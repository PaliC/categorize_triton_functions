[
    {
      "input": "@triton.jit\ndef liger_cross_entropy_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr,\n    loss_stride, n_cols, n_non_ignore, ignore_index, label_smoothing:\n    'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    This kernel computes both cross entropy loss and the gradient of the input.\n    We only consider hard label + mean reduction for now. Please refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for the math.\n\n    Parameters:\n    X_ptr: Pointer to input tensor.\n    X_stride (int): The stride of the input tensor.\n    Y_ptr: Pointer to target tensor.\n    Y_stride (int): The stride of the target tensor.\n    loss_ptr: Pointer to tensor to store the loss.\n    loss_stride (int): The stride of the loss tensor.\n    n_cols (int): The number of columns in the input tensor.\n    n_non_ignore (int): The number of non-ignored elements in the batch.\n    ignore_index (int): The index to ignore in the target.\n    label_smoothing (float): The amount of smoothing when computing the loss, where 0.0 means no smoothing.\n    reduction (str): The string for the reduction to apply\n    BLOCK_SIZE (int): The block size for Triton operations.\n    \"\"\"\n    program_id = tl.program_id(0)\n    Y_ptr += program_id * Y_stride\n    y = tl.load(Y_ptr)\n    X_ptr += program_id * X_stride\n    if y == ignore_index:\n        for i in range(0, n_cols, BLOCK_SIZE):\n            X_offsets = i + tl.arange(0, BLOCK_SIZE)\n            tl.store(X_ptr + X_offsets, 0.0, mask=X_offsets < n_cols)\n        return\n    loss_ptr += program_id * loss_stride\n    m = float('-inf')\n    d = 0.0\n    ori_X_y = tl.load(X_ptr + y)\n    scaled_x_sum = 0.0\n    eps = label_smoothing / n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        block_max = tl.max(X_block)\n        if label_smoothing > 0:\n            scaled_x_sum += tl.sum(tl.where(X_offsets < n_cols, -eps *\n                X_block, 0.0))\n        m_new = tl.maximum(m, block_max)\n        d = d * tl.exp(m - m_new) + tl.sum(tl.exp(X_block - m_new))\n        m = m_new\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        if reduction == 'mean':\n            X_block = (tl.exp(X_block - m) / d - eps) / n_non_ignore\n        else:\n            X_block = tl.exp(X_block - m) / d - eps\n        tl.store(X_ptr + X_offsets, X_block, mask=X_offsets < n_cols)\n    tl.debug_barrier()\n    loss = -(ori_X_y - m - tl.log(d))\n    if label_smoothing > 0:\n        smooth_loss = scaled_x_sum + label_smoothing * (m + tl.log(d))\n        loss = loss * (1 - label_smoothing) + smooth_loss\n    if reduction == 'mean':\n        loss = loss / n_non_ignore\n    X_y = tl.load(X_ptr + y)\n    if reduction == 'mean':\n        X_y += -(1 - label_smoothing) / n_non_ignore\n    else:\n        X_y += -(1 - label_smoothing)\n    tl.store(loss_ptr, loss)\n    tl.store(X_ptr + y, X_y)\n"
    },
    {
      "input": "@triton.jit\ndef embedding_forward_kernel(embeddings_ptr, indices_ptr, output_ptr,\n    n_elements, embedding_dim: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    embedding_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    embeddings = tl.load(embeddings_ptr + embedding_offsets, mask=mask_m[:,\n        None] & mask_n[None, :], other=0.0)\n    output_offsets = offsets_m[:, None] * embedding_dim + offsets_n[None, :]\n    tl.store(output_ptr + output_offsets, embeddings, mask=mask_m[:, None] &\n        mask_n[None, :])\n"
    },
    {
      "input": "@triton.jit\ndef embedding_backward_kernel(grad_output_ptr, grad_weight_ptr, indices_ptr,\n    n_elements, embedding_dim: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    grad_output = tl.load(grad_output_ptr + offsets_m[:, None] *\n        embedding_dim + offsets_n[None, :], mask=mask_m[:, None] & mask_n[\n        None, :], other=0.0)\n    grad_weight_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    tl.atomic_add(grad_weight_ptr + grad_weight_offsets, grad_output, mask=\n        mask_m[:, None] & mask_n[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K: 'tl.constexpr', stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    tl.static_assert(K % (4 * BLOCK_SIZE_K) == 0,\n        'K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K'\n        )\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \"\"\"\n        This part of the code generates pointers to the specific blocks of matrices A and B that the current thread block will process.\n\n        As described in the PyTorch documentation, a stride refers to the step size needed to move from one element to the next along a given dimension:\n\n        For matrix A: stride_am = A.stride(0) = K (stride along the rows), and stride_ak = A.stride(1) = 1 (stride along the columns).\n        For matrix B: stride_bk = B.stride(0) = N (stride along the rows), and stride_bn = B.stride(1) = 1 (stride along the columns).\n        Now, let's break down the pointer generation:\n\n        offs_am[:, None] creates a column of shape [BLOCK_SIZE_M, 1], which represents the row indices of matrix A that this block is processing. It is multiplied by K (the number of columns in matrix A) since A is stored in row-major order. So, the element at position (i, j) in A is located at index i*K + j in memory.\n        offs_k[None, BLOCK_SIZE_K] creates a row vector representing the column indices of the block, i.e., a range from 0 to BLOCK_SIZE_K. This is used to compute the positions of the columns within the block.\n        When combined, the result has the shape [BLOCK_SIZE_M, BLOCK_SIZE_K], where each entry (i, j) points to the element in matrix A at position (i, j) for the current block.\n\n        The same logic is applied to matrix B, but the resulting shape is [BLOCK_SIZE_K, BLOCK_SIZE_N], representing the block of matrix B that the thread block will work on.\n    \"\"\"\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    \"\"\"\n        We split the loop into two layers. The outer loop runs 4 times, and each iteration focuses on a specific portion of matrix A.\n\n        For example, when i = 0, we\u2019re only concerned with the blocks of matrix A that cover the range from 0 to K // (4 * BLOCK_SIZE_K).\n        Since matrix B is packed, its first dimension is effectively divided by 4. So, while we process the first segment of matrix A,\n        we still iterate over the entire first dimension of matrix B.\n\n        In each of the 4 iterations of the outer loop, we go through the full blocks of matrix B, but what changes is the data we extract.\n        Matrix B elements contain 4 weights, all packed into an int8 format, and during each iteration of the outer loop,\n        we extract a different weight by using bitwise shifting operations. This way, we access a unique weight on each pass.\n    \"\"\"\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n            stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n                other=0)\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n            mask = 3 << 2 * i\n            b = (b_uint8 & mask) >> 2 * i\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n            accumulator += tl.dot(a, b - tensor_full, out_dtype=tl.int32)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_forward_kernel(a, b, c, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_backward_kernel(dc, a, b, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    db_row = dc_row * geglu_a\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = 0.5 * a_row * (1 - tanh_sq) * (sqrt_2_over_pi * (1 + 3 * \n        0.044715 * a_row * a_row))\n    da_row = dc_row * b_row * (term1 + term2)\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _jsd_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr, loss_stride,\n    dX_ptr, dX_stride, label_ptr, beta, n_non_ignore, ignore_index:\n    'tl.constexpr', n_cols, BLOCK_SIZE: 'tl.constexpr', HAS_LABEL:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    X_ptr += pid * X_stride\n    dX_ptr += pid * dX_stride\n    Y_ptr += pid * Y_stride\n    loss_ptr += pid * loss_stride\n    label_ptr += pid\n    if HAS_LABEL:\n        label = tl.load(label_ptr)\n        if label == ignore_index:\n            for i in range(0, n_cols, BLOCK_SIZE):\n                offsets = i + tl.arange(0, BLOCK_SIZE)\n                tl.store(dX_ptr + offsets, 0.0, mask=offsets < n_cols)\n            return\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        X = tl.load(X_ptr + offsets, mask=mask, other=float('-inf'))\n        Y = tl.load(Y_ptr + offsets, mask=mask, other=float('-inf'))\n        Q = tl.exp(X)\n        P = tl.exp(Y)\n        M = beta * P + (1 - beta) * Q\n        log_M = tl.log(M)\n        loss = beta * P * Y + (1 - beta) * Q * X - M * log_M\n        loss = loss / n_non_ignore\n        tl.store(loss_ptr + offsets, loss, mask=mask)\n        dX = (1 - beta) * Q * (X - log_M) / n_non_ignore\n        tl.store(dX_ptr + offsets, dX, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _kldiv_kernel_forward(y_ptr, y_stride, gt_ptr, gt_stride, loss_ptr,\n    loss_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr', log_target:\n    'tl.constexpr'=False, reduction: 'tl.constexpr'=_REDUCTION_MODE_BATCHMEAN):\n    pid = tl.program_id(0)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n    loss_sum = 0.0\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            loss = y_true * (tl.log(tl.maximum(y_true, eps)) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n        if reduction == _REDUCTION_MODE_NONE:\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss_sum += tl.sum(loss, axis=0)\n    if reduction != _REDUCTION_MODE_NONE:\n        tl.store(loss_ptr, loss_sum)\n"
    },
    {
      "input": "@triton.jit\ndef _kldiv_kernel_backward(target_ptr, target_stride, new_grads_ptr,\n    new_grads_stride, n_cols, BLOCK_SIZE: 'tl.constexpr', log_target:\n    'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    target_ptr += pid * target_stride\n    new_grads_ptr += pid * new_grads_stride\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n        tl.store(new_grads_ptr + offsets, res, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, B_ptr, B_row_stride, Mean_ptr, Mean_row_stride,\n    RSTD_ptr, RSTD_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    Mean_ptr += row_idx * Mean_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    B_row = tl.load(B_ptr + col_offsets, mask=mask, other=0)\n    mean = tl.sum(X_row, axis=0) / n_cols\n    var = tl.sum((X_row - mean) * (X_row - mean), axis=0) / n_cols\n    rstd = rsqrt(var + eps)\n    tl.store(Mean_ptr, mean)\n    tl.store(RSTD_ptr, rstd)\n    Y_row = (X_row - mean) * rstd * W_row + B_row\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_backward_kernel(X_ptr, W_ptr, Mean_ptr, RSTD_ptr, DX_ptr,\n    DW_ptr, DB_ptr, DY_ptr, stride_x, stride_dx, stride_dw, stride_db,\n    stride_dy, n_rows, n_cols, rows_per_program: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', dtype: 'tl.constexpr'):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/ops/triton/layer_norm.py\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < n_cols\n    dw_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    db_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    X_ptr += row_start * stride_x\n    Mean_ptr += row_start\n    RSTD_ptr += row_start\n    DX_ptr += row_start * stride_dx\n    DY_ptr += row_start * stride_dy\n    for _ in range(row_start, row_end):\n        x = tl.load(X_ptr + cols, mask=mask, other=0.0)\n        w = tl.load(W_ptr + cols, mask=mask, other=0.0)\n        dy = tl.load(DY_ptr + cols, mask=mask, other=0.0)\n        mean = tl.load(Mean_ptr)\n        rstd = tl.load(RSTD_ptr)\n        x_hat = (x - mean) * rstd\n        wdy = w * dy\n        c1 = tl.sum(x_hat * wdy, axis=0) / n_cols\n        c2 = tl.sum(wdy, axis=0) / n_cols\n        dx = (wdy - (x_hat * c1 + c2)) * rstd\n        tl.store(DX_ptr + cols, dx, mask=mask)\n        dw_row += dy * x_hat\n        db_row += dy\n        X_ptr += stride_x\n        Mean_ptr += 1\n        RSTD_ptr += 1\n        DX_ptr += stride_dx\n        DY_ptr += stride_dy\n    tl.store(DW_ptr + row_block_id * stride_dw + cols, dw_row, mask=mask)\n    tl.store(DB_ptr + row_block_id * stride_db + cols, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, RSTD_ptr, RSTD_row_stride, n_cols, eps, offset,\n    casting_mode: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    y_i = (x_i / (RMS)) * (offset + wi), RMS = sqrt(sum(x_i^2) / N)\n\n    Reference:\n    1. https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    2. https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/rms_layernorm.py#L22\n    3. https://arxiv.org/pdf/1910.07467\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    X_row_dtype = X_row.dtype\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row\n    if casting_mode == _CASTING_MODE_GEMMA:\n        W_row = W_row\n        X_row = X_row\n    if casting_mode == _CASTING_MODE_NONE:\n        eps = eps\n        offset = offset\n    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols\n    rstd = rsqrt(mean_square + eps)\n    tl.store(RSTD_ptr, rstd)\n    X_row = X_row * rstd\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row\n    Y_row = X_row * (offset + W_row)\n    if casting_mode == _CASTING_MODE_GEMMA:\n        Y_row = Y_row\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_backward_kernel(dY_ptr, dY_row_stride, X_ptr, X_row_stride,\n    X_dtype: 'tl.constexpr', W_ptr, W_row_stride, RSTD_ptr, RSTD_row_stride,\n    dW_ptr, dW_row_stride, n_rows, n_cols, offset, rows_per_program:\n    'tl.constexpr', casting_mode: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    dx = (1 / RMS) * [dy * (w + offset - (1 / N) * (1 / RMS^2) * ((dy * (w + offset)) dot x) * x]. * means element-wise multiplication, whileas dot means dot product\n    dw = sum(dy * (x / RMS)). summation over BxT dimension\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dW_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    dY_ptr += row_start * dY_row_stride\n    X_ptr += row_start * X_row_stride\n    RSTD_ptr += row_start\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0.0)\n    W_row = W_row + offset\n    for _ in range(row_start, row_end):\n        dY_row = tl.load(dY_ptr + col_offsets, mask=mask, other=0.0)\n        X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0.0)\n        rstd_row = tl.load(RSTD_ptr)\n        X_row = X_row\n        if casting_mode == _CASTING_MODE_LLAMA:\n            m = dY_row * W_row\n        elif casting_mode == _CASTING_MODE_GEMMA:\n            dY_row = dY_row\n            m = dY_row * W_row\n        else:\n            m = dY_row * W_row\n        dX_row = rstd_row * m\n        dX_row += rstd_row * (-(1 / n_cols) * rstd_row * rstd_row * tl.sum(\n            m * X_row, axis=0) * X_row)\n        if casting_mode == _CASTING_MODE_LLAMA:\n            dW_row += dY_row * (X_row * rstd_row)\n        else:\n            dW_row += dY_row * (X_row * rstd_row)\n        tl.store(dY_ptr + col_offsets, dX_row, mask=mask)\n        dY_ptr += dY_row_stride\n        X_ptr += X_row_stride\n        RSTD_ptr += RSTD_row_stride\n    tl.store(dW_ptr + row_block_id * dW_row_stride + col_offsets, dW_row,\n        mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_rope(q_ptr, q_row_stride, k_ptr, k_row_stride, cos,\n    cos_row_stride, sin, sin_row_stride, sl, bs: 'tl.constexpr', n_qh:\n    'tl.constexpr', n_kh: 'tl.constexpr', hd: 'tl.constexpr', pad_n_qh:\n    'tl.constexpr', pad_n_kh: 'tl.constexpr', pad_hd: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', BACKWARD_PASS: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n    cos_row_idx = pid % sl\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        )\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        )\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.jit\ndef _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef element_mul_kernel(X_ptr, X_stride, grad_output_ptr, n_cols, BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by X_ptr with the value pointed by grad_output_ptr.\n    The multiplication is performed in-place on the tensor pointed by X_ptr.\n\n    Parameters:\n    X_ptr: Pointer to the input tensor.\n    X_stride (int): The stride of the input tensor.\n    grad_output_ptr: Pointer to the gradient output value.\n    n_cols (int): The number of columns in the input tensor.\n    BLOCK_SIZE (int): The block size for Triton operations.\n    \"\"\"\n    program_id = tl.program_id(0)\n    X_ptr += program_id * X_stride\n    grad_output = tl.load(grad_output_ptr)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols)\n        tl.store(X_ptr + X_offsets, X_block * grad_output, mask=X_offsets <\n            n_cols)\n"
    },
    {
      "input": "@triton.autotune(DEFAULT_DEQUANT_CONFIGS, key=['numels'])\n@triton.jit\ndef dequant_kernel_248(g_idx_ptr, scales_ptr, qweight_ptr, qzeros_ptr,\n    out_ptr, numels, maxq: 'tl.constexpr', bits: 'tl.constexpr',\n    outfeatures: 'tl.constexpr', num_groups: 'tl.constexpr', X_BLOCK:\n    'tl.constexpr'):\n    xoffset = tl.program_id(0) * X_BLOCK\n    x_index = xoffset + tl.arange(0, X_BLOCK)\n    xmask = x_index < numels\n    row_idx = x_index // outfeatures\n    col_idx = x_index % outfeatures\n    elements_per_feature: 'tl.constexpr' = 32 // bits\n    g_idx = tl.load(g_idx_ptr + row_idx, None, eviction_policy='evict_last')\n    qweights = tl.load(qweight_ptr + (col_idx + outfeatures * (row_idx //\n        elements_per_feature)), None)\n    wf_weights = row_idx % elements_per_feature * bits\n    wf_zeros = col_idx % elements_per_feature * bits\n    tmp1 = g_idx + num_groups\n    tmp2 = g_idx < 0\n    tl.device_assert(g_idx >= 0, 'index out of bounds: 0 <= tmp0 < 0')\n    groups = tl.where(tmp2, tmp1, g_idx)\n    scales = tl.load(scales_ptr + (col_idx + outfeatures * groups), None)\n    weights = qweights >> wf_weights\n    weights = weights & maxq\n    qzero_ncols: 'tl.constexpr' = outfeatures // elements_per_feature\n    qzeros = tl.load(qzeros_ptr + (qzero_ncols * groups + col_idx //\n        elements_per_feature), None, eviction_policy='evict_last')\n    zeros = qzeros >> wf_zeros\n    zeros = zeros & maxq\n    zeros = zeros + 1\n    weights = weights - zeros\n    weights = weights\n    weights = scales * weights\n    tl.store(out_ptr + x_index, weights, mask=xmask)\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.jit\ndef sigmoid(input):\n    \"\"\"\n    Applies sigmoid to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by sigmoid.\n    \"\"\"\n    return 1 / (1 + tl.exp(-input))\n"
    },
    {
      "input": "@triton.jit\ndef sigmoid_grad(input):\n    \"\"\"\n    Calculates the gradient of sigmoid.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of sigmoid.\n    \"\"\"\n    output_sigmoid = sigmoid(input)\n    return output_sigmoid * (1 - output_sigmoid)\n"
    },
    {
      "input": "@triton.jit\ndef tanh(input):\n    \"\"\"\n    Applies tanh to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by tanh.\n    \"\"\"\n    return 2 * sigmoid(2 * input) - 1\n"
    },
    {
      "input": "@triton.jit\ndef tanh_grad(input):\n    \"\"\"\n    Calculates the gradient of tanh.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of tanh.\n    \"\"\"\n    output_tanh = tanh(input)\n    return 1 - output_tanh * output_tanh\n"
    },
    {
      "input": "@triton.jit\ndef relu(input):\n    \"\"\"\n    Applies ReLU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by ReLU.\n    \"\"\"\n    return tl.maximum(0, input)\n"
    },
    {
      "input": "@triton.jit\ndef relu_grad(input):\n    \"\"\"\n    Calculates the gradient of ReLU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of ReLU.\n    \"\"\"\n    return tl.where(input <= 0, 0, 1)\n"
    },
    {
      "input": "@triton.jit\ndef gelu(input):\n    \"\"\"\n    Applies GELU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by GELU.\n    \"\"\"\n    cdf = 0.5 * (1 + tl.math.erf(0.707106781 * input))\n    return cdf * input\n"
    },
    {
      "input": "@triton.jit\ndef gelu_grad(input):\n    \"\"\"\n    Calculates the gradient of GELU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of GELU.\n    \"\"\"\n    cdf = 0.5 * (1 + tl.math.erf(0.707106781 * input))\n    cdf_grad = 0.39894228 * tl.exp(-0.5 * input * input)\n    return cdf_grad * input + cdf\n"
    },
    {
      "input": "@triton.jit\ndef silu(input):\n    \"\"\"\n    Applies SiLU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by SiLU.\n    \"\"\"\n    return input * sigmoid(input)\n"
    },
    {
      "input": "@triton.jit\ndef silu_grad(input):\n    \"\"\"\n    Calculates the gradient of SiLU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of SiLU.\n    \"\"\"\n    output_sigmoid = sigmoid(input)\n    return output_sigmoid * (input * (1 - output_sigmoid) + 1)\n"
    },
    {
      "input": "@triton.jit\ndef relu6(input):\n    \"\"\"\n    Applies ReLU6 to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by ReLU6.\n    \"\"\"\n    return tl.minimum(relu(input), 6)\n"
    },
    {
      "input": "@triton.jit\ndef relu6_grad(input):\n    \"\"\"\n    Calculates the gradient of ReLU6.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of ReLU6.\n    \"\"\"\n    return tl.where((0 < input) & (input < 6), 1, 0)\n"
    },
    {
      "input": "@triton.jit\ndef hardsigmoid(input):\n    \"\"\"\n    Applies hard sigmoid to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by hard sigmoid.\n    \"\"\"\n    return tl.maximum(0, tl.minimum(1, input / 6 + 0.5))\n"
    },
    {
      "input": "@triton.jit\ndef hardsigmoid_grad(input):\n    \"\"\"\n    Calculates the gradient of hard sigmoid.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of hard sigmoid.\n    \"\"\"\n    return tl.where((-3 < input) & (input < 3), 1 / 6, 0)\n"
    },
    {
      "input": "@triton.jit\ndef hardswish(input):\n    \"\"\"\n    Applies hard Swish to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by hard Swish.\n    \"\"\"\n    return input * relu6(input + 3) / 6\n"
    },
    {
      "input": "@triton.jit\ndef hardswish_grad(input):\n    \"\"\"\n    Calculates the gradient of hard Swish.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of hard Swish.\n    \"\"\"\n    return (relu6(input + 3) + input * relu6_grad(input + 3)) / 6\n"
    },
    {
      "input": "@triton.jit\ndef selu(input):\n    \"\"\"\n    Applies SELU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by SELU.\n    \"\"\"\n    scale = 1.0507009873554805\n    alpha = 1.6732632423543772\n    return scale * (tl.maximum(0, input) + tl.minimum(0, alpha * (tl.exp(\n        input) - 1)))\n"
    },
    {
      "input": "@triton.jit\ndef selu_grad(input):\n    \"\"\"\n    Calculates the gradient of SELU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of SELU.\n    \"\"\"\n    scale = 1.0507009873554805\n    alpha = 1.6732632423543772\n    return scale * tl.where(input <= 0, alpha * tl.exp(input), 1)\n"
    },
    {
      "input": "@triton.jit\ndef mish(input):\n    \"\"\"\n    Applies Mish to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by Mish.\n    \"\"\"\n    return input * tanh(tl.log(1 + tl.exp(input)))\n"
    },
    {
      "input": "@triton.jit\ndef mish_grad(input):\n    \"\"\"\n    Calculates the gradient of Mish.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of Mish.\n    \"\"\"\n    exp = tl.exp(input)\n    delta = exp * (exp + 2) + 2\n    return exp * (exp * (4 * input + 6 + exp * (exp + 4)) + 4 * (input + 1)\n        ) / (delta * delta)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu(input, negative_slope):\n    \"\"\"\n    Applies leaky ReLU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        negative_slope: Slope of the negative component.\n\n    Returns:\n        Input transformed by leaky ReLU.\n    \"\"\"\n    return relu(input) + negative_slope * tl.minimum(0, input)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu_grad(input, negative_slope):\n    \"\"\"\n    Calculates the gradient of leaky ReLU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        negative_slope: Slope of the negative component.\n\n    Returns:\n        Gradient of leaky ReLU.\n    \"\"\"\n    return tl.where(input <= 0, negative_slope, 1)\n"
    },
    {
      "input": "@triton.jit\ndef apply_dropout(input, drop_p, seed, offset):\n    \"\"\"\n    Randomly zeroes elements in the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element.\n        seed: Seed for generating the dropout mask.\n        offset: Offset to generate the mask for.\n\n    Returns:\n        Input with elements randomly zeroed out.\n    \"\"\"\n    random = tl.rand(seed, offset)\n    return tl.where(random < drop_p, 0, input / (1 - drop_p))\n"
    },
    {
      "input": "@triton.jit\ndef apply_act_func(input, drop_p, seed, offset, param, act_func:\n    'tl.constexpr', dropout: 'tl.constexpr'):\n    \"\"\"\n    Applies an activation function to the input, optionally fusing dropout.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        offset: Offset to generate the dropout mask for if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n\n    Returns:\n        Input transformed by the desired activation function,\n        potentially with fused dropout.\n    \"\"\"\n    if act_func == 'sigmoid':\n        input = input\n        output = sigmoid(input)\n    elif act_func == 'tanh':\n        input = input\n        output = tanh(input)\n    elif act_func == 'relu':\n        output = relu(input)\n    elif act_func == 'gelu':\n        input = input\n        output = gelu(input)\n    elif act_func == 'silu':\n        input = input\n        output = silu(input)\n    elif act_func == 'relu6':\n        output = relu6(input)\n    elif act_func == 'hardsigmoid':\n        output = hardsigmoid(input)\n    elif act_func == 'hardswish':\n        output = hardswish(input)\n    elif act_func == 'selu':\n        input = input\n        output = selu(input)\n    elif act_func == 'mish':\n        input = input\n        output = mish(input)\n    elif act_func == 'leaky_relu':\n        output = leaky_relu(input, param)\n    if dropout:\n        output = apply_dropout(output, drop_p, seed, offset)\n    return output\n"
    },
    {
      "input": "@triton.jit\ndef apply_dropout_grad(output_grad, drop_p, seed, offset):\n    \"\"\"\n    Calculates the input gradient of dropout.\n\n    Args:\n        output_grad: Output gradients. The output gradients must be\n            loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element.\n        seed: Seed for generating the dropout mask.\n        offset: Offset to generate the mask for.\n\n    Returns:\n        Gradient of dropout.\n    \"\"\"\n    random = tl.rand(seed, offset)\n    return tl.where(random < drop_p, 0, output_grad / (1 - drop_p))\n"
    },
    {
      "input": "@triton.jit\ndef apply_act_func_grad(output_grad, input, drop_p, seed, offset, param,\n    act_func: 'tl.constexpr', dropout: 'tl.constexpr'):\n    \"\"\"\n    Calculates the gradient of an activation function.\n\n    Args:\n        output_grad: Output gradients. The output gradients must be\n            loaded and cannot be a pointer.\n        input: Input. The input must be loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        offset: Offset to generate the dropout mask for if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function whose gradient is calculated.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n\n    Returns:\n        Gradient of the desired activation function.\n    \"\"\"\n    if act_func == 'sigmoid':\n        input = input\n        output = sigmoid_grad(input)\n    elif act_func == 'tanh':\n        input = input\n        output = tanh_grad(input)\n    elif act_func == 'relu':\n        output = relu_grad(input)\n    elif act_func == 'gelu':\n        input = input\n        output = gelu_grad(input)\n    elif act_func == 'silu':\n        input = input\n        output = silu_grad(input)\n    elif act_func == 'relu6':\n        output = relu6_grad(input)\n    elif act_func == 'hardsigmoid':\n        output = hardsigmoid_grad(input)\n    elif act_func == 'hardswish':\n        output = hardswish_grad(input)\n    elif act_func == 'selu':\n        input = input\n        output = selu_grad(input)\n    elif act_func == 'mish':\n        input = input\n        output = mish_grad(input)\n    elif act_func == 'leaky_relu':\n        output = leaky_relu_grad(input, param)\n    if dropout:\n        output_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)\n    return output_grad * output\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef act_func_forward_kernel(input_pointer, output_pointer, size, drop_p,\n    seed, param, act_func: 'tl.constexpr', dropout: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Applies an activation function to the input, optionally fusing dropout.\n\n    Args:\n        input_pointer: Pointer to the input to transform.\n            The input must be of shape [size].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input = tl.load(input_pointer + offset, mask=mask)\n    tl.store(output_pointer + offset, apply_act_func(input, drop_p, seed,\n        offset, param, act_func, dropout), mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef act_func_backward_kernel(output_grad_pointer, input_pointer,\n    input_grad_pointer, size, drop_p, seed, param, act_func: 'tl.constexpr',\n    dropout: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of an activation function.\n\n    Args:\n        output_grad_pointer: Pointer to the activation's output gradients.\n            The output gradients must be of shape [size].\n        input_pointer: Pointer to the activation's input.\n            The input must be of shape [size].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function whose gradient is calculated.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input = tl.load(input_pointer + offset, mask=mask)\n    tl.store(input_grad_pointer + offset, apply_act_func_grad(output_grad,\n        input, drop_p, seed, offset, param, act_func, dropout), mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'], restore_value=['running_mean_pointer',\n    'running_var_pointer'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args[\n    'batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})\n@triton.jit\ndef batch_norm_forward_kernel(input_pointer, weight_pointer, bias_pointer,\n    mean_pointer, inv_std_pointer, pre_act_add_pointer, pre_act_pointer,\n    output_pointer, running_mean_pointer, running_var_pointer, batch_dim,\n    spatial_dim, input_batch_stride, input_feat_stride,\n    input_spatial_stride, pre_act_add_batch_stride, pre_act_add_feat_stride,\n    pre_act_add_spatial_stride, pre_act_batch_stride, pre_act_feat_stride,\n    pre_act_spatial_stride, output_batch_stride, output_feat_stride,\n    output_spatial_stride, momentum, eps, param, affine: 'tl.constexpr',\n    save_stats: 'tl.constexpr', track_running_stats: 'tl.constexpr',\n    is_train: 'tl.constexpr', add_pre_act: 'tl.constexpr', act_func:\n    'tl.constexpr', save_pre_act: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_SPATIAL: 'tl.constexpr'):\n    \"\"\"\n    Batch-normalizes the input, optionally adding a residual and fusing an activation function.\n\n    Args:\n        input_pointer: Pointer to the input to layer-normalize.\n            The input must be of shape [batch_dim, feat_dim, spatial_dim].\n        weight_pointer: Pointer to optional weights for affine transform.\n            The weights, if provided, must be of shape [feat_dim].\n        bias_pointer: Pointer to an optional bias vector for affine transform.\n            The bias vector, if provided, must be of shape [feat_dim].\n        mean_pointer: Pointer to an optional container the input's mean\n            is written to if save_stats is True.\n            The container, if provided, must be of shape [feat_dim].\n        inv_std_pointer: Pointer to an optional container the input's inverse\n            standard deviation is written to if save_stats is True.\n            The container, if provided, must be of shape [feat_dim].\n        pre_act_add_pointer: Pointer to an optional residual added to the pre-activation result.\n            The residual, if provided, must be of shape [batch_dim, feat_dim, spatial_dim].\n        pre_act_pointer: Pointer to an optional container the pre-activation input\n            is written to if act_func is not None and save_pre_act is True.\n            The container, if provided, must be of shape [batch_dim, feat_dim, spatial_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim, spatial_dim].\n        running_mean_pointer: Pointer to an optional container the input's running\n            mean is written to if track_running_stats and is_train are True.\n            The container, if provided, must be of shape [feat_dim].\n        running_var_pointer: Pointer to an optional container the input's running\n            variance is written to if track_running_stats and is_train are True.\n            The container, if provided, must be of shape [feat_dim].\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        pre_act_add_batch_stride: Stride necessary to jump one element along the\n            residual's batch dimension.\n        pre_act_add_out_feat_stride: Stride necessary to jump one element along the\n            residual's feature dimension.\n        pre_act_add_spatial_stride: Stride necessary to jump one element along the\n            residual's spatial dimension.\n        pre_act_batch_stride: Stride necessary to jump one element along the\n            pre-activation input container's batch dimension.\n        pre_act_out_feat_stride: Stride necessary to jump one element along the\n            pre-activation input container's feature dimension.\n        pre_act_spatial_stride: Stride necessary to jump one element along the\n            pre-activation input container's spatial dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        output_spatial_stride: Stride necessary to jump one element along the\n            output container's spatial dimension.\n        momentum: Momentum for the running mean and variance.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        param: Parameter in the case of parameterized activation functions.\n        affine: Flag for performing an affine transformation on the normalized output.\n        save_stats: Flag for saving the mean and standard deviation.\n        track_running_stats: Flag for tracking running mean and variance if\n            is_train is also True.\n        is_train: Flag indicating if the model is in training mode.\n        add_pre_act: Flag for adding the residual to the pre-activation result.\n        act_func: Name of activation function to apply, with None for identity.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        save_pre_act: Flag for saving the pre-activation input.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    feat_pid = tl.program_id(axis=0)\n    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\n    batch_mask = batch_offset < batch_dim\n    if is_train or not track_running_stats:\n        count = 0\n        mean = 0.0\n        var = 0.0\n        for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n            spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n                BLOCK_SIZE_SPATIAL)\n            spatial_mask = spatial_offset < spatial_dim\n            curr_input_pointer = (input_pointer + input_feat_stride *\n                feat_pid + input_batch_stride * batch_offset[:, None] + \n                input_spatial_stride * spatial_offset[None, :])\n            curr_input = tl.load(curr_input_pointer, mask=batch_mask[:,\n                None] & spatial_mask[None, :])\n            spatial_count = min(BLOCK_SIZE_SPATIAL, spatial_dim - block_ind *\n                BLOCK_SIZE_SPATIAL)\n            curr_count = spatial_count * batch_dim\n            count += curr_count\n            prev_mean = mean\n            mean += (tl.sum(curr_input) - curr_count * mean) / count\n            deltas = tl.where(batch_mask[:, None] & spatial_mask[None, :], \n                (curr_input - mean) * (curr_input - prev_mean), 0.0)\n            var += tl.sum(deltas)\n        var /= count\n        inv_std = tl.rsqrt(var + eps)\n        if save_stats:\n            tl.store(feat_pid + mean_pointer, mean)\n            tl.store(feat_pid + inv_std_pointer, inv_std)\n        if track_running_stats:\n            running_mean_pointer += feat_pid\n            running_var_pointer += feat_pid\n            running_mean = tl.load(running_mean_pointer)\n            running_var = tl.load(running_var_pointer)\n            n = batch_dim * spatial_dim\n            tl.store(running_mean_pointer, (1 - momentum) * running_mean + \n                momentum * mean)\n            tl.store(running_var_pointer, (1 - momentum) * running_var + \n                momentum * var * n / (n - 1))\n    else:\n        mean = tl.load(feat_pid + running_mean_pointer)\n        inv_std = tl.rsqrt(tl.load(feat_pid + running_var_pointer) + eps)\n    if affine:\n        weight = tl.load(feat_pid + weight_pointer)\n        bias = tl.load(feat_pid + bias_pointer)\n    else:\n        weight = 1.0\n        bias = 0.0\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n            BLOCK_SIZE_SPATIAL)\n        spatial_mask = spatial_offset < spatial_dim\n        curr_input_pointer = (input_pointer + input_feat_stride * feat_pid +\n            input_batch_stride * batch_offset[:, None] + \n            input_spatial_stride * spatial_offset[None, :])\n        curr_output_pointer = (output_pointer + output_feat_stride *\n            feat_pid + output_batch_stride * batch_offset[:, None] + \n            output_spatial_stride * spatial_offset[None, :])\n        curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        output = weight * (curr_input - mean) * inv_std + bias\n        if add_pre_act:\n            curr_pre_act_add_pointer = (pre_act_add_pointer + \n                pre_act_add_feat_stride * feat_pid + \n                pre_act_add_batch_stride * batch_offset[:, None] + \n                pre_act_add_spatial_stride * spatial_offset[None, :])\n            curr_pre_act_add = tl.load(curr_pre_act_add_pointer, mask=\n                batch_mask[:, None] & spatial_mask[None, :])\n            output += curr_pre_act_add\n        if act_func is not None:\n            if save_pre_act:\n                curr_pre_act_pointer = (pre_act_pointer + \n                    pre_act_feat_stride * feat_pid + pre_act_batch_stride *\n                    batch_offset[:, None] + pre_act_spatial_stride *\n                    spatial_offset[None, :])\n                tl.store(curr_pre_act_pointer, output, mask=batch_mask[:,\n                    None] & spatial_mask[None, :])\n            output = apply_act_func(output, None, None, None, param,\n                act_func, False)\n        tl.store(curr_output_pointer, output, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args[\n    'batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})\n@triton.jit\ndef batch_norm_backward_kernel(output_grad_pointer, input_pointer,\n    mean_pointer, inv_std_pointer, weight_pointer, input_grad_pointer,\n    weight_grad_pointer, bias_grad_pointer, batch_dim, spatial_dim,\n    output_grad_batch_stride, output_grad_feat_stride,\n    output_grad_spatial_stride, input_batch_stride, input_feat_stride,\n    input_spatial_stride, input_grad_batch_stride, input_grad_feat_stride,\n    input_grad_spatial_stride, affine: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_SPATIAL: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of batch normalization.\n\n    Args:\n        output_grad_pointer: Pointer to layer normalization's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim, spatial_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim, spatial_dim].\n        mean_pointer: Pointer to the input's mean.\n            The mean should be of shape [feat_dim].\n        inv_std_pointer: Pointer to the input's inverse standard deviation.\n            The inverse standard deviation should be of shape [feat_dim].\n        weight_pointer: Pointer to optional weights if affine transform occurred.\n            The weights, if provided, must be of shape [feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim, spatial_dim].\n        weight_grad_pointer: Pointer to an optional container the weights' gradients\n            are written to if scale_by_weight is True.\n            The container, if provided, must be of shape [feat_dim].\n        bias_grad_pointer: Pointer to an optional container the bias vector's gradients\n            are written to if scale_by_weight is True.\n            The container, if provided, must be of shape [feat_dim].\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        output_grad_spatial_stride: Stride necessary to jump one element along the\n            output gradients' spatial dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        input_grad_spatial_stride: Stride necessary to jump one element along the\n            input gradient container's spatial dimension.\n        affine: Flag for performing an affine transformation on the normalized output.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    feat_pid = tl.program_id(axis=0)\n    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\n    batch_mask = batch_offset < batch_dim\n    mean = tl.load(feat_pid + mean_pointer)\n    inv_std = tl.load(feat_pid + inv_std_pointer)\n    term1 = 0.0\n    term2 = 0.0\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n            BLOCK_SIZE_SPATIAL)\n        spatial_mask = spatial_offset < spatial_dim\n        curr_output_grad_pointer = (output_grad_pointer + \n            output_grad_feat_stride * feat_pid + output_grad_batch_stride *\n            batch_offset[:, None] + output_grad_spatial_stride *\n            spatial_offset[None, :])\n        curr_input_pointer = (input_pointer + input_feat_stride * feat_pid +\n            input_batch_stride * batch_offset[:, None] + \n            input_spatial_stride * spatial_offset[None, :])\n        curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        curr_pre_lin = (curr_input - mean) * inv_std\n        curr_output_grad = tl.load(curr_output_grad_pointer, mask=\n            batch_mask[:, None] & spatial_mask[None, :])\n        term1 += tl.sum(curr_pre_lin * curr_output_grad)\n        term2 += tl.sum(curr_output_grad)\n    if affine:\n        weight = tl.load(feat_pid + weight_pointer)\n        weight_grad = 0.0\n        bias_grad = 0.0\n    else:\n        weight = 1.0\n    count = batch_dim * spatial_dim\n    term1 *= weight / count\n    term2 *= weight / count\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n            BLOCK_SIZE_SPATIAL)\n        spatial_mask = spatial_offset < spatial_dim\n        curr_output_grad_pointer = (output_grad_pointer + \n            output_grad_feat_stride * feat_pid + output_grad_batch_stride *\n            batch_offset[:, None] + output_grad_spatial_stride *\n            spatial_offset[None, :])\n        curr_input_pointer = (input_pointer + input_feat_stride * feat_pid +\n            input_batch_stride * batch_offset[:, None] + \n            input_spatial_stride * spatial_offset[None, :])\n        curr_input_grad_pointer = (input_grad_pointer + \n            input_grad_feat_stride * feat_pid + input_grad_batch_stride *\n            batch_offset[:, None] + input_grad_spatial_stride *\n            spatial_offset[None, :])\n        curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        curr_pre_lin = (curr_input - mean) * inv_std\n        curr_output_grad = tl.load(curr_output_grad_pointer, mask=\n            batch_mask[:, None] & spatial_mask[None, :])\n        curr_input_grad = inv_std * (weight * curr_output_grad - (term1 *\n            curr_pre_lin + term2))\n        tl.store(curr_input_grad_pointer, curr_input_grad, mask=batch_mask[\n            :, None] & spatial_mask[None, :])\n        if affine:\n            weight_grad += tl.sum(curr_pre_lin * curr_output_grad)\n            bias_grad += tl.sum(curr_output_grad)\n    if affine:\n        tl.store(feat_pid + weight_grad_pointer, weight_grad)\n        tl.store(feat_pid + bias_grad_pointer, bias_grad)\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef cross_entropy_loss_forward_kernel(input_pointer, target_pointer,\n    weight_pointer, sum_weights_pointer, output_pointer, batch_dim,\n    feat_dim, input_batch_stride, input_feat_stride, weighted:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the mean cross entropy loss between the input and target,\n    with optional reweighing of each class.\n\n    Args:\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to a container the sum of the class weights is written to.\n            The container must be of shape [batch_dim/BLOCK_SIZE_BATCH].\n        output_pointer: Pointer to a container the loss is written to.\n            The container must be of shape [batch_dim/BLOCK_SIZE_BATCH].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    target = tl.load(target_pointer + batch_offset, mask=batch_mask)\n    pred_pointer = (input_pointer + input_feat_stride * target + \n        input_batch_stride * batch_offset)\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :], other=-float('inf'))\n    pred = tl.load(pred_pointer, mask=batch_mask)\n    mx = tl.max(input, axis=1)\n    input -= mx[:, None]\n    loss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask)\n        loss *= weight\n        tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))\n    else:\n        loss /= batch_dim\n    tl.store(output_pointer + batch_pid, tl.sum(loss))\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef cross_entropy_loss_backward_kernel(output_grad_pointer, target_pointer,\n    input_pointer, weight_pointer, sum_weights_pointer, input_grad_pointer,\n    batch_dim, feat_dim, input_batch_stride, input_feat_stride,\n    input_grad_batch_stride, input_grad_feat_stride, weighted:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT:\n    'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of cross entropy loss.\n\n    Args:\n        output_grad_pointer: Pointer to the loss's output gradients.\n            The output gradient must be a scalar.\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to the sum of the class weights if the classes were weighed.\n            The sum of weights must be a scalar.\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :], other=-float('inf'))\n    input -= tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    softmax = numerator / tl.sum(numerator, axis=1)[:, None]\n    output_grad = tl.load(output_grad_pointer)\n    target = tl.load(target_pointer + batch_offset, mask=batch_mask)\n    broadcasted_feat_offset = tl.broadcast_to(feat_offset[None, :], (\n        BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT))\n    broadcasted_target = tl.broadcast_to(target[:, None], (BLOCK_SIZE_BATCH,\n        BLOCK_SIZE_FEAT))\n    input_grad = output_grad * (softmax - (broadcasted_feat_offset ==\n        broadcasted_target))\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask)\n        sum_weights = tl.load(sum_weights_pointer)\n        input_grad *= weight[:, None] / sum_weights\n    else:\n        input_grad /= batch_dim\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef dropout_forward_kernel(input_pointer, output_pointer, size, drop_p,\n    seed, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Randomly zeroes elements in the input.\n\n    Args:\n        input_pointer: Pointer to the input to perform dropout on.\n            The input must be of shape [size].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element.\n        seed: Seed for generating the dropout mask.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input = tl.load(input_pointer + offset, mask=mask)\n    output = apply_dropout(input, drop_p, seed, offset)\n    tl.store(output_pointer + offset, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef dropout_backward_kernel(output_grad_pointer, input_grad_pointer, size,\n    drop_p, seed, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of dropout.\n\n    Args:\n        output_grad_pointer: Pointer to dropout's output gradients.\n            The output gradients must be of shape [size].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element used in dropout.\n        seed: Seed for generating the dropout mask.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)\n    tl.store(input_grad_pointer + offset, input_grad, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef glu_forward_kernel(input1_pointer, input2_pointer, output_pointer, size,\n    param, act_func: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Applies the gated linear unit with an arbitrary activation function\n    to the input.\n\n    Args:\n        input1_pointer: Pointer to the first half of the input to gate.\n            The first half must be contiguous and contain size elements.\n        input2_pointer: Pointer to the second half of the input to gate.\n            The second half must be contiguous and contain size elements.\n        output_pointer: Pointer to a container the result is written to.\n            The container must be contiguous and contain size elements.\n        size: Number of elements in each half of the input.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input1 = tl.load(input1_pointer + offset, mask=mask)\n    input2 = tl.load(input2_pointer + offset, mask=mask)\n    output = input1 * apply_act_func(input2, None, None, None, param,\n        act_func, False)\n    tl.store(output_pointer + offset, output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef glu_backward_kernel(output_grad_pointer, input1_pointer, input2_pointer,\n    input1_grad_pointer, input2_grad_pointer, size, param, act_func:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of the gated linear unit.\n\n    Args:\n        output_grad_pointer: Pointer to the unit's output gradients.\n            The output gradients must be contiguous and contain size elements.\n        input1_pointer: Pointer to the first half of the input that was gated.\n            The first half must be contiguous and contain size elements.\n        input2_pointer: Pointer to the second half of the input that was gated.\n            The second half must be contiguous and contain size elements.\n        input1_grad_pointer: Pointer to a container the first half's gradients are written to.\n            The container must be contiguous and contain size elements.\n        input2_grad_pointer: Pointer to a container the second half's gradients are written to.\n            The container must be contiguous and contain size elements.\n        size: Number of elements in each half of the input.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input1 = tl.load(input1_pointer + offset, mask=mask)\n    input2 = tl.load(input2_pointer + offset, mask=mask)\n    input1_grad = output_grad * apply_act_func(input2, None, None, None,\n        param, act_func, False)\n    input2_grad = output_grad * input1 * apply_act_func_grad(1, input2,\n        None, None, None, param, act_func, False)\n    tl.store(input1_grad_pointer + offset, input1_grad, mask=mask)\n    tl.store(input2_grad_pointer + offset, input2_grad, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef layer_norm_forward_kernel(input_pointer, weight_pointer, bias_pointer,\n    mean_pointer, inv_std_pointer, output_pointer, batch_dim, feat_dim,\n    input_batch_stride, input_feat_stride, output_batch_stride,\n    output_feat_stride, eps, scale_by_weight: 'tl.constexpr', add_bias:\n    'tl.constexpr', save_stats: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Layer-normalizes the input.\n\n    Args:\n        input_pointer: Pointer to the input to layer-normalize.\n            The input must be of shape [batch_dim, feat_dim].\n        weight_pointer: Pointer to optional weights for affine transform.\n            The weights, if provided, must be of shape [feat_dim].\n        bias_pointer: Pointer to an optional bias vector for affine transform.\n            The bias vector, if provided, must be of shape [feat_dim].\n        mean_pointer: Pointer to an optional container the input's mean\n            is written to if save_stats is True.\n            The container, if provided, must be of shape [batch_dim].\n        inv_std_pointer: Pointer to an optional container the input's inverse\n            standard deviation is written to if save_stats is True.\n            The container, if provided, must be of shape [batch_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        add_bias: Flag for adding a bias vector to the normalized output\n            if scale_by_weight is True.\n        save_stats: Flag for saving the mean and standard deviation.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    mean = tl.sum(input, axis=1) / feat_dim\n    diff = tl.where(feat_mask[None, :], input - mean[:, None], 0)\n    inv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / feat_dim + eps)\n    if save_stats:\n        tl.store(mean_pointer + batch_offset, mean, mask=batch_mask)\n        tl.store(inv_std_pointer + batch_offset, inv_std, mask=batch_mask)\n    output = diff * inv_std[:, None]\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        output *= weight\n        if add_bias:\n            bias = tl.load(bias_pointer + feat_offset, mask=feat_mask)\n            output += bias\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef layer_norm_backward_kernel(output_grad_pointer, input_pointer,\n    mean_pointer, inv_std_pointer, weight_pointer, input_grad_pointer,\n    weight_grad_pointer, bias_grad_pointer, batch_dim, feat_dim,\n    output_grad_batch_stride, output_grad_feat_stride, input_batch_stride,\n    input_feat_stride, input_grad_batch_stride, input_grad_feat_stride,\n    weight_grad_batch_stride, weight_grad_feat_stride,\n    bias_grad_batch_stride, bias_grad_feat_stride, scale_by_weight:\n    'tl.constexpr', add_bias: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of layer normalization.\n\n    Args:\n        output_grad_pointer: Pointer to layer normalization's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        mean_pointer: Pointer to the input's mean.\n            The mean should be of shape [batch_dim].\n        inv_std_pointer: Pointer to the input's inverse standard deviation.\n            The inverse standard deviation should be of shape [batch_dim].\n        weight_pointer: Pointer to optional weights if affine transform occurred.\n            The weights, if provided, must be of shape [feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        weight_grad_pointer: Pointer to an optional container the weights' row-wise gradients\n            are written to if scale_by_weight is True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        bias_grad_pointer: Pointer to an optional container the bias vector's row-wise gradients\n            are written to if scale_by_weight and add_bias are True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        weight_grad_batch_stride: Stride necessary to jump one element along the\n            weight gradient container's batch dimension.\n        weight_grad_feat_stride: Stride necessary to jump one element along the\n            weight gradient container's feature dimension.\n        bias_grad_batch_stride: Stride necessary to jump one element along the\n            weight gradient container's batch dimension.\n        bias_grad_feat_stride: Stride necessary to jump one element along the\n            weight gradient container's feature dimension.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        add_bias: Flag for adding a bias vector to the normalized output\n            if scale_by_weight is True.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n        ] + output_grad_feat_stride * feat_offset[None, :]\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    mean = tl.load(mean_pointer + batch_offset, mask=batch_mask)\n    inv_std = tl.load(inv_std_pointer + batch_offset, mask=batch_mask)\n    pre_lin = (input - mean[:, None]) * inv_std[:, None]\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        weight_output_grad_prod = weight * output_grad\n    else:\n        weight_output_grad_prod = output_grad\n    term1 = tl.sum(pre_lin * weight_output_grad_prod, axis=1) / feat_dim\n    term1 = pre_lin * term1[:, None]\n    term2 = tl.sum(weight_output_grad_prod, axis=1) / feat_dim\n    input_grad = inv_std[:, None] * (weight_output_grad_prod - (term1 +\n        term2[:, None]))\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    if scale_by_weight:\n        weight_grad_pointer += (weight_grad_batch_stride * batch_pid + \n            weight_grad_feat_stride * feat_offset)\n        tl.store(weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0),\n            mask=feat_mask)\n        if add_bias:\n            bias_grad_pointer += (bias_grad_batch_stride * batch_pid + \n                bias_grad_feat_stride * feat_offset)\n            tl.store(bias_grad_pointer, tl.sum(output_grad, axis=0), mask=\n                feat_mask)\n"
    },
    {
      "input": "@triton.jit\ndef accum_linear(accum, input1, input2, fp16: 'tl.constexpr', tf32:\n    'tl.constexpr'):\n    \"\"\"\n    Accumulates matrix multiplications of input tensors for linear functions.\n\n    Args:\n        accum: Accumulator holding aggregation of matrix multiplications.\n            The accumulator must be of shape [BLOCK_SIZE1, BLOCK_SIZE3].\n        input1: First operand of matrix multiplication.\n            The operand must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        input2: Second operand of matrix multiplication.\n            The operand must be of shape [BLOCK_SIZE2, BLOCK_SIZE3].\n        fp16: Flag for converting operands to FP16.\n        tf32: Flag for performing matrix multiplication in TF32.\n\n    Returns:\n        Accumulator with the result of the new matrix multiplication added to it.\n    \"\"\"\n    if fp16:\n        input1 = input1\n        input2 = input2\n    return accum + tl.dot(input1, input2, allow_tf32=tf32)\n"
    },
    {
      "input": "@triton.jit\ndef glu(input1, input2, param, act_func: 'tl.constexpr'):\n    \"\"\"\n    Applies the gated linear unit with an arbitrary activation function\n    to the input.\n\n    Args:\n        input1: First half of input to gate.\n            The first half must be of the same shape as the second half.\n        input2: Second half of input to gate.\n            The second half must be of the same shape as the first half.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        param: Parameter in the case of parameterized activation functions.\n\n    Args:\n        Input transformed by the gated linear unit\n        with an arbitrary activation function.\n    \"\"\"\n    return input1 * apply_act_func(input2, None, None, None, param,\n        act_func, False)\n"
    },
    {
      "input": "@triton.jit\ndef softmax(input, log: 'tl.constexpr'):\n    \"\"\"\n    Normalizes the input using softmax along the last dimension.\n\n    Args:\n        input: Input to normalize.\n            The input must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        log: Flag for indicating if the log of softmax should be taken.\n\n    Returns:\n        Input normalized by softmax.\n    \"\"\"\n    input = input\n    input = input - tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    if log:\n        output = input - tl.log(denominator)\n    else:\n        output = numerator / denominator\n    return output\n"
    },
    {
      "input": "@triton.jit\ndef calc_mean_and_inv_std(input, last_dim, eps, last_dim_mask: 'tl.constexpr'):\n    \"\"\"\n    Calculates the mean and inverse standard deviation of the input\n    along the last dimension.\n\n    Args:\n        input: Input whose mean and inverse standard deviation are calculated.\n            The input must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        last_dim: Size of the last dimension of input.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        last_dim_mask: Mask for the last dimension indicating\n            which elements should be included in the calculations.\n            The mask must be of shape [BLOCK_SIZE2].\n\n    Returns:\n        Mean and inverse standard deviation of the input.\n    \"\"\"\n    input = input\n    mean = tl.sum(input, axis=1) / last_dim\n    diff = tl.where(last_dim_mask[None, :], input - mean[:, None], 0)\n    inv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / last_dim + eps)\n    return mean, inv_std\n"
    },
    {
      "input": "@triton.jit\ndef update_welford(input, prev_count, prev_mean, prev_var, curr_count, mask:\n    'tl.constexpr'):\n    \"\"\"\n    Updates count, mean, and variance (M2) statistics for Welford's algorithm.\n\n    Args:\n        input: Input used to update statistics.\n            The input must be of the same shape as the mask.\n        prev_count: Previous count statistic to update.\n        prev_mean: Previous mean statistic to update.\n        prev_var: Previous variance (M2) statistic to update.\n        curr_count: Count of elements in current input.\n        mask: Mask indicating which elements should be included in the calculations.\n            The mask must be of the same shape as the input.\n\n    Returns:\n        Updated count, mean, and variance (M2) statistics\n    \"\"\"\n    input = input\n    count = prev_count + curr_count\n    mean = (tl.sum(input) - curr_count * prev_mean) / count\n    deltas = tl.where(mask, (input - mean) * (input - prev_mean), 0.0)\n    var = prev_var + tl.sum(deltas)\n    return count, mean, var\n"
    },
    {
      "input": "@triton.jit\ndef update_ema(prev_ema, new_val, momentum):\n    \"\"\"\n    Updates exponential moving average.\n\n    Args:\n        prev_ema: Previous exponential moving average.\n        new_val: Value used to update the exponential moving average.\n        momentum: Momentum.\n\n    Returns:\n        Updated running statistic.\n    \"\"\"\n    return (1 - momentum) * prev_ema + momentum * new_val\n"
    },
    {
      "input": "@triton.jit\ndef standardize(input, mean, inv_std, weight, bias):\n    \"\"\"\n    Standardizes the input given its mean and inverse standard deviation,\n    multiplies the result by weights, and adds a bias vector.\n\n    Args:\n        input: Input to standardize.\n        mean: Mean of input.\n        inv_std: Inverse standard deviation of input.\n        weight: Weight multiplied by the standardized input.\n        bias: Bias added to the result of the weight multiplication.\n\n    Returns:\n        Standardized input.\n    \"\"\"\n    return weight * inv_std * (input - mean) + bias\n"
    },
    {
      "input": "@triton.jit\ndef calc_p_loss(input, target, size, p_loss: 'tl.constexpr', reduction:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the L1 or squared L2 norm of the difference between the input\n    and target (i.e., mean absolute error or mean squared error).\n\n    Args:\n        input: Input.\n            The input must be of shape [BLOCK_SIZE].\n        target: Target.\n            The target must be of shape [BLOCK_SIZE].\n        size: Number of elements in the input and target.\n            This value is used only if reduction is 'mean'.\n        p_loss: p-norm used to compute the error.\n            Options are 1 for MAE and 2 for MSE.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the error\n            across all entries, and 'sum' for summing the error across all entries.\n\n    Returns:\n        Error.\n    \"\"\"\n    input = input\n    target = target\n    diff = input - target\n    if p_loss == 1:\n        error = tl.abs(diff)\n    elif p_loss == 2:\n        error = diff * diff\n    if reduction == 'none':\n        output = error\n    elif reduction == 'mean':\n        output = tl.sum(error) / size\n    elif reduction == 'sum':\n        output = tl.sum(error)\n    return output\n"
    },
    {
      "input": "@triton.jit\ndef nll_loss(input, size, reduction: 'tl.constexpr'):\n    \"\"\"\n    Measures the negative log likelihood loss given log-probabilities of target class.\n\n    Args:\n        input: Input containing predicted log-probabilities corresponding to target class.\n            The input can have arbitrary shape.\n        size: Number of elements in the input.\n            This value is used only if reduction is 'mean'.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the loss\n            across all entries, and 'sum' for summing the loss across all entries.\n\n    Returns:\n        Loss.\n    \"\"\"\n    input = input\n    if reduction == 'none':\n        output = -input\n    elif reduction == 'mean':\n        output = -tl.sum(input) / size\n    elif reduction == 'sum':\n        output = -tl.sum(input)\n    return output\n"
    },
    {
      "input": "@triton.jit\ndef cross_entropy_loss(input, pred):\n    \"\"\"\n    Measures the per-row cross entropy loss given\n    input and predicted logits corresponding to target class.\n\n    Args:\n        input: Input.\n            The input must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        pred: Predicted logits corresponding to target class.\n            The predictions must be of shape [BLOCK_SIZE1].\n\n    Returns:\n        Loss.\n    \"\"\"\n    input = input\n    pred = pred\n    mx = tl.max(input, axis=1)\n    input -= mx[:, None]\n    loss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx\n    return loss\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, L, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vn, stride_vk, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, Z_H_N_CTX, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', IS_CAUSAL:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    qvk_offset = off_hz * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    K_block_ptr = tl.make_block_ptr(base=K, shape=(BLOCK_DMODEL, Z_H_N_CTX),\n        strides=(stride_kk, stride_kn), offsets=(0, vk_offset), block_shape\n        =(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_vn, stride_vk), offsets=(vk_offset, 0), block_shape\n        =(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :\n        ] * stride_qk\n    q = tl.load(Q_ptrs)\n    q = q * qk_scale\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if IS_CAUSAL:\n            qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n                float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n    O_block_ptr = tl.make_block_ptr(base=Out, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(vk_offset +\n        start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(\n        1, 0))\n    tl.store(O_block_ptr, acc)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_m, delta)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel_one_col_block(Q, K, V, sm_scale, qk_scale, Out, DO, DQ, DK,\n    DV, L, D, Q_block_ptr, K_block_ptr, V_block_ptr, DO_block_ptr,\n    DQ_block_ptr, DK_block_ptr, DV_block_ptr, stride_dqa, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, Z, H, N_CTX,\n    off_h, off_z, off_hz, start_n, num_block, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', CAUSAL: 'tl.constexpr', MMA_V3:\n    'tl.constexpr'):\n    if CAUSAL:\n        lo = start_n * BLOCK_M\n    else:\n        lo = 0\n    Q_offset = (off_z * stride_qz + off_h * stride_qh) // stride_qm\n    DQ_offset = off_z * stride_qz + off_h * stride_qh\n    K_offset = (off_z * stride_kz + off_h * stride_kh) // stride_kn\n    V_offset = (off_z * stride_vz + off_h * stride_vh) // stride_vn\n    if SEQUENCE_PARALLEL:\n        DQ_offset += stride_dqa * start_n\n    DQ_offset = DQ_offset // stride_qm\n    Q_block_ptr = tl.advance(Q_block_ptr, (lo + Q_offset, 0))\n    K_block_ptr = tl.advance(K_block_ptr, (start_n * BLOCK_M + K_offset, 0))\n    V_block_ptr = tl.advance(V_block_ptr, (start_n * BLOCK_M + V_offset, 0))\n    DO_block_ptr = tl.advance(DO_block_ptr, (lo + Q_offset, 0))\n    DQ_block_ptr = tl.advance(DQ_block_ptr, (lo + DQ_offset, 0))\n    DK_block_ptr = tl.advance(DK_block_ptr, (start_n * BLOCK_M + K_offset, 0))\n    DV_block_ptr = tl.advance(DV_block_ptr, (start_n * BLOCK_M + V_offset, 0))\n    offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_m = tl.arange(0, BLOCK_N)\n    D_ptrs = D + off_hz * N_CTX\n    l_ptrs = L + off_hz * N_CTX\n    dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    k = tl.load(K_block_ptr)\n    v = tl.load(V_block_ptr)\n    for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n        offs_m_curr = start_m + offs_m\n        q = tl.load(Q_block_ptr)\n        if CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], float(\n                0.0), float('-inf'))\n        else:\n            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        qk *= qk_scale\n        l_i = tl.load(l_ptrs + offs_m_curr)\n        p = tl.math.exp2(qk - l_i[:, None])\n        do = tl.load(DO_block_ptr)\n        dv += tl.dot(tl.trans(p), do)\n        Di = tl.load(D_ptrs + offs_m_curr)\n        dp = tl.dot(do, tl.trans(v))\n        ds = p * (dp - Di[:, None]) * sm_scale\n        dk += tl.dot(tl.trans(ds), q)\n        if not SEQUENCE_PARALLEL:\n            dq = tl.load(DQ_block_ptr)\n            dq += tl.dot(ds, k)\n            tl.store(DQ_block_ptr, dq)\n        elif SEQUENCE_PARALLEL:\n            if MMA_V3:\n                dq = tl.dot(ds, k)\n            else:\n                dq = tl.trans(tl.dot(tl.trans(k), tl.trans(ds)))\n            tl.store(DQ_block_ptr, dq)\n        DQ_block_ptr = tl.advance(DQ_block_ptr, (BLOCK_M, 0))\n        Q_block_ptr = tl.advance(Q_block_ptr, (BLOCK_M, 0))\n        DO_block_ptr = tl.advance(DO_block_ptr, (BLOCK_M, 0))\n    tl.store(DV_block_ptr, dv)\n    tl.store(DK_block_ptr, dk)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, D, stride_dqa,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, Z, H,\n    N_CTX, Z_H_N_CTX, SQ_Z_H_N_CTX, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', SEQUENCE_PARALLEL:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', MMA_V3: 'tl.constexpr'):\n    qk_scale = sm_scale * 1.44269504\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    Q_block_ptr = tl.make_block_ptr(base=Q, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk), offsets=(0, 0), block_shape=(\n        BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_kn, stride_kk), offsets=(0, 0), block_shape=(\n        BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_vn, stride_vk), offsets=(0, 0), block_shape=(\n        BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    DO_block_ptr = tl.make_block_ptr(base=DO, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    if SEQUENCE_PARALLEL:\n        DQ_block_ptr = tl.make_block_ptr(base=DQ, shape=(SQ_Z_H_N_CTX,\n            BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n            block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    else:\n        DQ_block_ptr = tl.make_block_ptr(base=DQ, shape=(Z_H_N_CTX,\n            BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n            block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    DK_block_ptr = tl.make_block_ptr(base=DK, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_kn, stride_kk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    DV_block_ptr = tl.make_block_ptr(base=DV, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_vn, stride_vk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    num_block_n = tl.cdiv(N_CTX, BLOCK_N)\n    if not SEQUENCE_PARALLEL:\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(Q, K, V, sm_scale, qk_scale, Out, DO,\n                DQ, DK, DV, L, D, Q_block_ptr, K_block_ptr, V_block_ptr,\n                DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,\n                stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n                stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n                stride_vh, stride_vn, stride_vk, Z, H, N_CTX, off_h, off_z,\n                off_hz, start_n, num_block_n, BLOCK_M=BLOCK_M, BLOCK_DMODEL\n                =BLOCK_DMODEL, BLOCK_N=BLOCK_N, SEQUENCE_PARALLEL=\n                SEQUENCE_PARALLEL, CAUSAL=CAUSAL, MMA_V3=MMA_V3)\n    else:\n        start_n = tl.program_id(1)\n        _bwd_kernel_one_col_block(Q, K, V, sm_scale, qk_scale, Out, DO, DQ,\n            DK, DV, L, D, Q_block_ptr, K_block_ptr, V_block_ptr,\n            DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,\n            stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n            stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n            stride_vh, stride_vn, stride_vk, Z, H, N_CTX, off_h, off_z,\n            off_hz, start_n, num_block_n, BLOCK_M=BLOCK_M, BLOCK_DMODEL=\n            BLOCK_DMODEL, BLOCK_N=BLOCK_N, SEQUENCE_PARALLEL=\n            SEQUENCE_PARALLEL, CAUSAL=CAUSAL, MMA_V3=MMA_V3)\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_SPATIAL': lambda args: next_power_of_2(args['spatial_dim'])})\n@triton.jit\ndef nll_loss_forward_kernel(input_pointer, target_pointer, weight_pointer,\n    sum_weights_pointer, output_pointer, batch_dim, spatial_dim,\n    input_batch_stride, input_feat_stride, input_spatial_stride,\n    target_batch_stride, target_spatial_stride, output_batch_stride,\n    output_spatial_stride, reduction: 'tl.constexpr', weighted:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_SPATIAL:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the negative log likelihood loss between the input and target,\n    with optional reweighing of each class.\n\n    Args:\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim, spatial_dim].\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim, spatial_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to a container the sum of the class weights is written to.\n            The container must be of shape [batch_dim/BLOCK_SIZE_BATCH].\n        output_pointer: Pointer to a container the loss is written to.\n            The container must be of shape [batch_dim, spatial_dim] if reduction is 'none',\n            and otherwise of shape [batch_dim/BLOCK_SIZE].\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        target_batch_stride: Stride necessary to jump one element along the\n            target's batch dimension.\n        target_spatial_stride: Stride necessary to jump one element along the\n            target's spatial dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_spatial_stride: Stride necessary to jump one element along the\n            output container's spatial dimension.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the loss\n            across all entries, and 'sum' for summing the loss across all entries.\n            If a reduction method is specified, the reduced result of each\n            program is written to a separate index in the summed weights and\n            output container, which should later be summed.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)\n    batch_mask = batch_offset < batch_dim\n    spatial_mask = spatial_offset < spatial_dim\n    target_pointer += target_batch_stride * batch_offset[:, None\n        ] + target_spatial_stride * spatial_offset[None, :]\n    target = tl.load(target_pointer, mask=batch_mask[:, None] &\n        spatial_mask[None, :])\n    input_pointer += (input_feat_stride * target + input_batch_stride *\n        batch_offset[:, None] + input_spatial_stride * spatial_offset[None, :])\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & spatial_mask[\n        None, :])\n    output = -input\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        output *= weight\n    if reduction == 'none':\n        output_pointer += output_batch_stride * batch_offset[:, None\n            ] + output_spatial_stride * spatial_offset[None, :]\n        tl.store(output_pointer, output, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n    elif reduction == 'mean':\n        if weighted:\n            tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))\n            tl.store(output_pointer + batch_pid, tl.sum(output))\n        else:\n            tl.store(output_pointer + batch_pid, tl.sum(output) / (\n                batch_dim * spatial_dim))\n    elif reduction == 'sum':\n        tl.store(output_pointer + batch_pid, tl.sum(output))\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_SPATIAL': lambda args: next_power_of_2(args['spatial_dim'])})\n@triton.jit\ndef nll_loss_backward_kernel(output_grad_pointer, target_pointer,\n    weight_pointer, sum_weights_pointer, input_grad_pointer, batch_dim,\n    spatial_dim, output_grad_batch_stride, output_grad_feat_stride,\n    target_batch_stride, target_spatial_stride, input_grad_batch_stride,\n    input_grad_feat_stride, input_grad_spatial_stride, reduction:\n    'tl.constexpr', weighted: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_SPATIAL: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of negative log likelihood loss.\n\n    Args:\n        output_grad_pointer: Pointer to the loss's output gradients.\n            The output gradients must be of shape [batch_dim, spatial_dim]\n            if reduction is 'none', and otherwise [batch_dim/BLOCK_SIZE_BATCH].\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim, spatial_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to the sum of the class weights if the classes were weighed.\n            The sum of weights must be a scalar.\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim, spatial_dim] and zeroed.\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        target_batch_stride: Stride necessary to jump one element along the\n            target's batch dimension.\n        target_spatial_stride: Stride necessary to jump one element along the\n            target's spatial dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        input_grad_spatial_stride: Stride necessary to jump one element along the\n            input gradient container's spatial dimension.\n        reduction: Reduction strategy for the output whose gradient is calculated.\n            Options are 'none' for no reduction, 'mean' for averaging the loss\n            across all entries, and 'sum' for summing the loss across all entries.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)\n    batch_mask = batch_offset < batch_dim\n    spatial_mask = spatial_offset < spatial_dim\n    output_grad_mask = None\n    if reduction == 'none':\n        output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n            ] + output_grad_feat_stride * spatial_offset[None, :]\n        output_grad_mask = batch_mask[:, None] & spatial_mask[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask)\n    input_grad = -output_grad\n    target_pointer += target_batch_stride * batch_offset[:, None\n        ] + target_spatial_stride * spatial_offset[None, :]\n    target = tl.load(target_pointer, mask=batch_mask[:, None] &\n        spatial_mask[None, :])\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        input_grad *= weight\n        if reduction == 'mean':\n            input_grad /= tl.load(sum_weights_pointer)\n    elif reduction == 'mean':\n        input_grad /= batch_dim * spatial_dim\n    input_grad_pointer += (input_grad_feat_stride * target + \n        input_grad_batch_stride * batch_offset[:, None] + \n        input_grad_spatial_stride * spatial_offset[None, :])\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        spatial_mask[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef p_loss_forward_kernel(input_pointer, target_pointer, output_pointer,\n    size, p_loss: 'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the L1 or squared L2 norm of the difference between the input\n    and target (i.e., mean absolute error or mean squared error).\n\n    Args:\n        input_pointer: Pointer to the input.\n            The input must be of shape [size].\n        target_pointer: Pointer to the target.\n            The target must be of shape [size].\n        output_pointer: Pointer to a container the error is written to.\n            The container must be of shape [size] if reduction is 'none',\n            and otherwise of shape [size/BLOCK_SIZE].\n        size: Number of elements in the input and target.\n        p_loss: p-norm used to compute the error.\n            Options are 1 for MAE and 2 for MSE.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the error\n            across all entries, and 'sum' for summing the error across all entries.\n            If a reduction method is specified, the reduced result of each\n            program is written to a separate index in the output container,\n            which should later be summed.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input = tl.load(input_pointer + offset, mask=mask)\n    target = tl.load(target_pointer + offset, mask=mask)\n    diff = input - target\n    if p_loss == 1:\n        error = tl.abs(diff)\n    elif p_loss == 2:\n        error = diff * diff\n    if reduction == 'none':\n        tl.store(output_pointer + offset, error, mask=mask)\n    elif reduction == 'mean':\n        tl.store(output_pointer + pid, tl.sum(error) / size)\n    elif reduction == 'sum':\n        tl.store(output_pointer + pid, tl.sum(error))\n"
    },
    {
      "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef p_loss_backward_kernel(output_grad_pointer, input_pointer,\n    target_pointer, input_grad_pointer, target_grad_pointer, size, p_loss:\n    'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of the mean absolute error or\n    mean squared error.\n\n    Args:\n        output_grad_pointer: Pointer to the error's output gradients.\n            The output gradients must be a scalar or of shape [size].\n        input_pointer: Pointer to the input.\n            The input must be of shape [size].\n        target_pointer: Pointer to the target.\n            The target must be of shape [size].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [size].\n        target_grad_pointer: Pointer to a container the target's gradients are written to.\n            The container must be of shape [size].\n        size: Number of elements in the input and target.\n        p_loss: p-norm used to compute the error whose gradient is calculated.\n            Options are 1 for MAE and 2 for MSE.\n        reduction: Reduction strategy for the output whose gradient is calculated.\n            Options are 'none' for no reduction, 'mean' for averaging the error\n            across all entries, and 'sum' for summing the error across all entries.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad_mask = None\n    if reduction == 'none':\n        output_grad_pointer += offset\n        output_grad_mask = mask\n    input = tl.load(input_pointer + offset, mask=mask)\n    target = tl.load(target_pointer + offset, mask=mask)\n    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask)\n    if p_loss == 1:\n        input_grad = tl.where(target <= input, 1, -1)\n    elif p_loss == 2:\n        input_grad = 2 * (input - target)\n    if reduction == 'mean':\n        input_grad /= size\n    input_grad *= output_grad\n    tl.store(input_grad_pointer + offset, input_grad, mask=mask)\n    tl.store(target_grad_pointer + offset, -input_grad, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef rms_norm_forward_kernel(input_pointer, weight_pointer, inv_rms_pointer,\n    output_pointer, batch_dim, feat_dim, input_batch_stride,\n    input_feat_stride, output_batch_stride, output_feat_stride, eps,\n    scale_by_weight: 'tl.constexpr', save_stats: 'tl.constexpr',\n    BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Root-mean-square-normalizes the input.\n\n    Args:\n        input_pointer: Pointer to the input to root-mean-square-normalize.\n            The input must be of shape [batch_dim, feat_dim].\n        weight_pointer: Pointer to optional weights for linear transform.\n            The weights, if provided, must be of shape [feat_dim].\n        inv_rms_pointer: Pointer to an optional container the input's inverse\n            root mean square is written to if save_stats is True.\n            The container, if provided, must be of shape [batch_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        save_stats: Flag for saving the root mean square.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    inv_rms = tl.rsqrt(tl.sum(input * input, axis=1) / feat_dim + eps)\n    output = input * inv_rms[:, None]\n    if save_stats:\n        tl.store(inv_rms_pointer + batch_offset, inv_rms, mask=batch_mask)\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        output *= weight\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef rms_norm_backward_kernel(output_grad_pointer, input_pointer,\n    inv_rms_pointer, weight_pointer, input_grad_pointer,\n    weight_grad_pointer, batch_dim, feat_dim, output_grad_batch_stride,\n    output_grad_feat_stride, input_batch_stride, input_feat_stride,\n    input_grad_batch_stride, input_grad_feat_stride,\n    weight_grad_batch_stride, weight_grad_feat_stride, scale_by_weight:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT:\n    'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of root mean square normalization.\n\n    Args:\n        output_grad_pointer: Pointer to root mean square normalization's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        inv_rms_pointer: Pointer to the input's inverse root mean square.\n            The inverse root mean square should be of shape [batch_dim].\n        weight_pointer: Pointer to optional weights if affine transform occurred.\n            The weights, if provided, must be of shape [feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        weight_grad_pointer: Pointer to an optional container the weights' row-wise gradients\n            are written to if scale_by_weight is True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        bias_grad_pointer: Pointer to an optional container the bias vector's row-wise gradients\n            are written to if scale_by_weight and add_bias are True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        weight_grad_batch_stride: Stride necessary to jump one element along the\n            weight gradient container's batch dimension.\n        weight_grad_feat_stride: Stride necessary to jump one element along the\n            weight gradient container's feature dimension.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n        ] + output_grad_feat_stride * feat_offset[None, :]\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    inv_rms = tl.load(inv_rms_pointer + batch_offset, mask=batch_mask)\n    pre_lin = input * inv_rms[:, None]\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        weight_output_grad_prod = weight * output_grad\n    else:\n        weight_output_grad_prod = output_grad\n    term1 = input * tl.sum(input * weight_output_grad_prod, axis=1)\n    term2 = inv_rms[:, None] * inv_rms[:, None]\n    input_grad = inv_rms[:, None] * (weight_output_grad_prod - term1 *\n        term2 / feat_dim)\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    if scale_by_weight:\n        weight_grad_pointer += (weight_grad_batch_stride * batch_pid + \n            weight_grad_feat_stride * feat_offset)\n        tl.store(weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0),\n            mask=feat_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef softmax_forward_kernel(input_pointer, output_pointer, batch_dim,\n    feat_dim, input_batch_stride, input_feat_stride, output_batch_stride,\n    output_feat_stride, log: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Normalizes the input using softmax.\n\n    Args:\n        input_pointer: Pointer to the input to normalize.\n            The input must be of shape [batch_dim, feat_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        log: Flag for indicating if the log of softmax should be taken.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :], other=-float('inf'))\n    input -= tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    if log:\n        output = input - tl.log(denominator)\n    else:\n        output = numerator / denominator\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef softmax_backward_kernel(output_grad_pointer, output_pointer,\n    input_grad_pointer, batch_dim, feat_dim, output_grad_batch_stride,\n    output_grad_feat_stride, output_batch_stride, output_feat_stride,\n    input_grad_batch_stride, input_grad_feat_stride, log: 'tl.constexpr',\n    BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of softmax.\n\n    Args:\n        output_grad_pointer: Pointer to softmax's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim].\n        output_pointer: Pointer to softmax's output.\n            The output must be of shape [batch_dim, feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        log: Flag indicating if log of softmax was taken.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n        ] + output_grad_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    output = tl.load(output_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    if log:\n        input_grad = output_grad - tl.exp(output) * tl.sum(output_grad, axis=1\n            )[:, None]\n    else:\n        input_grad = output * (output_grad - tl.sum(output_grad * output,\n            axis=1)[:, None])\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, TMP, L, M, Out, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh,\n    stride_om, stride_on, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(q_ptrs)\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + start_n * stride_kn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        qk *= sm_scale\n        qk += tl.where(offs_m[:, None] >= start_n + offs_n[None, :], 0,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale)\n        acc_scale = tl.load(t_ptrs)\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + start_n * stride_vk)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, M, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    num_block, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_qz + off_h * stride_qh\n    V += off_z * stride_qz + off_h * stride_qh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_qz + off_h * stride_qh\n    DV += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        lo = start_n * BLOCK_M\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        D_ptrs = D + off_hz * N_CTX\n        m_ptrs = M + off_hz * N_CTX\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, k, trans_b=True)\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n            m = tl.load(m_ptrs + offs_m_curr)\n            p = tl.exp(qk * sm_scale - m[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(p, do, trans_a=True)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, v, trans_b=True)\n            ds = p * dp * sm_scale\n            dk += tl.dot(ds, q, trans_a=True)\n            dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n            dq += tl.dot(ds, k)\n            tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, L, NewDO, Delta, BLOCK_M: 'tl.constexpr',\n    D_HEAD: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    denom = tl.load(L + off_m)\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n    tl.store(Delta + off_m, delta)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride, BLOCK_SIZE:\n    'tl.constexpr', HAS_SMOOTHING: 'tl.constexpr', SPLIT: 'tl.constexpr',\n    PRECOMPUTED_LSE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    sum_logits = 0.0\n    if not PRECOMPUTED_LSE:\n        m_i = -float('inf')\n        l_i = 0.0\n        for col_offset in range(0, n_cols, BLOCK_SIZE):\n            cols = col_offset + tl.arange(0, BLOCK_SIZE)\n            logits = tl.load(logits_ptr + cols, mask=cols < n_cols, other=-\n                float('inf')) * logit_scale\n            if HAS_SMOOTHING:\n                sum_logits += tl.sum(tl.where(cols < n_cols, logits, 0.0))\n            m_i_new = tl.maximum(m_i, tl.max(logits))\n            l_i = tl.exp(m_i - m_i_new) * l_i + tl.sum(tl.exp(logits - m_i_new)\n                )\n            m_i = m_i_new\n        lse = tl.log(l_i) + m_i\n        tl.store(lse_ptr + row_idx, lse)\n    else:\n        lse = tl.load(lse_ptr + row_idx)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= 0 and label_idx < n_cols:\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - smoothing * sum_logits / total_classes - (1 - smoothing\n                    ) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits /\n                total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + row_idx, z_loss)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: 'tl.constexpr',\n    HAS_SMOOTHING: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - smooth_positive,\n            probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n"
    },
    {
      "input": "@triton.jit\ndef cosh(x):\n    exp_x = tl.exp(x)\n    return (exp_x + 1.0 / exp_x) * 0.5\n"
    },
    {
      "input": "@triton.jit\ndef relu(x):\n    \"\"\"\n    ReLU_ activation function\n\n    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n    \"\"\"\n    zero = 0.0\n    return tl.where(x >= 0, x, zero)\n"
    },
    {
      "input": "@triton.jit\ndef relu_grad(x):\n    zero = 0.0\n    one = 1.0\n    return tl.where(x >= 0, one, zero)\n"
    },
    {
      "input": "@triton.jit\ndef squared_relu(x):\n    \"\"\"\n    Squared ReLU activation, as proposed in the Primer_ paper.\n\n    .. _Primer: https://arxiv.org/abs/2109.08668\n    \"\"\"\n    x_ = relu(x)\n    return x_ * x_\n"
    },
    {
      "input": "@triton.jit\ndef squared_relu_grad(x):\n    return tl.where(x >= 0, 2.0 * x, 0.0)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu(x):\n    \"\"\"\n    LeakyReLU_ activation\n\n    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html\n    \"\"\"\n    scale = 0.01 + 0.0\n    scale = scale\n    return tl.where(x >= 0, x, scale * x)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu_grad(x):\n    min_grad = 0.01\n    max_grad = 1\n    min_grad = min_grad\n    max_grad = max_grad\n    return tl.where(x >= 0, max_grad, min_grad)\n"
    },
    {
      "input": "@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU)\"\"\"\n    return x * 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n"
    },
    {
      "input": "@triton.jit\ndef gelu_grad(x):\n    cdf = 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n    pdf = tl.exp(-0.5 * x * x) * _gaussian_pdf_normalization\n    return cdf + x * pdf\n"
    },
    {
      "input": "@triton.jit\ndef gelu_approx(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit, with tanh approximation\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1.0 + tanh(_sqrt2pi * x * (1.0 + 0.044715 * x * x)))\n"
    },
    {
      "input": "@triton.jit\ndef gelu_approx_grad(x):\n    tanh_out = tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n    return 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + \n        0.1070322243 * x * x)) + 0.5 * (1 + tanh_out)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})\n@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})\n@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, X1, W1, B1, Y1,\n    RESIDUAL_OUT, ROWSCALE, SEEDS, DROPOUT_MASK, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_res_row, stride_res_out_row, stride_x1_row,\n    stride_y1_row, M, N, eps, dropout_p, IS_RMS_NORM: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', HAS_RESIDUAL: 'tl.constexpr',\n    STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS: 'tl.constexpr',\n    HAS_DROPOUT: 'tl.constexpr', STORE_DROPOUT_MASK: 'tl.constexpr',\n    HAS_ROWSCALE: 'tl.constexpr', HAS_X1: 'tl.constexpr', HAS_W1:\n    'tl.constexpr', HAS_B1: 'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row)\n        x *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + row), cols, n_rounds=7) > dropout_p\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + M + row), cols, n_rounds=7\n                ) > dropout_p\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask,\n                    mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS', 'HAS_DROPOUT'])\n@triton.heuristics({'HAS_ROWSCALE': lambda args: args['ROWSCALE'] is not None})\n@triton.heuristics({'HAS_DY1': lambda args: args['DY1'] is not None})\n@triton.heuristics({'HAS_DX1': lambda args: args['DX1'] is not None})\n@triton.heuristics({'HAS_B1': lambda args: args['DB1'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, W1, DY1,\n    DX1, DW1, DB1, DRESIDUAL_IN, ROWSCALE, SEEDS, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_dy_row, stride_dx_row, stride_dres_row,\n    stride_dy1_row, stride_dx1_row, stride_dres_in_row, M, N, eps,\n    dropout_p, rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', HAS_DROPOUT: 'tl.constexpr',\n    HAS_ROWSCALE: 'tl.constexpr', HAS_DY1: 'tl.constexpr', HAS_DX1:\n    'tl.constexpr', HAS_B1: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if HAS_DY1:\n        DY1 += row_start * stride_dy1_row\n    if HAS_DX1:\n        DX1 += row_start * stride_dx1_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    if HAS_DY1:\n        w1 = tl.load(W1 + cols, mask=mask)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_DY1:\n        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n        if HAS_B1:\n            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if HAS_DY1:\n            dy1 = tl.load(DY1 + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_DY1:\n            wdy += w1 * dy1\n            dw1 += dy1 * xhat\n            if HAS_B1:\n                db1 += dy1\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        if HAS_DX1:\n            if HAS_DROPOUT:\n                keep_mask = tl.rand(tl.load(SEEDS + M + row), cols, n_rounds=7\n                    ) > dropout_p\n                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n            else:\n                dx1 = dx\n            tl.store(DX1 + cols, dx1, mask=mask)\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + row), cols, n_rounds=7\n                ) > dropout_p\n            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + row)\n            dx *= rowscale\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n        if HAS_DY1:\n            DY1 += stride_dy1_row\n        if HAS_DX1:\n            DX1 += stride_dx1_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n    if HAS_DY1:\n        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)\n        if HAS_B1:\n            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_fwd(C, ACT_INPUT, A, B, bias, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bn, stride_bk,\n    BLOCK_M: 'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', A_ROWMAJOR: 'tl.constexpr', B_COLMAJOR:\n    'tl.constexpr', BIAS: 'tl.constexpr', SAVE_ACT_INPUT: 'tl.constexpr',\n    ACTIVATION: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Bias has shape (N,)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    if A_ROWMAJOR:\n        A = A + (ram[:, None] * stride_am + rk[None, :])\n    else:\n        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    if B_COLMAJOR:\n        B = B + (rk[:, None] + rbn[None, :] * stride_bn)\n    else:\n        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        if A_ROWMAJOR:\n            A += BLOCK_K\n        else:\n            A += BLOCK_K * stride_ak\n        if B_COLMAJOR:\n            B += BLOCK_K\n        else:\n            B += BLOCK_K * stride_bk\n    if BIAS:\n        bias = tl.load(bias + rn, mask=rn < N, other=0.0)\n        acc += bias[None, :]\n    if SAVE_ACT_INPUT:\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        tl.store(act_in_ptrs, acc)\n    if ACTIVATION == 'gelu':\n        acc = gelu(acc)\n    elif ACTIVATION == 'gelu_approx':\n        acc = gelu_approx(acc)\n    elif ACTIVATION == 'squared_relu':\n        acc = squared_relu(acc)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_bwd(C, ACT_INPUT, A, B, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bk, stride_bn,\n    BLOCK_M: 'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    if ACTIVATION != 'id':\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        act_input = tl.load(act_in_ptrs)\n    if ACTIVATION == 'gelu':\n        acc *= gelu_grad(act_input)\n    elif ACTIVATION == 'gelu_approx':\n        acc *= gelu_approx_grad(act_input)\n    elif ACTIVATION == 'squared_relu':\n        acc *= squared_relu_grad(act_input)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    rotary_dim, seqlen_ro, stride_out_batch, stride_out_seqlen,\n    stride_out_nheads, stride_out_headdim, stride_x_batch, stride_x_seqlen,\n    stride_x_nheads, stride_x_headdim, BLOCK_K: 'tl.constexpr',\n    IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr', IS_VARLEN: 'tl.constexpr',\n    INTERLEAVED: 'tl.constexpr', CONJUGATE: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q1, K1, Q2, K2, V, sm_scale, L, O, stride_q1z, stride_q1h,\n    stride_q1m, stride_q1k, stride_k1z, stride_k1h, stride_k1n, stride_k1k,\n    stride_q2z, stride_q2h, stride_q2m, stride_q2k, stride_k2z, stride_k2h,\n    stride_k2n, stride_k2k, stride_vz, stride_vh, stride_vn, stride_vk,\n    stride_oz, stride_oh, stride_om, stride_ok, Z, H, M, N, P_SEQ, w:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr', LARGER_M:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    input_dtype = Q1.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q1 += off_z * stride_q1z + off_h * stride_q1h\n    Q2 += off_z * stride_q2z + off_h * stride_q2h\n    K1 += off_z * stride_k1z + off_h * stride_k1h\n    K2 += off_z * stride_k2z + off_h * stride_k2h\n    V += off_z * stride_vz + off_h * stride_vh\n    O += off_z * stride_oz + off_h * stride_oh\n    L += (off_z * H + off_h) * M\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_n_init = offs_n_base\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q1_ptrs = Q1 + (offs_m[:, None] * stride_q1m + offs_k[None, :] * stride_q1k\n        )\n    q2_ptrs = Q2 + (offs_m[:, None] * stride_q2m + offs_k[None, :] * stride_q2k\n        )\n    k1_ptrs = K1 + (offs_n_init[:, None] * stride_k1n + offs_k[None, :] *\n        stride_k1k)\n    k2_ptrs = K2 + (offs_n_init[:, None] * stride_k2n + offs_k[None, :] *\n        stride_k2k)\n    v_ptrs = V + (offs_n_init[:, None] * stride_vn + offs_k[None, :] *\n        stride_vk)\n    o_ptrs = O + (offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)\n    l_ptrs = L + offs_m\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if DIVISIBLE_M:\n        q1 = tl.load(q1_ptrs)\n        q2 = tl.load(q2_ptrs)\n    else:\n        mask_m = offs_m < M\n        q1 = tl.load(q1_ptrs, mask=mask_m[:, None])\n        q2 = tl.load(q2_ptrs, mask=mask_m[:, None])\n    I = tl.where(offs_k[:, None] == offs_k, tl.full((BLOCK_DMODEL,\n        BLOCK_DMODEL), 1.0, dtype=input_dtype), tl.full((BLOCK_DMODEL,\n        BLOCK_DMODEL), 0.0, dtype=input_dtype))\n    q1 = tl.dot(q1, I)\n    q2 = tl.dot(q2, I)\n    if IS_CAUSAL:\n        hi = tl.minimum(N, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = N\n    for start_n in range(0, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        piecewise_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :] + w\n        if DIVISIBLE_N:\n            k1 = tl.load(k1_ptrs)\n            k2 = tl.load(k2_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            mask_n = offs_n < N\n            k1 = tl.load(k1_ptrs, mask=mask_n[:, None])\n            k2 = tl.load(k2_ptrs, mask=mask_n[:, None])\n            v = tl.load(v_ptrs, mask=mask_n[:, None])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.where(piecewise_mask, tl.dot(q2, tl.trans(k2)), tl.dot(q1,\n            tl.trans(k1)))\n        if not DIVISIBLE_N:\n            s = tl.where(mask_n, s, float('-inf'))\n        if IS_CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            s = tl.where(causal_mask, s, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(s, 1))\n        alpha = tl.math.exp2((m_i - m_i_new) * qk_scale)\n        p = tl.math.exp2(s * qk_scale - m_i_new[:, None] * qk_scale)\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        k1_ptrs += BLOCK_N * stride_k1n\n        k2_ptrs += BLOCK_N * stride_k2n\n        v_ptrs += BLOCK_N * stride_vn\n    if IS_CAUSAL and LARGER_M:\n        is_empty_line = offs_m + P_SEQ < 0\n        acc = tl.where(is_empty_line[:, None], 0.0, acc * (1.0 / l_i[:, None]))\n        l_i = tl.where(is_empty_line, float('-inf'), m_i * sm_scale + tl.\n            log(l_i))\n    else:\n        acc = acc * (1.0 / l_i[:, None])\n        l_i = m_i * sm_scale + tl.log(l_i)\n    if DIVISIBLE_M:\n        tl.store(l_ptrs, l_i)\n        tl.store(o_ptrs, acc)\n    else:\n        tl.store(l_ptrs, l_i, mask=mask_m)\n        tl.store(o_ptrs, acc, mask=mask_m[:, None])\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, stride_oz, stride_oh, stride_om,\n    stride_ok, stride_doz, stride_doh, stride_dom, stride_dok, stride_dz,\n    stride_dh, stride_dm, M, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr'):\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    Out += off_z * stride_oz + off_h * stride_oh\n    DO += off_z * stride_doz + off_h * stride_doh\n    Delta += off_z * stride_dz + off_h * stride_dh\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o_ptrs = Out + off_m[:, None] * stride_om + off_n[None, :] * stride_ok\n    do_ptrs = DO + off_m[:, None] * stride_dom + off_n[None, :] * stride_dok\n    if DIVISIBLE_M:\n        o = tl.load(o_ptrs)\n        do = tl.load(do_ptrs)\n    else:\n        mask_m = off_m < M\n        o = tl.load(o_ptrs, mask=mask_m[:, None])\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n    delta = tl.sum(o * do, axis=1)\n    d_ptrs = Delta + off_m * stride_dm\n    if DIVISIBLE_M:\n        tl.store(d_ptrs, delta)\n    else:\n        tl.store(d_ptrs, delta, mask=mask_m)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kv_kernel(Q1, K1, Q2, K2, V, sm_scale, DO, DK1, DK2, DV, L, D,\n    stride_q1z, stride_q1h, stride_q1m, stride_q1k, stride_k1z, stride_k1h,\n    stride_k1n, stride_k1k, stride_q2z, stride_q2h, stride_q2m, stride_q2k,\n    stride_k2z, stride_k2h, stride_k2n, stride_k2k, stride_vz, stride_vh,\n    stride_vn, stride_vk, stride_doz, stride_doh, stride_dom, stride_dok,\n    stride_dk1z, stride_dk1h, stride_dk1n, stride_dk1k, stride_dk2z,\n    stride_dk2h, stride_dk2n, stride_dk2k, stride_dvz, stride_dvh,\n    stride_dvn, stride_dvk, Z, H, M, N, P_SEQ, w: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    CAUSAL: 'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N:\n    'tl.constexpr'):\n    input_dtype = Q1.dtype.element_ty\n    start_n = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q1 += off_z * stride_q1z + off_h * stride_q1h\n    Q2 += off_z * stride_q2z + off_h * stride_q2h\n    K1 += off_z * stride_k1z + off_h * stride_k1h\n    K2 += off_z * stride_k2z + off_h * stride_k2h\n    V += off_z * stride_vz + off_h * stride_vh\n    DO += off_z * stride_doz + off_h * stride_doh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    DK1 += off_z * stride_dk1z + off_h * stride_dk1h\n    DK2 += off_z * stride_dk2z + off_h * stride_dk2h\n    DV += off_z * stride_dvz + off_h * stride_dvh\n    if CAUSAL:\n        lo = tl.maximum(start_n * BLOCK_N - P_SEQ, 0)\n        lo = lo // BLOCK_M * BLOCK_M\n    else:\n        lo = 0\n    offs_m_init = lo + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q1_ptrs = Q1 + (offs_m_init[:, None] * stride_q1m + offs_k[None, :] *\n        stride_q1k)\n    q2_ptrs = Q2 + (offs_m_init[:, None] * stride_q2m + offs_k[None, :] *\n        stride_q2k)\n    k1_ptrs = K1 + (offs_k[:, None] * stride_k1k + offs_n[None, :] * stride_k1n\n        )\n    k2_ptrs = K2 + (offs_k[:, None] * stride_k2k + offs_n[None, :] * stride_k2n\n        )\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n    do_ptrs = DO + (offs_m_init[:, None] * stride_dom + offs_k[None, :] *\n        stride_dok)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_k[None, :] * stride_dvk\n        )\n    dk1_ptrs = DK1 + (offs_n[:, None] * stride_dk1n + offs_k[None, :] *\n        stride_dk1k)\n    dk2_ptrs = DK2 + (offs_n[:, None] * stride_dk2n + offs_k[None, :] *\n        stride_dk2k)\n    if DIVISIBLE_N:\n        k1 = tl.load(k1_ptrs)\n        k2 = tl.load(k2_ptrs)\n        v = tl.load(v_ptrs)\n    else:\n        mask_n = offs_n < N\n        k1 = tl.load(k1_ptrs, mask=mask_n[None, :])\n        k2 = tl.load(k2_ptrs, mask=mask_n[None, :])\n        v = tl.load(v_ptrs, mask=mask_n[:, None])\n    dk1 = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dk2 = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    for start_m in range(lo, M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m = start_m + offs_m_base\n        if DIVISIBLE_M:\n            q1 = tl.load(q1_ptrs)\n            q2 = tl.load(q2_ptrs)\n            do = tl.load(do_ptrs)\n            delta = tl.load(D + offs_m)\n            l = tl.load(L + offs_m)\n        else:\n            mask_m = offs_m < M\n            q1 = tl.load(q1_ptrs, mask=mask_m[:, None])\n            q2 = tl.load(q2_ptrs, mask=mask_m[:, None])\n            do = tl.load(do_ptrs, mask=mask_m[:, None])\n            delta = tl.load(D + offs_m, mask=mask_m)\n            l = tl.load(L + offs_m, mask=mask_m)\n        piecewise_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :] + w\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.where(piecewise_mask, tl.dot(q2, k2), tl.dot(q1, k1))\n        p = tl.math.exp2(s * qk_scale - l[:, None] * log2e)\n        if not DIVISIBLE_M:\n            valid_mask = mask_m[:, None]\n            p = tl.where(valid_mask, p, 0.0)\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(causal_mask, p, 0.0)\n        dv += tl.dot(tl.trans(p), do)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_M:\n            ds = tl.where(valid_mask, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        ds2 = tl.where(piecewise_mask, ds, 0.0)\n        ds1 = tl.where(piecewise_mask, 0.0, ds)\n        dk1 += tl.dot(tl.trans(ds1), q1)\n        dk2 += tl.dot(tl.trans(ds2), q2)\n        q1_ptrs += BLOCK_M * stride_q1m\n        q2_ptrs += BLOCK_M * stride_q2m\n        do_ptrs += BLOCK_M * stride_dom\n    dk1 *= sm_scale\n    dk2 *= sm_scale\n    if DIVISIBLE_N:\n        tl.store(dk1_ptrs, dk1)\n        tl.store(dk2_ptrs, dk2)\n        tl.store(dv_ptrs, dv)\n    else:\n        tl.store(dk1_ptrs, dk1, mask=mask_n[:, None])\n        tl.store(dk2_ptrs, dk2, mask=mask_n[:, None])\n        tl.store(dv_ptrs, dv, mask=mask_n[:, None])\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_q_kernel(Q1, K1, Q2, K2, V, sm_scale, DO, DQ1, DQ2, L, D,\n    stride_q1z, stride_q1h, stride_q1m, stride_q1k, stride_k1z, stride_k1h,\n    stride_k1n, stride_k1k, stride_q2z, stride_q2h, stride_q2m, stride_q2k,\n    stride_k2z, stride_k2h, stride_k2n, stride_k2k, stride_vz, stride_vh,\n    stride_vn, stride_vk, stride_doz, stride_doh, stride_dom, stride_dok,\n    stride_dq1z, stride_dq1h, stride_dq1m, stride_dq1k, stride_dq2z,\n    stride_dq2h, stride_dq2m, stride_dq2k, Z, H, M, N, P_SEQ, w:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', CAUSAL: 'tl.constexpr', LARGER_M:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    input_dtype = Q1.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q1 += off_z * stride_q1z + off_h * stride_q1h\n    Q2 += off_z * stride_q2z + off_h * stride_q2h\n    K1 += off_z * stride_k1z + off_h * stride_k1h\n    K2 += off_z * stride_k2z + off_h * stride_k2h\n    V += off_z * stride_vz + off_h * stride_vh\n    DO += off_z * stride_doz + off_h * stride_doh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    DQ1 += off_z * stride_dq1z + off_h * stride_dq1h\n    DQ2 += off_z * stride_dq2z + off_h * stride_dq2h\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_n_init = offs_n_base\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q1_ptrs = Q1 + (offs_m[:, None] * stride_q1m + offs_k[None, :] * stride_q1k\n        )\n    q2_ptrs = Q2 + (offs_m[:, None] * stride_q2m + offs_k[None, :] * stride_q2k\n        )\n    k1_ptrs = K1 + (offs_n_init[:, None] * stride_k1n + offs_k[None, :] *\n        stride_k1k)\n    k2_ptrs = K2 + (offs_n_init[:, None] * stride_k2n + offs_k[None, :] *\n        stride_k2k)\n    v_ptrs = V + (offs_n_init[:, None] * stride_vn + offs_k[None, :] *\n        stride_vk)\n    dq1_ptrs = DQ1 + (offs_m[:, None] * stride_dq1m + offs_k[None, :] *\n        stride_dq1k)\n    dq2_ptrs = DQ2 + (offs_m[:, None] * stride_dq2m + offs_k[None, :] *\n        stride_dq2k)\n    do_ptrs = DO + (offs_m[:, None] * stride_dom + offs_k[None, :] * stride_dok\n        )\n    d_ptrs = D + offs_m\n    l_ptrs = L + offs_m\n    if DIVISIBLE_M:\n        q1 = tl.load(q1_ptrs)\n        q2 = tl.load(q2_ptrs)\n        do = tl.load(do_ptrs)\n        delta = tl.load(d_ptrs)\n        l = tl.load(l_ptrs)\n    else:\n        mask_m = offs_m < M\n        q1 = tl.load(q1_ptrs, mask=mask_m[:, None])\n        q2 = tl.load(q2_ptrs, mask=mask_m[:, None])\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n        delta = tl.load(d_ptrs, mask=mask_m)\n        l = tl.load(l_ptrs, mask=mask_m)\n    dq1 = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    dq2 = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if CAUSAL:\n        hi = tl.minimum(N, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = N\n    for start_n in range(0, hi, BLOCK_N):\n        offs_n = start_n + offs_n_base\n        if DIVISIBLE_N:\n            v = tl.load(v_ptrs)\n            k1 = tl.load(k1_ptrs)\n            k2 = tl.load(k2_ptrs)\n        else:\n            mask_n = offs_n < N\n            v = tl.load(v_ptrs, mask=mask_n[:, None])\n            k1 = tl.load(k1_ptrs, mask=mask_n[:, None])\n            k2 = tl.load(k2_ptrs, mask=mask_n[:, None])\n        piecewise_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :] + w\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.where(piecewise_mask, tl.dot(q2, tl.trans(k2)), tl.dot(q1,\n            tl.trans(k1)))\n        p = tl.math.exp2(s * qk_scale - l[:, None] * log2e)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_N:\n            ds = tl.where(mask_n, ds, 0.0)\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            ds = tl.where(causal_mask, ds, 0.0)\n        ds2 = tl.where(piecewise_mask, ds, 0.0)\n        ds1 = tl.where(piecewise_mask, 0.0, ds)\n        dq1 += tl.dot(ds1, k1)\n        dq2 += tl.dot(ds2, k2)\n        k1_ptrs += BLOCK_N * stride_k1n\n        k2_ptrs += BLOCK_N * stride_k2n\n        v_ptrs += BLOCK_N * stride_vn\n    dq1 *= sm_scale\n    dq2 *= sm_scale\n    if DIVISIBLE_M:\n        tl.store(dq1_ptrs, dq1)\n        tl.store(dq2_ptrs, dq2)\n    else:\n        tl.store(dq1_ptrs, dq1, mask=mask_m[:, None])\n        tl.store(dq2_ptrs, dq2, mask=mask_m[:, None])\n"
    },
    {
      "input": "@triton.heuristics({'num_warps': lambda args: get_num_warps(args[\n    'QUERY_GROUP_SIZE'], args['HEAD_SIZE'], args['KV_BLOCK_SIZE']),\n    'num_stages': lambda args: get_num_stages(args['QUERY_GROUP_SIZE'],\n    args['KV_BLOCK_SIZE'])})\n@triton.jit\ndef _paged_attn_kernel(m_i_ptr, l_i_ptr, out_ptr, q_ptr, k_cache_ptr,\n    v_cache_ptr, context_lens_ptr, block_tables_ptr, attn_scale, stride_bt0,\n    stride_bt1, stride_q0, stride_q1, stride_q2, stride_kv0, stride_kv1,\n    stride_kv2, stride_kv3, stride_o0, stride_o1, stride_o2, stride_o3,\n    stride_o4, HEAD_SIZE: 'tl.constexpr', QUERY_GROUP_SIZE: 'tl.constexpr',\n    PADDED_QUERY_GROUP_SIZE: 'tl.constexpr', NUM_KV_HEADS: 'tl.constexpr',\n    KV_BLOCK_SIZE: 'tl.constexpr', PARTITION_SIZE: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    part_idx = tl.program_id(2)\n    max_num_partitions = tl.num_programs(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    USE_PARTITIONING = PARTITION_SIZE > 0\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    if USE_PARTITIONING:\n        context_start_idx = part_idx * PARTITION_SIZE\n        if context_start_idx >= context_len:\n            return\n        context_end_idx = tl.minimum(context_start_idx + PARTITION_SIZE,\n            context_len)\n        num_blocks = tl.cdiv(context_end_idx - context_start_idx, KV_BLOCK_SIZE\n            )\n    else:\n        num_blocks = tl.cdiv(context_len, KV_BLOCK_SIZE)\n    block_offset = tl.arange(0, KV_BLOCK_SIZE)\n    head_offset = tl.arange(0, HEAD_SIZE)\n    padding_group_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)\n    kv_offset = kv_head_idx * stride_kv1 + block_offset[:, None\n        ] * stride_kv2 + head_offset[None, :] * stride_kv3\n    q_offset = seq_idx * stride_q0 + (kv_head_idx * QUERY_GROUP_SIZE +\n        padding_group_offset[:, None]) * stride_q1 + head_offset[None, :\n        ] * stride_q2\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n    q = tl.load(q_ptr + q_offset, mask=group_mask, other=0.0)\n    m_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32)\n    acc = tl.zeros([PADDED_QUERY_GROUP_SIZE, HEAD_SIZE], dtype=tl.float32)\n    num_prev_blocks = part_idx * (PARTITION_SIZE // KV_BLOCK_SIZE)\n    for i in range(num_blocks):\n        block_idx = num_prev_blocks + i\n        block_number = tl.load(block_tables_ptr + seq_idx * stride_bt0 + \n            block_idx * stride_bt1)\n        kv_block_offset = block_number * stride_kv0 + kv_offset\n        mask_offset = block_idx * KV_BLOCK_SIZE + block_offset\n        kv_mask = mask_offset[:, None] < context_len\n        k = tl.load(k_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            qk = tl.sum(q[:, None, :] * k[None, :, :], axis=2)\n        else:\n            qk = tl.dot(q, k.T, out_dtype=tl.float32)\n        qk *= attn_scale\n        qk = tl.where(mask_offset < context_len, qk, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n        p = tl.math.exp2((qk - m_i_new[:, None]) * log2e)\n        alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n        acc *= alpha[:, None]\n        v = tl.load(v_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            acc += tl.sum(p.T[:, :, None] * v[:, None, :], axis=0)\n        else:\n            p = p\n            acc += tl.dot(p, v, out_dtype=tl.float32)\n        l_i = l_i * alpha + tl.sum(p, axis=1)\n        m_i = m_i_new\n    acc = acc / l_i[:, None]\n    if USE_PARTITIONING:\n        part_offset = ((seq_idx * NUM_KV_HEADS + kv_head_idx) *\n            max_num_partitions * QUERY_GROUP_SIZE + part_idx *\n            QUERY_GROUP_SIZE + padding_group_offset)\n        mask = padding_group_offset < QUERY_GROUP_SIZE\n        tl.store(m_i_ptr + part_offset, m_i, mask=mask)\n        tl.store(l_i_ptr + part_offset, l_i, mask=mask)\n    out_offset = seq_idx * stride_o0\n    if USE_PARTITIONING:\n        out_offset += kv_head_idx * stride_o1\n    else:\n        out_offset += kv_head_idx * QUERY_GROUP_SIZE * stride_o1\n    out_offset += part_idx * stride_o2 + padding_group_offset[:, None\n        ] * stride_o3 + head_offset[None, :] * stride_o4\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n    tl.store(out_ptr + out_offset, acc, mask=group_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _paged_attn_v2_reduce_kernel(out_ptr, m_i_ptr, l_i_ptr, tmp_out_ptr,\n    context_lens_ptr, max_num_partitions, stride_o0, stride_o1, stride_o2,\n    HEAD_SIZE: 'tl.constexpr', QUERY_GROUP_SIZE: 'tl.constexpr',\n    NUM_KV_HEADS: 'tl.constexpr', PARTITION_SIZE: 'tl.constexpr',\n    NUM_PARTITIONS: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    num_partitions = tl.cdiv(context_len, PARTITION_SIZE)\n    group_head_offset = tl.arange(0, QUERY_GROUP_SIZE)[:, None\n        ] * HEAD_SIZE + tl.arange(0, HEAD_SIZE)[None, :]\n    if num_partitions == 1:\n        tmp_out_offset = ((seq_idx * NUM_KV_HEADS + kv_head_idx) *\n            max_num_partitions * QUERY_GROUP_SIZE * HEAD_SIZE +\n            group_head_offset)\n        tmp_out = tl.load(tmp_out_ptr + tmp_out_offset)\n        out_offset = (seq_idx * stride_o0 + kv_head_idx * QUERY_GROUP_SIZE *\n            stride_o1 + group_head_offset * stride_o2)\n        tl.store(out_ptr + out_offset, tmp_out)\n        return\n    ml_offset = (seq_idx * NUM_KV_HEADS + kv_head_idx\n        ) * max_num_partitions * QUERY_GROUP_SIZE + tl.arange(0, NUM_PARTITIONS\n        )[:, None] * QUERY_GROUP_SIZE + tl.arange(0, QUERY_GROUP_SIZE)[None, :]\n    mask = tl.arange(0, NUM_PARTITIONS)[:, None] < num_partitions\n    m_i = tl.load(m_i_ptr + ml_offset, mask=mask, other=float('-inf'))\n    m = tl.max(m_i, axis=0)\n    l_i = tl.load(l_i_ptr + ml_offset, mask=mask, other=0.0)\n    l_i *= tl.exp(m_i - m[None, :])\n    l = tl.sum(l_i, axis=0)\n    r = l_i / l[None, :]\n    r = tl.reshape(r, (NUM_PARTITIONS, QUERY_GROUP_SIZE, 1))\n    tmp_out_offset = (seq_idx * NUM_KV_HEADS + kv_head_idx\n        ) * max_num_partitions * QUERY_GROUP_SIZE * HEAD_SIZE + tl.arange(0,\n        NUM_PARTITIONS)[:, None, None\n        ] * QUERY_GROUP_SIZE * HEAD_SIZE + tl.arange(0, QUERY_GROUP_SIZE)[\n        None, :, None] * HEAD_SIZE + tl.arange(0, HEAD_SIZE)[None, None, :]\n    tmp_out = tl.load(tmp_out_ptr + tmp_out_offset, mask=mask[:, :, None],\n        other=0.0)\n    out = tl.sum(tmp_out * r, axis=0)\n    out_offset = (seq_idx * stride_o0 + kv_head_idx * QUERY_GROUP_SIZE *\n        stride_o1 + group_head_offset * stride_o2)\n    tl.store(out_ptr + out_offset, out)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_split_kv_kernel(Q, K, V, sm_scale, L, O, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk, stride_oz, stride_oh,\n    stride_os, stride_om, stride_ok, Z, H, M, N, P_SEQ, N_SPLIT_SIZE, S,\n    num_groups, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr', LARGER_M:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_m = tl.program_id(0)\n    n_split_id = tl.program_id(1)\n    off_zh = tl.program_id(2)\n    off_h = off_zh % H\n    off_z = off_zh // H\n    off_hk = off_h // num_groups\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_hk * stride_kh\n    V += off_z * stride_vz + off_hk * stride_vh\n    O += off_z * stride_oz + off_h * stride_oh + n_split_id * stride_os\n    L += ((off_z * H + off_h) * S + n_split_id) * M\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_m = start_m * BLOCK_M + offs_m_base\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    o_ptrs = O + (offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)\n    l_ptrs = L + offs_m\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if DIVISIBLE_M:\n        q = tl.load(q_ptrs)\n    else:\n        mask_m = offs_m < M\n        q = tl.load(q_ptrs, mask=mask_m[:, None])\n    if BLOCK_DMODEL < 128:\n        I = tl.where(offs_k[:, None] == offs_k, tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 1.0, dtype=input_dtype), tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 0.0, dtype=input_dtype))\n        q = tl.dot(q, I)\n    N_LEFT = n_split_id * N_SPLIT_SIZE\n    N_RIGHT = tl.minimum(N_LEFT + N_SPLIT_SIZE, N)\n    if IS_CAUSAL:\n        hi = tl.minimum(N_RIGHT, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(N_LEFT, hi)\n    else:\n        hi = N_RIGHT\n    offs_n_init = N_LEFT + offs_n_base\n    k_ptrs = K + (offs_k[:, None] * stride_vk + offs_n_init[None, :] *\n        stride_vn)\n    v_ptrs = V + (offs_n_init[:, None] * stride_kn + offs_k[None, :] *\n        stride_kk)\n    for start_n in range(N_LEFT, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        if DIVISIBLE_N:\n            k = tl.load(k_ptrs, cache_modifier='.cg')\n            v = tl.load(v_ptrs, cache_modifier='.cg')\n        else:\n            mask_n = offs_n < N\n            k = tl.load(k_ptrs, mask=mask_n[None, :], cache_modifier='.cg')\n            v = tl.load(v_ptrs, mask=mask_n[:, None], cache_modifier='.cg')\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, k)\n        if not DIVISIBLE_N:\n            s = tl.where(mask_n[None, :], s, float('-inf'))\n        if IS_CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            s = tl.where(causal_mask, s, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(s, 1))\n        alpha = tl.math.exp2((m_i - m_i_new) * qk_scale)\n        p = tl.math.exp2(s * qk_scale - m_i_new[:, None] * qk_scale)\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vn\n    if IS_CAUSAL and LARGER_M:\n        is_empty_line = offs_m + P_SEQ < 0\n        acc = tl.where(is_empty_line[:, None], 0.0, acc * (1.0 / l_i[:, None]))\n        l = tl.where(is_empty_line, float('-inf'), m_i * sm_scale + tl.log(l_i)\n            )\n    else:\n        acc = acc * (1.0 / l_i[:, None])\n        l = m_i * sm_scale + tl.log(l_i)\n    if DIVISIBLE_M:\n        tl.store(l_ptrs, l, cache_modifier='.cg')\n        tl.store(o_ptrs, acc, cache_modifier='.cg')\n    else:\n        tl.store(l_ptrs, l, mask=mask_m, cache_modifier='.cg')\n        tl.store(o_ptrs, acc, mask=mask_m[:, None], cache_modifier='.cg')\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_combine_kv_splits(multiple_o, multiple_l, final_o, final_l,\n    stride_mul_oz, stride_mul_oh, stride_mul_os, stride_mul_om,\n    stride_mul_ok, stride_fin_oz, stride_fin_oh, stride_fin_om,\n    stride_fin_ok, Z, H, M, S, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    offs_h = tl.program_id(1)\n    offs_z = tl.program_id(2)\n    multiple_o += offs_z * stride_mul_oz + offs_h * stride_mul_oh\n    multiple_l += (offs_z * H + offs_h) * S * M\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not DIVISIBLE_M:\n        mask_m = offs_m < M\n    m = tl.full([BLOCK_M], value=float('-inf'), dtype=tl.float32)\n    acc = tl.full([BLOCK_M], value=float(0.0), dtype=tl.float32)\n    l_ptrs = multiple_l + offs_m\n    for _ in range(0, S):\n        if DIVISIBLE_M:\n            l = tl.load(l_ptrs)\n        else:\n            l = tl.load(l_ptrs, mask=mask_m)\n        m_new = tl.maximum(m, l)\n        acc = acc * tl.exp(m - m_new) + tl.exp(l - m_new)\n        m = m_new\n        l_ptrs += M\n    l_acc = m + tl.log(acc)\n    o_acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    l_ptrs = multiple_l + offs_m\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    o_ptrs = multiple_o + offs_m[:, None] * stride_mul_om + offs_k[None, :\n        ] * stride_mul_ok\n    for _ in range(0, S):\n        l = tl.load(l_ptrs, mask=offs_m < M)\n        rescale = tl.exp(l - l_acc)\n        if DIVISIBLE_M:\n            o = tl.load(o_ptrs)\n        else:\n            o = tl.load(o_ptrs, mask=mask_m[:, None])\n        o_acc += o * rescale[:, None]\n        l_ptrs += M\n        o_ptrs += stride_mul_os\n    final_o += offs_z * stride_fin_oz + offs_h * stride_fin_oh\n    final_l += (offs_z * H + offs_h) * M\n    a_ptrs = final_o + offs_m[:, None] * stride_fin_om + offs_k * stride_fin_ok\n    b_ptrs = final_l + offs_m\n    if DIVISIBLE_M:\n        tl.store(a_ptrs, o_acc)\n        tl.store(b_ptrs, l_acc)\n    else:\n        tl.store(a_ptrs, o_acc, mask=mask_m[:, None])\n        tl.store(b_ptrs, l_acc, mask=mask_m)\n"
    },
    {
      "input": "@triton.jit\ndef recompute_mask_kernel(mask, B, H, M, N, dropout_p, seed, offset):\n    row, b, h = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    offs_base = b * H * M * N + h * M * N + row * N\n    BLOCK: 'tl.constexpr' = 1024\n    offs_base += tl.arange(0, BLOCK)\n    for start_n in range(0, N, BLOCK):\n        offs = start_n + offs_base\n        rng_offs = offset + offs\n        pmask = tl.rand(seed, rng_offs, n_rounds=6) > dropout_p\n        row_mask = start_n + tl.arange(0, BLOCK) < N\n        tl.store(mask + offs, pmask, mask=row_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _total_attention_kernel(Q, K, L, TA, sm_scale, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, Z, H,\n    M, N, P_SEQ, num_groups, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL: 'tl.constexpr',\n    DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    off_hk = off_h // num_groups\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_hk * stride_kh\n    L += (off_z * H + off_h) * M\n    TA += (off_z * H + off_h) * N\n    if CAUSAL:\n        lo = tl.maximum(start_n * BLOCK_N - P_SEQ, 0)\n        lo = lo // BLOCK_M * BLOCK_M\n    else:\n        lo = 0\n    offs_m_init = lo + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m_init[:, None] * stride_qm + offs_k[None, :] *\n        stride_qk)\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n    ta_ptrs = TA + offs_n\n    if DIVISIBLE_N:\n        k = tl.load(k_ptrs)\n    else:\n        mask_n = offs_n < N\n        k = tl.load(k_ptrs, mask=mask_n[:, None])\n    tot_attn = tl.zeros([BLOCK_N], dtype=tl.float32)\n    for start_m in range(lo, M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m = start_m + offs_m_base\n        causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n        if DIVISIBLE_M:\n            q = tl.load(q_ptrs)\n        else:\n            mask_m = offs_m < M\n            valid_mask = mask_m[:, None]\n            q = tl.load(q_ptrs, mask=mask_m[:, None])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k))\n        if DIVISIBLE_M:\n            l = tl.load(L + offs_m)\n        else:\n            l = tl.load(L + offs_m, mask=mask_m)\n        p = tl.math.exp2(s * qk_scale - l[:, None] * log2e)\n        if not DIVISIBLE_M:\n            p = tl.where(valid_mask, p, 0.0)\n        if CAUSAL:\n            p = tl.where(causal_mask, p, 0.0)\n        tot_attn += tl.sum(p, 0)\n        q_ptrs += BLOCK_M * stride_qm\n    if DIVISIBLE_N:\n        tl.store(ta_ptrs, tot_attn)\n    else:\n        tl.store(ta_ptrs, tot_attn, mask=mask_n)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, Rstd, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_bwd_dx_fused(DX, DY, DW, X, W, Rstd, Lock, stride, N, eps,\n    GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    rstd = tl.load(Rstd + row)\n    xhat = x * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    dx = (wdy - xhat * c1) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_bwd_dwdb(DW, FINAL_DW, M, N, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n"
    },
    {
      "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    rotary_dim, seqlen_ro, stride_out_batch, stride_out_seqlen,\n    stride_out_nheads, stride_out_headdim, stride_x_batch, stride_x_seqlen,\n    stride_x_nheads, stride_x_headdim, BLOCK_K: 'tl.constexpr',\n    IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr', IS_VARLEN: 'tl.constexpr',\n    INTERLEAVED: 'tl.constexpr', CONJUGATE: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n"
    },
    {
      "input": "@triton.jit\ndef _class_indices_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n    WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS:\n    'tl.constexpr', CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING:\n    'tl.constexpr', IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE:\n    'tl.constexpr', BLOCK: 'tl.constexpr'):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK)\n    logit_start_ptrs = LOGITS + row * N\n    logit_ptrs = logit_start_ptrs + cols\n    m_prev = -float('inf')\n    l_prev = 0.0\n    m_prev = m_prev\n    l_prev = l_prev\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=-float('inf'))\n        m_curr = tl.maximum(tl.max(row_logits, 0), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(row_logits - m_curr)\n        l_curr = tl.sum(p, 0) + l_prev\n        l_prev = l_curr\n        m_prev = m_curr\n        logit_ptrs += BLOCK\n    logit_ptrs = logit_start_ptrs + cols\n    output_ptrs = PROBS + row * N + cols\n    WRIT_PROBS = PROBS + row * N + cols\n    if LABEL_SMOOTHING:\n        sum_total = 0.0\n        sum_total = sum_total\n        weights_total = 0.0\n        weights_total = weights_total\n        if WEIGHTS:\n            weight_ptr = weight + cols\n    l_prev_log = tl.log(l_prev)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=l_prev_log + m_prev)\n        if LABEL_SMOOTHING and WEIGHTS:\n            full_weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            weights_total += tl.sum(full_weights_val, 0)\n        row_minus_max = row_logits - m_prev\n        log_softmax = l_prev_log - row_minus_max\n        if LABEL_SMOOTHING and WEIGHTS:\n            log_softmax *= full_weights_val\n        if LABEL_SMOOTHING:\n            sum_total += tl.sum(log_softmax, 0)\n        tl.store(WRIT_PROBS, log_softmax, mask=cols < N - start_n * BLOCK)\n        logit_ptrs += BLOCK\n        WRIT_PROBS += BLOCK\n        if LABEL_SMOOTHING and WEIGHTS:\n            weight_ptr += BLOCK\n    idx = tl.load(IDX + row)\n    use_class = 0.0\n    if IGNORE_INDEX >= 0:\n        use_class = idx == IGNORE_INDEX\n    READ_PROBS = PROBS + row * N + idx\n    tl.debug_barrier()\n    probs = tl.load(READ_PROBS)\n    if WEIGHTS and not LABEL_SMOOTHING:\n        weight_ptr = weight + idx\n        weights_val = tl.load(weight_ptr)\n        probs = weights_val * probs\n    if LABEL_SMOOTHING:\n        tl.store(WEIGHT_BUFFER + row, weights_total)\n        probs = (1 - smoothing_factor\n            ) * probs + smoothing_factor * sum_total / N\n    probs = probs * (1.0 - use_class)\n    tl.store(LOSS + row, probs)\n"
    },
    {
      "input": "@triton.jit\ndef _class_probs_forward(LOGITS, PROBS, IDX, LOSS, weight, N, WEIGHT_BUFFER,\n    smoothing_factor, log_size_logits, WEIGHTS: 'tl.constexpr',\n    CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING: 'tl.constexpr',\n    IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE: 'tl.constexpr', BLOCK:\n    'tl.constexpr'):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK)\n    logit_start_ptrs = LOGITS + row * N\n    logit_ptrs = logit_start_ptrs + cols\n    m_prev = -float('inf')\n    l_prev = 0.0\n    m_prev = m_prev\n    l_prev = l_prev\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=-float('inf'))\n        m_curr = tl.maximum(tl.max(row_logits, 0), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(row_logits - m_curr)\n        l_curr = tl.sum(p, 0) + l_prev\n        l_prev = l_curr\n        m_prev = m_curr\n        logit_ptrs += BLOCK\n    logit_ptrs = logit_start_ptrs + cols\n    output_ptrs = PROBS + row * N + cols\n    WRIT_PROBS = PROBS + row * N + cols\n    sum_total = 0.0\n    weights_total = 0.0\n    sum_total = sum_total\n    weights_total = weights_total\n    idx_ptr = IDX + row * N + cols\n    if WEIGHTS:\n        weight_ptr = weight + cols\n    l_prev_log = tl.log(l_prev)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=l_prev_log + m_prev)\n        idx = tl.load(idx_ptr, mask=cols < N - start_n * BLOCK, other=0.0)\n        full_weights_val = (1.0 - smoothing_factor\n            ) * idx + smoothing_factor / N\n        if WEIGHTS:\n            weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            full_weights_val = weights_val * full_weights_val\n        else:\n            full_weights_val = tl.where(cols < N - start_n * BLOCK,\n                full_weights_val, 0.0)\n        weights_total += tl.sum(full_weights_val, 0)\n        row_minus_max = row_logits - m_prev\n        log_softmax = l_prev_log - row_minus_max\n        log_softmax *= full_weights_val\n        sum_total += tl.sum(log_softmax, 0)\n        tl.store(WRIT_PROBS, log_softmax, mask=cols < N - start_n * BLOCK)\n        logit_ptrs += BLOCK\n        WRIT_PROBS += BLOCK\n        idx_ptr += BLOCK\n        if WEIGHTS:\n            weight_ptr += BLOCK\n    tl.store(WEIGHT_BUFFER + row, weights_total)\n    probs = sum_total\n    tl.store(LOSS + row, probs)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK': 1024}, num_stages=\n    FORWARD_NUM_STAGES, num_warps=1), triton.Config({'BLOCK': 2048},\n    num_stages=FORWARD_NUM_STAGES, num_warps=8), triton.Config({'BLOCK': \n    4096}, num_stages=FORWARD_NUM_STAGES, num_warps=8), triton.Config({\n    'BLOCK': 8192}, num_stages=FORWARD_NUM_STAGES, num_warps=16), triton.\n    Config({'BLOCK': 16384}, num_stages=FORWARD_NUM_STAGES, num_warps=16)],\n    key=['N', 'CLASS_INDICES', 'log_size_logits', 'BUFFER_DTYPE'])\n@triton.jit\ndef _forward(LOGITS, PROBS, IDX, LOSS, weight, N, WEIGHT_BUFFER,\n    smoothing_factor, log_size_logits, WEIGHTS: 'tl.constexpr',\n    CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING: 'tl.constexpr',\n    IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE: 'tl.constexpr', BLOCK:\n    'tl.constexpr'):\n    if CLASS_INDICES:\n        _class_indices_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n            WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS,\n            CLASS_INDICES, LABEL_SMOOTHING, IGNORE_INDEX, BUFFER_DTYPE, BLOCK)\n    else:\n        _class_probs_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n            WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS,\n            CLASS_INDICES, LABEL_SMOOTHING, IGNORE_INDEX, BUFFER_DTYPE, BLOCK)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK': 1024}, num_stages=1,\n    num_warps=1), triton.Config({'BLOCK': 2048}, num_stages=1, num_warps=8),\n    triton.Config({'BLOCK': 4096}, num_stages=1, num_warps=8), triton.\n    Config({'BLOCK': 8192}, num_stages=1, num_warps=16), triton.Config({\n    'BLOCK': 16384}, num_stages=1, num_warps=16)], key=['N',\n    'CLASS_INDICES', 'log_size_logits', 'BUFFER_DTYPE'])\n@triton.jit\ndef _backward(PROBS, IDX, DPROBS, dprob_stride, DIN, weight, N,\n    WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS:\n    'tl.constexpr', CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING:\n    'tl.constexpr', IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE:\n    'tl.constexpr', BLOCK: 'tl.constexpr'):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    start_n = tl.program_id(1)\n    cols = tl.arange(0, BLOCK)\n    PROBS = PROBS + row * N\n    probs_start = PROBS + cols + BLOCK * start_n\n    probs = -tl.load(probs_start, mask=cols < N - start_n * BLOCK, other=\n        float('inf'))\n    DIN = DIN + row * N + cols + BLOCK * start_n\n    dout = tl.load(DPROBS + row * dprob_stride)\n    if CLASS_INDICES:\n        idx = tl.load(IDX + row)\n        delta = start_n * BLOCK + cols == idx\n        if IGNORE_INDEX >= 0:\n            use_class = idx == IGNORE_INDEX\n            dout = dout * (1 - use_class)\n        if LABEL_SMOOTHING:\n            if WEIGHTS:\n                weight_ptr = weight + cols + BLOCK * start_n\n                full_weights_val = tl.load(weight_ptr, mask=cols < N - \n                    start_n * BLOCK, other=0.0)\n                weights_val = tl.load(weight + idx)\n                probs = probs / full_weights_val\n            probs = tl.exp(probs)\n            if WEIGHTS:\n                weights_total = tl.load(WEIGHT_BUFFER + row)\n                numerator_contrib = weights_val * (1.0 - smoothing_factor) * (\n                    probs - delta)\n                mean_contrib = (weights_total * probs - full_weights_val\n                    ) * smoothing_factor / N\n            else:\n                numerator_contrib = (1.0 - smoothing_factor) * (probs - delta)\n                mean_contrib = smoothing_factor * probs - smoothing_factor / N\n            din = (numerator_contrib + mean_contrib) * dout\n        else:\n            probs = tl.exp(probs)\n            din = (probs - delta) * dout\n            if WEIGHTS:\n                weight_ptr = weight + idx\n                weights_val = tl.load(weight_ptr)\n                din = weights_val * din\n    else:\n        idx = tl.load(IDX + row * N + cols + BLOCK * start_n, mask=cols < N -\n            start_n * BLOCK, other=0.0)\n        full_weights_val = (1.0 - smoothing_factor\n            ) * idx + smoothing_factor / N\n        weights_total = tl.load(WEIGHT_BUFFER + row)\n        if WEIGHTS:\n            weight_ptr = weight + cols + BLOCK * start_n\n            weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            full_weights_val = weights_val * full_weights_val\n        probs = probs / full_weights_val\n        probs = tl.exp(probs)\n        weighted_probs = probs * weights_total\n        weighted_probs_per_class = weighted_probs - full_weights_val\n        din = weighted_probs_per_class * dout\n    tl.store(DIN, din, mask=cols + BLOCK * start_n < N)\n"
    },
    {
      "input": "@triton.jit\ndef MSAFwdFused(v_si_ptr, b_ij_ptr, g_si_ptr, output_ptr, vw_ptr,\n    logsumexp_ptr, C_hidden, N_head, C_LEN_POW2: 'tl.constexpr',\n    RES_LEN_POW2: 'tl.constexpr', SEQ_LEN: 'tl.constexpr', RES_LEN:\n    'tl.constexpr', BLOCK_SIZE_ROW: 'tl.constexpr', BLOCK_SIZE_COL:\n    'tl.constexpr', BLOCK_SIZE_SEQ: 'tl.constexpr'):\n    pid_z = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_i = tl.program_id(2)\n    z_off = pid_z\n    h_off = pid_h\n    i_off = pid_i * BLOCK_SIZE_ROW\n    offs_i = i_off + tl.arange(0, BLOCK_SIZE_ROW)\n    offs_c = tl.arange(0, C_LEN_POW2)\n    log2_e = 1.44269504089\n    prev_row_max = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    new_row_max = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    l = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    for j in range(0, RES_LEN, BLOCK_SIZE_COL):\n        offs_j = j + tl.arange(0, BLOCK_SIZE_COL)\n        b_offs = z_off * RES_LEN * RES_LEN * N_head + offs_i[:, None\n            ] * RES_LEN * N_head + offs_j[None, :] * N_head + h_off\n        ij_mask = (offs_i < RES_LEN)[:, None] & (offs_j < RES_LEN)[None, :]\n        b = tl.load(b_ij_ptr + b_offs, ij_mask, -INF)\n        new_row_max = tl.maximum(tl.max(b, axis=1, keep_dims=True),\n            prev_row_max)\n        w = tl.exp2(log2_e * (b - new_row_max))\n        l *= tl.exp2(log2_e * (prev_row_max - new_row_max))\n        l += tl.sum(w, axis=1, keep_dims=True)\n        for s in range(0, SEQ_LEN, BLOCK_SIZE_SEQ):\n            for ch in range(0, C_hidden, 1):\n                offs_s = s + tl.arange(0, BLOCK_SIZE_SEQ)\n                si_off = (z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + \n                    offs_s[None, :] * RES_LEN * N_head * C_hidden + offs_i[\n                    :, None] * N_head * C_hidden + h_off * C_hidden + ch)\n                sj_off = (z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + \n                    offs_s[None, :] * RES_LEN * N_head * C_hidden + offs_j[\n                    :, None] * N_head * C_hidden + h_off * C_hidden + ch)\n                si_mask = (offs_s < SEQ_LEN)[None, :] & (offs_i < RES_LEN)[\n                    :, None]\n                sj_mask = (offs_s < SEQ_LEN)[None, :] & (offs_j < RES_LEN)[\n                    :, None]\n                v = tl.load(v_si_ptr + sj_off, sj_mask, 0)\n                vw = tl.load(output_ptr + si_off, si_mask, 0)\n                vw = vw * tl.exp2(log2_e * (prev_row_max - new_row_max))\n                vw = tl.dot(w, v, acc=vw)\n                tl.store(output_ptr + si_off, vw, si_mask)\n        prev_row_max = new_row_max\n    for s in range(0, SEQ_LEN, BLOCK_SIZE_SEQ):\n        for ch in range(0, C_hidden, 1):\n            offs_s = s + tl.arange(0, BLOCK_SIZE_SEQ)\n            si_off = z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + offs_s[\n                None, :] * RES_LEN * N_head * C_hidden + offs_i[:, None\n                ] * N_head * C_hidden + h_off * C_hidden + ch\n            si_mask = (offs_s < SEQ_LEN)[None, :] & (offs_i < RES_LEN)[:, None]\n            g = tl.load(g_si_ptr + si_off, si_mask, 0)\n            g = tl.sigmoid(g)\n            vw = tl.load(output_ptr + si_off, si_mask, 0)\n            vw = vw / l\n            out = g * vw\n            tl.store(output_ptr + si_off, out, si_mask)\n            tl.store(vw_ptr + si_off, vw, si_mask)\n    lse_off = z_off * RES_LEN * N_head + offs_i[:, None] * N_head + h_off\n    lse_mask = (offs_i < RES_LEN)[:, None]\n    tl.store(logsumexp_ptr + lse_off, new_row_max + tl.log(l), lse_mask)\n"
    },
    {
      "input": "@triton.jit\ndef MSABwdFused(b_ij_ptr, logsumexp_ptr, N_head, RES_LEN: 'tl.constexpr',\n    BLOCK_SIZE_ROW: 'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr'):\n    pid_zh = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_z = pid_zh // N_head\n    pid_h = pid_zh % N_head\n    log2_e = 1.44269504089\n    z_off = pid_z\n    h_off = pid_h\n    i_off = pid_i * BLOCK_SIZE_ROW\n    offs_i = i_off + tl.arange(0, BLOCK_SIZE_ROW)\n    lse_off = z_off * RES_LEN * N_head + offs_i[:, None] * N_head + h_off\n    lse_mask = (offs_i < RES_LEN)[:, None]\n    logsumexp = tl.load(logsumexp_ptr + lse_off, lse_mask, 0)\n    for j in range(0, RES_LEN, BLOCK_SIZE_COL):\n        offs_j = j + tl.arange(0, BLOCK_SIZE_COL)\n        b_offs = z_off * RES_LEN * RES_LEN * N_head + offs_i[:, None\n            ] * RES_LEN * N_head + offs_j[None, :] * N_head + h_off\n        ij_mask = (offs_i < RES_LEN)[:, None] & (offs_j < RES_LEN)[None, :]\n        b = tl.load(b_ij_ptr + b_offs, ij_mask, -INF)\n        b = tl.exp2(log2_e * (b - logsumexp))\n        tl.store(b_ij_ptr + b_offs, b, ij_mask)\n"
    },
    {
      "input": "@triton.autotune(DEFAULT_DEQUANT_CONFIGS, key=['numels'])\n@triton.jit\ndef dequant_kernel_248(g_idx_ptr, scales_ptr, qweight_ptr, qzeros_ptr,\n    out_ptr, numels, maxq: 'tl.constexpr', bits: 'tl.constexpr',\n    outfeatures: 'tl.constexpr', num_groups: 'tl.constexpr', X_BLOCK:\n    'tl.constexpr'):\n    xoffset = tl.program_id(0) * X_BLOCK\n    x_index = xoffset + tl.arange(0, X_BLOCK)\n    xmask = x_index < numels\n    row_idx = x_index // outfeatures\n    col_idx = x_index % outfeatures\n    elements_per_feature: 'tl.constexpr' = 32 // bits\n    g_idx = tl.load(g_idx_ptr + row_idx, None, eviction_policy='evict_last')\n    qweights = tl.load(qweight_ptr + (col_idx + outfeatures * (row_idx //\n        elements_per_feature)), None)\n    wf_weights = row_idx % elements_per_feature * bits\n    wf_zeros = col_idx % elements_per_feature * bits\n    tmp1 = g_idx + num_groups\n    tmp2 = g_idx < 0\n    tl.device_assert(g_idx >= 0, 'index out of bounds: 0 <= tmp0 < 0')\n    groups = tl.where(tmp2, tmp1, g_idx)\n    scales = tl.load(scales_ptr + (col_idx + outfeatures * groups), None)\n    weights = qweights >> wf_weights\n    weights = weights & maxq\n    qzero_ncols: 'tl.constexpr' = outfeatures // elements_per_feature\n    qzeros = tl.load(qzeros_ptr + (qzero_ncols * groups + col_idx //\n        elements_per_feature), None, eviction_policy='evict_last')\n    zeros = qzeros >> wf_zeros\n    zeros = zeros & maxq\n    weights = weights - zeros\n    weights = weights\n    weights = scales * weights\n    tl.store(out_ptr + x_index, weights, mask=xmask)\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_apply_penalty(Logits, presence_penalty, freqency_penalty,\n    repetition_penalty, p_token_ids, p_token_counts, p_cumsum_seq_len,\n    stride_logit_b, stride_logit_s, BLOCK_P: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_repetition = tl.load(repetition_penalty + cur_batch)\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset <\n        cur_batch_end_index, other=0.0)\n    rep_logits = tl.where(cur_logits > 0, cur_logits / cur_repetition, \n        cur_logits * cur_repetition)\n    freq_logits = rep_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset <\n        cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_copy_kv_index_to_req(req_to_token_indexs, b_req_idx,\n    b_split_seq_len, cumsum_split_seq_len, b_seq_len, memindex,\n    stride_req_to_token_b, stride_req_to_token_s, BLOCK_M: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    cur_req_idx = tl.load(b_req_idx + cur_index)\n    q_split_len = tl.load(b_split_seq_len + cur_index)\n    q_mem_end = tl.load(cumsum_split_seq_len + cur_index)\n    q_mem_start = q_mem_end - q_split_len\n    store_end = tl.load(b_seq_len + cur_index)\n    store_start = store_end - q_split_len\n    off_m = tl.arange(0, BLOCK_M)\n    for block_start in range(0, q_split_len, BLOCK_M):\n        read_index = tl.load(memindex + q_mem_start + block_start + off_m,\n            mask=q_mem_start + block_start + off_m < q_mem_end, other=0)\n        tl.store(req_to_token_indexs + cur_req_idx * stride_req_to_token_b +\n            (block_start + store_start + off_m), read_index, mask=\n            block_start + store_start + off_m < store_end)\n    return\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K',\n    'NO_GROUPS'])\n@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // infearure_per_bits * stride_zeros_n\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = zeros * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_asm, stride_bk, stride_bn, stride_bsn, stride_cm,\n    stride_cn, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', SPLIT_K:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    c = accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128,\n    'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, fpb_ptr, K, N, stride_bk,\n    stride_bn, stride_fpbk, stride_fpbn, BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, head_dim,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=(offs_h[:, None] < head_num) & (offs_d[None, :\n        ] < head_dim), other=0.0)\n    tl.store(o_ptrs, k, mask=(offs_h[:, None] < head_num) & (offs_d[None, :\n        ] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num, head_dim,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=(offs_h[:, None] <\n        head_num) & (offs_d[None, :] < head_dim), other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0)[:, None]\n    q_src_data = src_data / data_scale\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=(offs_h[:, None] < head_num) & (\n        offs_d[None, :] < head_dim))\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out,\n    Req_to_tokens, B_req_idx, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, stride_req_to_tokens_b, stride_req_to_tokens_s,\n    kv_group_num, b_prompt_cache_len, head_dim: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim), other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum((start_m + 1) * BLOCK_M + prompt_cache_len, \n        cur_batch_seq_len + prompt_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=start_n + offs_n < block_end_loc, other=0)\n        off_k = kv_loc[None, :\n            ] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None\n            ] * stride_kd\n        k = tl.load(K + off_k, mask=(start_n + offs_n[None, :] <\n            block_end_loc) & (offs_d[:, None] < head_dim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] + prompt_cache_len >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc_scale = tl.where(offs_m + prompt_cache_len >= start_n,\n            acc_scale, 1.0)\n        acc = acc * acc_scale[:, None]\n        off_v = kv_loc[:, None\n            ] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :\n            ] * stride_vd\n        v = tl.load(V + off_v, mask=(start_n + offs_n[:, None] <\n            block_end_loc) & (offs_d[None, :] < head_dim), other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8),\n    triton.Config({}, num_stages=2, num_warps=4), triton.Config({},\n    num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1\n    )], key=['K'])\n@triton.jit\ndef quantize_int8_perrow_kernel(fpa_ptr, a_ptr, as_ptr, M, K, stride_fpam,\n    stride_fpak, stride_am, stride_ak, stride_asm, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        inta = fpa / a_scale[:, None]\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, B, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, Att_Out,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, stride_qbs, stride_qh,\n    stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, kv_group_num, sliding_window, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_att_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value = att_value\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, sliding_window, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_att_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=start_n + offs_n <\n            cur_att_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n +\n            cur_batch_start_index < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] + cur_batch_start_index <\n            cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rotary_kernel(Q, K, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd, stride_cosbs, stride_cosd,\n    stride_sinbs, stride_sind, max_total_len, HEAD_Q, HEAD_K, rot_dim,\n    head_dim, BLOCK_HEAD: 'tl.constexpr', BLOCK_SEQ: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr'):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL)\n    dim_range1 = rot_dim + tl.arange(0, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range0[None, None, :] < rot_dim), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range1[None, None, :] < head_dim), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range0[None, None, :] < rot_dim))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range1[None, None, :] < head_dim))\n    off_k0 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range0[None, None, :] * stride_kd\n    off_k1 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range1[None, None, :] * stride_kd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    k0 = tl.load(K + off_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range0[None, None, :] < rot_dim), other=0.0)\n    k1 = tl.load(K + off_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range1[None, None, :] < head_dim), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out_k0 = k0 * cos - k1 * sin\n    out_k1 = k0 * sin + k1 * cos\n    tl.store(K + off_k0, out_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range0[None, None, :] < rot_dim))\n    tl.store(K + off_k1, out_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range1[None, None, :] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_kernel(X, W, Y, stride_x_N, stride_x_hn, stride_x_hd,\n    stride_y_N, stride_y_hn, stride_y_hd, stride_w_hn, stride_w_hd, N, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    Seq = tl.program_id(0)\n    H = tl.program_id(1)\n    X += Seq * stride_x_N + H * stride_x_hn\n    Y += Seq * stride_y_N + H * stride_y_hn\n    W += H * stride_w_hn\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage1(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    start_idx = pid * tokens_per_thread\n    off_c = (pid + 1) * num_experts\n    for i in range(tokens_per_thread):\n        if start_idx + i < numel:\n            idx = tl.load(topk_ids_ptr + start_idx + i)\n            token_cnt = tl.load(tokens_cnts_ptr + off_c + idx)\n            tl.store(tokens_cnts_ptr + off_c + idx, token_cnt + 1)\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage2(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    last_cnt = 0\n    for i in range(1, num_experts + 1):\n        token_cnt = tl.load(tokens_cnts_ptr + i * num_experts + pid)\n        last_cnt = last_cnt + token_cnt\n        tl.store(tokens_cnts_ptr + i * num_experts + pid, last_cnt)\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage3(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    last_cumsum = 0\n    off_cnt = num_experts * num_experts\n    for i in range(1, num_experts + 1):\n        token_cnt = tl.load(tokens_cnts_ptr + off_cnt + i - 1)\n        last_cumsum = last_cumsum + tl.cdiv(token_cnt, block_size) * block_size\n        tl.store(cumsum_ptr + i, last_cumsum)\n    tl.store(total_tokens_post_pad_ptr, last_cumsum)\n"
    },
    {
      "input": "@triton.jit\ndef moe_align_block_size_stage4(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    start_idx = tl.load(cumsum_ptr + pid)\n    end_idx = tl.load(cumsum_ptr + pid + 1)\n    for i in range(start_idx, end_idx, block_size):\n        tl.store(expert_ids_ptr + i // block_size, pid)\n    start_idx = pid * tokens_per_thread\n    off_t = pid * num_experts\n    for i in range(start_idx, tl.minimum(start_idx + tokens_per_thread, numel)\n        ):\n        expert_id = tl.load(topk_ids_ptr + i)\n        token_cnt = tl.load(tokens_cnts_ptr + off_t + expert_id)\n        rank_post_pad = token_cnt + tl.load(cumsum_ptr + expert_id)\n        tl.store(sorted_token_ids_ptr + rank_post_pad, i)\n        tl.store(tokens_cnts_ptr + off_t + expert_id, token_cnt + 1)\n"
    },
    {
      "input": "@triton.jit\ndef fused_moe_kernel(a_ptr, b_ptr, c_ptr, a_scale_ptr, b_scale_ptr,\n    topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr,\n    num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am,\n    stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr',\n    MUL_ROUTED_WEIGHT: 'tl.constexpr', top_k: 'tl.constexpr', compute_type:\n    'tl.constexpr', use_fp8: 'tl.constexpr'):\n    \"\"\"\n    Implements the fused computation for a Mixture of Experts (MOE) using\n    token and expert matrices.\n\n    Key Parameters:\n    - A: The input tensor representing tokens with shape (*, K), where '*' can\n        be any shape representing batches and K is the feature dimension of\n        each token.\n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is\n        the number of experts, K is the input feature dimension, and N is\n        the output feature dimension.\n    - C: The output cache tensor with shape (M, topk, N), where M is the\n        total number of tokens post padding, topk is the number of times\n        each token is repeated, and N is the output feature dimension.\n    - sorted_token_ids: A tensor containing the sorted indices of tokens,\n        repeated topk times and arranged by the expert index they are\n        assigned to.\n    - expert_ids: A tensor containing the indices of the expert for each\n        block. It determines which expert matrix from B should be used for\n        each block in A.\n    This kernel performs the multiplication of a token by its corresponding\n    expert matrix as determined by `expert_ids`. The sorting of\n    `sorted_token_ids` by expert index and padding ensures divisibility by\n    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix\n    multiplication across different blocks processed by the same expert.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am + offs_k[\n        None, :] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    if use_fp8:\n        a_scale = tl.load(a_scale_ptr)\n        b_scale = tl.load(b_scale_ptr + off_experts)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        if use_fp8:\n            accumulator = tl.dot(a, b, acc=accumulator)\n        else:\n            accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    if use_fp8:\n        accumulator = accumulator * a_scale * b_scale\n    else:\n        accumulator = accumulator\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_no_prompt_cache(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen,\n    Out, stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, head_dim, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim), other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=(start_n + offs_n[None, :] < cur_batch_seq_len\n            ) & (offs_d[:, None] < head_dim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=(start_n + offs_n[:, None] < cur_batch_seq_len\n            ) & (offs_d[None, :] < head_dim), other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage1(Q, K, V, sm_scale, Req_to_tokens,\n    B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_mid_ob,\n    stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb,\n    stride_mid_o_eh, stride_mid_o_es, gqa_group_size, head_dim, BLOCK_SEQ:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    seq_start_block = tl.program_id(2)\n    cur_kv_head = cur_head // gqa_group_size\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(cur_batch_seq_len, \n        cur_batch_start_index + BLOCK_SEQ)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = cur_batch_start_index + tl.arange(0, BLOCK_N)\n    q = tl.load(Q + off_q, mask=offs_d < head_dim, other=0.0)\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, block_n_size, 1):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :]\n        k = tl.load(K + off_k, mask=(offs_n_new[:, None] <\n            cur_batch_end_index) & (offs_d[None, :] < head_dim), other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new < cur_batch_end_index, att_value,\n            float('-inf'))\n        v = tl.load(V + off_k, mask=(offs_n_new[:, None] <\n            cur_batch_end_index) & (offs_d[None, :] < head_dim), other=0.0)\n        cur_max_logic = tl.max(att_value, axis=0)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic)\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale\n        acc += tl.sum(exp_logic[:, None] * v, axis=0)\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=0)\n        max_logic = new_max_logic\n    need_store = tl.where(block_n_size == 0, 0, 1)\n    for _ in range(0, need_store, 1):\n        off_mid_o = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +\n            seq_start_block * stride_mid_os + offs_d)\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + cur_head *\n            stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp, mask=offs_d < head_dim)\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, Out,\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs,\n    stride_oh, stride_od, head_dim, BLOCK_SEQ: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len +\n        BLOCK_SEQ - 1) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os, mask=\n            offs_d < head_dim, other=0.0)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(Out + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp, mask=offs_d < head_dim)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n"
    },
    {
      "input": "@triton.jit\ndef gelu(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1 + tanh(_kAlpha * (x + 0.044715 * x * x * x)))\n"
    },
    {
      "input": "@triton.jit\ndef _gelu_and_mul_kernel(input_ptr, stride_input_m, stride_input_n,\n    stride_output_m, stride_output_n, size_m, size_n, BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    tid = tl.program_id(0)\n    input_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    output_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    pid = tl.program_id(1)\n    input_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    up_offsets = input_m_offsets[:, None] * stride_input_m + (input_n_offsets\n        [None, :] + size_n) * stride_input_n\n    gate_offsets = input_m_offsets[:, None] * stride_input_m + input_n_offsets[\n        None, :] * stride_input_n\n    res_offsets = output_m_offsets[:, None\n        ] * stride_output_m + output_n_offsets[None, :] * stride_output_n\n    up = tl.load(input_ptr + up_offsets, mask=(input_n_offsets < size_n)[\n        None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = tl.load(input_ptr + gate_offsets, mask=(input_n_offsets < size_n\n        )[None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = gelu(gate)\n    gate = gate\n    tl.store(input_ptr + res_offsets, up * gate, mask=(output_n_offsets <\n        size_n)[None, :] * (output_m_offsets < size_m)[:, None])\n"
    },
    {
      "input": "@triton.jit\ndef embedding_kernel(weight, input_ids, out, vob_start_id, vob_end_id,\n    stride_weight_seq, stride_out_seq, n_ctx, hiden_size: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_NN:\n    'tl.constexpr'):\n    start_n = tl.program_id(0) * BLOCK_N\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=\n            vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq +\n            offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :],\n            vecs, mask=store_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_int4_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_g, stride_k_d, stride_o_bs,\n    stride_o_h, stride_o_g, stride_o_d, stride_os_bs, stride_os_h,\n    stride_os_g, group_size, BLOCK_GROUP_NUM: 'tl.constexpr',\n    BLOCK_GROUP_DIM: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_g = tl.arange(0, BLOCK_GROUP_NUM)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM // 2)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data_0 = tl.load(K + cur_index * stride_k_bs + cur_head *\n        stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :] * 2,\n        mask=offs_g[:, None] < group_size, other=0.0)\n    src_data_1 = tl.load(K + cur_index * stride_k_bs + cur_head *\n        stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :] * 2 + 1,\n        mask=offs_g[:, None] < group_size, other=0.0)\n    abs_data_0 = tl.abs(src_data_0)\n    abs_data_1 = tl.abs(src_data_1)\n    data_scale = tl.maximum(tl.max(abs_data_0, axis=1), tl.max(abs_data_1,\n        axis=1)) / 7.0\n    q_src_data_0 = src_data_0 / data_scale[:, None]\n    q_src_data_0 = tl.where(q_src_data_0 > 7, 7, q_src_data_0)\n    q_src_data_0 = tl.where(q_src_data_0 < -7, -7, q_src_data_0)\n    q_src_data_1 = src_data_1 / data_scale[:, None]\n    q_src_data_1 = tl.where(q_src_data_1 > 7, 7, q_src_data_1)\n    q_src_data_1 = tl.where(q_src_data_1 < -7, -7, q_src_data_1)\n    low_4 = (q_src_data_0 & 128) >> 4 | q_src_data_0 & 15\n    high_4 = ((q_src_data_1 & 128) >> 4 | q_src_data_1 & 15) << 4\n    out_data = low_4 | high_4\n    o_ptrs = Out + dest_index * stride_o_bs + cur_head * stride_o_h + offs_g[\n        :, None] * stride_o_g + offs_d[None, :]\n    os_ptrs = (Out_scale + dest_index * stride_os_bs + cur_head *\n        stride_os_h + offs_g)\n    tl.store(o_ptrs, out_data, mask=offs_g[:, None] < group_size)\n    tl.store(os_ptrs, data_scale, mask=offs_g < group_size)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _silu_and_mul_kernel(input_ptr, stride_input_m, stride_input_n,\n    stride_output_m, stride_output_n, size_m, size_n, BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    stride_input_m = stride_input_m\n    stride_output_m = stride_output_m\n    tid = tl.program_id(0)\n    input_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    output_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    pid = tl.program_id(1)\n    input_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    up_offsets = input_m_offsets[:, None] * stride_input_m + (input_n_offsets\n        [None, :] + size_n) * stride_input_n\n    gate_offsets = input_m_offsets[:, None] * stride_input_m + input_n_offsets[\n        None, :] * stride_input_n\n    res_offsets = output_m_offsets[:, None\n        ] * stride_output_m + output_n_offsets[None, :] * stride_output_n\n    up = tl.load(input_ptr + up_offsets, mask=(input_n_offsets < size_n)[\n        None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = tl.load(input_ptr + gate_offsets, mask=(input_n_offsets < size_n\n        )[None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = gate / (1 + tl.exp(-gate))\n    gate = gate\n    tl.store(input_ptr + res_offsets, up * gate, mask=(output_n_offsets <\n        size_n)[None, :] * (output_m_offsets < size_m)[:, None])\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_int8(Q, K, K_scale, V, V_scale, sm_scale, Req_to_tokens,\n    B_req_idx, B_split_start_loc, B_split_ready_cache_len, B_seqlen, Out,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_ksbs, stride_ksh, stride_ksd, stride_vbs, stride_vh, stride_vd,\n    stride_vsbs, stride_vsh, stride_vsd, stride_obs, stride_oh, stride_od,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, kv_group_num, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_q_split_start_loc = tl.load(B_split_start_loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_seqlen + cur_batch)\n    cur_batch_seq_start = tl.load(B_split_ready_cache_len + cur_batch)\n    cur_batch_q_split_seq_len = cur_batch_seq_len - cur_batch_seq_start\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :]\n    off_k = cur_kv_head * stride_kh + offs_d[:, None]\n    off_v = cur_kv_head * stride_vh + offs_d[None, :]\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_q_split_seq_len,\n        other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    ks_ptrs = K_scale + cur_kv_head * stride_ksh\n    vs_ptrs = V_scale + cur_kv_head * stride_vsh\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(start_m * BLOCK_M < cur_batch_q_split_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (cur_batch_seq_start + (start_m + \n        1) * BLOCK_M), BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_tokens_b + start_n + offs_n, mask=start_n +\n            offs_n < cur_batch_seq_len, other=0)\n        k = tl.load(k_ptrs + kv_loc[None, :] * stride_kbs, mask=start_n +\n            offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        k_scale = tl.load(ks_ptrs + kv_loc[None, :] * stride_ksbs, mask=\n            start_n + offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k_scale * k)\n        qk *= sm_scale\n        qk = tl.where(cur_batch_seq_start + offs_m[:, None] >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + kv_loc[:, None] * stride_vbs, mask=start_n +\n            offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        v_scale = tl.load(vs_ptrs + kv_loc[:, None] * stride_vsbs, mask=(\n            start_n + offs_n)[:, None] < cur_batch_seq_len, other=0.0)\n        p = p\n        acc += tl.dot(p, v * v_scale)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :]\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_q_split_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1_int8(Q, K, K_scale, sm_scale, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, Att_Out, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_ksbs, stride_ksh, stride_ksd, att_stride_h,\n    att_stride_bs, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        off_ks = k_loc[:, None] * stride_ksbs + cur_kv_head * stride_ksh\n        k_scale = tl.load(K_scale + off_ks, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k * k_scale, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2_int8v(Prob, V, V_scale, Out, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_ph, stride_pbs, stride_vbs, stride_vh,\n    stride_vd, stride_vsbs, stride_vsh, stride_vsd, stride_obs, stride_oh,\n    stride_od, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    vs_offs = cur_kv_head * stride_vsh\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        vs_value = tl.load(V_scale + vs_offs + v_loc[:, None] * stride_vsbs,\n            mask=start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value * vs_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_init_att_window_info(b_seq_len, b_att_seq_len, batch_size,\n    sliding_window, BLOCK_SIZE: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    cur_start = cur_index * BLOCK_SIZE\n    offsets = cur_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < batch_size\n    cur_seq_len = tl.load(b_seq_len + offsets, mask=mask)\n    b_att_seq_len_data = tl.minimum(cur_seq_len, sliding_window)\n    tl.store(b_att_seq_len + offsets, b_att_seq_len_data, mask=mask)\n    return\n"
    },
    {
      "input": "@autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'N'],\n    nearest_power_of_two=True)\n@triton.jit\ndef matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M,\n    N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm,\n    stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_k\n    scales_ptrs = scales_ptr + offs_bn[None, :]\n    zeros_ptrs = zeros_ptr + offs_bn[None, :] // infearure_per_bits\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        g_idx = tl.load(g_ptrs)\n        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)\n        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n        g_ptrs += BLOCK_SIZE_K\n    c = accumulator\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_K': 64,\n    'BLOCK_SIZE_N': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_N':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'K'],\n    nearest_power_of_two=True)\n@triton.jit\ndef trans_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr,\n    g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn,\n    stride_cm, stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, N) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, K) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_k = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_bk[:, None] // infearure_per_bits * stride_bk + \n        offs_n[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_bk\n    g_idx = tl.load(g_ptrs)\n    scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n    zeros_ptrs = zeros_ptr + offs_n[None, :] // infearure_per_bits + g_idx[\n        :, None] * stride_zeros\n    shifter = offs_bk % infearure_per_bits * bits\n    zeros_shifter = offs_n % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for k in range(0, num_pid_n):\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        b = tl.trans(b)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_N\n        b_ptrs += BLOCK_SIZE_N\n        scales_ptrs += BLOCK_SIZE_N\n        zeros_ptrs += BLOCK_SIZE_N // infearure_per_bits\n    c = accumulator\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, Out, b: 'tl.constexpr', h: 'tl.constexpr', n:\n    'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', BLOCK_MODEL: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_bh % h\n    off_e = tl.program_id(1)\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    e_offset = off_e * BLOCK_MODEL\n    Q_block_ptr = Q + qk_offset + tl.arange(0, d)[None, :]\n    K_trans_block_ptr = K + qk_offset + tl.arange(0, d)[:, None]\n    V_block_ptr = V + v_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n    O_block_ptr = Out + o_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :\n        ]\n    off_block = tl.arange(0, BLOCK)\n    index = off_block[:, None] - off_block[None, :]\n    kv = tl.zeros([d, BLOCK_MODEL], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        q = tl.load(Q_block_ptr + off_block[:, None] * d, mask=off_block[:,\n            None] < n, other=0.0)\n        k_trans = tl.load(K_trans_block_ptr + off_block[None, :] * d, mask=\n            off_block[None, :] < n, other=0.0)\n        v = tl.load(V_block_ptr + off_block[:, None] * e, mask=off_block[:,\n            None] < n, other=0.0)\n        qk = tl.dot(q, k_trans)\n        qk = tl.where(index >= 0, qk, 0)\n        o_intra = tl.dot(qk, v)\n        o_inter = tl.dot(q, kv)\n        o = o_intra + o_inter\n        tl.store(O_block_ptr + off_block[:, None] * e, o, mask=off_block[:,\n            None] < n)\n        kv += tl.dot(k_trans, v)\n        off_block += BLOCK\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_intra_kernel(Q, K, V, DO, DQ, DK, DV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK + tl.arange(0, BLOCK)\n    Q_trans_block_ptr = Q + qk_offset + block_offset[None, :] * d + tl.arange(\n        0, d)[:, None]\n    K_block_ptr = K + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[\n        None, :]\n    V_trans_block_ptr = V + v_offset + block_offset[None, :] * e + tl.arange(\n        0, e)[:, None]\n    DQ_block_ptr = DQ + qk_offset + block_offset[:, None] * d + tl.arange(0, d\n        )[None, :]\n    DK_trans_block_ptr = DK + qk_offset + block_offset[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + block_offset[:, None] * e + tl.arange(0, e)[\n        None, :]\n    DO_block_ptr = DO + o_offset + block_offset[:, None] * e + tl.arange(0, e)[\n        None, :]\n    array = tl.arange(0, BLOCK)\n    index = array[:, None] - array[None, :]\n    k = tl.load(K_block_ptr, mask=block_offset[:, None] < n, other=0.0)\n    v_trans = tl.load(V_trans_block_ptr, mask=block_offset[None, :] < n,\n        other=0.0)\n    do = tl.load(DO_block_ptr, mask=block_offset[:, None] < n, other=0.0)\n    q_trans = tl.load(Q_trans_block_ptr, mask=block_offset[None, :] < n,\n        other=0.0)\n    dqk = tl.dot(do, v_trans)\n    dqk = tl.where(index >= 0, dqk, 0)\n    dq_intra = tl.dot(dqk, k)\n    dk_intra_trans = tl.dot(q_trans, dqk)\n    qk_trans = tl.dot(k, q_trans)\n    qk_trans = tl.where(index <= 0, qk_trans, 0)\n    dv_intra = tl.dot(qk_trans, do)\n    dq = dq_intra\n    dk_trans = dk_intra_trans\n    dv = dv_intra\n    tl.store(DQ_block_ptr, dq, mask=block_offset[:, None] < n)\n    tl.store(DK_trans_block_ptr, dk_trans, mask=block_offset[None, :] < n)\n    tl.store(DV_block_ptr, dv, mask=block_offset[:, None] < n)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_inter_kernel(Q, K, V, DO, DQ, DK, DV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    DQ_block_ptr = DQ + qk_offset + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    K_block_ptr = K + qk_offset + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + tl.arange(0, CBLOCK)[None, :\n        ] * e + tl.arange(0, e)[:, None]\n    DO_block_ptr = DO + o_offset + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    off_block1 = tl.arange(0, CBLOCK)\n    off_block2 = tl.arange(0, CBLOCK)\n    kv_trans = tl.zeros([e, d], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        for j in range(NUM_CBLOCK):\n            if i > 0:\n                do = tl.load(DO_block_ptr, mask=off_block1[:, None] < n,\n                    other=0.0)\n                dq_inter = tl.dot(do, kv_trans)\n                dq = dq_inter + tl.load(DQ_block_ptr, mask=off_block1[:,\n                    None] < n, other=0.0)\n                tl.store(DQ_block_ptr, dq, mask=off_block1[:, None] < n)\n            DQ_block_ptr += CBLOCK * d\n            DO_block_ptr += CBLOCK * e\n            off_block1 += CBLOCK\n        kv_trans_current = tl.zeros([e, d], dtype=tl.float32)\n        for j in range(NUM_CBLOCK):\n            v_trans = tl.load(V_trans_block_ptr, mask=off_block2[None, :] <\n                n, other=0.0)\n            k = tl.load(K_block_ptr, mask=off_block2[:, None] < n, other=0.0)\n            kv_trans_current += tl.dot(v_trans, k)\n            K_block_ptr += CBLOCK * d\n            V_trans_block_ptr += CBLOCK * e\n            off_block2 += CBLOCK\n        kv_trans += kv_trans_current\n    m = NUM_BLOCK * BLOCK\n    off_block1 = m + tl.arange(0, CBLOCK)\n    off_block2 = m + tl.arange(0, CBLOCK)\n    Q_trans_block_ptr = Q + qk_offset + m * d + tl.arange(0, CBLOCK)[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    K_block_ptr = K + qk_offset + m * d + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + m * e + tl.arange(0, CBLOCK)[None, :\n        ] * e + tl.arange(0, e)[:, None]\n    DK_trans_block_ptr = DK + qk_offset + m * d + tl.arange(0, CBLOCK)[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + m * e + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + m * e + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    dkv = tl.zeros([d, e], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            K_block_ptr -= CBLOCK * d\n            V_trans_block_ptr -= CBLOCK * e\n            DK_trans_block_ptr -= CBLOCK * d\n            DV_block_ptr -= CBLOCK * e\n            off_block1 -= CBLOCK\n            if i < NUM_BLOCK - 1:\n                k = tl.load(K_block_ptr, mask=off_block1[:, None] < n,\n                    other=0.0)\n                v_trans = tl.load(V_trans_block_ptr, mask=off_block1[None,\n                    :] < n, other=0.0)\n                dk_inter_trans = tl.dot(dkv, v_trans)\n                dv_inter = tl.dot(k, dkv)\n                dk_trans = dk_inter_trans + tl.load(DK_trans_block_ptr,\n                    mask=off_block1[None, :] < n, other=0.0)\n                dv = dv_inter + tl.load(DV_block_ptr, mask=off_block1[:,\n                    None] < n, other=0.0)\n                tl.store(DK_trans_block_ptr, dk_trans, mask=off_block1[None,\n                    :] < n)\n                tl.store(DV_block_ptr, dv, mask=off_block1[:, None] < n)\n        dkv_current = tl.zeros([d, e], dtype=tl.float32)\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            DO_block_ptr -= CBLOCK * e\n            Q_trans_block_ptr -= CBLOCK * d\n            off_block2 -= CBLOCK\n            do = tl.load(DO_block_ptr, mask=off_block2[:, None] < n, other=0.0)\n            q_trans = tl.load(Q_trans_block_ptr, mask=off_block2[None, :] <\n                n, other=0.0)\n            dkv_current += tl.dot(q_trans, do)\n        dkv += dkv_current\n"
    },
    {
      "input": "@triton.jit\ndef srms_norm_fw(X, Y, V, stride, N, eps, BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    x_ptrs = X + row * stride + cols\n    x = tl.load(x_ptrs, mask=mask, other=0.0)\n    x_zm = tl.where(mask, x, 0.0)\n    x_var = tl.sum(x_zm * x_zm, axis=0) / N\n    rstd = 1.0 / tl.sqrt(x_var + eps)\n    y = x_zm * rstd\n    tl.store(V + row, rstd)\n    y_ptrs = Y + row * stride + cols\n    tl.store(y_ptrs, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef srms_norm_bwd_dx_fused(DX, DY, X, V, stride, N, BLOCK_SIZE_N:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    x_ptrs = X + row * stride + cols\n    dy_ptrs = DY + row * stride + cols\n    x = tl.load(x_ptrs, mask=mask, other=0)\n    dy = tl.load(dy_ptrs, mask=mask, other=0)\n    rstd = tl.load(V + row)\n    xhat = x * rstd\n    wdy = dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    mean1 = tl.sum(xhat * wdy, axis=0) / N\n    dx = (wdy - xhat * mean1) * rstd\n    mask = cols < N\n    dx_ptrs = DX + row * stride + cols\n    tl.store(dx_ptrs, dx, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_apply_penalty(Logits, presence_penalty, freqency_penalty,\n    p_token_ids, p_token_counts, p_cumsum_seq_len, stride_logit_b,\n    stride_logit_s, BLOCK_P: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset <\n        cur_batch_end_index, other=0.0)\n    freq_logits = cur_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset <\n        cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K',\n    'NO_GROUPS'])\n@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // infearure_per_bits * stride_zeros_n\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = zeros * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_asm, stride_bk, stride_bn, stride_bsn, stride_cm,\n    stride_cn, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', SPLIT_K:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    c = accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128,\n    'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, fpb_ptr, K, N, stride_bk,\n    stride_bn, stride_fpbk, stride_fpbn, BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=offs_h[:, None] < head_num, other=0.0)\n    tl.store(o_ptrs, k, mask=offs_h[:, None] < head_num)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=offs_h[:, None] <\n        head_num, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0)[:, None]\n    q_src_data = src_data / data_scale\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=offs_h[:, None] < head_num)\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8),\n    triton.Config({}, num_stages=2, num_warps=4), triton.Config({},\n    num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1\n    )], key=['K'])\n@triton.jit\ndef quantize_int8_perrow_kernel(fpa_ptr, a_ptr, as_ptr, M, K, stride_fpam,\n    stride_fpak, stride_am, stride_ak, stride_asm, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        inta = fpa / a_scale[:, None]\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rotary_kernel(Q, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_cosbs, stride_cosd, stride_sinbs, stride_sind, max_total_len, H,\n    BLOCK_HEAD: 'tl.constexpr', BLOCK_SEQ: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr'):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2)\n    dim_range1 = tl.arange(BLOCK_DMODEL // 2, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att1_int8(Q, K, K_scale, sm_scale, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_ksbs, stride_ksh, stride_ksd, att_stride_h, att_stride_bs,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        off_ks = k_loc[:, None] * stride_ksbs + cur_head * stride_ksh\n        k_scale = tl.load(K_scale + off_ks, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k * k_scale, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_att2_int8v(Prob, V, V_scale, Out, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph,\n    stride_pbs, stride_vbs, stride_vh, stride_vd, stride_vsbs, stride_vsh,\n    stride_vsd, stride_obs, stride_oh, stride_od, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    vs_offs = cur_head * stride_vsh\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        vs_value = tl.load(V_scale + vs_offs + v_loc[:, None] * stride_vsbs,\n            mask=start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value * vs_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Logics, V, Out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs, stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od, stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_v = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len\n        ) * stride_b_loc_s\n    v_ptrs = V + off_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) *\n            stride_b_loc_s, mask=start_n + offs_n < cur_batch_seq_len,\n            other=other_kv_index)\n        qk = tl.load(Logics + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, mask\n            =start_n + offs_n < cur_batch_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef var_len_copy_kernel_triton(old_a_start, old_a_len, old_a_location,\n    new_a_start, new_a_location, BLOCK_SIZE: 'tl.constexpr'):\n    a_id = tl.program_id(0)\n    length = tl.load(old_a_len + a_id)\n    old_start = tl.load(old_a_start + a_id)\n    new_start = tl.load(new_a_start + a_id)\n    old_offset = tl.arange(0, BLOCK_SIZE)\n    new_offset = tl.arange(0, BLOCK_SIZE)\n    for i in range(0, length, BLOCK_SIZE):\n        v = tl.load(old_a_location + old_start + i + old_offset, mask=\n            old_offset < length)\n        tl.store(new_a_location + new_start + i + new_offset, v, mask=\n            new_offset < length)\n"
    },
    {
      "input": "@triton.jit\ndef triton_batch_lora_B(output, x, w, a_start, a_len, a_loc, batch_req_bins,\n    a_scaling, qkvo_offset: 'tl.constexpr', NUM_TOKENS: 'tl.constexpr',\n    HIDDEN: 'tl.constexpr', MAX_LORA_RANK: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    return\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'UNUSED': 1}, num_stages=\n    num_stages, num_warps=num_warps) for num_stages in (1, 2, 3, 4, 5) for\n    num_warps in (1, 2, 4, 8)], key=['in_features', 'out_features',\n    'num_codebooks', 'codebook_size', 'out_group_size', 'in_group_size',\n    'num_input_groups', 'num_input_groups_next_power_of_2',\n    'compute_in_fp32', 'has_output_scale', 'has_bias'])\n@triton.jit\ndef _aqlm_gemv_simple(input_vec_ptr, output_vec_ptr, codes_ptr,\n    codebooks_ptr, scales_ptr, bias_ptr, in_features: 'tl.constexpr',\n    out_features: 'tl.constexpr', num_codebooks: 'tl.constexpr',\n    codebook_size: 'tl.constexpr', out_group_size: 'tl.constexpr',\n    in_group_size: 'tl.constexpr', num_input_groups: 'tl.constexpr',\n    num_input_groups_next_power_of_2: 'tl.constexpr', compute_in_fp32:\n    'tl.constexpr', has_output_scale: 'tl.constexpr', has_bias:\n    'tl.constexpr', UNUSED: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    input_vec = tl.load(input_vec_ptr + tl.arange(0,\n        num_input_groups_next_power_of_2)[:, None, None, None] *\n        in_group_size + tl.arange(0, in_group_size)[None, None, None, :],\n        mask=tl.arange(0, num_input_groups_next_power_of_2)[:, None, None,\n        None] < num_input_groups)\n    dtype = input_vec.dtype\n    codes_i_ptrs = (codes_ptr + pid * num_input_groups * num_codebooks + tl\n        .arange(0, num_input_groups_next_power_of_2)[:, None] *\n        num_codebooks + tl.arange(0, num_codebooks)[None, :])\n    codes_i_mask_1d = tl.arange(0, num_input_groups_next_power_of_2\n        ) < num_input_groups\n    codes_i = tl.load(codes_i_ptrs, mask=codes_i_mask_1d[:, None])\n    codes_i = codes_i\n    codes_i = codes_i + (codes_i < 0) * codebook_size\n    codes_i += tl.arange(0, num_codebooks)[None, :] * codebook_size\n    out_group_ix = tl.arange(0, out_group_size)[None, None, :, None]\n    in_group_ix = tl.arange(0, in_group_size)[None, None, None, :]\n    weight_i_ptrs = (codebooks_ptr + codes_i[:, :, None, None] *\n        out_group_size * in_group_size + out_group_ix * in_group_size +\n        in_group_ix)\n    weights_i = tl.load(weight_i_ptrs, mask=codes_i_mask_1d[:, None, None,\n        None], other=0)\n    if compute_in_fp32:\n        weights_i = weights_i\n        input_vec = input_vec\n    output_i = weights_i * input_vec\n    if out_group_size == 1:\n        output_i = tl.sum(output_i)\n    else:\n        output_i = tl.sum(output_i, axis=1)\n        output_i = tl.sum(output_i, axis=2)\n        output_i = tl.sum(output_i, axis=0)\n    if has_output_scale:\n        output_i *= tl.load(scales_ptr + pid)\n    if has_bias:\n        output_i += tl.load(bias_ptr + pid)\n    if out_group_size == 1:\n        tl.store(output_vec_ptr + pid, output_i)\n    else:\n        tl.store(output_vec_ptr + pid * out_group_size + tl.arange(0,\n            out_group_size), output_i)\n"
    },
    {
      "input": "@triton.jit\ndef dtw_kernel(cost, trace, x, x_stride, cost_stride, trace_stride, N, M,\n    BLOCK_SIZE: 'tl.constexpr'):\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < M\n    for k in range(1, N + M + 1):\n        tl.debug_barrier()\n        p0 = cost + (k - 1) * cost_stride\n        p1 = cost + k * cost_stride\n        p2 = cost + k * cost_stride + 1\n        c0 = tl.load(p0 + offsets, mask=mask)\n        c1 = tl.load(p1 + offsets, mask=mask)\n        c2 = tl.load(p2 + offsets, mask=mask)\n        x_row = tl.load(x + (k - 1) * x_stride + offsets, mask=mask, other=0)\n        cost_row = x_row + tl.minimum(tl.minimum(c0, c1), c2)\n        cost_ptr = cost + (k + 1) * cost_stride + 1\n        tl.store(cost_ptr + offsets, cost_row, mask=mask)\n        trace_ptr = trace + (k + 1) * trace_stride + 1\n        tl.store(trace_ptr + offsets, 2, mask=mask & (c2 <= c0) & (c2 <= c1))\n        tl.store(trace_ptr + offsets, 1, mask=mask & (c1 <= c0) & (c1 <= c2))\n        tl.store(trace_ptr + offsets, 0, mask=mask & (c0 <= c1) & (c0 <= c2))\n"
    },
    {
      "input": "@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, out_ptr, stride_x_batch, stride_x_m,\n    stride_x_k, stride_rms_w, stride_out_batch, stride_out_m, stride_out_k,\n    N_SIZE: 'tl.constexpr', eps: 'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr'):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n    offset_m = pid_batch * stride_x_batch + pid_m * stride_x_m\n    block_n_size = tl.arange(0, BLOCK_N_SIZE)\n    var = tl.zeros((BLOCK_N_SIZE,), tl.float32)\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=\n            x_ptr_mask, other=0.0)\n        xf = x\n        var += xf * xf\n    var = tl.sum(var, axis=0) / N_SIZE\n    std = tl.sqrt(var + eps)\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n        rms_w_offset = tl.load(rms_w_ptr + offset_n * stride_rms_w, mask=\n            x_ptr_mask)\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=\n            x_ptr_mask, other=0.0)\n        x_new = x / std\n        out = x_new * rms_w_offset\n        out_offset = (pid_batch * stride_out_batch + pid_m * stride_out_m +\n            offset_n * stride_out_k)\n        tl.store(out_ptr + out_offset, out, mask=x_ptr_mask)\n"
    },
    {
      "input": "@triton.jit\ndef awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size,\n    result_ptr, num_cols, num_rows, BLOCK_SIZE_X: 'tl.constexpr',\n    BLOCK_SIZE_Y: 'tl.constexpr'):\n    pid_x = tl.program_id(axis=0)\n    pid_y = tl.program_id(axis=1)\n    offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n    offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    offsets = num_cols * offsets_y[:, None] + offsets_x[None, :]\n    masks_y = offsets_y < num_rows\n    masks_x = offsets_x < num_cols\n    masks = masks_y[:, None] & masks_x[None, :]\n    result_offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n    result_offsets_x = pid_x * BLOCK_SIZE_X * 8 + tl.arange(0, BLOCK_SIZE_X * 8\n        )\n    result_offsets = 8 * num_cols * result_offsets_y[:, None\n        ] + result_offsets_x[None, :]\n    result_masks_y = result_offsets_y < num_rows\n    result_masks_x = result_offsets_x < num_cols * 8\n    result_masks = result_masks_y[:, None] & result_masks_x[None, :]\n    iweights = tl.load(qweight_ptr + offsets, masks)\n    iweights = tl.interleave(iweights, iweights)\n    iweights = tl.interleave(iweights, iweights)\n    iweights = tl.interleave(iweights, iweights)\n    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] + tl.arange(\n        0, 4)[:, None]).reshape(8)\n    shifts = reverse_awq_order_tensor * 4\n    shifts = tl.broadcast_to(shifts[None, :], (BLOCK_SIZE_Y * BLOCK_SIZE_X, 8))\n    shifts = tl.reshape(shifts, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    iweights = iweights >> shifts & 15\n    zero_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)\n    zero_offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    zero_offsets = num_cols * zero_offsets_y[:, None] + zero_offsets_x[None, :]\n    zero_masks_y = zero_offsets_y < num_rows // group_size\n    zero_masks_x = zero_offsets_x < num_cols\n    zero_masks = zero_masks_y[:, None] & zero_masks_x[None, :]\n    zeros = tl.load(zeros_ptr + zero_offsets, zero_masks)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    zeros = zeros >> shifts & 15\n    scale_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)\n    scale_offsets_x = pid_x * BLOCK_SIZE_X * 8 + tl.arange(0, BLOCK_SIZE_X * 8)\n    scale_offsets = num_cols * 8 * scale_offsets_y[:, None] + scale_offsets_x[\n        None, :]\n    scale_masks_y = scale_offsets_y < num_rows // group_size\n    scale_masks_x = scale_offsets_x < num_cols * 8\n    scale_masks = scale_masks_y[:, None] & scale_masks_x[None, :]\n    scales = tl.load(scales_ptr + scale_offsets, scale_masks)\n    scales = tl.broadcast_to(scales, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    iweights = (iweights - zeros) * scales\n    iweights = iweights\n    tl.store(result_ptr + result_offsets, iweights, result_masks)\n"
    },
    {
      "input": "@triton.jit\ndef awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K,\n    group_size, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_z = tl.program_id(1)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    accumulator_dtype = c_ptr.type.element_ty\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=\n        accumulator_dtype)\n    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] + tl.arange(\n        0, 4)[:, None]).reshape(8)\n    shifts = reverse_awq_order_tensor * 4\n    shifts = tl.broadcast_to(shifts[None, :], (BLOCK_SIZE_K * (BLOCK_SIZE_N //\n        8), 8))\n    shifts = tl.reshape(shifts, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n    offsets_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    masks_am = offsets_am < M\n    offsets_bn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)\n    masks_bn = offsets_bn < N // 8\n    offsets_zn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)\n    masks_zn = offsets_zn < N // 8\n    offsets_sn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    masks_sn = offsets_sn < N\n    offsets_k = pid_z * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offsets_a = K * offsets_am[:, None] + offsets_k[None, :]\n    offsets_b = N // 8 * offsets_k[:, None] + offsets_bn[None, :]\n    a_ptrs = a_ptr + offsets_a\n    b_ptrs = b_ptr + offsets_b\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        masks_k = offsets_k < K\n        masks_a = masks_am[:, None] & masks_k[None, :]\n        a = tl.load(a_ptrs, mask=masks_a)\n        masks_b = masks_k[:, None] & masks_bn[None, :]\n        b = tl.load(b_ptrs, mask=masks_b)\n        b = tl.interleave(b, b)\n        b = tl.interleave(b, b)\n        b = tl.interleave(b, b)\n        offsets_szk = (BLOCK_SIZE_K * SPLIT_K * k + pid_z * BLOCK_SIZE_K\n            ) // group_size + tl.arange(0, 1)\n        offsets_z = N // 8 * offsets_szk[:, None] + offsets_zn[None, :]\n        masks_zk = offsets_szk < K // group_size\n        masks_z = masks_zk[:, None] & masks_zn[None, :]\n        zeros_ptrs = zeros_ptr + offsets_z\n        zeros = tl.load(zeros_ptrs, mask=masks_z)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n        offsets_s = N * offsets_szk[:, None] + offsets_sn[None, :]\n        masks_sk = offsets_szk < K // group_size\n        masks_s = masks_sk[:, None] & masks_sn[None, :]\n        scales_ptrs = scales_ptr + offsets_s\n        scales = tl.load(scales_ptrs, mask=masks_s)\n        scales = tl.broadcast_to(scales, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n        b = b >> shifts & 15\n        zeros = zeros >> shifts & 15\n        b = (b - zeros) * scales\n        b = b\n        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n        offsets_k += BLOCK_SIZE_K * SPLIT_K\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * (N // 8)\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + N * offs_cm[:, None] + offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef identity(x):\n    return x\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x.to(tl.float32))\n"
    },
    {
      "input": "@triton.jit\ndef relu(x):\n    return tl.max(x, 0.0)\n"
    },
    {
      "input": "@triton.jit\ndef gelu(x):\n    return 0.5 * x * (1.0 + tl.tanh(0.7978845608028654 * (x + 0.044715 * x *\n        x * x)))\n"
    },
    {
      "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x_hwc(x, w, bias, y, stride_xn, stride_xc, stride_xh,\n    stride_xw, stride_wn, stride_wc, stride_wh, stride_ww, stride_yn,\n    stride_yc, stride_yh, stride_yw, delta_xh_ptr, delta_xw_ptr,\n    delta_xc_ptr, BATCH, IN_C, IN_H, IN_W, KERNEL_N, KERNEL_H, KERNEL_W,\n    OUT_H, OUT_W, stride_h, stride_w, padding_h, padding_w, dilation_h,\n    dilation_w, output_padding_h, output_padding_w, groups, ACC_TYPE:\n    'tl.constexpr', CONV1X1_NHWC: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_H:\n    'tl.constexpr', WITH_BIAS: 'tl.constexpr'):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_xh_ptrs = delta_xh_ptr + off_x_crs\n        delta_xw_ptrs = delta_xw_ptr + off_x_crs\n        delta_xc_ptrs = delta_xc_ptr + off_x_crs\n        delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n        off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw * stride_xw +\n            delta_xc * stride_xc)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n        delta_xh = 0\n        delta_xw = 0\n    mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n        off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n        delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None, :] >= 0\n        ) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w, out_dtype=ACC_TYPE)\n        w_ptrs += BLOCK_K\n        off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n        if not CONV1X1_NHWC:\n            delta_xh_ptrs += BLOCK_K\n            delta_xw_ptrs += BLOCK_K\n            delta_xc_ptrs += BLOCK_K\n            delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n            off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw *\n                stride_xw + delta_xc * stride_xc)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            x_ptrs += BLOCK_K\n        mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n            off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n            delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None,\n            :] >= 0) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    if WITH_BIAS:\n        acc += tl.load(bias + off_y_k)[None, :]\n    acc = acc\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n    return\n"
    },
    {
      "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x(x, w, bias, y, stride_xn, stride_xc, stride_xh,\n    stride_xw, stride_wn, stride_wc, stride_wh, stride_ww, stride_yn,\n    stride_yc, stride_yh, stride_yw, delta_x_ptr, BATCH, IN_C, IN_H, IN_W,\n    KERNEL_N, KERNEL_H, KERNEL_W, OUT_H, OUT_W, stride_h, stride_w,\n    padding_h, padding_w, dilation_h, dilation_w, output_padding_h,\n    output_padding_w, groups, ACC_TYPE: 'tl.constexpr', CONV1X1_NHWC:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', GROUP_H: 'tl.constexpr', WITH_BIAS: 'tl.constexpr'\n    ):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_x_ptrs = delta_x_ptr + off_x_crs\n        off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n    mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n        off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[None, :]\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w, out_dtype=ACC_TYPE)\n        w_ptrs += BLOCK_K\n        if not CONV1X1_NHWC:\n            delta_x_ptrs += BLOCK_K\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS,\n                other=0)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            x_ptrs += BLOCK_K\n        mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n            off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[\n            None, :]\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    if WITH_BIAS:\n        acc += tl.load(bias + off_y_k)[None, :]\n    acc = acc\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n    return\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(4096, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_2d_kernel(output_ptr, input_ptr, bs, size_inp_0, batch_stride_inp,\n    stride_inp_0, batch_stride_out, stride_out_0, BATCH_STRIDE_INP_IS_1:\n    'tl.constexpr', STRIDE_INP_0_IS_1: 'tl.constexpr',\n    BATCH_STRIDE_OUT_IS_1: 'tl.constexpr', STRIDE_OUT_0_IS_1:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    pid_m = pid\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + rm * (1 if STRIDE_INP_0_IS_1 else stride_inp_0)\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + rm * (1 if STRIDE_OUT_0_IS_1 else stride_out_0)\n    mask = rm < size_inp_0\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(64, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BLOCK_N': lambda kwargs: min(64, triton.next_power_of_2(kwargs['size_inp_1'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'STRIDE_INP_1_IS_1': lambda kwargs: kwargs['stride_inp_1'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n    'STRIDE_OUT_1_IS_1': lambda kwargs: kwargs['stride_out_1'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] * kwargs['BLOCK_N'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_3d_kernel(output_ptr, input_ptr, bs, size_inp_0, size_inp_1,\n    batch_stride_inp, stride_inp_0, stride_inp_1, batch_stride_out,\n    stride_out_0, stride_out_1, BATCH_STRIDE_INP_IS_1: 'tl.constexpr',\n    STRIDE_INP_0_IS_1: 'tl.constexpr', STRIDE_INP_1_IS_1: 'tl.constexpr',\n    BATCH_STRIDE_OUT_IS_1: 'tl.constexpr', STRIDE_OUT_0_IS_1:\n    'tl.constexpr', STRIDE_OUT_1_IS_1: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    grid_n = tl.cdiv(size_inp_1, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid - pid_m * grid_n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + (rm[:, None] * (1 if STRIDE_INP_0_IS_1 else\n        stride_inp_0) + rn[None, :] * (1 if STRIDE_INP_1_IS_1 else\n        stride_inp_1))\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + (rm[:, None] * (1 if STRIDE_OUT_0_IS_1 else\n        stride_out_0) + rn[None, :] * (1 if STRIDE_OUT_1_IS_1 else\n        stride_out_1))\n    mask = (rm < size_inp_0)[:, None] & (rn < size_inp_1)[None, :]\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BLOCK_N': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_1'])),\n    'BLOCK_K': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_2'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'STRIDE_INP_1_IS_1': lambda kwargs: kwargs['stride_inp_1'] == 1,\n    'STRIDE_INP_2_IS_1': lambda kwargs: kwargs['stride_inp_2'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n    'STRIDE_OUT_1_IS_1': lambda kwargs: kwargs['stride_out_1'] == 1,\n    'STRIDE_OUT_2_IS_1': lambda kwargs: kwargs['stride_out_2'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] * kwargs['BLOCK_N'] * kwargs['BLOCK_K'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_4d_kernel(output_ptr, input_ptr, bs, size_inp_0, size_inp_1,\n    size_inp_2, batch_stride_inp, stride_inp_0, stride_inp_1, stride_inp_2,\n    batch_stride_out, stride_out_0, stride_out_1, stride_out_2,\n    BATCH_STRIDE_INP_IS_1: 'tl.constexpr', STRIDE_INP_0_IS_1:\n    'tl.constexpr', STRIDE_INP_1_IS_1: 'tl.constexpr', STRIDE_INP_2_IS_1:\n    'tl.constexpr', BATCH_STRIDE_OUT_IS_1: 'tl.constexpr',\n    STRIDE_OUT_0_IS_1: 'tl.constexpr', STRIDE_OUT_1_IS_1: 'tl.constexpr',\n    STRIDE_OUT_2_IS_1: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    grid_n = tl.cdiv(size_inp_1, BLOCK_N)\n    grid_k = tl.cdiv(size_inp_2, BLOCK_K)\n    pid_m = pid // (grid_n * grid_k)\n    pid_nk = pid - pid_m * (grid_n * grid_k)\n    pid_n = pid_nk // grid_k\n    pid_k = pid_nk - pid_n * grid_k\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = pid_k * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + (rm[:, None, None] * (1 if STRIDE_INP_0_IS_1 else\n        stride_inp_0) + rn[None, :, None] * (1 if STRIDE_INP_1_IS_1 else\n        stride_inp_1) + rk[None, None, :] * (1 if STRIDE_INP_2_IS_1 else\n        stride_inp_2))\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + (rm[:, None, None] * (1 if STRIDE_OUT_0_IS_1 else\n        stride_out_0) + rn[None, :, None] * (1 if STRIDE_OUT_1_IS_1 else\n        stride_out_1) + rk[None, None, :] * (1 if STRIDE_OUT_2_IS_1 else\n        stride_out_2))\n    mask = (rm < size_inp_0)[:, None, None] & (rn < size_inp_1)[None, :, None\n        ] & (rk < size_inp_2)[None, None, :]\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n    delta = mean_2 - mean_1\n    new_weight = weight_1 + weight_2\n    w2_over_w = tl.where(new_weight == 0.0, 0.0, weight_2 / new_weight)\n    return (mean_1 + delta * w2_over_w, m2_1 + m2_2 + delta * delta *\n        weight_1 * w2_over_w, new_weight)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'ROW_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['C'] // kwargs['groups']),\n    'BLOCK_SIZE':\n    lambda kwargs: max(\n        1, min(triton.next_power_of_2(kwargs['HxW']),\n               4096 // (triton.next_power_of_2(kwargs['C'] // kwargs['groups']))\n               )),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['ROW_SIZE'] * kwargs['BLOCK_SIZE'] // 128)),\n    'C_G': lambda kwargs: kwargs['C'] // kwargs['groups'],\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel(input_ptr, N,\n    C, HxW, groups, eps, mean_ptr, rstd_ptr, C_G, ROW_SIZE: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    group = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    offset = pid_batch * C * HxW + group * C_G\n    X = input_ptr + offset\n    _mean = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _m2 = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _weight = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    row = tl.arange(0, ROW_SIZE)\n    for off in range(0, HxW, BLOCK_SIZE):\n        r = off + tl.arange(0, BLOCK_SIZE)\n        m2_ = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n        mask = (r < HxW)[:, None] & (row[None, :] < C_G)\n        weight_ = mask\n        x = tl.load(X + (r * C)[:, None] + row[None, :], mask=mask)\n        _mean, _m2, _weight = welford_combine(_mean, _m2, _weight, x, m2_,\n            weight_)\n    _mean = tl.view(_mean, (BLOCK_SIZE * ROW_SIZE,))\n    _m2 = tl.view(_m2, (BLOCK_SIZE * ROW_SIZE,))\n    _weight = tl.view(_weight, (BLOCK_SIZE * ROW_SIZE,))\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    var = m2 / weight\n    rstd = 1.0 / tl.sqrt(var + eps)\n    offset = pid_batch * groups + group\n    tl.store(mean_ptr + offset, mean)\n    tl.store(rstd_ptr + offset, rstd)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'ROW_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['C'] // kwargs['groups']),\n    'BLOCK_SIZE':\n    lambda kwargs: max(\n        1, min(triton.next_power_of_2(kwargs['cluster_size']),\n               4096 // (triton.next_power_of_2(kwargs['C'] // kwargs['groups']))\n               )),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['ROW_SIZE'] * kwargs['BLOCK_SIZE'] // 128)),\n    'C_G': lambda kwargs: kwargs['C'] // kwargs['groups'],\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel_stage_1(input_ptr,\n    N, C, HxW, groups, cluster_size, cluster_num, cluster_mean_ptr,\n    cluster_m2_ptr, cluster_weight_ptr, C_G, ROW_SIZE: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    group = tl.program_id(0)\n    cluster = tl.program_id(1)\n    pid_batch = tl.program_id(2)\n    offset = pid_batch * C * HxW + group * C_G\n    X = input_ptr + offset\n    _mean = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _m2 = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _weight = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    row = tl.arange(0, ROW_SIZE)\n    start = cluster * cluster_size\n    end = start + cluster_size\n    end = min(end, HxW)\n    for off in range(start, end, BLOCK_SIZE):\n        r = off + tl.arange(0, BLOCK_SIZE)\n        m2_ = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n        mask = (r < end)[:, None] & (row[None, :] < C_G)\n        weight_ = mask\n        x = tl.load(X + (r * C)[:, None] + row[None, :], mask=mask)\n        _mean, _m2, _weight = welford_combine(_mean, _m2, _weight, x, m2_,\n            weight_)\n    _mean = tl.view(_mean, (BLOCK_SIZE * ROW_SIZE,))\n    _m2 = tl.view(_m2, (BLOCK_SIZE * ROW_SIZE,))\n    _weight = tl.view(_weight, (BLOCK_SIZE * ROW_SIZE,))\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    offset = pid_batch * groups * cluster_num + group * cluster_num + cluster\n    tl.store(cluster_mean_ptr + offset, mean)\n    tl.store(cluster_m2_ptr + offset, m2)\n    tl.store(cluster_weight_ptr + offset, weight)\n"
    },
    {
      "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['cluster_num']),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['BLOCK_SIZE'] // 128)),\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel_stage_2(\n    cluster_mean_ptr, cluster_m2_ptr, cluster_weight_ptr, N, groups,\n    cluster_num, eps, mean_ptr, rstd_ptr, BLOCK_SIZE: 'tl.constexpr'):\n    group = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    block = tl.arange(0, BLOCK_SIZE)\n    mask = block < cluster_num\n    offset = pid_batch * groups * cluster_num + group * cluster_num + block\n    cluster_mean = tl.load(cluster_mean_ptr + offset, mask=mask)\n    cluster_m2 = tl.load(cluster_m2_ptr + offset, mask=mask)\n    cluster_weight = tl.load(cluster_weight_ptr + offset, mask=mask)\n    mean, m2, weight = tl.reduce((cluster_mean, cluster_m2, cluster_weight),\n        0, welford_combine)\n    var = m2 / weight\n    rstd = 1.0 / tl.sqrt(var + eps)\n    offset = pid_batch * groups + group\n    tl.store(mean_ptr + offset, mean)\n    tl.store(rstd_ptr + offset, rstd)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, B, Mean, Rstd, stride: 'tl.constexpr', N:\n    'tl.constexpr', eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    if BLOCK_SIZE >= N:\n        cols = tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N)\n        m2_ = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        weight_ = cols < N\n        _mean, _m2, _weight = x, m2_, weight_\n    else:\n        _mean = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        _m2 = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        _weight = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        for off in range(0, N, BLOCK_SIZE):\n            cols = off + tl.arange(0, BLOCK_SIZE)\n            x = tl.load(X + cols, mask=cols < N)\n            m2_ = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n            weight_ = cols < N\n            if off == 0:\n                _mean, _m2, _weight = x, m2_, weight_\n            else:\n                _mean, _m2, _weight = welford_combine(_mean, _m2, _weight,\n                    x, m2_, weight_)\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    var = m2 / weight\n    rstd = 1 / tl.sqrt(var + eps)\n    mean = mean\n    rstd = rstd\n    if Mean is not None:\n        tl.store(Mean + row, mean)\n    if Rstd is not None:\n        tl.store(Rstd + row, rstd)\n    if BLOCK_SIZE >= N:\n        cols = tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        if W is None:\n            w = tl.full((BLOCK_SIZE,), 1.0, dtype=x.dtype)\n        else:\n            w = tl.load(W + cols, mask=mask)\n        if B is None:\n            b = tl.zeros((BLOCK_SIZE,), dtype=x.dtype)\n        else:\n            b = tl.load(B + cols, mask=mask)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n    else:\n        for off in range(0, N, BLOCK_SIZE):\n            cols = off + tl.arange(0, BLOCK_SIZE)\n            mask = cols < N\n            if W is None:\n                w = tl.full((BLOCK_SIZE,), 1.0, dtype=x.dtype)\n            else:\n                w = tl.load(W + cols, mask=mask)\n            if B is None:\n                b = tl.zeros((BLOCK_SIZE,), dtype=x.dtype)\n            else:\n                b = tl.load(B + cols, mask=mask)\n            x = tl.load(X + cols, mask=mask)\n            x_hat = (x - mean) * rstd\n            y = x_hat * w + b\n            tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, Mean, Rstd, Lock,\n    stride: 'tl.constexpr', N: 'tl.constexpr', eps, GROUP_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    c2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    partial_db = dy\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_s(q, k, s, rk, ck, pk, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, scale, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BM: 'tl.constexpr', DK: 'tl.constexpr', DM:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_m, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_s = tl.make_block_ptr(s + (i_k * n_bh + i_bh) * s_sk_h, (T, DM), (\n        s_sk_t, s_sk_m), (0, i_m * BM), (BT, BM), (1, 0))\n    p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_ck = tl.make_block_ptr(ck + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    p_pk = tl.make_block_ptr(pk + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hk = tl.zeros([BK, BM], dtype=tl.float32)\n    for _ in range(NT):\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_rk = tl.load(p_rk, boundary_check=(0,))\n        b_ck = tl.load(p_ck, boundary_check=(0, 1))\n        b_pk = tl.load(p_pk, boundary_check=(0, 1))\n        b_inter = tl.dot(b_q, b_hk, allow_tf32=False) * b_rk[None, :]\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_q, b_k, allow_tf32=False), \n            0), b_ck, allow_tf32=False)\n        b_s = (b_inter + b_intra) * b_pk\n        b_hk = b_hk * b_rk[None, :] + tl.dot(b_k, b_ck, allow_tf32=False)\n        tl.store(p_s, b_s, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_s = tl.advance(p_s, (BT, 0))\n        p_rk = tl.advance(p_rk, (DM,))\n        p_ck = tl.advance(p_ck, (BT, 0))\n        p_pk = tl.advance(p_pk, (BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_o(p, v, o, rv, cv, pv, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BM: 'tl.constexpr', BV:\n    'tl.constexpr', DM: 'tl.constexpr', DV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_v, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_p = tl.make_block_ptr(p + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_qk_h, (T, DV), (s_qk_t, s_qk_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_m * n_bh + i_bh) * s_qk_h, (T, DV), (\n        s_qk_t, s_qk_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_cv = tl.make_block_ptr(cv + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t),\n        (i_m * BM, 0), (BM, BT), (0, 1))\n    p_pv = tl.make_block_ptr(pv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hv = tl.zeros([BM, BV], dtype=tl.float32)\n    for _ in range(NT):\n        b_p = tl.load(p_p, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_rv = tl.load(p_rv, boundary_check=(0,))\n        b_cv = tl.load(p_cv, boundary_check=(0, 1))\n        b_pv = tl.load(p_pv, boundary_check=(0, 1))\n        b_p = b_p * b_pv\n        b_inter = tl.dot(b_p * b_rv[None, :], b_hv, allow_tf32=False)\n        b_intra = tl.where(m_s, tl.dot(b_p, b_cv, allow_tf32=False), 0)\n        b_intra = tl.dot(b_intra, b_v, allow_tf32=False)\n        b_o = b_inter + b_intra\n        b_hv = b_hv * b_rv[:, None] + tl.dot(b_cv, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_p = tl.advance(p_p, (BT, 0))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_rv = tl.advance(p_rv, (DM,))\n        p_cv = tl.advance(p_cv, (0, BT))\n        p_pv = tl.advance(p_pv, (BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dp(v, rv, cv, pv, do, dp, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BV: 'tl.constexpr', BM:\n    'tl.constexpr', DV: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_m, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_v = tl.make_block_ptr(v + i_bh * s_qk_h, (DV, T), (s_qk_d, s_qk_t), (\n        i_v * BV, 0), (BV, BT), (0, 1))\n    p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_cv = tl.make_block_ptr(cv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    p_pv = tl.make_block_ptr(pv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * s_qk_h, (T, DV), (s_qk_t, s_qk_d),\n        (0, i_v * BV), (BT, BV), (1, 0))\n    p_dp = tl.make_block_ptr(dp + (i_v * n_bh + i_bh) * s_sk_h, (T, DM), (\n        s_sk_t, s_sk_m), (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hv = tl.zeros([BV, BM], dtype=tl.float32)\n    for _ in range(NT):\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_rv = tl.load(p_rv, boundary_check=(0,))\n        b_cv = tl.load(p_cv, boundary_check=(0, 1))\n        b_pv = tl.load(p_pv, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_inter = tl.dot(b_do, b_hv, allow_tf32=False) * b_rv[None, :]\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_do, b_v, allow_tf32=False),\n            0), b_cv, allow_tf32=False)\n        b_dp = (b_inter + b_intra) * b_pv\n        b_hv = b_hv * b_rv[None, :] + tl.dot(b_v, b_cv, allow_tf32=False)\n        tl.store(p_dp, b_dp, boundary_check=(0, 1))\n        p_v = tl.advance(p_v, (0, BT))\n        p_rv = tl.advance(p_rv, (DM,))\n        p_cv = tl.advance(p_cv, (BT, 0))\n        p_pv = tl.advance(p_pv, (BT, 0))\n        p_do = tl.advance(p_do, (BT, 0))\n        p_dp = tl.advance(p_dp, (BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dq(k, rk, ck, dq, ds, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BK: 'tl.constexpr', BM:\n    'tl.constexpr', DK: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_ck = tl.make_block_ptr(ck + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t),\n        (i_m * BM, 0), (BM, BT), (0, 1))\n    p_dq = tl.make_block_ptr(dq + (i_m * n_bh + i_bh) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (0, i_k * BK), (BT, BK), (1, 0))\n    p_ds = tl.make_block_ptr(ds + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hk = tl.zeros([BM, BK], dtype=tl.float32)\n    for _ in range(NT):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_rk = tl.load(p_rk, boundary_check=(0,))\n        b_ck = tl.load(p_ck, boundary_check=(0, 1))\n        b_ds = tl.load(p_ds, boundary_check=(0, 1))\n        b_inter = tl.dot(b_ds * b_rk[None, :], b_hk, allow_tf32=False)\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_ds, b_ck, allow_tf32=False),\n            0), b_k, allow_tf32=False)\n        b_dq = b_inter + b_intra\n        b_hk = b_hk * b_rk[:, None] + tl.dot(b_ck, b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        p_k = tl.advance(p_k, (BT, 0))\n        p_rk = tl.advance(p_rk, (DM,))\n        p_ck = tl.advance(p_ck, (0, BT))\n        p_dq = tl.advance(p_dq, (BT, 0))\n        p_ds = tl.advance(p_ds, (BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dk(q, k, rk, ck, ds, dk, dsk, s_qk_h, s_qk_t,\n    s_qk_d, s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BM: 'tl.constexpr', DK: 'tl.constexpr', DM:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        (NT - 1) * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, (NT - 1) * BT), (BK, BT), (0, 1))\n    p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_ck = tl.make_block_ptr(ck + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_ds = tl.make_block_ptr(ds + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t),\n        (i_m * BM, (NT - 1) * BT), (BM, BT), (0, 1))\n    p_dk = tl.make_block_ptr(dk + (i_m * n_bh + i_bh) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), ((NT - 1) * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dsk = tl.make_block_ptr(dsk + (i_k * n_bh + i_bh) * s_sk_h, (T, DM),\n        (s_sk_t, s_sk_m), ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s, m_t = o_i[:, None] <= o_i[None, :], o_i[:, None] >= o_i[None, :]\n    b_dhk = tl.zeros([BM, BK], dtype=tl.float32)\n    for i in range(NT):\n        p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (\n            s_sk_m,), ((NT - i) % NT * DM + i_m * BM,), (BM,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_rk = tl.load(p_rk, boundary_check=(0,))\n        b_ck = tl.load(p_ck, boundary_check=(0, 1))\n        b_ds = tl.load(p_ds, boundary_check=(0, 1))\n        b_inter = tl.dot(b_ck * b_rk[None, :], b_dhk, allow_tf32=False)\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_ck, b_ds, allow_tf32=False),\n            0.0), b_q, allow_tf32=False)\n        b_dk = b_inter + b_intra\n        b_inter = tl.dot(b_dhk, b_k, allow_tf32=False) * b_rk[:, None]\n        b_intra = tl.dot(b_ds, tl.where(m_t, tl.dot(b_q, b_k, allow_tf32=\n            False), 0.0), allow_tf32=False)\n        b_dsk = b_ck * tl.trans(b_inter + b_intra)\n        b_dhk = b_dhk * b_rk[:, None] + tl.dot(b_ds, b_q, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dsk, b_dsk, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (-BT, 0))\n        p_k = tl.advance(p_k, (0, -BT))\n        p_ck = tl.advance(p_ck, (-BT, 0))\n        p_ds = tl.advance(p_ds, (0, -BT))\n        p_dk = tl.advance(p_dk, (-BT, 0))\n        p_dsk = tl.advance(p_dsk, (-BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dv(do, v, rv, cv, p, dv, dsv, s_qk_h, s_qk_t,\n    s_qk_d, s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BV:\n    'tl.constexpr', BM: 'tl.constexpr', DV: 'tl.constexpr', DM:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_v, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_do = tl.make_block_ptr(do + i_bh * s_qk_h, (T, DV), (s_qk_t, s_qk_d),\n        ((NT - 1) * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_qk_h, (DV, T), (s_qk_d, s_qk_t), (\n        i_v * BV, (NT - 1) * BT), (BV, BT), (0, 1))\n    p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_cv = tl.make_block_ptr(cv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_p = tl.make_block_ptr(p + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t), (\n        i_m * BM, (NT - 1) * BT), (BM, BT), (0, 1))\n    p_dv = tl.make_block_ptr(dv + (i_m * n_bh + i_bh) * s_qk_h, (T, DV), (\n        s_qk_t, s_qk_d), ((NT - 1) * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dsv = tl.make_block_ptr(dsv + (i_v * n_bh + i_bh) * s_sk_h, (T, DM),\n        (s_sk_t, s_sk_m), ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s, m_t = o_i[:, None] <= o_i[None, :], o_i[:, None] >= o_i[None, :]\n    b_dhv = tl.zeros([BM, BV], dtype=tl.float32)\n    for i in range(NT):\n        p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (\n            s_sk_m,), ((NT - i) % NT * DM + i_m * BM,), (BM,), (0,))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_rv = tl.load(p_rv, boundary_check=(0,))\n        b_cv = tl.load(p_cv, boundary_check=(0, 1))\n        b_p = tl.load(p_p, boundary_check=(0, 1))\n        b_inter = tl.dot(b_cv * b_rv[None, :], b_dhv, allow_tf32=False)\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_cv, b_p, allow_tf32=False),\n            0.0), b_do, allow_tf32=False)\n        b_dv = b_inter + b_intra\n        b_inter = tl.dot(b_dhv, b_v, allow_tf32=False) * b_rv[:, None]\n        b_intra = tl.dot(b_p, tl.where(m_t, tl.dot(b_do, b_v, allow_tf32=\n            False), 0.0), allow_tf32=False)\n        b_dsv = b_cv * tl.trans(b_inter + b_intra)\n        b_dhv = b_dhv * b_rv[:, None] + tl.dot(b_p, b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        tl.store(p_dsv, b_dsv, boundary_check=(0, 1))\n        p_do = tl.advance(p_do, (-BT, 0))\n        p_v = tl.advance(p_v, (0, -BT))\n        p_cv = tl.advance(p_cv, (-BT, 0))\n        p_p = tl.advance(p_p, (0, -BT))\n        p_dv = tl.advance(p_dv, (-BT, 0))\n        p_dsv = tl.advance(p_dsv, (-BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_cum(s, r, c, p, s_sk_h, s_sk_t, s_sk_m, T, BT:\n    'tl.constexpr', BM: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    p_r = tl.make_block_ptr(r + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_c = tl.make_block_ptr(c + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    p_p = tl.make_block_ptr(p + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    b_mp = tl.zeros([BM], dtype=tl.float32)\n    b_zp = tl.zeros([BM], dtype=tl.float32)\n    for i in range(NT):\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_m = tl.max(b_s, 0)\n        if i == 0:\n            b_r = tl.exp(-b_m)\n        else:\n            b_m = tl.maximum(b_mp, b_m)\n            b_r = tl.exp(b_mp - b_m)\n        b_c = tl.exp(b_s - b_m[None, :])\n        b_z = tl.cumsum(b_c, 0) + (b_zp * b_r)[None, :]\n        b_p = tl.exp(-tl.log(b_z))\n        b_mp = b_m\n        b_zp = tl.max(b_z, 0)\n        tl.store(p_r, b_r, boundary_check=(0,))\n        tl.store(p_c, b_c, boundary_check=(0, 1))\n        tl.store(p_p, b_p, boundary_check=(0, 1))\n        p_s = tl.advance(p_s, (BT, 0))\n        p_r = tl.advance(p_r, (DM,))\n        p_c = tl.advance(p_c, (BT, 0))\n        p_p = tl.advance(p_p, (BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum(s, r, c, o, s_sk_h, s_sk_t, s_sk_m, T, BT:\n    'tl.constexpr', BM: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        (NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_c = tl.make_block_ptr(c + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        (NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        (NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_t = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BM], dtype=tl.float32)\n    for i in range(NT):\n        p_r = tl.make_block_ptr(r + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m\n            ,), ((NT - i) % NT * DM + i_m * BM,), (BM,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_r = tl.load(p_r, boundary_check=(0,))\n        b_c = tl.load(p_c, boundary_check=(0, 1))\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_z = b_z * b_r\n        b_o -= b_c * (b_z[None, :] + tl.dot(m_t, b_s, allow_tf32=False))\n        b_z += tl.sum(b_s, 0)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_s = tl.advance(p_s, (-BT, 0))\n        p_c = tl.advance(p_c, (-BT, 0))\n        p_o = tl.advance(p_o, (-BT, 0))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'\n    ):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_0o = tl.zeros([BV], dtype=tl.float32)\n    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_0o = 0\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT])\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1)) * scale\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_z = tl.zeros([BT], dtype=tl.float32)\n        b_o += b_h_0o\n        b_z += k_0o\n        b_o += tl.dot(b_q, b_h_1o, allow_tf32=False)\n        b_z += tl.sum(b_q * k_1o, axis=1)\n        b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK])\n        b_o += tl.dot(b_q_2o, b_h_2o, allow_tf32=False) * 0.5\n        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n        k_1o += tl.sum(b_k, axis=1)[None, :]\n        k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n        k_0o += BT\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        tl.store(p_z, b_z, mask=i * BT + tl.arange(0, BT) < T)\n        b_h_2o = b_h_2o + tl.dot(b_k_2o, b_v, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_z += BT\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_1o = tl.zeros([BV, BK], dtype=tl.float32)\n    b_h_2o = tl.zeros([BV, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i * BT\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i * BT < T)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h_1o, allow_tf32=False)\n        if i_v == 0:\n            b_dq += b_dz[:, None] * k_1o\n        b_dq_2o = tl.dot(b_do, b_h_2o, allow_tf32=False) * 0.5\n        if i_v == 0:\n            b_dq_2o += b_dz[:, None] * k_2o * 0.5\n        b_dq_2o = tl.reshape(b_dq_2o, [BT, BK, BK])\n        b_dq += tl.sum(b_dq_2o * b_q[:, :, None], axis=1)\n        b_dq += tl.sum(b_dq_2o * b_q[:, None, :], axis=2)\n        b_dq *= scale\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_h_2o = b_h_2o + tl.dot(b_v, b_k_2o, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_v, b_k, allow_tf32=False)\n        if i_v == 0:\n            k_1o += tl.sum(b_k, axis=0)[None, :]\n            k_2o += tl.sum(b_k_2o, axis=0)[None, :]\n    tl.debug_barrier()\n    b_h_1o = None\n    b_h_2o = None\n    b_dh_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_dh_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    b_dh_0o = tl.zeros([BV], dtype=tl.float32)\n    m_s = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    dq_1o = tl.zeros([1, BK], dtype=tl.float32)\n    dq_2o = tl.zeros([BK * BK, 1], dtype=tl.float32)\n    for i in range(tl.cdiv(T, BT) * BT - BT, -BT, -BT):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i < T)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds *= 1 + b_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s2, b_do, allow_tf32=False)\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_dv += tl.dot(b_k, b_dh_1o, allow_tf32=False)\n        b_dv += tl.dot(b_k_2o, b_dh_2o, allow_tf32=False)\n        b_dv += b_dh_0o\n        b_dk += tl.dot(b_v, tl.trans(b_dh_1o), allow_tf32=False)\n        if i_v == 0:\n            b_dk += dq_1o\n        b_dk_2o = tl.dot(b_dh_2o, tl.trans(b_v), allow_tf32=False)\n        if i_v == 0:\n            b_dk_2o += dq_2o\n        b_dk_2o = tl.reshape(b_dk_2o, [BK, BK, BT])\n        b_k_fp32 = tl.trans(b_k)\n        b_dk2 = tl.sum(b_dk_2o * b_k_fp32[:, None, :], axis=0)\n        b_dk2 += tl.sum(b_dk_2o * b_k_fp32[None, :, :], axis=1)\n        b_dk += tl.trans(b_dk2)\n        b_dh_0o += tl.sum(b_do, axis=0)\n        b_dh_1o = b_dh_1o + tl.dot(b_q, b_do, allow_tf32=False)\n        b_q_2o = b_q[None, :, :] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BK * BK, BT])\n        b_dh_2o = b_dh_2o + tl.dot(b_q_2o, b_do, allow_tf32=False) * 0.5\n        if i_v == 0:\n            dq_1o += tl.sum(b_dz[None, :] * b_q, axis=1)[None, :]\n            dq_2o += (tl.sum(b_dz[None, :] * b_q_2o, axis=1) * 0.5)[:, None]\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds + b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_recurrence(S, p2, O, NUM_BLOCK, D_MODEL_K: 'tl.constexpr',\n    D_MODEL_V: 'tl.constexpr', BLOCK_MODEL: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)\n    S = (S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None, :]\n        )\n    O = (O + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None,\n        :] + D_MODEL_K * D_MODEL_V)\n    p2 = p2 + offset_bh * NUM_BLOCK * D_MODEL_V + tl.arange(0, BLOCK_MODEL\n        ) + offset_s * BLOCK_MODEL + D_MODEL_V\n    acc = tl.zeros([BLOCK_MODEL, BLOCK_MODEL], dtype=tl.float32)\n    acc += tl.load(S)\n    S += D_MODEL_K * D_MODEL_V\n    tl.store(O, acc)\n    O += D_MODEL_K * D_MODEL_V\n    for i in range(NUM_BLOCK - 2):\n        p_v = tl.load(p2)\n        S_i = tl.load(S)\n        acc = acc * p_v[None, :] + S_i\n        tl.store(O, acc)\n        p2 += D_MODEL_V\n        S += D_MODEL_K * D_MODEL_V\n        O += D_MODEL_K * D_MODEL_V\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_recurrence(S, p2, DS, Dp2, NUM_BLOCK, NUM_SPLIT_K, NUM_SPLIT_V,\n    D_MODEL_K: 'tl.constexpr', D_MODEL_V: 'tl.constexpr', BLOCK_MODEL:\n    'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)\n    S = (S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None,\n        :] + (NUM_BLOCK - 2) * D_MODEL_K * D_MODEL_V)\n    DS = (DS + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None,\n        :] + (NUM_BLOCK - 1) * D_MODEL_K * D_MODEL_V)\n    p2 = p2 + offset_bh * NUM_BLOCK * D_MODEL_V + tl.arange(0, BLOCK_MODEL\n        ) + offset_s * BLOCK_MODEL + (NUM_BLOCK - 2) * D_MODEL_V\n    Dp2 = (Dp2 + offset_bh * NUM_BLOCK * D_MODEL_V * NUM_SPLIT_K + offset_d *\n        D_MODEL_V + tl.arange(0, BLOCK_MODEL) + offset_s * BLOCK_MODEL + (\n        NUM_BLOCK - 2) * D_MODEL_V * NUM_SPLIT_K)\n    Dacc = tl.zeros([BLOCK_MODEL, BLOCK_MODEL], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1):\n        p_value = tl.load(p2)\n        S_i = tl.load(S)\n        DS_i = tl.load(DS)\n        Dacc += DS_i\n        dp_i = Dacc * S_i\n        dp_value = tl.sum(dp_i, axis=0)\n        tl.store(Dp2, dp_value)\n        tl.store(S, Dacc)\n        Dacc *= p_value[None, :]\n        S -= D_MODEL_K * D_MODEL_V\n        DS -= D_MODEL_K * D_MODEL_V\n        p2 -= D_MODEL_V\n        Dp2 -= D_MODEL_V * NUM_SPLIT_K\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_preprocess_cumsum_gk(Q, K, GK, GK_cumsum, Q_exp, K_reduce,\n    GK_last_exp, NUM_CHUNK, L, D_MODEL_K: 'tl.constexpr', D_BLOCK_K:\n    'tl.constexpr', CHUNK_SIZE: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    offset_nk = tl.program_id(2)\n    Q_ptr = (Q + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    Q_exp_ptr = (Q_exp + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_ptr = (GK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_cumsum_ptr = (GK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    GK_last_exp_ptr = (GK_last_exp + offset_bh * NUM_CHUNK * D_MODEL_K + \n        offset_c * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    cumsum = tl.zeros([D_BLOCK_K], dtype=tl.float32)\n    mask = D_BLOCK_K * offset_nk + tl.arange(0, D_BLOCK_K) < D_MODEL_K\n    for _ in range(CHUNK_SIZE):\n        gk = tl.load(GK_ptr, mask=mask, other=0)\n        cumsum += gk\n        tl.store(GK_cumsum_ptr, cumsum, mask=mask)\n        cumsum_exp = tl.exp(cumsum)\n        q = tl.load(Q_ptr, mask=mask, other=0)\n        q_exp = q * cumsum_exp\n        tl.store(Q_exp_ptr, q_exp, mask=mask)\n        Q_ptr += D_MODEL_K\n        Q_exp_ptr += D_MODEL_K\n        GK_ptr += D_MODEL_K\n        GK_cumsum_ptr += D_MODEL_K\n    tl.store(GK_last_exp_ptr, tl.exp(cumsum), mask=mask)\n    tl.debug_barrier()\n    GK_cumsum_ptr = (GK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    K_ptr = (K + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    K_reduce_ptr = (K_reduce + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    for _ in range(CHUNK_SIZE):\n        gk_cumsum = tl.load(GK_cumsum_ptr, mask=mask, other=0)\n        k = tl.load(K_ptr, mask=mask, other=0)\n        k_reduce = k * tl.exp(cumsum - gk_cumsum)\n        tl.store(K_reduce_ptr, k_reduce, mask=mask)\n        K_ptr += D_MODEL_K\n        GK_cumsum_ptr += D_MODEL_K\n        K_reduce_ptr += D_MODEL_K\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess_cumsum_gk(Q, K, GK, GK_cumsum, DQ_exp, DK_reduce,\n    DGK_last_exp, DGK_cumsum, DQ, DK, DGK, NUM_CHUNK, L, D_MODEL_K:\n    'tl.constexpr', D_BLOCK_K: 'tl.constexpr', CHUNK_SIZE: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    offset_nk = tl.program_id(2)\n    mask = D_BLOCK_K * offset_nk + tl.arange(0, D_BLOCK_K) < D_MODEL_K\n    Q_ptr = (Q + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    K_ptr = (K + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_ptr = (GK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_cumsum_ptr = (GK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DQ_ptr = (DQ + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    DK_ptr = (DK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    DQ_exp_ptr = (DQ_exp + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DK_reduce_ptr = (DK_reduce + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DGK_cumsum_ptr = (DGK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DGK_ptr = (DGK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    D_GK_last_exp_ptr = (DGK_last_exp + offset_bh * NUM_CHUNK * D_MODEL_K +\n        offset_c * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    cumsum_gradient = tl.zeros([D_BLOCK_K], dtype=tl.float32)\n    grad_gk_last = tl.zeros([D_BLOCK_K], dtype=tl.float32)\n    gk_last = tl.load(GK_cumsum_ptr + (CHUNK_SIZE - 1) * D_MODEL_K, mask=\n        mask, other=0)\n    cumsum_gradient += tl.load(D_GK_last_exp_ptr, mask=mask, other=0) * tl.exp(\n        gk_last)\n    GK_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    GK_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    Q_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    K_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DQ_exp_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DK_reduce_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DGK_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DQ_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DK_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DGK_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        gk_cs = tl.load(GK_cumsum_ptr, mask=mask, other=0)\n        k = tl.load(K_ptr, mask=mask, other=0)\n        grad_k = tl.exp(gk_last - gk_cs) * tl.load(DK_reduce_ptr, mask=mask,\n            other=0)\n        tl.store(DK_ptr, grad_k, mask=mask)\n        grad_k *= k\n        cumsum_gradient -= grad_k\n        grad_gk_last += grad_k\n        q = tl.load(Q_ptr, mask=mask, other=0)\n        grad_q = tl.exp(gk_cs) * tl.load(DQ_exp_ptr, mask=mask, other=0)\n        tl.store(DQ_ptr, grad_q, mask=mask)\n        cumsum_gradient += grad_q * q\n        cumsum_gradient += tl.load(DGK_cumsum_ptr, mask=mask, other=0)\n        tl.store(DGK_ptr, cumsum_gradient, mask=mask)\n        Q_ptr -= D_MODEL_K\n        DQ_exp_ptr -= D_MODEL_K\n        K_ptr -= D_MODEL_K\n        DK_reduce_ptr -= D_MODEL_K\n        GK_cumsum_ptr -= D_MODEL_K\n        DGK_cumsum_ptr -= D_MODEL_K\n        DQ_ptr -= D_MODEL_K\n        DK_ptr -= D_MODEL_K\n        DGK_ptr -= D_MODEL_K\n    DGK_ptr = (DGK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + (CHUNK_SIZE - 1) * D_MODEL_K +\n        D_BLOCK_K * offset_nk)\n    GK_ptr = (GK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + (CHUNK_SIZE - 1) * D_MODEL_K +\n        D_BLOCK_K * offset_nk)\n    grad_gk_last = grad_gk_last + 0.0\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        dgk = tl.load(DGK_ptr, mask=mask, other=0)\n        dgk += grad_gk_last\n        tl.store(DGK_ptr, dgk, mask=mask)\n        DGK_ptr -= D_MODEL_K\n        GK_ptr -= D_MODEL_K\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_preprocess_cumsum_gv(V, GV, GV_cumsum, GV_exp, V_reduce,\n    GV_last_exp, NUM_CHUNK, L, D_MODEL_V: 'tl.constexpr', CHUNK_SIZE:\n    'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    GV_ptr = (GV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_last_exp_ptr = (GV_last_exp + offset_bh * NUM_CHUNK * D_MODEL_V + \n        offset_c * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_cumsum_ptr = (GV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_exp_ptr = (GV_exp + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    cumsum = tl.zeros([D_MODEL_V], dtype=tl.float32)\n    for _ in range(CHUNK_SIZE):\n        gv = tl.load(GV_ptr)\n        cumsum += gv\n        tl.store(GV_cumsum_ptr, cumsum)\n        tl.store(GV_exp_ptr, tl.exp(cumsum))\n        GV_cumsum_ptr += D_MODEL_V\n        GV_exp_ptr += D_MODEL_V\n        GV_ptr += D_MODEL_V\n    tl.store(GV_last_exp_ptr, tl.exp(cumsum))\n    tl.debug_barrier()\n    V_ptr = (V + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_cumsum_ptr = (GV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    V_reduce_ptr = (V_reduce + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    for _ in range(CHUNK_SIZE):\n        v = tl.load(V_ptr)\n        gv = tl.load(GV_cumsum_ptr)\n        v_reduce = v * tl.exp(cumsum - gv)\n        tl.store(V_reduce_ptr, v_reduce)\n        V_ptr += D_MODEL_V\n        V_reduce_ptr += D_MODEL_V\n        GV_cumsum_ptr += D_MODEL_V\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess_cumsum_gv(V, GV, GV_cumsum, DGV_cumsum_exp, DV_reduce,\n    DGV_last_exp, DGV_cumsum, DV, DGV, NUM_CHUNK, L, D_MODEL_V:\n    'tl.constexpr', CHUNK_SIZE: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    V_ptr = (V + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_ptr = (GV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_cumsum_ptr = (GV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DV_ptr = (DV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DV_reduce_ptr = (DV_reduce + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DGV_cumsum_ptr = (DGV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DGV_cumsum_exp_ptr = (DGV_cumsum_exp + offset_bh * L * D_MODEL_V + \n        offset_c * CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DGV_ptr = (DGV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    D_GV_last_exp_ptr = (DGV_last_exp + offset_bh * NUM_CHUNK * D_MODEL_V +\n        offset_c * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    cumsum_gradient = tl.zeros([D_MODEL_V], dtype=tl.float32)\n    grad_gv_last = tl.zeros([D_MODEL_V], dtype=tl.float32)\n    gv_last = tl.load(GV_cumsum_ptr + (CHUNK_SIZE - 1) * D_MODEL_V)\n    cumsum_gradient += tl.load(D_GV_last_exp_ptr) * tl.exp(gv_last)\n    GV_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    GV_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    V_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DV_reduce_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DGV_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DGV_cumsum_exp_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DV_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DGV_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        gv_cs = tl.load(GV_cumsum_ptr)\n        v = tl.load(V_ptr)\n        grad_v = tl.exp(gv_last - gv_cs) * tl.load(DV_reduce_ptr)\n        tl.store(DV_ptr, grad_v)\n        grad_v *= v\n        cumsum_gradient -= grad_v\n        grad_gv_last += grad_v\n        grad_v = tl.exp(gv_cs) * tl.load(DGV_cumsum_exp_ptr)\n        cumsum_gradient += grad_v\n        cumsum_gradient += tl.load(DGV_cumsum_ptr)\n        tl.store(DGV_ptr, cumsum_gradient)\n        V_ptr -= D_MODEL_V\n        DV_reduce_ptr -= D_MODEL_V\n        GV_cumsum_ptr -= D_MODEL_V\n        DGV_cumsum_ptr -= D_MODEL_V\n        DV_ptr -= D_MODEL_V\n        DGV_ptr -= D_MODEL_V\n        DGV_cumsum_exp_ptr -= D_MODEL_V\n    DGV_ptr = (DGV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V) + (CHUNK_SIZE - 1) * D_MODEL_V)\n    GV_ptr = (GV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V) + (CHUNK_SIZE - 1) * D_MODEL_V)\n    grad_gv_last = grad_gv_last + 0.0\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        dgv = tl.load(DGV_ptr)\n        dgv += grad_gv_last\n        tl.store(DGV_ptr, dgv)\n        DGV_ptr -= D_MODEL_V\n        GV_ptr -= D_MODEL_V\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_compute_A(Q, K, GK, A, stride_q1, stride_q2, stride_q3,\n    stride_q4, stride_a1, stride_a2, stride_a3, stride_a4, Z, H, N_CTX, D,\n    BLOCK_DMODEL_QK: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_k = tl.program_id(2)\n    qk_offset = off_hz * stride_q2 + off_k * BLOCK_DMODEL_QK\n    a_offset = (off_k * Z * H + off_hz) * stride_a2\n    lo = 0\n    hi = BLOCK_N\n    Q_ptr = Q + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    K_ptr = K + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[:, None] + tl.arange(0, 16)[None, :] * stride_q4\n    GK_K_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[:, None] + tl.arange(0, 16)[None, :] * stride_q4\n    GK_Q_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(16, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        q_gk2 = tl.exp(q_gk - q_normalizer[None, :])\n        q = q * q_gk2\n        for k_high in range(0, q_high, 16):\n            k = tl.load(K_ptr + k_high * stride_q4)\n            k_gk = tl.load(GK_K_ptr + k_high * stride_q4)\n            k_gk = tl.exp(q_normalizer[:, None] - k_gk)\n            k = k * k_gk\n            qk = tl.dot(q, k, allow_tf32=False)\n            tl.store(A_ptr + q_high * stride_a4 + k_high, qk)\n    for q_high in range(lo, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        q_gk2 = tl.exp(q_gk - q_normalizer[None, :])\n        q = q * q_gk2\n        q_gk3 = tl.exp(q_normalizer[None, :] - q_gk)\n        k = tl.load(K_ptr + q_high * stride_q4)\n        k = k * tl.trans(q_gk3)\n        qk = tl.dot(q, k, allow_tf32=False)\n        qk = tl.where(tl.arange(0, 16)[:, None] >= tl.arange(0, 16)[None, :\n            ], qk, 0.0)\n        tl.store(A_ptr + q_high * stride_a4 + q_high, qk)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel_dqk(Q, K, GK, DA, DQ, DK, DGK, stride_q1, stride_q2,\n    stride_q3, stride_q4, stride_a1, stride_a2, stride_a3, stride_a4, Z, H,\n    N_CTX, D, BLOCK_DMODEL_QK: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_k = tl.program_id(2)\n    qk_offset = off_hz * stride_q2 + BLOCK_DMODEL_QK * off_k\n    a_offset = off_hz * stride_a2\n    lo = 0\n    hi = BLOCK_N\n    Q_ptr = Q + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DQ_ptr = DQ + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    K_ptr = K + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    GK_K_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    GK_Q_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DA_ptr = DA + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(lo + 16, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        dq2 = tl.zeros([16, BLOCK_DMODEL_QK], dtype=tl.float32)\n        for k_high in range(0, q_high, 16):\n            k = tl.load(K_ptr + k_high * stride_q4)\n            k_gk = tl.load(GK_K_ptr + k_high * stride_q4)\n            dqk = tl.load(DA_ptr + q_high * stride_a4 + k_high)\n            k_gk = tl.exp(q_normalizer[None, :] - k_gk)\n            k = k * k_gk\n            dq2 += tl.dot(dqk, k, allow_tf32=False)\n        dq2 = dq2\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_gk = tl.exp(q_gk - q_normalizer[None, :])\n        dq = dq2 * q_gk\n        dq_gk = dq * q\n        DQ_ptr = DQ + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + q_high * stride_q4\n        tl.store(DQ_ptr, dq)\n        DGK_Q_ptr = DGK + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + q_high * stride_q4\n        tl.store(DGK_Q_ptr, dq_gk)\n    tl.debug_barrier()\n    for k_high in range(lo, hi - 16, 16):\n        k = tl.load(K_ptr + k_high * stride_q4)\n        k_gk = tl.load(GK_K_ptr + k_high * stride_q4)\n        dk = tl.zeros([16, BLOCK_DMODEL_QK], dtype=tl.float32)\n        dgk = tl.zeros([16, BLOCK_DMODEL_QK], dtype=tl.float32)\n        for q_high in range(k_high + 16, hi, 16):\n            q = tl.load(Q_ptr + q_high * stride_q4)\n            q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n                q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n            q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n            q_gk = tl.exp(q_gk - q_normalizer[None, :])\n            q = q * q_gk\n            dqk = tl.load(DA_ptr + q_high * stride_a4 + k_high)\n            k_gk2 = tl.exp(q_normalizer[None, :] - k_gk)\n            dk2 = tl.dot(tl.trans(dqk), q, allow_tf32=False)\n            dk += dk2 * k_gk2\n            dgk -= dk2 * k * k_gk2\n        DK_ptr = DK + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + k_high * stride_q4\n        tl.store(DK_ptr, dk)\n        DGK_K_ptr = DGK + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + k_high * stride_q4\n        prev = tl.load(DGK_K_ptr)\n        tl.store(DGK_K_ptr, prev + dgk)\n    tl.debug_barrier()\n    DK_ptr = DK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DGK_K_ptr = DGK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DQ_ptr = DQ + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    for q_high in range(lo, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        q_gk2 = tl.exp(q_gk - q_normalizer[None, :])\n        q2 = q * q_gk2\n        q_gk3 = tl.exp(q_normalizer[None, :] - q_gk)\n        k = tl.load(K_ptr + q_high * stride_q4)\n        k2 = k * q_gk3\n        dqk = tl.load(DA_ptr + q_high * stride_a4 + q_high)\n        dqk = tl.where(tl.arange(0, 16)[:, None] >= tl.arange(0, 16)[None,\n            :], dqk, 0.0)\n        dk2 = tl.dot(tl.trans(dqk), q2, allow_tf32=False)\n        dk = dk2 * q_gk3\n        prev_dk = tl.load(DK_ptr + q_high * stride_q4)\n        tl.store(DK_ptr + q_high * stride_q4, dk + prev_dk)\n        dgk = -dk * k\n        dq2 = tl.dot(dqk, k2, allow_tf32=False)\n        dq = dq2 * q_gk2\n        prev_dq = tl.load(DQ_ptr + q_high * stride_q4)\n        tl.store(DQ_ptr + q_high * stride_q4, dq + prev_dq)\n        dgk += dq * q\n        prev_dq_gk = tl.load(DGK_K_ptr + q_high * stride_q4)\n        tl.store(DGK_K_ptr + q_high * stride_q4, dgk + prev_dq_gk)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_compute_O(A, V, GV, O, stride_a2, stride_a3, stride_a4, stride_v2,\n    stride_v3, stride_v4, BLOCK_N: 'tl.constexpr', BLOCK_DMODEL_V:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_v = tl.program_id(2)\n    a_offset = off_hz * stride_a2\n    v_offset = off_hz * stride_v2 + off_v * BLOCK_DMODEL_V\n    lo = 0\n    hi = BLOCK_N\n    V_ptr = V + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    O_ptr = O + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(lo + 16, hi, 16):\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        acc = tl.zeros([16, BLOCK_DMODEL_V], dtype=tl.float32)\n        for k_high in range(0, q_high, 16):\n            qk = tl.load(A_ptr + q_high * stride_a4 + k_high)\n            v = tl.load(V_ptr + k_high * stride_v4)\n            k_gv = tl.load(GV_ptr + k_high * stride_v4)\n            k_gv = tl.exp(q_gv_normalizer[None, :] - k_gv)\n            v = v * k_gv\n            output = tl.dot(qk, v, allow_tf32=False)\n            acc += output\n        tl.store(O_ptr + q_high * stride_v4, acc)\n    tl.store(O_ptr, tl.zeros([16, BLOCK_DMODEL_V], dtype=tl.float32))\n    tl.debug_barrier()\n    for q_high in range(lo, hi, 16):\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        qk = tl.load(A_ptr + q_high * stride_a4 + q_high)\n        v = tl.load(V_ptr + q_high * stride_v4)\n        k_gv = tl.load(GV_ptr + q_high * stride_v4)\n        k_gv2 = tl.exp(q_gv_normalizer[None, :] - k_gv)\n        v = v * k_gv2\n        output = tl.dot(qk, v, allow_tf32=False)\n        q_gv = tl.exp(k_gv - q_gv_normalizer[None, :])\n        prev = tl.load(O_ptr + q_high * stride_v4)\n        output += prev\n        output = output * q_gv\n        tl.store(O_ptr + q_high * stride_v4, output)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel_dav(V, GV, A, O, DO, DA, DV, DGV, Z, H, stride_a1,\n    stride_a2, stride_a3, stride_a4, stride_v1, stride_v2, stride_v3,\n    stride_v4, BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_DMODEL_V: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_v = tl.program_id(2)\n    a_offset = off_hz * stride_a2\n    da_offset = (off_v * Z * H + off_hz) * stride_a2\n    v_offset = off_hz * stride_v2 + off_v * BLOCK_DMODEL_V\n    lo = 0\n    hi = BLOCK_N\n    DO_ptr = DO + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    O_ptr = O + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    DV_ptr = DV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    DGV_ptr = DGV + v_offset + start_m * stride_v3 + tl.arange(0,\n        BLOCK_DMODEL_V)[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    DA_ptr = DA + da_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(lo, hi, 16):\n        do = tl.load(DO_ptr + q_high * stride_v4)\n        o = tl.load(O_ptr + q_high * stride_v4)\n        tl.store(DGV_ptr + q_high * stride_v4, do * o)\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        q_gv = tl.load(GV_ptr + q_high * stride_v4)\n        q_gv = tl.exp(q_gv - q_gv_normalizer[None, :])\n        do = do * q_gv\n        tl.store(DO_ptr + q_high * stride_v4, do)\n    tl.debug_barrier()\n    V_ptr = V + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        :, None] + tl.arange(0, 16)[None, :] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[:, None] + tl.arange(0, 16)[None, :] * stride_v4\n    for q_high in range(lo + 16, hi, 16):\n        do = tl.load(DO_ptr + q_high * stride_v4)\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        for k_high in range(0, q_high, 16):\n            v = tl.load(V_ptr + k_high * stride_v4)\n            k_gv = tl.load(GV_ptr + k_high * stride_v4)\n            k_gv = tl.exp(q_gv_normalizer[:, None] - k_gv)\n            v2 = v * k_gv\n            dqk = tl.dot(do, v2, allow_tf32=False)\n            tl.store(DA_ptr + q_high * stride_a4 + k_high, dqk)\n    tl.debug_barrier()\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[:, None\n        ] + tl.arange(0, 16)[None, :] * stride_a4\n    V_ptr = V + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    for k_high in range(0, hi, 16):\n        dv = tl.zeros([16, BLOCK_DMODEL_V], dtype=tl.float32)\n        k_gv = tl.load(GV_ptr + k_high * stride_v4)\n        for q_high in range(k_high + 16, BLOCK_N, 16):\n            do = tl.load(DO_ptr + q_high * stride_v4)\n            kq = tl.load(A_ptr + q_high * stride_a4 + k_high)\n            q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 +\n                q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n            k_gv2 = tl.exp(q_gv_normalizer[None, :] - k_gv)\n            dv2 = tl.dot(kq, do, allow_tf32=False)\n            dv += dv2 * k_gv2\n        v = tl.load(V_ptr + k_high * stride_v4)\n        tl.store(DV_ptr + k_high * stride_v4, dv)\n        prev_dv = tl.load(DGV_ptr + k_high * stride_v4)\n        tl.store(DGV_ptr + k_high * stride_v4, prev_dv - dv * v)\n    tl.debug_barrier()\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[:, None\n        ] + tl.arange(0, 16)[None, :] * stride_a4\n    for q_high in range(lo, hi, 16):\n        do = tl.load(DO_ptr + q_high * stride_v4)\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        v = tl.load(V_ptr + q_high * stride_v4)\n        k_gv = tl.load(GV_ptr + q_high * stride_v4)\n        k_gv = tl.exp(q_gv_normalizer[None, :] - k_gv)\n        v2 = v * k_gv\n        dqk = tl.dot(do, tl.trans(v2), allow_tf32=False)\n        dqk = tl.where(tl.arange(0, 16)[:, None] >= tl.arange(0, 16)[None,\n            :], dqk, 0.0)\n        tl.store(DA_ptr + q_high * stride_a4 + q_high, dqk)\n        kq = tl.load(A_ptr + q_high * stride_a4 + q_high)\n        dv2 = tl.dot(kq, do, allow_tf32=False)\n        dv = dv2 * k_gv\n        prev_dv = tl.load(DV_ptr + q_high * stride_v4)\n        tl.store(DV_ptr + q_high * stride_v4, prev_dv + dv)\n        prev_gdv = tl.load(DGV_ptr + q_high * stride_v4)\n        prev_gdv -= dv * v\n        tl.store(DGV_ptr + q_high * stride_v4, prev_gdv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_gla_fwd_kernel(q, k, v, g, o, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_db = g + i_bh * s_qk_h + (BT - 1) * s_qk_t + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_g *= inv_ln2\n        d_b = tl.load(p_db) * inv_ln2\n        b_q = b_q * scale * tl.math.exp2(b_g)\n        b_k = b_k * tl.trans(tl.math.exp2(-b_g + d_b[None, :]))\n        b_o = tl.dot(b_q, b_h, allow_tf32=False)\n        b_h *= tl.math.exp2(d_b)[:, None]\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_g = tl.advance(p_g, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_db += BT * DK\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, initial_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + ((i + 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1)) * inv_ln2\n        d_b = tl.load(p_db) * inv_ln2\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_k *= tl.math.exp2(d_b[None, :] - b_g)\n        b_h *= tl.math.exp2(d_b)[None, :]\n        b_h += tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale * tl.math.exp2(b_g)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + (T - (i - 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1)) * inv_ln2\n        b_db = tl.load(p_db) * inv_ln2\n        g_k = tl.math.exp2(b_db[None, :] - b_g)\n        b_k *= g_k\n        b_q *= tl.math.exp2(tl.trans(b_g))\n        b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False)\n            ) * scale * g_k\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * scale\n        b_dh *= tl.math.exp2(b_db)[:, None]\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_gla_fwd_kernel(q, k, v, gk, gv, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * DV if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if REVERSE else 0)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            h = h * _gk[None, :]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            h = h * _gv[:, None]\n        h += _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += -DK if REVERSE else DK\n        p_k += -DK if REVERSE else DK\n        p_o += -DV if REVERSE else DV\n        p_v += -DV if REVERSE else DV\n        if USE_GK:\n            p_gk += -DK if REVERSE else DK\n        if USE_GV:\n            p_gv += -DV if REVERSE else DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_gla_bwd_kernel(q, k, v, gk, gv, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', REVERSE:\n    'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * DK if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if REVERSE else 0)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[:, None] & mask_bv[None, :]\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            h = h * _gk[:, None]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            h = h * _gv[None, :]\n        h += _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += -DK if REVERSE else DK\n        p_v += -DV if REVERSE else DV\n        p_q += -DK if REVERSE else DK\n        p_do += -DV if REVERSE else DV\n        p_dq += -DK if REVERSE else DK\n        if USE_GK:\n            p_gk += -DK if REVERSE else DK\n        if USE_GV:\n            p_gv += -DV if REVERSE else DV\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        not REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        not REVERSE else 0)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        not REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        not REVERSE else 0)\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * DK if not REVERSE else 0)\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * DV if not REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if not REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if not REVERSE else 0)\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            d_h *= _gk[:, None]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            d_h *= _gv[None, :]\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do += DV if REVERSE else -DV\n        p_q += DK if REVERSE else -DK\n        p_k += DK if REVERSE else -DK\n        p_v += DV if REVERSE else -DV\n        p_dk += DK if REVERSE else -DK\n        p_dv += DV if REVERSE else -DV\n        if USE_GK:\n            p_gk += DK if REVERSE else -DK\n        if USE_GV:\n            p_gv += DV if REVERSE else -DV\n"
    },
    {
      "input": "@triton.jit\ndef parallel_rebased_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_rebased_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_retention_fwd_kernel_h(k, v, h, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_hh, s_ht, H, T, TD,\n    DK, DV, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + i_bh * s_hh, (TD, DV), (s_ht, 1), (i_k * BK,\n        i_v * BV), (BK, BV), (1, 0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for _ in range(0, T, BT):\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = d_b * b_h + tl.dot(b_k, b_v * d_i[:, None], allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_h = tl.advance(p_h, (DK, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_retention_fwd_kernel_o(q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_hh, s_ht, H, T, TD, scale, DK, DV, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    for i_v in range(0, tl.cdiv(DV, BV)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i_t * BT, 0), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (0, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (TD, DV), (s_ht, 1), (i_t *\n            DK, i_v * BV), (BK, BV), (1, 0))\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_s = tl.zeros([BT, BT], dtype=tl.float32)\n        for _ in range(0, tl.cdiv(DK, BK)):\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_q = b_q * scale\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_o += tl.dot(b_q * d_i[:, None], b_h, allow_tf32=False)\n            b_s += tl.dot(b_q, b_k, allow_tf32=False)\n            p_q = tl.advance(p_q, (0, BK))\n            p_k = tl.advance(p_k, (BK, 0))\n            p_h = tl.advance(p_h, (BK, 0))\n        b_s *= d_s\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_retention_bwd_kernel_dh(q, do, dh, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, s_hh, s_ht, H, T, scale, DK, DV, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_hh, ((i + 1) * DK, DV), (\n            s_ht, 1), (i * DK + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, b_do * d_i[:, None], allow_tf32=False)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_hh, s_ht, H, T, TDK, scale,\n    DK, DV, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b\n        )\n    d_q = d_q * scale\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    for i_k in range(0, tl.cdiv(DK, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i_t * BT, 0), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (DV, TDK), (1, s_ht), (0, \n            i_t * DK + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i_t * BT, 0), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_hh, (TDK, DV), (s_ht, 1), (\n            i_t * DK + i_k * BK, 0), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i_t * BT, 0), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, DK), (s_qk_t,\n            s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, DK), (s_qk_t,\n            s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        for _ in range(tl.cdiv(DV, BV)):\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_dh = tl.load(p_dh, boundary_check=(0, 1))\n            b_ds = tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n            b_ds = b_ds * d_s\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False) * d_q[:, None\n                ] + tl.dot(b_ds, b_k, allow_tf32=False)\n            b_ds = tl.trans(b_ds)\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(\n                b_s, b_do, allow_tf32=False)\n            b_dv += tl.load(p_dv, boundary_check=(0, 1))\n            tl.store(p_dv, b_dv, boundary_check=(0, 1))\n            p_v = tl.advance(p_v, (0, BV))\n            p_h = tl.advance(p_h, (BV, 0))\n            p_do = tl.advance(p_do, (0, BV))\n            p_dh = tl.advance(p_dh, (0, BV))\n            p_dv = tl.advance(p_dv, (0, BV))\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_retention_fwd_kernel(q, k, v, o, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b, d_o, d_h = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b\n        ), tl.math.exp2((BT - o_i - 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_retention_bwd_kernel(q, k, v, do, dq, dk, dv, initial_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT -\n        o_i - 1) * b_b)\n    d_b = tl.math.exp2(BT * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    d_s = tl.trans(d_s)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_retention_fwd_kernel(q, k, v, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    o_k = tl.arange(0, BTS)\n    d_h = tl.math.exp2((BTS - o_k) * b_b)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_h[None, :]\n        b_o = b_o * tl.math.exp2(b_b * BTS)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    d_q = tl.math.exp2(tl.arange(0, BTL) * b_b)\n    b_o *= d_q[:, None]\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    d_h = tl.math.exp2((BTS - tl.arange(0, BTS)) * b_b)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        b_dq *= d_b\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= tl.math.exp2(tl.arange(0, BTL) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    d_h = tl.math.exp2((BTL - tl.arange(0, BTL)) * b_b)\n    b_kd = b_k * d_h[:, None]\n    d_q = tl.math.exp2(tl.arange(0, BTS) * b_b)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * d_q[None, :]\n        b_dv *= d_b\n        b_s = tl.dot(b_kd, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        b_dk *= d_b\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((-o_k[:, None] + o_q[None, :]) *\n            b_b), 0) * scale\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_retention_bwd_kernel(q, k, v, do, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_retention_fwd_kernel(q, k, v, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.pow(2, -5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        h = b_b * h + _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_retention_bwd_kernel(q, k, v, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.pow(2, -5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        h = b_b * h + _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dq += DK\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        d_h *= b_b\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n"
    },
    {
      "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    nheads, rotary_dim, seqlen_ro, CACHE_KEY_SEQLEN, stride_out_batch,\n    stride_out_seqlen, stride_out_nheads, stride_out_headdim,\n    stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim,\n    BLOCK_K: 'tl.constexpr', IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr',\n    IS_VARLEN: 'tl.constexpr', INTERLEAVED: 'tl.constexpr', CONJUGATE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n"
    },
    {
      "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate,\n    stride_state_batch, stride_state_dim, stride_state_dstate,\n    stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim,\n    stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch,\n    stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim,\n    stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim,\n    DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS:\n    'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    state_ptr += pid_b * stride_state_batch\n    x_ptr += pid_b * stride_x_batch\n    dt_ptr += pid_b * stride_dt_batch\n    B_ptr += pid_b * stride_B_batch\n    C_ptr += pid_b * stride_C_batch\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch\n    out_ptr += pid_b * stride_out_batch\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n    if DT_SOFTPLUS:\n        dt = tl.log(1.0 + tl.exp(dt))\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n        dstate), other=0.0)\n    dA = tl.exp(A * dt[:, None])\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    dB = B[None, :] * dt[:, None]\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, Out, sqz, sqh, sqm, sqd, skz, skh, skn,\n    skd, svz, svh, svn, svd, soz, soh, som, sod, L, M, Z, H, N_CTX_Q,\n    N_CTX_KV, BLOCK: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    N_PREFIX_Q: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    BLOCK_M: 'tl.constexpr' = BLOCK\n    BLOCK_N: 'tl.constexpr' = BLOCK\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_m_real = (start_m + N_PREFIX_Q) * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_q = off_hz * sqh + offs_m[:, None] * sqm + offs_d[None, :] * sqd\n    offs_k = off_hz * skh + offs_n[None, :] * skn + offs_d[:, None] * skd\n    offs_v = off_hz * svh + offs_n[:, None] * svn + offs_d[None, :] * svd\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q_vals = tl.load(Q + offs_q, mask=offs_m[:, None] < N_CTX_Q, other=0)\n    for start_n in range(0, N_PREFIX_Q + start_m):\n        k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n        qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n        qk *= sm_scale\n        qk = tl.where(offs_m_real[:, None] >= offs_n[None, :], qk, float(\n            '-inf'))\n        landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] == \n            BLOCK_N - 1, qk, float('-inf')), 1)\n        normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1,\n            float('-inf'), qk)\n        normal_m = tl.max(normal_qk, 1)\n        normal_p = tl.exp(normal_qk - normal_m[:, None])\n        normal_denom = tl.sum(normal_p, 1)\n        m_curr = tl.maximum(landmark_qk, m_prev)\n        m_curr_ = m_curr\n        l_prev *= tl.exp(m_prev - m_curr_)\n        landmark_p = tl.exp(landmark_qk - m_curr_)\n        l_curr = landmark_p + l_prev\n        l_rcp = 1.0 / l_curr\n        landmark_p *= l_rcp\n        acc *= (l_prev * l_rcp)[:, None]\n        v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n        acc += tl.dot(landmark_p[:, None] * normal_p / normal_denom[:, None\n            ], v_vals, allow_tf32=False)\n        l_prev = l_curr\n        m_prev = m_curr\n        offs_n += BLOCK_N\n        offs_k += BLOCK_N * skn\n        offs_v += BLOCK_N * svn\n    k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n    qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n    qk *= sm_scale\n    qk = tl.where(offs_m_real[:, None] >= offs_n[None, :], qk, float('-inf'))\n    m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n    m_curr_ = m_curr\n    l_prev *= tl.exp(m_prev - m_curr_)\n    p = tl.exp(qk - m_curr_[:, None])\n    l_curr = tl.sum(p, 1) + l_prev\n    l_rcp = 1.0 / l_curr\n    p *= l_rcp[:, None]\n    acc *= (l_prev * l_rcp)[:, None]\n    p = p\n    v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n    acc += tl.dot(p, v_vals, allow_tf32=False)\n    l_prev = l_curr\n    m_prev = m_curr\n    offs_L = off_hz * N_CTX_Q + offs_m\n    offs_M = off_hz * N_CTX_Q + offs_m\n    tl.store(L + offs_L, l_prev, mask=offs_m < N_CTX_Q)\n    tl.store(M + offs_M, m_prev, mask=offs_m < N_CTX_Q)\n    offs_o = off_hz * soh + offs_m[:, None] * som + offs_d[None, :] * sod\n    tl.store(Out + offs_o, acc, mask=offs_m[:, None] < N_CTX_Q)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess(Out, soz, soh, som, sod, DO, L, slzh, slm, NewDO, Delta,\n    N_CTX_Q, BLOCK_M: 'tl.constexpr', D_HEAD: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_d = tl.arange(0, D_HEAD)\n    off_o = off_hz * soh + off_m[:, None] * som + off_d[None, :] * sod\n    off_l = off_hz * slzh + off_m * slm\n    o = tl.load(Out + off_o)\n    do = tl.load(DO + off_o)\n    denom = tl.load(L + off_l)\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    tl.store(NewDO + off_o, do)\n    tl.store(Delta + off_l, delta)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, M, D, sqz, sqh,\n    sqm, sqd, skz, skh, skn, skd, svz, svh, svn, svd, Z, H, N_CTX_Q,\n    N_CTX_KV, BLOCK: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    N_PREFIX_Q: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    BLOCK_M: 'tl.constexpr' = BLOCK\n    BLOCK_N: 'tl.constexpr' = BLOCK\n    Q += off_z * sqz + off_h * sqh\n    K += off_z * skz + off_h * skh\n    V += off_z * svz + off_h * svh\n    DO += off_z * sqz + off_h * sqh\n    DQ += off_z * sqz + off_h * sqh\n    DK += off_z * skz + off_h * skh\n    DV += off_z * svz + off_h * svh\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    D_ptrs = D + off_hz * N_CTX_Q\n    m_ptrs = M + off_hz * N_CTX_Q\n    for start_n in range(0, N_CTX_KV, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + tl.arange(0, BLOCK_N)\n        k_ptrs = K + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        v_ptrs = V + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        if start_n < N_PREFIX_Q * BLOCK_M:\n            start_q_index = 0\n        elif N_CTX_Q <= start_n - N_PREFIX_Q * BLOCK_M:\n            start_q_index = start_n - N_PREFIX_Q * BLOCK_M\n        else:\n            first_start_m = start_n - N_PREFIX_Q * BLOCK_M\n            first_start_m = tl.multiple_of(first_start_m, BLOCK_M)\n            offs_m = first_start_m + tl.arange(0, BLOCK_M)\n            offs_m_real = offs_m + N_PREFIX_Q * BLOCK_M\n            offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0\n                )\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk = tl.where(offs_m_real[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n            m = tl.load(m_ptrs + offs_m)\n            m_ = m\n            last_p = tl.exp(qk * sm_scale - m_[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(tl.trans(last_p), do, allow_tf32=False)\n            Di = tl.load(D_ptrs + offs_m)\n            last_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:,\n                None]\n            last_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            ds = last_p * last_dp * sm_scale\n            dk += tl.dot(tl.trans(ds), q, allow_tf32=False)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k, allow_tf32=False)\n            tl.store(dq_ptrs, dq)\n            start_q_index = first_start_m + BLOCK_M\n        for start_m in range(start_q_index, N_CTX_Q, BLOCK_M):\n            start_m = tl.multiple_of(start_m, BLOCK_M)\n            offs_m = start_m + tl.arange(0, BLOCK_M)\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk *= sm_scale\n            landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] ==\n                BLOCK_N - 1, qk, float('-inf')), 1)\n            normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N -\n                1, float('-inf'), qk)\n            m = tl.load(m_ptrs + offs_m)\n            m_ = m\n            p = tl.exp(landmark_qk - m_)\n            do = tl.load(do_ptrs)\n            normal_m = tl.max(normal_qk, 1)\n            normal_p = tl.exp(normal_qk - normal_m[:, None])\n            normal_p_normalized = normal_p / tl.sum(normal_p, 1)[:, None]\n            normal_kv = tl.dot(normal_p_normalized, v, allow_tf32=False)\n            normal_D = tl.sum(do * normal_kv, 1)\n            dv += tl.dot(tl.trans(p[:, None] * normal_p_normalized), do,\n                allow_tf32=False)\n            Di = tl.load(D_ptrs + offs_m)\n            dp = tl.zeros([BLOCK_M], dtype=tl.float32) - Di\n            dp += normal_D\n            landmark_ds = p * dp\n            normal_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32\n                ) - normal_D[:, None]\n            normal_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            normal_ds = p[:, None] * normal_p_normalized * normal_dp\n            ds = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1,\n                landmark_ds[:, None], normal_ds)\n            ds *= sm_scale\n            dk += tl.dot(tl.trans(ds), q, allow_tf32=False)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k, allow_tf32=False)\n            tl.store(dq_ptrs, dq)\n        dv_ptrs = DV + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dk_ptrs = DK + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n"
    },
    {
      "input": "@triton.autotune(configs=_get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef _triton_gemm(a_ptr, b_ptr, c_ptr, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, M, N, K, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a_data = tl.load(a_ptrs, mask=offs_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        b_data = tl.load(b_ptrs, mask=offs_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a_data, b_data)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    acc = acc\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] *\n        stride_cn)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=_get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef _gemm_activation_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', activation: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a_data = tl.load(a_ptrs, mask=offs_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        b_data = tl.load(b_ptrs, mask=offs_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a_data, b_data)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    acc = activation(acc)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] *\n        stride_cn)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _fwd_gqa_attn_kernel_block_ptr(Q, K, V, B, softmax_scale:\n    'tl.constexpr', stride_qb, stride_qh, stride_qg, stride_qm, stride_kb,\n    stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb,\n    stride_bh, stride_bg, stride_bm, stride_bn, stride_ob, stride_oh,\n    stride_og, stride_om, stride_lb, stride_lh, stride_lg, headdim:\n    'tl.constexpr', num_kv_heads: 'tl.constexpr', num_groups:\n    'tl.constexpr', seqlen_q, seqlen_k, O, L, HAVE_BIAS: 'tl.constexpr',\n    BIAS_SINGLE_HEAD: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m, off_bh, off_gp = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    off_h = off_bh % num_kv_heads\n    off_b = off_bh // num_kv_heads\n    if not EVEN_N:\n        offs_n = tl.arange(0, BLOCK_N)\n    Q_Block_ptr = tl.make_block_ptr(base=Q + (off_b * stride_qb + off_h *\n        stride_qh + off_gp * stride_qg), shape=(seqlen_q, headdim),\n        block_shape=(BLOCK_M, BLOCK_HEADDIM), strides=(stride_qm, 1),\n        offsets=(start_m * BLOCK_M, 0), order=(0, 1))\n    O_Block_ptr = tl.make_block_ptr(base=O + (off_b * stride_ob + off_h *\n        stride_oh + off_gp * stride_og), shape=(seqlen_q, headdim),\n        block_shape=(BLOCK_M, BLOCK_HEADDIM), strides=(stride_om, 1),\n        offsets=(start_m * BLOCK_M, 0), order=(0, 1))\n    L_Block_ptr = tl.make_block_ptr(base=L + (off_b * stride_lb + off_h *\n        stride_lh + off_gp * stride_lg), shape=(seqlen_q,), strides=(1,),\n        offsets=(start_m * BLOCK_M,), block_shape=(BLOCK_M,), order=(0,))\n    kv_stride = off_b * stride_kb + off_h * stride_kh\n    K_Block_ptr = tl.make_block_ptr(base=K + kv_stride, shape=(headdim,\n        seqlen_k), block_shape=(BLOCK_HEADDIM, BLOCK_N), strides=(1,\n        stride_kn), offsets=(0, 0), order=(1, 0))\n    V_Block_ptr = tl.make_block_ptr(base=V + kv_stride, shape=(seqlen_k,\n        headdim), block_shape=(BLOCK_N, BLOCK_HEADDIM), strides=(stride_vn,\n        1), offsets=(0, 0), order=(0, 1))\n    q = tl.load(Q_Block_ptr, boundary_check=(0, 1))\n    softmax_scale = softmax_scale\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = (0 if BIAS_SINGLE_HEAD else off_h *\n            stride_bh + off_gp * stride_bg)\n        B_Block_ptr = tl.make_block_ptr(base=B + (off_b * stride_bb +\n            bias_h_pos), shape=(seqlen_q, seqlen_k), block_shape=(BLOCK_M,\n            BLOCK_N), strides=(stride_bm, stride_bn), offsets=(start_m *\n            BLOCK_M, 0), order=(0, 1))\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    max_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    for j in range(0, seqlen_k, BLOCK_N):\n        j = tl.multiple_of(j, BLOCK_N)\n        k = tl.load(K_Block_ptr, boundary_check=(0, 1))\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k) * softmax_scale\n        if not EVEN_N:\n            qk += tl.where((j + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n        if HAVE_BIAS:\n            b = tl.load(B_Block_ptr, boundary_check=(0, 1))\n            B_Block_ptr = tl.advance(B_Block_ptr, (0, BLOCK_N))\n            qk = qk + b\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        else:\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(max_i - max_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v = tl.load(V_Block_ptr, boundary_check=(0, 1))\n        acc_o += tl.dot(p, v)\n        max_i = max_ij\n        lse_i = max_ij + tl.log(tl.exp(lse_i - max_ij) + l_ij)\n        K_Block_ptr = tl.advance(K_Block_ptr, (0, BLOCK_N))\n        V_Block_ptr = tl.advance(V_Block_ptr, (BLOCK_N, 0))\n    o_scale = tl.exp(max_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    tl.store(L_Block_ptr, lse_i, boundary_check=(0,))\n    tl.store(O_Block_ptr, acc_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _fwd_gqa_attn_kernel_ptr_block(Q, K, V, B, softmax_scale:\n    'tl.constexpr', stride_qb, stride_qh, stride_qg, stride_qm, stride_kb,\n    stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb,\n    stride_bh, stride_bg, stride_bm, stride_bn, stride_ob, stride_oh,\n    stride_og, stride_om, stride_lb, stride_lh, stride_lg, headdim:\n    'tl.constexpr', num_kv_heads: 'tl.constexpr', num_groups:\n    'tl.constexpr', seqlen_q, seqlen_k, O, L, HAVE_BIAS: 'tl.constexpr',\n    BIAS_SINGLE_HEAD: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m, off_bh, off_gp = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    off_h = off_bh % num_kv_heads\n    off_b = off_bh // num_kv_heads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (off_b * stride_qb + off_h * stride_qh + off_gp * stride_qg\n        ) + (offs_m[:, None] * stride_qm + offs_d[None, :])\n    o_ptrs = O + (off_b * stride_ob + off_h * stride_oh + off_gp * stride_og\n        ) + (offs_m[:, None] * stride_om + offs_d[None, :])\n    l_ptrs = L + (off_b * stride_lb + off_h * stride_lh + offs_m + off_gp *\n        stride_lg)\n    k_ptrs = K + (off_b * stride_kb + off_h * stride_kh) + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + (off_b * stride_vb + off_h * stride_vh) + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    softmax_scale = softmax_scale\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = (0 if BIAS_SINGLE_HEAD else off_h *\n            stride_bh + off_gp * stride_bg)\n        b_ptrs = B + (off_b * stride_bb + bias_h_pos) + (offs_m[:, None] *\n            stride_bm + offs_n[None, :] * stride_bn)\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    max_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    for j in range(0, seqlen_k, BLOCK_N):\n        j = tl.multiple_of(j, BLOCK_N)\n        current_k = offs_n + j\n        k = tl.load(k_ptrs + j * stride_kn, mask=(current_k[:, None] <\n            seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k.T) * softmax_scale\n        if not EVEN_N:\n            qk += tl.where((j + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n        if HAVE_BIAS:\n            b = tl.load(b_ptrs + j, mask=(offs_m[:, None] < seqlen_q) & (\n                current_k[None, :] < seqlen_k), other=0.0)\n            qk = qk + b\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        else:\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(max_i - max_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v = tl.load(v_ptrs + j * stride_vn, mask=(current_k[:, None] <\n            seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n        acc_o += tl.dot(p, v)\n        max_i = max_ij\n        lse_i = max_ij + tl.log(tl.exp(lse_i - max_ij) + l_ij)\n    o_scale = tl.exp(max_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    tl.store(l_ptrs, lse_i, mask=offs_m < seqlen_q)\n    tl.store(o_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n        None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_do_attn_kernel(O, Do, De, stride_ob: 'int', stride_om: 'int',\n    stride_oh: 'int', stride_dob: 'int', stride_dom: 'int', stride_doh:\n    'int', stride_deb: 'int', stride_deh: 'int', nheads: 'int', headdim:\n    'int', seqlen_q: 'int', BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    \"\"\"Triton kernel for the backward pass of the attention mechanism with respect to the output gradient.\n\n\tArgs:\n\t\tO: Output array.\n\t\tDo: Output gradient array.\n\t\tDe: Delta array.\n\t\tstride_ob: Stride for the output batch dimension.\n\t\tstride_om: Stride for the output sequence dimension.\n\t\tstride_oh: Stride for the output head dimension.\n\t\tstride_dob: Stride for the output gradient batch dimension.\n\t\tstride_dom: Stride for the output gradient sequence dimension.\n\t\tstride_doh: Stride for the output gradient head dimension.\n\t\tstride_deb: Stride for the delta batch dimension.\n\t\tstride_deh: Stride for the delta head dimension.\n\t\tnheads: Number of heads.\n\t\theaddim: Head dimension.\n\t\tseqlen_q: Sequence length of the query.\n\t\tBLOCK_M: Block size for the query sequence dimension.\n\t\tBLOCK_HEADDIM: Block size for the head dimension.\n\t\"\"\"\n    off_q = tl.program_id(0)\n    off_bh = tl.program_id(1)\n    off_b = off_bh // nheads\n    off_h = off_bh % nheads\n    offs_m = off_q * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o_ptrs = O + off_b * stride_ob + off_h * stride_oh + offs_m[:, None\n        ] * stride_om + offs_d[None, :]\n    do_ptrs = Do + off_b * stride_dob + off_h * stride_doh + offs_m[:, None\n        ] * stride_dom + offs_d[None, :]\n    o = tl.load(o_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    do = tl.load(do_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None,\n        :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(De + (off_b * stride_deb + off_h * stride_deh + offs_m), delta,\n        mask=offs_m < seqlen_q)\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_attn_kernel(Q, K, V, B, Do, L, D, softmax_scale: 'float',\n    stride_qb: 'int', stride_qm: 'int', stride_qh: 'int', stride_kb: 'int',\n    stride_kn: 'int', stride_kh: 'int', stride_vb: 'int', stride_vn: 'int',\n    stride_vh: 'int', stride_bb: 'int', stride_bh: 'int', stride_bm: 'int',\n    stride_dob: 'int', stride_dom: 'int', stride_doh: 'int', stride_dqb:\n    'int', stride_dqm: 'int', stride_dqh: 'int', stride_dkb: 'int',\n    stride_dkn: 'int', stride_dkh: 'int', stride_dvb: 'int', stride_dvn:\n    'int', stride_dvh: 'int', stride_lb: 'int', stride_lh: 'int', seqlen_q:\n    'int', seqlen_k: 'int', headdim: 'int', nheads: 'int', Dq: 'chex.Array',\n    Dk: 'chex.Array', Dv: 'chex.Array', HAVE_BIAS: 'tl.constexpr',\n    BIAS_SINGLE_HEAD: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_n, off_bh = tl.program_id(0), tl.program_id(2)\n    softmax_scale = softmax_scale\n    off_h = off_bh % nheads\n    off_b = off_bh // nheads\n    offs_qm = tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    l_ptrs = L + (off_b * stride_lb + off_h * stride_lh + offs_qm)\n    d_ptrs = D + (off_b * stride_lb + off_h * stride_lh + offs_qm)\n    q_ptrs = Q + (off_b * stride_qb + off_h * stride_qh) + (offs_qm[:, None\n        ] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (off_b * stride_kb + off_h * stride_kh) + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + (off_b * stride_vb + off_h * stride_vh) + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    do_ptrs = Do + (off_b * stride_dob + off_h * stride_doh) + (offs_qm[:,\n        None] * stride_dom + offs_d[None, :])\n    dq_ptrs = Dq + (off_b * stride_dqb + off_h * stride_dqh) + (offs_qm[:,\n        None] * stride_dqm + offs_d[None, :])\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = 0 if BIAS_SINGLE_HEAD else off_h\n        b_ptrs = B + (off_b * stride_bb + bias_h_pos * stride_bh) + (\n            offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(0, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k.T) * softmax_scale\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if HAVE_BIAS:\n            bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk + bias\n        lse_i = tl.load(l_ptrs + start_m, mask=offs_m_curr < seqlen_q,\n            other=0.0)\n        p = tl.exp(qk - lse_i[:, None])\n        do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p.T, do)\n        dp = tl.dot(do, v.T)\n        Di = tl.load(d_ptrs + start_m, mask=offs_m_curr < seqlen_q, other=0.0)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds.T, q)\n        dq = tl.dot(ds, k)\n        tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q) &\n            (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if HAVE_BIAS:\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = Dv + (off_b * stride_dvb + off_h * stride_dvh) + (offs_n[:,\n        None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = Dk + (off_b * stride_dkb + off_h * stride_dkh) + (offs_n[:,\n        None] * stride_dkn + offs_d[None, :])\n    tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n    tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _fwd_attn_kernel_block_ptr(Q, K, V, B, softmax_scale: 'tl.constexpr',\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_bn, stride_ob, stride_oh, stride_om, stride_lb, stride_lh,\n    headdim: 'tl.constexpr', nheads: 'tl.constexpr', seqlen_q, seqlen_k, O,\n    L, HAVE_BIAS: 'tl.constexpr', BIAS_SINGLE_HEAD: 'tl.constexpr',\n    BLOCK_HEADDIM: 'tl.constexpr', EVEN_N: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m, off_bh = tl.program_id(0), tl.program_id(1)\n    off_h = off_bh % nheads\n    off_b = off_bh // nheads\n    if not EVEN_N:\n        offs_n = tl.arange(0, BLOCK_N)\n    Q_Block_ptr = tl.make_block_ptr(base=Q + (off_b * stride_qb + off_h *\n        stride_qh), shape=(seqlen_q, headdim), block_shape=(BLOCK_M,\n        BLOCK_HEADDIM), strides=(stride_qm, 1), offsets=(start_m * BLOCK_M,\n        0), order=(0, 1))\n    O_Block_ptr = tl.make_block_ptr(base=O + (off_b * stride_ob + off_h *\n        stride_oh), shape=(seqlen_q, headdim), block_shape=(BLOCK_M,\n        BLOCK_HEADDIM), strides=(stride_om, 1), offsets=(start_m * BLOCK_M,\n        0), order=(0, 1))\n    L_Block_ptr = tl.make_block_ptr(base=L + (off_b * stride_lb + off_h *\n        stride_lh), shape=(seqlen_q,), strides=(1,), offsets=(start_m *\n        BLOCK_M,), block_shape=(BLOCK_M,), order=(0,))\n    kv_stride = off_b * stride_kb + off_h * stride_kh\n    K_Block_ptr = tl.make_block_ptr(base=K + kv_stride, shape=(headdim,\n        seqlen_k), block_shape=(BLOCK_HEADDIM, BLOCK_N), strides=(1,\n        stride_kn), offsets=(0, 0), order=(1, 0))\n    V_Block_ptr = tl.make_block_ptr(base=V + kv_stride, shape=(seqlen_k,\n        headdim), block_shape=(BLOCK_N, BLOCK_HEADDIM), strides=(stride_vn,\n        1), offsets=(0, 0), order=(0, 1))\n    q = tl.load(Q_Block_ptr, boundary_check=(0, 1))\n    softmax_scale = softmax_scale\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = 0 if BIAS_SINGLE_HEAD else off_h\n        B_Block_ptr = tl.make_block_ptr(base=B + (off_b * stride_bb + \n            bias_h_pos * stride_bh), shape=(seqlen_q, seqlen_k),\n            block_shape=(BLOCK_M, BLOCK_N), strides=(stride_bm, stride_bn),\n            offsets=(start_m * BLOCK_M, 0), order=(0, 1))\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    max_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    for j in range(0, seqlen_k, BLOCK_N):\n        j = tl.multiple_of(j, BLOCK_N)\n        k = tl.load(K_Block_ptr, boundary_check=(0, 1))\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k) * softmax_scale\n        if not EVEN_N:\n            qk += tl.where((j + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n        if HAVE_BIAS:\n            b = tl.load(B_Block_ptr, boundary_check=(0, 1))\n            B_Block_ptr = tl.advance(B_Block_ptr, (0, BLOCK_N))\n            qk = qk + b\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        else:\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(max_i - max_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v = tl.load(V_Block_ptr, boundary_check=(0, 1))\n        acc_o += tl.dot(p, v)\n        max_i = max_ij\n        lse_i = max_ij + tl.log(tl.exp(lse_i - max_ij) + l_ij)\n        K_Block_ptr = tl.advance(K_Block_ptr, (0, BLOCK_N))\n        V_Block_ptr = tl.advance(V_Block_ptr, (BLOCK_N, 0))\n    o_scale = tl.exp(max_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    tl.store(L_Block_ptr, lse_i, boundary_check=(0,))\n    tl.store(O_Block_ptr, acc_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _fwd_attn_kernel_ptr_block(Q, K, V, B, softmax_scale: 'tl.constexpr',\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_bn, stride_ob, stride_oh, stride_om, stride_lb, stride_lh,\n    headdim: 'tl.constexpr', nheads: 'tl.constexpr', seqlen_q, seqlen_k, O,\n    L, HAVE_BIAS: 'tl.constexpr', BIAS_SINGLE_HEAD: 'tl.constexpr',\n    BLOCK_HEADDIM: 'tl.constexpr', EVEN_N: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m, off_bh = tl.program_id(0), tl.program_id(1)\n    off_h = off_bh % nheads\n    off_b = off_bh // nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (off_b * stride_qb + off_h * stride_qh) + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    o_ptrs = O + (off_b * stride_ob + off_h * stride_oh) + (offs_m[:, None] *\n        stride_om + offs_d[None, :])\n    l_ptrs = L + (off_b * stride_lb + off_h * stride_lh + offs_m)\n    k_ptrs = K + (off_b * stride_kb + off_h * stride_kh) + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + (off_b * stride_vb + off_h * stride_vh) + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    softmax_scale = softmax_scale\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = 0 if BIAS_SINGLE_HEAD else off_h\n        b_ptrs = B + (off_b * stride_bb + bias_h_pos * stride_bh) + (offs_m\n            [:, None] * stride_bm + offs_n[None, :] * stride_bn)\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    max_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    for j in range(0, seqlen_k, BLOCK_N):\n        j = tl.multiple_of(j, BLOCK_N)\n        current_k = offs_n + j\n        k = tl.load(k_ptrs + j * stride_kn, mask=(current_k[:, None] <\n            seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k.T) * softmax_scale\n        if not EVEN_N:\n            qk += tl.where((j + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n        if HAVE_BIAS:\n            b = tl.load(b_ptrs + j, mask=(offs_m[:, None] < seqlen_q) & (\n                current_k[None, :] < seqlen_k), other=0.0)\n            qk = qk + b\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        else:\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(max_i - max_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v = tl.load(v_ptrs + j * stride_vn, mask=(current_k[:, None] <\n            seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n        acc_o += tl.dot(p, v)\n        max_i = max_ij\n        lse_i = max_ij + tl.log(tl.exp(lse_i - max_ij) + l_ij)\n    o_scale = tl.exp(max_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    tl.store(l_ptrs, lse_i, mask=offs_m < seqlen_q)\n    tl.store(o_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n        None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _sample_2d(image, w, batch_index, ix, iy, IH: 'tl.constexpr', IW:\n    'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    val = tl.view(tl.load((image + batch_index * IW * IH * C + iy_ * IW * C +\n        ix_ * C)[:, None] + Coffs[None, :]), (BLOCK_SIZE, C))\n    return val * tl.view((w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW\n        )))[:, None] * channel_bcast, (BLOCK_SIZE, C))\n"
    },
    {
      "input": "@triton.jit\ndef _is_in_bounds(x, y, z, W: 'tl.constexpr', H: 'tl.constexpr', D:\n    'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (x + 1) / 2 * W - 0.5\n    iy = (y + 1) / 2 * H - 0.5\n    iz = (z + 1) / 2 * D - 0.5\n    in_bounds = (iy >= 0) * (iy < H) * (ix >= 0) * (ix < W) * (iz >= 0) * (iz <\n        D)\n    in_bounds_mask = tl.broadcast_to(in_bounds[:, None], (BLOCK_SIZE, C))\n    return in_bounds_mask\n"
    },
    {
      "input": "@triton.jit\ndef _hash(x):\n    x = (x >> 16 ^ x) * 73244475\n    x = (x >> 16 ^ x) * 73244475\n    x = x >> 16 ^ x\n    return x\n"
    },
    {
      "input": "@triton.jit\ndef _pair_hash(x, h):\n    h = h ^ x\n    h = (h << 24) + h * 403\n    return h\n"
    },
    {
      "input": "@triton.jit\ndef _int_to_randn(x1, x2, seed):\n    x_hash_1 = _hash(x1)\n    x_hash_2 = _hash(x2)\n    x_hash_1 = _pair_hash(_pair_hash(2166136261, seed), x_hash_1)\n    x_hash_2 = _pair_hash(_pair_hash(2166136261, seed + 1), x_hash_2)\n    x_01_1 = (x_hash_1 + 10) / (4294967295.0 + 10)\n    x_01_2 = (x_hash_2 + 10) / (4294967295.0 + 10)\n    z = tl.sqrt(-2 * tl.log(x_01_1)) * tl.cos(6.28318530718 * x_01_2)\n    return z\n"
    },
    {
      "input": "@triton.jit\ndef _int_to_randn_kernel(x1, x2, out, N: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', seed: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) < N\n    x1_buffer = tl.load(x1 + offs, mask=offs_mask)\n    x2_buffer = tl.load(x2 + offs, mask=offs_mask)\n    seed_buffer = tl.full((BLOCK_SIZE,), seed, dtype=tl.int64)\n    r = _int_to_randn(x1_buffer, x2_buffer, seed_buffer)\n    tl.store(out + offs, r, mask=offs_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _grid_sample(image, batch_index, ix, iy, N: 'tl.constexpr', C:\n    'tl.constexpr', IH: 'tl.constexpr', IW: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    ix_nw = ix - ix % 1\n    iy_nw = iy - iy % 1\n    ix_ne = ix_nw + 1\n    iy_ne = iy_nw\n    ix_sw = ix_nw\n    iy_sw = iy_nw + 1\n    ix_se = ix_nw + 1\n    iy_se = iy_nw + 1\n    nw = (ix_se - ix) * (iy_se - iy)\n    ne = (ix - ix_sw) * (iy_sw - iy)\n    sw = (ix_ne - ix) * (iy - iy_ne)\n    se = (ix - ix_nw) * (iy - iy_nw)\n    out_val = _sample_2d(image, nw, batch_index, ix_nw, iy_nw, IH, IW, C,\n        BLOCK_SIZE) + _sample_2d(image, ne, batch_index, ix_ne, iy_ne, IH,\n        IW, C, BLOCK_SIZE) + _sample_2d(image, se, batch_index, ix_se,\n        iy_se, IH, IW, C, BLOCK_SIZE) + _sample_2d(image, sw, batch_index,\n        ix_sw, iy_sw, IH, IW, C, BLOCK_SIZE)\n    return out_val\n"
    },
    {
      "input": "@triton.jit\ndef _splat_2d(to_splat, grad_image, w, batch_index, ix, iy, IH:\n    'tl.constexpr', IW: 'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    w = tl.view((w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW)))[:,\n        None] * channel_bcast, (BLOCK_SIZE, C))\n    offs = tl.view((batch_index * IW * IH * C + iy_ * IW * C + ix_ * C)[:,\n        None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n"
    },
    {
      "input": "@triton.jit\ndef _grid_splat(to_splat, grad_image, batch_index, ix, iy, N:\n    'tl.constexpr', C: 'tl.constexpr', IH: 'tl.constexpr', IW:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    ix_nw = ix - ix % 1\n    iy_nw = iy - iy % 1\n    ix_ne = ix_nw + 1\n    iy_ne = iy_nw\n    ix_sw = ix_nw\n    iy_sw = iy_nw + 1\n    ix_se = ix_nw + 1\n    iy_se = iy_nw + 1\n    nw = (ix_se - ix) * (iy_se - iy)\n    ne = (ix - ix_sw) * (iy_sw - iy)\n    sw = (ix_ne - ix) * (iy - iy_ne)\n    se = (ix - ix_nw) * (iy - iy_nw)\n    _splat_2d(to_splat, grad_image, nw, batch_index, ix_nw, iy_nw, IH, IW,\n        C, BLOCK_SIZE)\n    _splat_2d(to_splat, grad_image, ne, batch_index, ix_ne, iy_ne, IH, IW,\n        C, BLOCK_SIZE)\n    _splat_2d(to_splat, grad_image, sw, batch_index, ix_sw, iy_sw, IH, IW,\n        C, BLOCK_SIZE)\n    _splat_2d(to_splat, grad_image, se, batch_index, ix_se, iy_se, IH, IW,\n        C, BLOCK_SIZE)\n"
    },
    {
      "input": "@triton.jit\ndef _splat_3d(to_splat, grad_image, w, batch_index, ix, iy, iz, ID:\n    'tl.constexpr', IH: 'tl.constexpr', IW: 'tl.constexpr', C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    w = tl.view(w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW) * (iz <\n        ID) * (iz >= 0))[:, None] * channel_bcast, (BLOCK_SIZE, C))\n    offs = tl.view((batch_index * ID * IW * IH * C + iz_ * IW * IH * C + \n        iy_ * IW * C + ix_ * C)[:, None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n"
    },
    {
      "input": "@triton.jit\ndef _voxel_grid_splat(to_splat, grad_image, batch_index, ix, iy, iz, N:\n    'tl.constexpr', C: 'tl.constexpr', ID: 'tl.constexpr', IH:\n    'tl.constexpr', IW: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    iz = (iz + 1) / 2 * ID - 0.5\n    ix0 = ix - ix % 1\n    iy0 = iy - iy % 1\n    iz0 = iz - iz % 1\n    V000x = ix0\n    V000y = iy0\n    V000z = iz0\n    V100x = ix0\n    V100y = iy0\n    V100z = iz0 + 1\n    V010x = ix0\n    V010y = iy0 + 1\n    V010z = iz0\n    V001x = ix0 + 1\n    V001y = iy0\n    V001z = iz0\n    V101x = ix0 + 1\n    V101y = iy0\n    V101z = iz0 + 1\n    V011x = ix0 + 1\n    V011y = iy0 + 1\n    V011z = iz0\n    V110x = ix0\n    V110y = iy0 + 1\n    V110z = iz0 + 1\n    V111x = ix0 + 1\n    V111y = iy0 + 1\n    V111z = iz0 + 1\n    x = ix - ix0\n    y = iy - iy0\n    z = iz - iz0\n    _splat_3d(to_splat, grad_image, (1 - x) * (1 - y) * (1 - z),\n        batch_index, V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, (1 - x) * (1 - y) * z, batch_index,\n        V100x, V100y, V100z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, (1 - x) * y * (1 - z), batch_index,\n        V010x, V010y, V010z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * (1 - y) * (1 - z), batch_index,\n        V001x, V001y, V001z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * (1 - y) * z, batch_index, V101x,\n        V101y, V101z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * y * (1 - z), batch_index, V011x,\n        V011y, V011z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, (1 - x) * y * z, batch_index, V110x,\n        V110y, V110z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * y * z, batch_index, V111x, V111y,\n        V111z, ID, IH, IW, C, BLOCK_SIZE)\n"
    },
    {
      "input": "@triton.jit\ndef _sample_3d(image, w, batch_index, ix, iy, iz, ID: 'tl.constexpr', IH:\n    'tl.constexpr', IW: 'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    val = tl.view(tl.load((image + batch_index * ID * IW * IH * C + iz_ *\n        IW * IH * C + iy_ * IW * C + ix_ * C)[:, None] + Coffs[None, :]), (\n        BLOCK_SIZE, C))\n    return val * tl.view((w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW\n        ) * (iz < ID) * (iz >= 0)))[:, None] * channel_bcast, (BLOCK_SIZE, C))\n"
    },
    {
      "input": "@triton.jit\ndef _voxel_grid_sample(image, batch_index, ix, iy, iz, N: 'tl.constexpr', C:\n    'tl.constexpr', ID: 'tl.constexpr', IH: 'tl.constexpr', IW:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    iz = (iz + 1) / 2 * ID - 0.5\n    ix0 = ix - ix % 1\n    iy0 = iy - iy % 1\n    iz0 = iz - iz % 1\n    V000x = ix0\n    V000y = iy0\n    V000z = iz0\n    V100x = ix0\n    V100y = iy0\n    V100z = iz0 + 1\n    V010x = ix0\n    V010y = iy0 + 1\n    V010z = iz0\n    V001x = ix0 + 1\n    V001y = iy0\n    V001z = iz0\n    V101x = ix0 + 1\n    V101y = iy0\n    V101z = iz0 + 1\n    V011x = ix0 + 1\n    V011y = iy0 + 1\n    V011z = iz0\n    V110x = ix0\n    V110y = iy0 + 1\n    V110z = iz0 + 1\n    V111x = ix0 + 1\n    V111y = iy0 + 1\n    V111z = iz0 + 1\n    x = ix - ix0\n    y = iy - iy0\n    z = iz - iz0\n    out_val = _sample_3d(image, (1 - x) * (1 - y) * (1 - z), batch_index,\n        V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(image,\n        (1 - x) * (1 - y) * z, batch_index, V100x, V100y, V100z, ID, IH, IW,\n        C, BLOCK_SIZE) + _sample_3d(image, (1 - x) * y * (1 - z),\n        batch_index, V010x, V010y, V010z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(image, x * (1 - y) * (1 - z), batch_index, V001x,\n        V001y, V001z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(image, x * (1 -\n        y) * z, batch_index, V101x, V101y, V101z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(image, x * y * (1 - z), batch_index, V011x, V011y,\n        V011z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(image, (1 - x) * y *\n        z, batch_index, V110x, V110y, V110z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(image, x * y * z, batch_index, V111x, V111y, V111z,\n        ID, IH, IW, C, BLOCK_SIZE)\n    return out_val\n"
    },
    {
      "input": "@triton.jit\ndef _sample_grid_rep(xy, yz, zx, batch_index, sample_x, sample_y, sample_z,\n    batch_size: 'tl.constexpr', C: 'tl.constexpr', D: 'tl.constexpr', H:\n    'tl.constexpr', W: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    shape_representation: 'tl.constexpr'):\n    if shape_representation == 0:\n        a = _grid_sample(xy, batch_index, sample_x, sample_y, batch_size, C,\n            H, W, BLOCK_SIZE)\n        b = _grid_sample(yz, batch_index, sample_y, sample_z, batch_size, C,\n            D, H, BLOCK_SIZE)\n        c = _grid_sample(zx, batch_index, sample_z, sample_x, batch_size, C,\n            W, D, BLOCK_SIZE)\n        vec = a + b + c\n    else:\n        vec = _voxel_grid_sample(xy, batch_index, sample_x, sample_y,\n            sample_z, batch_size, C, D, H, W, BLOCK_SIZE)\n    vec = tl.view(vec, (BLOCK_SIZE, C))\n    return vec\n"
    },
    {
      "input": "@triton.jit\ndef _splat_grid_rep(to_splat, xy, yz, zx, batch_index, sample_x, sample_y,\n    sample_z, batch_size: 'tl.constexpr', C: 'tl.constexpr', D:\n    'tl.constexpr', H: 'tl.constexpr', W: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', shape_representation: 'tl.constexpr'):\n    if shape_representation == 0:\n        _grid_splat(to_splat, xy, batch_index, sample_x, sample_y,\n            batch_size, C, H, W, BLOCK_SIZE)\n        _grid_splat(to_splat, yz, batch_index, sample_y, sample_z,\n            batch_size, C, D, H, BLOCK_SIZE)\n        _grid_splat(to_splat, zx, batch_index, sample_z, sample_x,\n            batch_size, C, W, D, BLOCK_SIZE)\n    else:\n        _voxel_grid_splat(to_splat, xy, batch_index, sample_x, sample_y,\n            sample_z, batch_size, C, D, H, W, BLOCK_SIZE)\n"
    },
    {
      "input": "@triton.jit\ndef _color_activation(x):\n    return tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.jit\ndef _d_color_activation(dy, x):\n    return dy * tl.sigmoid(x) * (1 - tl.sigmoid(x))\n"
    },
    {
      "input": "@triton.jit\ndef _softplus(x):\n    return tl.where(x >= 0, x + tl.log(1 + tl.exp(-x)), tl.log(1 + tl.exp(x)))\n"
    },
    {
      "input": "@triton.jit\ndef _d_softplus(grad, x):\n    z = tl.where(x >= 0, 1 / (1 + tl.exp(-x)), 1 - 1 / (1 + tl.exp(x)))\n    return grad * z\n"
    },
    {
      "input": "@triton.jit\ndef _d_linear(d_y, w, b, x):\n    d_x = tl.dot(d_y, tl.trans(w), allow_tf32=ALLOW_TF32)\n    d_w = tl.dot(tl.trans(d_y), x, allow_tf32=ALLOW_TF32)\n    d_b = tl.sum(d_y, axis=0)\n    return d_x, d_w, d_b\n"
    },
    {
      "input": "@triton.jit\ndef _d_linear_relu(d_y, w, b, xwb, x):\n    d_y_relu = d_y * (xwb > 0.0)\n    return _d_linear(d_y_relu, w, b, x)\n"
    },
    {
      "input": "@triton.jit\ndef _load_mlp_bias_bcast(biases, C, offs, BLOCK_SIZE):\n    return tl.view(tl.load((biases + offs + tl.arange(0, C))[None, :] + tl.\n        zeros((BLOCK_SIZE, 1), dtype=tl.int32)), (BLOCK_SIZE, C))\n"
    },
    {
      "input": "@triton.jit\ndef _contract_pi_one(x, n, perc_foreground):\n    x_c = tl.where(n <= 1.0, x, tl.where(tl.abs(tl.abs(x) - n) <= 1e-08, (\n        1.0 / perc_foreground - (1.0 / perc_foreground - 1) / tl.abs(x)) *\n        (x / tl.abs(x)), x / n))\n    x_c = x_c * perc_foreground\n    return x_c\n"
    },
    {
      "input": "@triton.jit\ndef _contract_pi(x, y, z, perc_foreground):\n    n = tl.maximum(tl.maximum(tl.abs(x), tl.abs(y)), tl.abs(z))\n    x_c = _contract_pi_one(x, n, perc_foreground)\n    y_c = _contract_pi_one(y, n, perc_foreground)\n    z_c = _contract_pi_one(z, n, perc_foreground)\n    return x_c, y_c, z_c\n"
    },
    {
      "input": "@triton.jit\ndef _depth_inv_sphere(far, disparity_at_inf, n, step):\n    frac_step = (step + 1) / n\n    n_disp = (disparity_at_inf - 1) * frac_step + 1\n    return far * (1 / n_disp)\n"
    },
    {
      "input": "@triton.jit\ndef _depth_lin(near, far, n, step):\n    frac_step = step / (n - 1)\n    return (far - near) * frac_step + near\n"
    },
    {
      "input": "@triton.jit\ndef _fw_kernel(xy, yz, zx, xy_color, yz_color, zx_color, rays, centers,\n    weights, biases, weight_opacity, bias_opacity, weight_color, bias_color,\n    rays_encoding, negative_log_transmittance, expected_depth,\n    expected_features, near, far, effective_num_samples, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', gain: 'tl.constexpr',\n    batch_size: 'tl.constexpr', num_rays_per_batch: 'tl.constexpr', C:\n    'tl.constexpr', OUT_C: 'tl.constexpr', H: 'tl.constexpr', W:\n    'tl.constexpr', D: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    transmittance_thr: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr', inject_noise: 'tl.constexpr', inject_noise_sigma:\n    'tl.constexpr', inject_noise_seed, contract_coords: 'tl.constexpr',\n    contract_perc_foreground: 'tl.constexpr', disparity_at_inf:\n    'tl.constexpr', shape_representation: 'tl.constexpr', activation_fun:\n    'tl.constexpr', use_separate_color_rep: 'tl.constexpr'):\n    tot_num_samples = num_samples + num_samples_inf\n    num_rays = num_rays_per_batch * batch_size\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = pid * BLOCK_SIZE * OUT_C + OUT_C * tl.arange(0, BLOCK_SIZE\n        )[:, None] + tl.arange(0, OUT_C)[None, :]\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    center_x = tl.load(centers + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(centers + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(centers + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(rays + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(rays + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(rays + offs_z, mask=offs_z < num_rays * 3)\n    batch_index = (pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        ) // num_rays_per_batch\n    near_buffer = tl.load(near + offs, mask=offs < num_rays)\n    far_buffer = tl.load(far + offs, mask=offs < num_rays)\n    effective_num_samples_buffer = tl.zeros((1,), dtype=tl.int32)\n    depth = near_buffer\n    seed_buffer = tl.load(inject_noise_seed + offs, mask=offs < num_rays)\n    sample_index_buffer = tl.arange(0, BLOCK_SIZE\n        ) * tot_num_samples + pid * BLOCK_SIZE * tot_num_samples + 1\n    expected_depth_buffer = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    expected_features_buffer = tl.zeros((BLOCK_SIZE, OUT_C), dtype=tl.float32)\n    prev_transmittance = tl.full((BLOCK_SIZE,), 1.0, dtype=tl.float32)\n    negative_log_transmittance_buffer = tl.zeros((BLOCK_SIZE,), dtype=tl.\n        float32)\n    w1, w2, wr, wo, wc, b1, b2, br, bo, bc, w2c, b2c = _load_mlp_weights(\n        weights, biases, weight_opacity, bias_opacity, weight_color,\n        bias_color, C, OUT_C, BLOCK_SIZE)\n    rays_encoding_buffer = tl.load(rays_encoding + pid * BLOCK_SIZE * C + C *\n        tl.arange(0, BLOCK_SIZE)[:, None] + tl.arange(0, C)[None, :])\n    transmittance = tl.exp(-negative_log_transmittance_buffer)\n    zero_value = tl.zeros((BLOCK_SIZE,), tl.float32)\n    zero_color = tl.zeros((BLOCK_SIZE, OUT_C), tl.float32)\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = _depth_lin(near_buffer, far_buffer, num_samples, step)\n            depth_prev = _depth_lin(near_buffer, far_buffer, num_samples, \n                step - 1)\n        else:\n            depth = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n            depth_prev = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples - 1)\n        delta = depth - depth_prev\n        if tl.sum(transmittance > transmittance_thr, axis=0):\n            sample_x = center_x + depth * ray_x\n            sample_y = center_y + depth * ray_y\n            sample_z = center_z + depth * ray_z\n            if contract_coords:\n                sample_x, sample_y, sample_z = _contract_pi(sample_x,\n                    sample_y, sample_z, contract_perc_foreground)\n            vec = _sample_grid_rep(xy, yz, zx, batch_index, sample_x,\n                sample_y, sample_z, batch_size, C, D, H, W, BLOCK_SIZE,\n                shape_representation)\n            if mask_out_of_bounds_samples:\n                in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z,\n                    W, H, D, C, BLOCK_SIZE)\n                vec = vec * in_bounds_mask\n            vec = tl.maximum(tl.dot(vec, w1, allow_tf32=ALLOW_TF32) + b1, 0.0)\n            vec = tl.maximum(tl.dot(vec, w2, allow_tf32=ALLOW_TF32) + b2, 0.0)\n            value = tl.view(tl.sum(wo * vec, axis=1), (BLOCK_SIZE,)) + bo\n            if inject_noise:\n                r = _int_to_randn(sample_index_buffer, sample_index_buffer +\n                    num_rays * tot_num_samples, seed_buffer)\n                value = value + r * inject_noise_sigma\n            if activation_fun == 0:\n                value_act = _softplus(value)\n            else:\n                value_act = tl.maximum(value, 0.0)\n            value = delta * gain * value_act\n            if use_separate_color_rep:\n                vec_color = _sample_grid_rep(xy_color, yz_color, zx_color,\n                    batch_index, sample_x, sample_y, sample_z, batch_size,\n                    C, D, H, W, BLOCK_SIZE, shape_representation)\n                vec_color = vec_color + rays_encoding_buffer\n                if mask_out_of_bounds_samples:\n                    in_bounds_mask = _is_in_bounds(sample_x, sample_y,\n                        sample_z, W, H, D, C, BLOCK_SIZE)\n                    vec_color = vec_color * in_bounds_mask\n                vec_color1 = tl.maximum(tl.dot(vec_color, wr, allow_tf32=\n                    ALLOW_TF32) + br, 0.0)\n                vec_color2 = tl.maximum(tl.dot(vec_color1, w2c, allow_tf32=\n                    ALLOW_TF32) + b2c, 0.0)\n                log_color = tl.dot(vec_color2, wc, allow_tf32=ALLOW_TF32) + bc\n            else:\n                vecr = tl.maximum(tl.dot(vec, wr, allow_tf32=ALLOW_TF32) +\n                    br + rays_encoding_buffer, 0.0)\n                log_color = tl.dot(vecr, wc, allow_tf32=ALLOW_TF32) + bc\n            color = _color_activation(log_color)\n            effective_ns_increment = 1\n        else:\n            value = zero_value\n            color = zero_color\n            effective_ns_increment = 0\n        negative_log_transmittance_buffer = (\n            negative_log_transmittance_buffer + value)\n        transmittance = tl.exp(-negative_log_transmittance_buffer)\n        render_weights = prev_transmittance - transmittance\n        expected_depth_buffer = expected_depth_buffer + render_weights * depth\n        render_weights_bcast = tl.broadcast_to(prev_transmittance[:, None],\n            (BLOCK_SIZE, OUT_C)) - tl.broadcast_to(transmittance[:, None],\n            (BLOCK_SIZE, OUT_C))\n        feature_render = color * render_weights_bcast\n        expected_features_buffer = expected_features_buffer + feature_render\n        prev_transmittance = transmittance\n        sample_index_buffer = sample_index_buffer + 1\n        effective_num_samples_buffer = (effective_num_samples_buffer +\n            effective_ns_increment)\n    tl.store(negative_log_transmittance + offs,\n        negative_log_transmittance_buffer, mask=offs < num_rays)\n    tl.store(expected_depth + offs, expected_depth_buffer, mask=offs < num_rays\n        )\n    tl.store(effective_num_samples + pid + tl.arange(0, 1),\n        effective_num_samples_buffer)\n    tl.store(expected_features + offs_features, expected_features_buffer,\n        mask=offs_features_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _bw_kernel(xy, yz, zx, xy_color, yz_color, zx_color, rays, centers,\n    weights, biases, weight_opacity, bias_opacity, weight_color, bias_color,\n    rays_encoding, negative_log_transmittance, expected_depth,\n    expected_features, near, far, effective_num_samples, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', gain: 'tl.constexpr',\n    batch_size: 'tl.constexpr', num_rays_per_batch: 'tl.constexpr', C:\n    'tl.constexpr', OUT_C: 'tl.constexpr', H: 'tl.constexpr', W:\n    'tl.constexpr', D: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    transmittance_thr: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr', inject_noise: 'tl.constexpr', inject_noise_sigma:\n    'tl.constexpr', inject_noise_seed, contract_coords: 'tl.constexpr',\n    contract_perc_foreground: 'tl.constexpr', disparity_at_inf:\n    'tl.constexpr', shape_representation: 'tl.constexpr', activation_fun:\n    'tl.constexpr', use_separate_color_rep: 'tl.constexpr',\n    grad_negative_log_transmittance, grad_expected_depth,\n    grad_expected_features, grad_xy, grad_yz, grad_zx, grad_xy_color,\n    grad_yz_color, grad_zx_color, grad_weights, grad_biases,\n    grad_weight_opacity, grad_bias_opacity, grad_weight_color,\n    grad_bias_color, grad_rays_enc):\n    tot_num_samples = num_samples + num_samples_inf\n    num_rays = num_rays_per_batch * batch_size\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = offs < num_rays\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = pid * BLOCK_SIZE * OUT_C + OUT_C * tl.arange(0, BLOCK_SIZE\n        )[:, None] + tl.arange(0, OUT_C)[None, :]\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    offs_CC = tl.arange(0, C)[:, None] * C + tl.arange(0, C)[None, :]\n    center_x = tl.load(centers + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(centers + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(centers + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(rays + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(rays + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(rays + offs_z, mask=offs_z < num_rays * 3)\n    rays_enc_offs = pid * BLOCK_SIZE * C + C * tl.arange(0, BLOCK_SIZE)[:, None\n        ] + tl.arange(0, C)[None, :]\n    rays_enc_mask = rays_enc_offs < num_rays * C\n    rays_encoding_buffer = tl.load(rays_encoding + rays_enc_offs, mask=\n        rays_enc_mask)\n    batch_index = (pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        ) // num_rays_per_batch\n    near_buffer = tl.load(near + offs, mask=offs < num_rays)\n    far_buffer = tl.load(far + offs, mask=offs < num_rays)\n    seed_buffer = tl.load(inject_noise_seed + offs, mask=offs < num_rays)\n    sample_index_buffer = (tl.arange(0, BLOCK_SIZE) * tot_num_samples + pid *\n        BLOCK_SIZE * tot_num_samples + 1 + tot_num_samples - 1)\n    depth = far_buffer\n    grad_negative_log_transmittance_buffer = tl.load(\n        grad_negative_log_transmittance + offs, mask=offs_mask, other=0.0)\n    grad_expected_features_buffer = tl.load(grad_expected_features +\n        offs_features, mask=offs_features_mask, other=0.0)\n    grad_expected_depth_buffer = tl.load(grad_expected_depth + offs, mask=\n        offs_mask, other=0.0)\n    prev_proj_depth = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    prev_proj_features = tl.zeros((BLOCK_SIZE, OUT_C), dtype=tl.float32)\n    negative_log_transmittance_buffer = tl.load(negative_log_transmittance +\n        offs, mask=offs_mask, other=0.0)\n    transmittance = tl.exp(-negative_log_transmittance_buffer)\n    prev_grad_opacity = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    d_w2 = tl.zeros((C, C), dtype=tl.float32)\n    d_w1 = tl.zeros((C, C), dtype=tl.float32)\n    d_b1 = tl.zeros((C,), dtype=tl.float32)\n    d_b2 = tl.zeros((C,), dtype=tl.float32)\n    d_w2c = tl.zeros((C, C), dtype=tl.float32)\n    d_b2c = tl.zeros((C,), dtype=tl.float32)\n    d_wr = tl.zeros((C, C), dtype=tl.float32)\n    d_br = tl.zeros((C,), dtype=tl.float32)\n    d_wo = tl.zeros((C,), dtype=tl.float32)\n    d_bo = tl.zeros((1,), dtype=tl.float32)\n    d_wc = tl.zeros((C, OUT_C), dtype=tl.float32)\n    d_wc = tl.zeros((OUT_C, C), dtype=tl.float32)\n    d_bc = tl.zeros((OUT_C,), dtype=tl.float32)\n    d_rays_enc = tl.zeros((BLOCK_SIZE, C), dtype=tl.float32)\n    zero_w = tl.zeros((C, C), dtype=tl.float32)\n    zero_b = tl.zeros((C,), dtype=tl.float32)\n    zero_vec = tl.zeros((BLOCK_SIZE, C), dtype=tl.float32)\n    w1, w2, wr, wo, wc, b1, b2, br, bo, bc, w2c, b2c = _load_mlp_weights(\n        weights, biases, weight_opacity, bias_opacity, weight_color,\n        bias_color, C, OUT_C, BLOCK_SIZE)\n    prev_transmittance = transmittance\n    for step in range(tot_num_samples):\n        if step < num_samples_inf:\n            depth = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, num_samples_inf - step - 1)\n            depth_prev = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, num_samples_inf - step - 2)\n        else:\n            depth = _depth_lin(near_buffer, far_buffer, num_samples, \n                num_samples - (step - num_samples_inf) - 1)\n            depth_prev = _depth_lin(near_buffer, far_buffer, num_samples, \n                num_samples - (step - num_samples_inf) - 2)\n        delta = depth - depth_prev\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = _contract_pi(sample_x, sample_y,\n                sample_z, contract_perc_foreground)\n        vec = _sample_grid_rep(xy, yz, zx, batch_index, sample_x, sample_y,\n            sample_z, batch_size, C, D, H, W, BLOCK_SIZE, shape_representation)\n        if mask_out_of_bounds_samples:\n            in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z, W,\n                H, D, C, BLOCK_SIZE)\n            vec = vec * in_bounds_mask\n        vec1 = tl.maximum(tl.dot(vec, w1, allow_tf32=ALLOW_TF32) + b1, 0.0)\n        vec2 = tl.maximum(tl.dot(vec1, w2, allow_tf32=ALLOW_TF32) + b2, 0.0)\n        value = tl.sum(vec2 * wo, axis=1) + bo\n        if inject_noise:\n            r = _int_to_randn(sample_index_buffer, sample_index_buffer + \n                num_rays * tot_num_samples, seed_buffer)\n            value = value + r * inject_noise_sigma\n        if activation_fun == 0:\n            value_act = _softplus(value)\n        else:\n            value_act = tl.maximum(value, 0.0)\n        delta_value = gain * value_act * delta\n        if use_separate_color_rep:\n            vec_color = _sample_grid_rep(xy_color, yz_color, zx_color,\n                batch_index, sample_x, sample_y, sample_z, batch_size, C, D,\n                H, W, BLOCK_SIZE, shape_representation)\n            vec_color = vec_color + rays_encoding_buffer\n            if mask_out_of_bounds_samples:\n                in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z,\n                    W, H, D, C, BLOCK_SIZE)\n                vec_color = vec_color * in_bounds_mask\n            vec_color1 = tl.maximum(tl.dot(vec_color, wr, allow_tf32=\n                ALLOW_TF32) + br, 0.0)\n            vecr = tl.maximum(tl.dot(vec_color1, w2c, allow_tf32=ALLOW_TF32\n                ) + b2c, 0.0)\n        else:\n            vecr = tl.maximum(tl.dot(vec2, wr, allow_tf32=ALLOW_TF32) + br +\n                rays_encoding_buffer, 0.0)\n        log_color = tl.dot(vecr, wc, allow_tf32=ALLOW_TF32) + bc\n        color = _color_activation(log_color)\n        proj_features = color * grad_expected_features_buffer\n        proj_depth = depth * grad_expected_depth_buffer\n        prev_transmittance = transmittance\n        opacity_grad_now = prev_transmittance * (proj_depth -\n            prev_proj_depth + tl.sum(proj_features - prev_proj_features,\n            axis=1))\n        prev_grad_opacity = prev_grad_opacity + opacity_grad_now\n        grad_value_act = delta * (prev_grad_opacity +\n            grad_negative_log_transmittance_buffer)\n        if activation_fun == 0:\n            grad_value_act = _d_softplus(grad_value_act, value)\n        else:\n            grad_value_act = grad_value_act * (value > 0.0)\n        grad_value = gain * grad_value_act\n        grad_value = tl.expand_dims(grad_value, 1)\n        d_wo_ = tl.sum(vec2 * tl.broadcast_to(grad_value, (BLOCK_SIZE, C)),\n            axis=0)\n        d_bo_ = tl.sum(grad_value, axis=0)\n        d_vec2_1 = wo * grad_value\n        negative_log_transmittance_buffer = (\n            negative_log_transmittance_buffer - delta_value)\n        transmittance = tl.exp(-negative_log_transmittance_buffer)\n        \"\"\"\n        transmittance_diff = tl.broadcast_to(\n            tl.view(transmittance, (BLOCK_SIZE, 1)), (BLOCK_SIZE, OUT_C)\n        ) - tl.broadcast_to(\n            tl.view(prev_transmittance, (BLOCK_SIZE, 1)), (BLOCK_SIZE, OUT_C)\n        )  # = rendering weights for the given step\n        \"\"\"\n        transmittance_diff = tl.broadcast_to(tl.expand_dims(transmittance, \n            1), (BLOCK_SIZE, OUT_C)) - tl.broadcast_to(tl.expand_dims(\n            prev_transmittance, 1), (BLOCK_SIZE, OUT_C))\n        d_color = grad_expected_features_buffer * transmittance_diff\n        d_log_color = _d_color_activation(d_color, log_color)\n        d_vecr, d_wc_, d_bc_ = _d_linear(d_log_color, wc, bc, vecr)\n        if use_separate_color_rep:\n            d_vec2_12 = tl.view(d_vec2_1, (BLOCK_SIZE, C))\n        else:\n            d_vec2_2, d_wr_, d_br_ = _d_linear_relu(d_vecr, wr, br, vecr, vec2)\n            d_vec2_12 = tl.view(d_vec2_1, (BLOCK_SIZE, C)) + tl.view(d_vec2_2,\n                (BLOCK_SIZE, C))\n        d_vec1, d_w2_, d_b2_ = _d_linear_relu(d_vec2_12, w2, b2, vec2, vec1)\n        d_vec, d_w1_, d_b1_ = _d_linear_relu(d_vec1, w1, b1, vec1, vec)\n        if mask_out_of_bounds_samples:\n            in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z, W,\n                H, D, C, BLOCK_SIZE)\n            d_vec = d_vec * in_bounds_mask\n        _splat_grid_rep(d_vec, grad_xy, grad_yz, grad_zx, batch_index,\n            sample_x, sample_y, sample_z, batch_size, C, D, H, W,\n            BLOCK_SIZE, shape_representation)\n        if use_separate_color_rep:\n            d_vec_color1, d_w2c_, d_b2c_ = _d_linear_relu(d_vecr, w2c, b2c,\n                vecr, vec_color1)\n            d_vec_color, d_wr_, d_br_ = _d_linear_relu(d_vec_color1, wr, br,\n                vec_color1, vec_color)\n            if mask_out_of_bounds_samples:\n                in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z,\n                    W, H, D, C, BLOCK_SIZE)\n                d_vec_color = d_vec_color * in_bounds_mask\n            d_rays_enc_ = tl.view(d_vec_color, (BLOCK_SIZE, C))\n            _splat_grid_rep(d_vec_color, grad_xy_color, grad_yz_color,\n                grad_zx_color, batch_index, sample_x, sample_y, sample_z,\n                batch_size, C, D, H, W, BLOCK_SIZE, shape_representation)\n        else:\n            d_vec_color = zero_vec\n            d_w2c_ = zero_w\n            d_b2c_ = zero_b\n            d_rays_enc_ = d_vecr * (vecr > 0.0)\n        d_wc += d_wc_\n        d_bc += d_bc_\n        d_wr += d_wr_\n        d_br += d_br_\n        d_w2 += d_w2_\n        d_w1 += d_w1_\n        d_b1 += d_b1_\n        d_b2 += d_b2_\n        d_wo += d_wo_\n        d_bo += d_bo_\n        d_w2c += d_w2c_\n        d_b2c += d_b2c_\n        d_rays_enc += d_rays_enc_\n        prev_proj_depth = proj_depth\n        prev_proj_features = proj_features\n        sample_index_buffer = sample_index_buffer - 1\n    tl.atomic_add(grad_weights + offs_CC, d_w1)\n    tl.atomic_add(grad_weights + C * C + offs_CC, d_w2)\n    tl.atomic_add(grad_weights + 2 * C * C + offs_CC, d_wr)\n    tl.atomic_add(grad_weights + 3 * C * C + offs_CC, d_w2c)\n    tl.atomic_add(grad_biases + tl.arange(0, C), d_b1)\n    tl.atomic_add(grad_biases + C + tl.arange(0, C), d_b2)\n    tl.atomic_add(grad_biases + 2 * C + tl.arange(0, C), d_br)\n    tl.atomic_add(grad_biases + 3 * C + tl.arange(0, C), d_b2c)\n    tl.atomic_add(grad_weight_opacity + tl.arange(0, C), d_wo)\n    tl.atomic_add(grad_bias_opacity + tl.arange(0, 1), d_bo)\n    tl.atomic_add(grad_weight_color + tl.arange(0, OUT_C)[:, None] * C + tl\n        .arange(0, C)[None, :], d_wc)\n    tl.atomic_add(grad_bias_color + tl.arange(0, OUT_C), d_bc)\n    tl.store(grad_rays_enc + rays_enc_offs, d_rays_enc, mask=rays_enc_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=get_mm_configs(), key=['N', 'K'])\n@triton.jit\ndef _addmm_fwd(x_ptr, w_ptr, y_ptr, z_ptr, M, N, K, stride_xm, stride_xk,\n    stride_wk, stride_wn, stride_ym, stride_yn, stride_zm, stride_zn,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K:\n    'tl.constexpr', GROUP_M: 'tl.constexpr', ALLOW_TF32: 'tl.constexpr',\n    BROADCAST_Y: 'tl.constexpr'):\n    pid_0, pid_1 = tl.program_id(axis=0), tl.program_id(axis=1)\n    pid = pid_0 * tl.num_programs(axis=1) + pid_1\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_K)\n    offs_n = tl.arange(0, BLOCK_N)\n    mask_m = (pid_m * BLOCK_M + offs_m)[:, None] < M\n    mask_n = (pid_n * BLOCK_N + offs_n)[None, :] < N\n    x_ptr += pid_m * BLOCK_M * stride_xm\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_xm + offs_k[None, :] * stride_xk\n        )\n    w_ptr += pid_n * BLOCK_N * stride_wn\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_n[None, :] * stride_wn\n        )\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_K)):\n        mask_k = offs_k[None, :] < K - k * BLOCK_K\n        x = tl.load(x_ptrs, mask=mask_k & mask_m, other=0.0)\n        mask_k = offs_k[:, None] < K - k * BLOCK_K\n        w = tl.load(w_ptrs, mask=mask_k & mask_n, other=0.0)\n        accumulator += tl.dot(x, w, allow_tf32=ALLOW_TF32)\n        x_ptrs += BLOCK_K * stride_xk\n        w_ptrs += BLOCK_K * stride_wk\n    z_mask = mask_m & mask_n\n    if BROADCAST_Y:\n        y_ptr += pid_n * BLOCK_N * stride_yn\n        y_ptrs = y_ptr + stride_yn * offs_n[None, :]\n        y = tl.load(y_ptrs, mask=mask_n)\n    else:\n        y_ptr += pid_m * BLOCK_M * stride_ym\n        y_ptr += pid_n * BLOCK_N * stride_yn\n        y_ptrs = y_ptr + stride_ym * offs_m[:, None] + stride_yn * offs_n[\n            None, :]\n        y = tl.load(y_ptrs, mask=z_mask)\n    z = accumulator + y.to(tl.float32)\n    z_ptr += pid_m * BLOCK_M * stride_zm\n    z_ptr += pid_n * BLOCK_N * stride_zn\n    z_ptrs = z_ptr + stride_zm * offs_m[:, None] + stride_zn * offs_n[None, :]\n    tl.store(z_ptrs, z, mask=z_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _ragged_hstu_attn_fwd_one_block(start_n, seq_len, offs_m, offs_n,\n    mask_m, mask_n, q, K_block_ptr, V_block_ptr, n_targets, ts_1_ptrs, ts_0,\n    TW, PW, alpha, MAX_SEQ_LEN, num_buckets, max_pos_ind, max_attn_len,\n    time_bucket_incr, time_bucket_div, time_delta, bias_ptrs, attn_scale,\n    contextual_seq_len, INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', BUCKET_FN: 'tl.constexpr', ATTN_BIAS_TYPE:\n    'tl.constexpr', USE_TIME_BIAS: 'tl.constexpr', USE_POS_BIAS:\n    'tl.constexpr', HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS:\n    'tl.constexpr', HAS_ATTN_SCALE: 'tl.constexpr', HAS_MAX_ATTN_LEN:\n    'tl.constexpr', HAS_CONTEXTUAL_SEQ_LEN: 'tl.constexpr', IS_DELTA_Q:\n    'tl.constexpr', ALLOW_TF32: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    k = tl.load(K_block_ptr, boundary_check=(1,), padding_option='zero')\n    qk = tl.dot(q, k, allow_tf32=ALLOW_TF32) * alpha\n    invalid_mask = offs_m[:, None] == offs_n[None, :]\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            offs_m = tl.where(offs_m < seq_len - n_targets, offs_m, seq_len -\n                n_targets)\n            offs_n = tl.where(offs_n < seq_len - n_targets, offs_n, seq_len -\n                n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            offs_m = tl.where(offs_m > n_targets - 1, offs_m, n_targets - 1)\n            offs_n = tl.where(offs_n > n_targets - 1, offs_n, n_targets - 1)\n    offs_n_minus_m = offs_n[None, :] - offs_m[:, None]\n    if HAS_MAX_ATTN_LEN:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            invalid_mask = (invalid_mask or offs_n_minus_m < 0 and \n                offs_n_minus_m >= -max_attn_len)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            invalid_mask = (invalid_mask or offs_n_minus_m > 0 and \n                offs_n_minus_m <= max_attn_len)\n    elif INVALID_MASK_TYPE == 'lower_triangular':\n        invalid_mask = invalid_mask or offs_n_minus_m < 0\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        invalid_mask = invalid_mask or offs_n_minus_m > 0\n    if HAS_CONTEXTUAL_SEQ_LEN:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            row_filter = offs_m < contextual_seq_len\n            if HAS_MULTIPLE_TARGETS:\n                col_filter = offs_n < seq_len - n_targets\n            else:\n                col_filter = offs_n < seq_len\n            invalid_mask = invalid_mask or row_filter[:, None] and col_filter[\n                None, :]\n    if ATTN_BIAS_TYPE == 'fused':\n        attn_bias = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if USE_TIME_BIAS:\n            if CAUSAL:\n                ts_1 = tl.load(ts_1_ptrs + start_n, mask=mask_n)\n            else:\n                ts_1 = tl.load(ts_1_ptrs + start_n + 1, mask=mask_n)\n            ts = ts_0[:, None] - ts_1[None, :]\n            ts = ts + time_delta\n            ts = tl.where(ts > 1e-06, ts, 1e-06)\n            ts = ts * (1.0 / time_bucket_incr)\n            if BUCKET_FN == 'log':\n                ts = tl.log(ts)\n            elif BUCKET_FN == 'sqrt':\n                ts = tl.sqrt(ts)\n            ts = ts * (1.0 / time_bucket_div)\n            ts = ts\n            ts = tl.where(ts > 0, ts, 0)\n            ts = tl.where(ts < num_buckets, ts, num_buckets)\n            ts_w = tl.load(TW + ts, mask=mask_m[:, None] and mask_n[None, :])\n            attn_bias = attn_bias + ts_w\n        if USE_POS_BIAS:\n            if HAS_MAX_POS_IND:\n                offs_pos_w = offs_n_minus_m + max_pos_ind - 1\n                offs_pos_w = tl.where(offs_pos_w > 0, offs_pos_w, 0)\n                offs_pos_w = tl.where(offs_pos_w < 2 * max_pos_ind - 2,\n                    offs_pos_w, 2 * max_pos_ind - 2)\n            else:\n                offs_pos_w = offs_n_minus_m + MAX_SEQ_LEN - 1\n            pos_w = tl.load(PW + offs_pos_w, mask=mask_m[:, None] and\n                mask_n[None, :])\n            attn_bias = attn_bias + pos_w\n        qk = qk + attn_bias\n    elif ATTN_BIAS_TYPE == 'separate':\n        attn_bias = tl.load(bias_ptrs + start_n, mask=mask_m[:, None] &\n            mask_n[None, :], other=0.0)\n        qk = qk + attn_bias\n    silu = fast_dividef(qk, 1.0 + tl.exp(-qk)) * (1.0 / MAX_SEQ_LEN)\n    silu = tl.where(invalid_mask, silu, 0)\n    if HAS_ATTN_SCALE:\n        silu = silu * attn_scale[:, None]\n    v = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero')\n    silu = silu\n    return tl.dot(silu, v, allow_tf32=ALLOW_TF32)\n"
    },
    {
      "input": "@triton.jit\ndef _ragged_hstu_attn_fwd_compute(Q, K, V, seq_offsets, TS, TW, PW, Bias,\n    seq2_offsets, delta_x_offsets, num_targets, Scale, Out, stride_qm,\n    stride_qh, stride_kn, stride_kh, stride_vn, stride_vh, stride_sz,\n    stride_sm, stride_ts, stride_om, stride_oh, alpha, Z, H, MAX_SEQ_LEN,\n    DimQ, DimV, DeltaSize, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, contextual_seq_len, off_z, off_h, pid,\n    INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL: 'tl.constexpr', BUCKET_FN:\n    'tl.constexpr', ATTN_BIAS_TYPE: 'tl.constexpr', USE_TIME_BIAS:\n    'tl.constexpr', USE_POS_BIAS: 'tl.constexpr', HAS_MAX_POS_IND:\n    'tl.constexpr', HAS_MULTIPLE_TARGETS: 'tl.constexpr', HAS_ATTN_SCALE:\n    'tl.constexpr', IS_DELTA_Q: 'tl.constexpr', ALLOW_TF32: 'tl.constexpr',\n    BLOCK_D_Q: 'tl.constexpr', BLOCK_D_V: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', max_attn_len: 'tl.constexpr',\n    HAS_MAX_ATTN_LEN: 'tl.constexpr', HAS_CONTEXTUAL_SEQ_LEN: 'tl.constexpr'):\n    seq_start = tl.load(seq_offsets + off_z)\n    seq_end = tl.load(seq_offsets + off_z + 1)\n    seq_len = seq_end - seq_start\n    if IS_DELTA_Q:\n        start_m_delta = pid * BLOCK_M\n        delta_start = tl.load(delta_x_offsets + off_z * DeltaSize)\n        start_m = start_m_delta + delta_start - seq_start\n    else:\n        start_m_delta = 0\n        start_m = pid * BLOCK_M\n    if start_m < seq_len:\n        if HAS_MULTIPLE_TARGETS:\n            n_targets = tl.load(num_targets + off_z)\n        else:\n            n_targets = None\n        offs_m = start_m + tl.arange(0, BLOCK_M)\n        offs_n = tl.arange(0, BLOCK_N)\n        if IS_DELTA_Q:\n            Q_block_ptr = tl.make_block_ptr(base=Q + off_h * stride_qh + \n                off_z * DeltaSize * stride_qm, shape=(DeltaSize, BLOCK_D_Q),\n                strides=(stride_qm, 1), offsets=(start_m_delta, 0),\n                block_shape=(BLOCK_M, BLOCK_D_Q), order=(1, 0))\n        else:\n            Q_block_ptr = tl.make_block_ptr(base=Q + off_h * stride_qh + \n                seq_start * stride_qm, shape=(seq_len, BLOCK_D_Q), strides=\n                (stride_qm, 1), offsets=(start_m, 0), block_shape=(BLOCK_M,\n                BLOCK_D_Q), order=(1, 0))\n        K_block_ptr = tl.make_block_ptr(base=K + off_h * stride_kh + \n            seq_start * stride_kn, shape=(BLOCK_D_Q, seq_len), strides=(1,\n            stride_kn), offsets=(0, 0), block_shape=(BLOCK_D_Q, BLOCK_N),\n            order=(0, 1))\n        V_block_ptr = tl.make_block_ptr(base=V + off_h * stride_vh + \n            seq_start * stride_vn, shape=(seq_len, BLOCK_D_V), strides=(\n            stride_vn, 1), offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_D_V),\n            order=(1, 0))\n        mask_m = offs_m < seq_len\n        if ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS:\n            ts_0_ptrs = TS + off_z * stride_ts + offs_m\n            ts_1_ptrs = TS + off_z * stride_ts + offs_n\n            if CAUSAL:\n                ts_0 = tl.load(ts_0_ptrs + 1, mask=mask_m)\n            else:\n                ts_0 = tl.load(ts_0_ptrs, mask=mask_m)\n        elif ATTN_BIAS_TYPE == 'separate':\n            seq2_start = tl.load(seq2_offsets + off_z)\n            bias_start = seq2_start * H + off_h * seq_len * seq_len\n            off_bias = offs_m[:, None] * seq_len + offs_n[None, :]\n            bias_ptrs = Bias + bias_start + off_bias\n        if HAS_ATTN_SCALE:\n            scale_ptrs = Scale + off_z * stride_sz\n            attn_scale = tl.load(scale_ptrs + offs_m * stride_sm, mask=\n                offs_m < seq_len)\n        q = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n        acc = tl.zeros([BLOCK_M, BLOCK_D_V], dtype=tl.float32)\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            if HAS_MULTIPLE_TARGETS:\n                if HAS_MAX_ATTN_LEN:\n                    start_m_index = (seq_len - n_targets if start_m > \n                        seq_len - n_targets else start_m)\n                    low = start_m_index - max_attn_len\n                    low = low if low > 0 else 0\n                else:\n                    low = 0\n                uih_end = (seq_len - n_targets + BLOCK_N - 1\n                    ) // BLOCK_N * BLOCK_N\n                if uih_end < start_m:\n                    high = seq_len - n_targets\n                else:\n                    high = start_m + BLOCK_M\n                if HAS_CONTEXTUAL_SEQ_LEN:\n                    if start_m < contextual_seq_len:\n                        high = seq_len - n_targets\n            else:\n                if HAS_MAX_ATTN_LEN:\n                    low = start_m - max_attn_len\n                    low = low if low > 0 else 0\n                else:\n                    low = 0\n                high = start_m + BLOCK_M\n                if HAS_CONTEXTUAL_SEQ_LEN:\n                    if start_m < contextual_seq_len:\n                        high = seq_len\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            low = start_m\n            high = seq_len\n        if low > 0:\n            K_block_ptr = tl.advance(K_block_ptr, (0, low))\n            V_block_ptr = tl.advance(V_block_ptr, (low, 0))\n        for start_n in range(low, high, BLOCK_N):\n            cur_offs_n = offs_n + start_n\n            mask_n = cur_offs_n < seq_len\n            acc += _ragged_hstu_attn_fwd_one_block(start_n=start_n, seq_len\n                =seq_len, offs_m=offs_m, offs_n=cur_offs_n, mask_m=mask_m,\n                mask_n=mask_n, q=q, K_block_ptr=K_block_ptr, V_block_ptr=\n                V_block_ptr, n_targets=n_targets if HAS_MULTIPLE_TARGETS else\n                None, ts_1_ptrs=ts_1_ptrs if ATTN_BIAS_TYPE == 'fused' and\n                USE_TIME_BIAS else None, ts_0=ts_0 if ATTN_BIAS_TYPE ==\n                'fused' and USE_TIME_BIAS else None, TW=TW, PW=PW, alpha=\n                alpha, MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=num_buckets,\n                max_pos_ind=max_pos_ind, max_attn_len=max_attn_len,\n                time_bucket_incr=time_bucket_incr, time_bucket_div=\n                time_bucket_div, time_delta=time_delta, bias_ptrs=bias_ptrs if\n                ATTN_BIAS_TYPE == 'separate' else None, attn_scale=\n                attn_scale if HAS_ATTN_SCALE else None, contextual_seq_len=\n                contextual_seq_len, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n                HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS, HAS_ATTN_SCALE=\n                HAS_ATTN_SCALE, HAS_MAX_ATTN_LEN=HAS_MAX_ATTN_LEN,\n                HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN, IS_DELTA_Q=\n                IS_DELTA_Q, ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M, BLOCK_N\n                =BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        if HAS_MULTIPLE_TARGETS and INVALID_MASK_TYPE == 'lower_triangular':\n            if uih_end < start_m:\n                low_delta = start_m\n                high_delta = start_m + BLOCK_M\n                offset = low_delta - uih_end\n                K_block_ptr = tl.advance(K_block_ptr, (0, offset))\n                V_block_ptr = tl.advance(V_block_ptr, (offset, 0))\n                for start_delta in tl.range(low_delta, high_delta, BLOCK_N,\n                    num_stages=0):\n                    cur_offs_n = offs_n + start_delta\n                    mask_n = cur_offs_n < seq_len\n                    acc += _ragged_hstu_attn_fwd_one_block(start_n=\n                        start_delta, seq_len=seq_len, offs_m=offs_m, offs_n\n                        =cur_offs_n, mask_m=mask_m, mask_n=mask_n, q=q,\n                        K_block_ptr=K_block_ptr, V_block_ptr=V_block_ptr,\n                        n_targets=n_targets if HAS_MULTIPLE_TARGETS else\n                        None, ts_1_ptrs=ts_1_ptrs if ATTN_BIAS_TYPE ==\n                        'fused' and USE_TIME_BIAS else None, ts_0=ts_0 if \n                        ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS else\n                        None, TW=TW, PW=PW, alpha=alpha, MAX_SEQ_LEN=\n                        MAX_SEQ_LEN, num_buckets=num_buckets, max_pos_ind=\n                        max_pos_ind, max_attn_len=max_attn_len,\n                        time_bucket_incr=time_bucket_incr, time_bucket_div=\n                        time_bucket_div, time_delta=time_delta, bias_ptrs=\n                        bias_ptrs if ATTN_BIAS_TYPE == 'separate' else None,\n                        attn_scale=attn_scale if HAS_ATTN_SCALE else None,\n                        contextual_seq_len=contextual_seq_len,\n                        INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL,\n                        BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n                        USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                        USE_POS_BIAS, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n                        HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,\n                        HAS_ATTN_SCALE=HAS_ATTN_SCALE, HAS_MAX_ATTN_LEN=\n                        HAS_MAX_ATTN_LEN, HAS_CONTEXTUAL_SEQ_LEN=\n                        HAS_CONTEXTUAL_SEQ_LEN, IS_DELTA_Q=IS_DELTA_Q,\n                        ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N\n                        )\n                    K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n                    V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        if IS_DELTA_Q:\n            start_m_delta = pid * BLOCK_M\n            offs_m_delta = start_m_delta + tl.arange(0, BLOCK_M)\n            offs_v_d = tl.arange(0, BLOCK_D_V)\n            off_o = (off_z * DeltaSize + offs_m_delta[:, None]\n                ) * stride_om + off_h * stride_oh + offs_v_d[None, :]\n            out_ptrs = Out + off_o\n            tl.store(out_ptrs, acc, mask=(offs_m_delta < DeltaSize)[:, None])\n        else:\n            start_m = pid * BLOCK_M\n            offs_m = start_m + tl.arange(0, BLOCK_M)\n            offs_v_d = tl.arange(0, BLOCK_D_V)\n            off_o = (seq_start + offs_m[:, None]\n                ) * stride_om + off_h * stride_oh + offs_v_d[None, :]\n            out_ptrs = Out + off_o\n            tl.store(out_ptrs, acc, mask=(offs_m < seq_len)[:, None])\n"
    },
    {
      "input": "@triton.autotune(configs=_get_fw_configs(), key=['Z', 'H',\n    'AUTOTUNE_MAX_SEQ_LEN', 'DimQ', 'DimV', 'BUCKET_FN', 'ATTN_BIAS_TYPE',\n    'DeltaSize', 'IS_DELTA_Q'])\n@triton.jit\ndef _ragged_hstu_attn_fwd(Q, K, V, sort_by_length_indices, seq_offsets, TS,\n    TW, PW, Bias, seq2_offsets, delta_x_offsets, num_targets, Scale, Out,\n    stride_qm, stride_qh, stride_kn, stride_kh, stride_vn, stride_vh,\n    stride_sz, stride_sm, stride_ts, stride_om, stride_oh, alpha, Z, H,\n    MAX_SEQ_LEN, AUTOTUNE_MAX_SEQ_LEN, DimQ, DimV, DeltaSize, num_buckets,\n    max_pos_ind, time_bucket_incr, time_bucket_div, time_delta,\n    contextual_seq_len, INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', BUCKET_FN: 'tl.constexpr', ATTN_BIAS_TYPE:\n    'tl.constexpr', USE_TIME_BIAS: 'tl.constexpr', USE_POS_BIAS:\n    'tl.constexpr', HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS:\n    'tl.constexpr', HAS_ATTN_SCALE: 'tl.constexpr', IS_DELTA_Q:\n    'tl.constexpr', ALLOW_TF32: 'tl.constexpr', BLOCK_D_Q: 'tl.constexpr',\n    BLOCK_D_V: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', max_attn_len: 'tl.constexpr', HAS_MAX_ATTN_LEN:\n    'tl.constexpr', HAS_CONTEXTUAL_SEQ_LEN: 'tl.constexpr',\n    HAS_SORT_BY_LENGTH_INDICES: 'tl.constexpr'):\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    if HAS_SORT_BY_LENGTH_INDICES:\n        off_z = tl.load(sort_by_length_indices + off_z)\n    off_h = off_hz % H\n    pid = tl.program_id(0)\n    _ragged_hstu_attn_fwd_compute(Q=Q, K=K, V=V, seq_offsets=seq_offsets,\n        TS=TS, TW=TW, PW=PW, Bias=Bias, seq2_offsets=seq2_offsets,\n        delta_x_offsets=delta_x_offsets, num_targets=num_targets, Scale=\n        Scale, Out=Out, stride_qm=stride_qm, stride_qh=stride_qh, stride_kn\n        =stride_kn, stride_kh=stride_kh, stride_vn=stride_vn, stride_vh=\n        stride_vh, stride_sz=stride_sz, stride_sm=stride_sm, stride_ts=\n        stride_ts, stride_om=stride_om, stride_oh=stride_oh, alpha=alpha, Z\n        =Z, H=H, MAX_SEQ_LEN=MAX_SEQ_LEN, DimQ=DimQ, DimV=DimV, DeltaSize=\n        DeltaSize, num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n        time_bucket_incr=time_bucket_incr, time_bucket_div=time_bucket_div,\n        time_delta=time_delta, contextual_seq_len=contextual_seq_len, off_z\n        =off_z, off_h=off_h, pid=pid, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n        CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n        USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=USE_POS_BIAS,\n        HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n        HAS_MULTIPLE_TARGETS, HAS_ATTN_SCALE=HAS_ATTN_SCALE, IS_DELTA_Q=\n        IS_DELTA_Q, ALLOW_TF32=ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q, BLOCK_D_V=\n        BLOCK_D_V, max_attn_len=max_attn_len, HAS_MAX_ATTN_LEN=\n        HAS_MAX_ATTN_LEN, HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN,\n        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n"
    },
    {
      "input": "@triton.autotune(configs=_get_fw_configs(), key=['Z', 'H',\n    'AUTOTUNE_MAX_SEQ_LEN', 'DimQ', 'DimV', 'BUCKET_FN', 'ATTN_BIAS_TYPE',\n    'DeltaSize', 'IS_DELTA_Q'])\n@triton.jit\ndef _ragged_hstu_attn_fwd_persistent(Q, K, V, sort_by_length_indices,\n    seq_offsets, TS, TW, PW, Bias, seq2_offsets, delta_x_offsets,\n    num_targets, Scale, Out, stride_qm, stride_qh, stride_kn, stride_kh,\n    stride_vn, stride_vh, stride_sz, stride_sm, stride_ts, stride_om,\n    stride_oh, alpha, Z, H, MAX_SEQ_LEN, AUTOTUNE_MAX_SEQ_LEN, DimQ, DimV,\n    DeltaSize, num_buckets, max_pos_ind, time_bucket_incr, time_bucket_div,\n    time_delta, contextual_seq_len, INVALID_MASK_TYPE: 'tl.constexpr',\n    CAUSAL: 'tl.constexpr', BUCKET_FN: 'tl.constexpr', ATTN_BIAS_TYPE:\n    'tl.constexpr', USE_TIME_BIAS: 'tl.constexpr', USE_POS_BIAS:\n    'tl.constexpr', HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS:\n    'tl.constexpr', HAS_ATTN_SCALE: 'tl.constexpr', IS_DELTA_Q:\n    'tl.constexpr', ALLOW_TF32: 'tl.constexpr', BLOCK_D_Q: 'tl.constexpr',\n    BLOCK_D_V: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', max_attn_len: 'tl.constexpr', HAS_MAX_ATTN_LEN:\n    'tl.constexpr', HAS_CONTEXTUAL_SEQ_LEN: 'tl.constexpr',\n    HAS_SORT_BY_LENGTH_INDICES: 'tl.constexpr'):\n    n_tile_num = tl.cdiv(MAX_SEQ_LEN, BLOCK_M)\n    prog_id = tl.program_id(0)\n    num_progs = tl.num_programs(0)\n    total_tiles = n_tile_num * Z * H\n    tiles_per_sm = total_tiles // num_progs\n    if prog_id < total_tiles % num_progs:\n        tiles_per_sm += 1\n    tile_idx = prog_id\n    for _ in range(0, tiles_per_sm):\n        pid = (total_tiles - tile_idx - 1) // (Z * H)\n        off_hz = (total_tiles - tile_idx - 1) % (Z * H)\n        off_z = off_hz // H\n        off_h = off_hz % H\n        _ragged_hstu_attn_fwd_compute(Q=Q, K=K, V=V, seq_offsets=\n            seq_offsets, TS=TS, TW=TW, PW=PW, Bias=Bias, seq2_offsets=\n            seq2_offsets, delta_x_offsets=delta_x_offsets, num_targets=\n            num_targets, Scale=Scale, Out=Out, stride_qm=stride_qm,\n            stride_qh=stride_qh, stride_kn=stride_kn, stride_kh=stride_kh,\n            stride_vn=stride_vn, stride_vh=stride_vh, stride_sz=stride_sz,\n            stride_sm=stride_sm, stride_ts=stride_ts, stride_om=stride_om,\n            stride_oh=stride_oh, alpha=alpha, Z=Z, H=H, MAX_SEQ_LEN=\n            MAX_SEQ_LEN, DimQ=DimQ, DimV=DimV, DeltaSize=DeltaSize,\n            num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n            time_bucket_incr=time_bucket_incr, time_bucket_div=\n            time_bucket_div, time_delta=time_delta, contextual_seq_len=\n            contextual_seq_len, off_z=off_z, off_h=off_h, pid=pid,\n            INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL, BUCKET_FN=\n            BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE, USE_TIME_BIAS=\n            USE_TIME_BIAS, USE_POS_BIAS=USE_POS_BIAS, HAS_MAX_POS_IND=\n            HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,\n            HAS_ATTN_SCALE=HAS_ATTN_SCALE, IS_DELTA_Q=IS_DELTA_Q,\n            ALLOW_TF32=ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q, BLOCK_D_V=BLOCK_D_V,\n            max_attn_len=max_attn_len, HAS_MAX_ATTN_LEN=HAS_MAX_ATTN_LEN,\n            HAS_CONTEXTUAL_SEQ_LEN=HAS_CONTEXTUAL_SEQ_LEN, BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N)\n        tile_idx += num_progs\n"
    },
    {
      "input": "@triton.jit\ndef d_sigmoid(dy, x):\n    s = tl.sigmoid(x)\n    return dy * s * (1 - s)\n"
    },
    {
      "input": "@triton.jit\ndef _softplus(x):\n    z = tl.where(x >= 0, x + tl.log(1 + tl.exp(-x)), tl.log(1 + tl.exp(x)))\n    return z\n"
    },
    {
      "input": "@triton.jit\ndef _d_softplus(grad, x):\n    z = tl.where(x >= 0, 1 / (1 + tl.exp(-x)), 1 - 1 / (1 + tl.exp(x)))\n    return grad * z\n"
    },
    {
      "input": "@triton.jit\ndef d_linear(d_y, w, b, x):\n    d_x = tl.dot(d_y, tl.trans(w), allow_tf32=ALLOW_TF32)\n    d_w = tl.dot(tl.trans(d_y), x, allow_tf32=ALLOW_TF32)\n    d_b = tl.sum(d_y, axis=0)\n    return d_x, d_w, d_b\n"
    },
    {
      "input": "@triton.jit\ndef d_linear_relu(d_y, w, b, xwb, x):\n    d_y_relu = d_y * (xwb > 0.0)\n    return d_linear(d_y_relu, w, b, x)\n"
    },
    {
      "input": "@triton.jit\ndef fwbw_init(directions, origins, grid_idx, near, far, rays_encoding,\n    inject_noise_seed, DIM_IN_COLOR: 'tl.constexpr', DIM_OUT_COLOR:\n    'tl.constexpr', num_samples: 'tl.constexpr', num_samples_inf:\n    'tl.constexpr', num_rays: 'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    tot_num_samples = num_samples + num_samples_inf\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = offs < num_rays\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = (pid * BLOCK_SIZE * DIM_OUT_COLOR + DIM_OUT_COLOR * tl.\n        arange(0, BLOCK_SIZE)[:, None] + tl.arange(0, DIM_OUT_COLOR)[None, :])\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    center_x = tl.load(origins + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(origins + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(origins + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(directions + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(directions + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(directions + offs_z, mask=offs_z < num_rays * 3)\n    near_buffer = tl.load(near + offs, mask=offs_mask)\n    far_buffer = tl.load(far + offs, mask=offs_mask)\n    grid_idx_buffer = tl.load(grid_idx + offs, mask=offs_mask)\n    seed_buffer = tl.load(inject_noise_seed + offs, mask=offs < num_rays)\n    sample_index_buffer = tl.arange(0, BLOCK_SIZE\n        ) * tot_num_samples + pid * BLOCK_SIZE * tot_num_samples + 1\n    rays_encoding_buffer = tl.load(rays_encoding + pid * BLOCK_SIZE *\n        DIM_IN_COLOR + DIM_IN_COLOR * tl.arange(0, BLOCK_SIZE)[:, None] +\n        tl.arange(0, DIM_IN_COLOR)[None, :], mask=offs_features_mask)\n    one_scaffold = tl.full((BLOCK_SIZE,), 1.0, tl.float32)\n    zero_value = tl.zeros((BLOCK_SIZE,), tl.float32)\n    one_vec = tl.full((BLOCK_SIZE, C), 1.0, tl.float32)\n    zero_color = tl.zeros((BLOCK_SIZE, DIM_OUT_COLOR), tl.float32)\n    return (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer, seed_buffer,\n        sample_index_buffer, rays_encoding_buffer, one_scaffold, zero_value,\n        one_vec, zero_color)\n"
    },
    {
      "input": "@triton.jit\ndef fwbw_splatter_init(directions, origins, grid_idx, near, far,\n    splatting_feature, mask, num_samples: 'tl.constexpr', num_samples_inf:\n    'tl.constexpr', num_rays: 'tl.constexpr', grid_channel: 'tl.constexpr',\n    feature_channel: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    tot_num_samples = num_samples + num_samples_inf\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = offs < num_rays\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = (pid * BLOCK_SIZE * feature_channel + feature_channel *\n        tl.arange(0, BLOCK_SIZE)[:, None] + tl.arange(0, feature_channel)[\n        None, :])\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    center_x = tl.load(origins + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(origins + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(origins + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(directions + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(directions + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(directions + offs_z, mask=offs_z < num_rays * 3)\n    near_buffer = tl.load(near + offs, mask=offs_mask)\n    far_buffer = tl.load(far + offs, mask=offs_mask)\n    grid_idx_buffer = tl.load(grid_idx + offs, mask=offs_mask)\n    sample_index_buffer = tl.arange(0, BLOCK_SIZE\n        ) * tot_num_samples + pid * BLOCK_SIZE * tot_num_samples + 1\n    feature = tl.load(splatting_feature + offs_features, mask=\n        offs_features_mask)\n    mask = tl.load(mask + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:,\n        None], mask=offs_features_mask)\n    mask = tl.view(mask, (BLOCK_SIZE, 1))\n    return (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature, mask)\n"
    },
    {
      "input": "@triton.jit\ndef _floor(x):\n    return x - x % 1\n"
    },
    {
      "input": "@triton.jit\ndef _round(x):\n    return _floor(x + 0.5)\n"
    },
    {
      "input": "@triton.jit\ndef is_in_bounds(x, y, z, C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    in_bounds = (tl.abs(x) <= 1) * (tl.abs(y) <= 1) * (tl.abs(z) <= 1)\n    if C == 1:\n        in_bounds_mask = tl.view(in_bounds, (BLOCK_SIZE,))\n    else:\n        in_bounds_mask = tl.broadcast_to(in_bounds[:, None], (BLOCK_SIZE, C))\n    return in_bounds_mask\n"
    },
    {
      "input": "@triton.jit\ndef _splat_3d(to_splat, grad_image, w, batch_index, ix, iy, iz, ID, IH, IW,\n    C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW) * (iz < ID) * (\n        iz >= 0))\n    w = tl.view(w[:, None], (BLOCK_SIZE, 1))\n    offs = tl.view((batch_index * ID * IW * IH * C + iz_ * IW * IH * C + \n        iy_ * IW * C + ix_ * C)[:, None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n"
    },
    {
      "input": "@triton.jit\ndef _splat_2d(to_splat, grad_image, w, batch_index, ix, iy, IH, IW, C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW))\n    w = tl.view(w[:, None], (BLOCK_SIZE, 1))\n    offs = tl.view((batch_index * IW * IH * C + iy_ * IW * C + ix_ * C)[:,\n        None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n"
    },
    {
      "input": "@triton.jit\ndef _get_plane_grid_sample_info(gi, ix_in, iy_in, IH, IW, feature_grid_size,\n    C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    BS = tl.load(feature_grid_size + offs + 0)\n    grid_numel = BS * IH * IW * C\n    grid_numel = tl.sum(grid_numel, axis=0) // BLOCK_SIZE\n    ix11 = (ix_in + 1) / 2 * IW - 0.5\n    iy11 = (iy_in + 1) / 2 * IH - 0.5\n    ix = ix11 * (IW > 1)\n    iy = iy11 * (IH > 1)\n    ix0 = _floor(ix)\n    iy0 = _floor(iy)\n    return ix, iy, ix0, iy0, grid_numel\n"
    },
    {
      "input": "@triton.jit\ndef _get_plane_grid_sample_locs_weights(ix, iy, ix0, iy0):\n    return (ix0, iy0, ix0, iy0 + 1, ix0 + 1, iy0, ix0 + 1, iy0 + 1, ix -\n        ix0, iy - iy0)\n"
    },
    {
      "input": "@triton.jit\ndef _plane_grid_splat_one(gi, to_splat, grad_feature_grid,\n    feature_grid_size, batch_index, ix_in, iy_in, IH, IW, C: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    ix, iy, ix0, iy0, grid_numel = _get_plane_grid_sample_info(gi, ix_in,\n        iy_in, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    V00x, V00y, V10x, V10y, V01x, V01y, V11x, V11y, x, y = (\n        _get_plane_grid_sample_locs_weights(ix, iy, ix0, iy0))\n    to_splat_now = to_splat\n    _splat_2d(to_splat_now, grad_feature_grid, (1 - x) * (1 - y),\n        batch_index, V00x, V00y, IH, IW, C, BLOCK_SIZE)\n    _splat_2d(to_splat_now, grad_feature_grid, (1 - x) * y, batch_index,\n        V10x, V10y, IH, IW, C, BLOCK_SIZE)\n    _splat_2d(to_splat_now, grad_feature_grid, x * (1 - y), batch_index,\n        V01x, V01y, IH, IW, C, BLOCK_SIZE)\n    _splat_2d(to_splat_now, grad_feature_grid, x * y, batch_index, V11x,\n        V11y, IH, IW, C, BLOCK_SIZE)\n    return grid_numel\n"
    },
    {
      "input": "@triton.jit\ndef _get_voxel_grid_sample_info(gi, ix_in, iy_in, iz_in, ID, IH, IW,\n    feature_grid_size, C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    BS = tl.load(feature_grid_size + offs + 0)\n    grid_numel = BS * ID * IH * IW * C\n    grid_numel = tl.sum(grid_numel, axis=0) // BLOCK_SIZE\n    ix11 = (ix_in + 1) / 2 * IW - 0.5\n    iy11 = (iy_in + 1) / 2 * IH - 0.5\n    iz11 = (iz_in + 1) / 2 * ID - 0.5\n    ix = ix11 * (IW > 1)\n    iy = iy11 * (IH > 1)\n    iz = iz11 * (ID > 1)\n    ix0 = _floor(ix)\n    iy0 = _floor(iy)\n    iz0 = _floor(iz)\n    return ix, iy, iz, ix0, iy0, iz0, grid_numel\n"
    },
    {
      "input": "@triton.jit\ndef _get_voxel_grid_sample_locs_weights(ix, iy, iz, ix0, iy0, iz0):\n    return (ix0, iy0, iz0, ix0, iy0, iz0 + 1, ix0, iy0 + 1, iz0, ix0 + 1,\n        iy0, iz0, ix0 + 1, iy0, iz0 + 1, ix0 + 1, iy0 + 1, iz0, ix0, iy0 + \n        1, iz0 + 1, ix0 + 1, iy0 + 1, iz0 + 1, ix - ix0, iy - iy0, iz - iz0)\n"
    },
    {
      "input": "@triton.jit\ndef _voxel_grid_splat_one(gi, to_splat, grad_feature_grid,\n    feature_grid_size, batch_index, ix_in, iy_in, iz_in, IH, IW, ID, C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr'):\n    ix, iy, iz, ix0, iy0, iz0, grid_numel = _get_voxel_grid_sample_info(gi,\n        ix_in, iy_in, iz_in, ID, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    (V000x, V000y, V000z, V100x, V100y, V100z, V010x, V010y, V010z, V001x,\n        V001y, V001z, V101x, V101y, V101z, V011x, V011y, V011z, V110x,\n        V110y, V110z, V111x, V111y, V111z, x, y, z\n        ) = _get_voxel_grid_sample_locs_weights(ix, iy, iz, ix0, iy0, iz0)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * (1 - y) * (1 - z),\n        batch_index, V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * (1 - y) * z,\n        batch_index, V100x, V100y, V100z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * y * (1 - z),\n        batch_index, V010x, V010y, V010z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * (1 - y) * (1 - z),\n        batch_index, V001x, V001y, V001z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * (1 - y) * z, batch_index,\n        V101x, V101y, V101z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * y * (1 - z), batch_index,\n        V011x, V011y, V011z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * y * z, batch_index,\n        V110x, V110y, V110z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * y * z, batch_index, V111x,\n        V111y, V111z, ID, IH, IW, C, BLOCK_SIZE)\n    return grid_numel\n"
    },
    {
      "input": "@triton.jit\ndef _voxel_grid_splat(to_splat, grad_feature_grid, feature_grid_size,\n    batch_index, ix_in, iy_in, iz_in, C: 'tl.constexpr', NUM_GRIDS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr'):\n    feature_grid_offs = tl.zeros((1,), dtype=tl.int32)\n    for gi in range(NUM_GRIDS):\n        offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n        ID = tl.load(feature_grid_size + offs + 1)\n        IH = tl.load(feature_grid_size + offs + 2)\n        IW = tl.load(feature_grid_size + offs + 3)\n        ID_ = tl.sum(ID, axis=0) // BLOCK_SIZE\n        IH_ = tl.sum(IH, axis=0) // BLOCK_SIZE\n        IW_ = tl.sum(IW, axis=0) // BLOCK_SIZE\n        voxel_grid = (ID_ - 1) * (IH_ - 1) * (IW_ - 1)\n        if mask_out_of_bounds_samples:\n            in_bounds_mask = is_in_bounds(ix_in, iy_in, iz_in, C, BLOCK_SIZE)\n            if C == 1:\n                in_bounds_mask = in_bounds_mask[:, None]\n            to_splat = to_splat * in_bounds_mask\n        else:\n            to_splat = to_splat\n        if voxel_grid > 0:\n            grid_numel = _voxel_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, ix_in, iy_in, iz_in, IH, IW, ID, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        elif ID_ == 1:\n            grid_numel = _plane_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, ix_in, iy_in, IH, IW, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        elif IH_ == 1:\n            grid_numel = _plane_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, ix_in, iz_in, ID, IW, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        else:\n            grid_numel = _plane_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, iy_in, iz_in, ID, IH, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        feature_grid_offs += grid_numel\n"
    },
    {
      "input": "@triton.jit\ndef _sample_3d(image, w, batch_index, ix, iy, iz, ID, IH, IW, C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    image_offs = (image + batch_index * ID * IW * IH * C + iz_ * IW * IH *\n        C + iy_ * IW * C + ix_ * C)\n    mask_w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW) * (iz < ID) *\n        (iz >= 0))\n    if C == 1:\n        val = tl.view(tl.load(image_offs), (BLOCK_SIZE,))\n        out = tl.view(val * mask_w, (BLOCK_SIZE,))\n        return out\n    else:\n        val = tl.view(tl.load(image_offs[:, None] + Coffs[None, :]), (\n            BLOCK_SIZE, C))\n        mask_w_bcast = tl.view(mask_w[:, None], (BLOCK_SIZE, 1))\n        return val * mask_w_bcast\n"
    },
    {
      "input": "@triton.jit\ndef _sample_2d(image, w, batch_index, ix, iy, IH, IW, C: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    image_offs = image + batch_index * IW * IH * C + iy_ * IW * C + ix_ * C\n    mask_w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW))\n    if C == 1:\n        val = tl.view(tl.load(image_offs), (BLOCK_SIZE,))\n        out = tl.view(val * mask_w, (BLOCK_SIZE,))\n        return out\n    else:\n        val = tl.view(tl.load(image_offs[:, None] + Coffs[None, :]), (\n            BLOCK_SIZE, C))\n        mask_w_bcast = tl.view(mask_w[:, None], (BLOCK_SIZE, 1))\n        return val * mask_w_bcast\n"
    },
    {
      "input": "@triton.jit\ndef voxel_grid_sample_one_nearest(gi, feature_grid, feature_grid_size,\n    batch_index, ix_in, iy_in, iz_in, C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    ID = tl.load(feature_grid_size + offs + 1)\n    IH = tl.load(feature_grid_size + offs + 2)\n    IW = tl.load(feature_grid_size + offs + 3)\n    ix11 = (ix_in + 1) / 2 * IW - 0.5\n    iy11 = (iy_in + 1) / 2 * IH - 0.5\n    iz11 = (iz_in + 1) / 2 * ID - 0.5\n    ix = ix11 * (ID > 1)\n    iy = iy11 * (IH > 1)\n    iz = iz11 * (IW > 1)\n    unit_weight = ix * 0.0 + 1.0\n    ix = _round(ix)\n    iy = _round(iy)\n    iz = _round(iz)\n    sampled = _sample_3d(feature_grid, unit_weight, batch_index, ix, iy, iz,\n        ID, IH, IW, C, BLOCK_SIZE)\n    if mask_out_of_bounds_samples:\n        in_bounds_mask = is_in_bounds(ix_in, iy_in, iz_in, C, BLOCK_SIZE)\n        sampled *= in_bounds_mask\n    return sampled\n"
    },
    {
      "input": "@triton.jit\ndef _voxel_grid_sample_one(gi, feature_grid, feature_grid_size, batch_index,\n    ix_in, iy_in, iz_in, ID, IH, IW, C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    ix, iy, iz, ix0, iy0, iz0, grid_numel = _get_voxel_grid_sample_info(gi,\n        ix_in, iy_in, iz_in, ID, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    (V000x, V000y, V000z, V100x, V100y, V100z, V010x, V010y, V010z, V001x,\n        V001y, V001z, V101x, V101y, V101z, V011x, V011y, V011z, V110x,\n        V110y, V110z, V111x, V111y, V111z, x, y, z\n        ) = _get_voxel_grid_sample_locs_weights(ix, iy, iz, ix0, iy0, iz0)\n    sampled = _sample_3d(feature_grid, (1 - x) * (1 - y) * (1 - z),\n        batch_index, V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(feature_grid, (1 - x) * (1 - y) * z, batch_index,\n        V100x, V100y, V100z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(\n        feature_grid, (1 - x) * y * (1 - z), batch_index, V010x, V010y,\n        V010z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(feature_grid, x * (1 -\n        y) * (1 - z), batch_index, V001x, V001y, V001z, ID, IH, IW, C,\n        BLOCK_SIZE) + _sample_3d(feature_grid, x * (1 - y) * z, batch_index,\n        V101x, V101y, V101z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(\n        feature_grid, x * y * (1 - z), batch_index, V011x, V011y, V011z, ID,\n        IH, IW, C, BLOCK_SIZE) + _sample_3d(feature_grid, (1 - x) * y * z,\n        batch_index, V110x, V110y, V110z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(feature_grid, x * y * z, batch_index, V111x, V111y,\n        V111z, ID, IH, IW, C, BLOCK_SIZE)\n    return sampled, grid_numel\n"
    },
    {
      "input": "@triton.jit\ndef _plane_grid_sample_one(gi, feature_grid, feature_grid_size, batch_index,\n    ix_in, iy_in, IH, IW, C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    mask_out_of_bounds_samples: 'tl.constexpr'):\n    ix, iy, ix0, iy0, grid_numel = _get_plane_grid_sample_info(gi, ix_in,\n        iy_in, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    V00x, V00y, V10x, V10y, V01x, V01y, V11x, V11y, x, y = (\n        _get_plane_grid_sample_locs_weights(ix, iy, ix0, iy0))\n    sampled = _sample_2d(feature_grid, (1 - x) * (1 - y), batch_index, V00x,\n        V00y, IH, IW, C, BLOCK_SIZE) + _sample_2d(feature_grid, x * (1 - y),\n        batch_index, V01x, V01y, IH, IW, C, BLOCK_SIZE) + _sample_2d(\n        feature_grid, (1 - x) * y, batch_index, V10x, V10y, IH, IW, C,\n        BLOCK_SIZE) + _sample_2d(feature_grid, x * y, batch_index, V11x,\n        V11y, IH, IW, C, BLOCK_SIZE)\n    return sampled, grid_numel\n"
    },
    {
      "input": "@triton.jit\ndef _voxel_grid_sample(feature_grid, feature_grid_size, batch_index, ix_in,\n    iy_in, iz_in, C: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    out_val = tl.zeros((BLOCK_SIZE, C), dtype=tl.float32)\n    feature_grid_offs = tl.zeros((1,), dtype=tl.int32)\n    for gi in range(NUM_GRIDS):\n        offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n        ID = tl.load(feature_grid_size + offs + 1)\n        IH = tl.load(feature_grid_size + offs + 2)\n        IW = tl.load(feature_grid_size + offs + 3)\n        ID_ = tl.sum(ID, axis=0) // BLOCK_SIZE\n        IH_ = tl.sum(IH, axis=0) // BLOCK_SIZE\n        IW_ = tl.sum(IW, axis=0) // BLOCK_SIZE\n        voxel_grid = (ID_ - 1) * (IH_ - 1) * (IW_ - 1)\n        if voxel_grid > 0:\n            sampled, grid_numel = _voxel_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, ix_in,\n                iy_in, iz_in, ID, IH, IW, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        elif ID_ == 1:\n            sampled, grid_numel = _plane_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, ix_in,\n                iy_in, IH, IW, C, BLOCK_SIZE, mask_out_of_bounds_samples)\n        elif IH_ == 1:\n            sampled, grid_numel = _plane_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, ix_in,\n                iz_in, ID, IW, C, BLOCK_SIZE, mask_out_of_bounds_samples)\n        else:\n            sampled, grid_numel = _plane_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, iy_in,\n                iz_in, ID, IH, C, BLOCK_SIZE, mask_out_of_bounds_samples)\n        out_val += sampled\n        feature_grid_offs += grid_numel\n    if mask_out_of_bounds_samples:\n        in_bounds_mask = is_in_bounds(ix_in, iy_in, iz_in, C, BLOCK_SIZE)\n        out_val *= in_bounds_mask\n    return out_val\n"
    },
    {
      "input": "@triton.jit\ndef sample_grid_rep(feature_grid, feature_grid_sizes, grid_idx, sample_x,\n    sample_y, sample_z, C: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    vec = _voxel_grid_sample(feature_grid, feature_grid_sizes, grid_idx,\n        sample_x, sample_y, sample_z, C, NUM_GRIDS, BLOCK_SIZE,\n        mask_out_of_bounds_samples)\n    return vec\n"
    },
    {
      "input": "@triton.jit\ndef splat_grid_rep(feature_grid, grad_image, feature_grid_sizes, grid_idx,\n    sample_x, sample_y, sample_z, C: 'tl.constexpr', NUM_GRIDS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr'):\n    _voxel_grid_splat(feature_grid, grad_image, feature_grid_sizes,\n        grid_idx, sample_x, sample_y, sample_z, C, NUM_GRIDS, BLOCK_SIZE,\n        mask_out_of_bounds_samples)\n"
    },
    {
      "input": "@triton.jit\ndef hash(x):\n    x = (x >> 16 ^ x) * 73244475\n    x = (x >> 16 ^ x) * 73244475\n    x = x >> 16 ^ x\n    return x\n"
    },
    {
      "input": "@triton.jit\ndef int32_to_float01(x):\n    x_01 = (x + MAX_INT_32_F + MAX_UINT_32_F_EPS) / (MAX_UINT_32_F +\n        MAX_UINT_32_F_EPS)\n    return x_01\n"
    },
    {
      "input": "@triton.jit\ndef pair_hash(x, h):\n    h = h ^ x\n    h = (h << 24) + h * 403\n    return h\n"
    },
    {
      "input": "@triton.jit\ndef int_to_randn(x1, x2, seed):\n    x_hash_1 = hash(x1)\n    x_hash_2 = hash(x2)\n    x_hash_1 = pair_hash(pair_hash(INT32_PRIME, seed), x_hash_1)\n    x_hash_2 = pair_hash(pair_hash(INT32_PRIME, seed + 1), x_hash_2)\n    x_01_1 = int32_to_float01(x_hash_1)\n    x_01_2 = int32_to_float01(x_hash_2)\n    z = tl.sqrt(-2 * tl.log(x_01_1)) * tl.cos(6.28318530718 * x_01_2)\n    return z\n"
    },
    {
      "input": "@triton.jit\ndef int_to_randn_kernel(x1, x2, out, N: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', seed: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) < N\n    x1_buffer = tl.load(x1 + offs, mask=offs_mask)\n    x2_buffer = tl.load(x2 + offs, mask=offs_mask)\n    seed_buffer = tl.full((BLOCK_SIZE,), seed, dtype=tl.int64)\n    r = int_to_randn(x1_buffer, x2_buffer, seed_buffer)\n    tl.store(out + offs, r, mask=offs_mask)\n"
    },
    {
      "input": "@triton.jit\ndef get_sample_randn(pid, step, n_rays, n_steps, BLOCK_SIZE, seed_buffer):\n    offs = pid * BLOCK_SIZE * n_steps + 1\n    i1 = offs + step + tl.arange(0, BLOCK_SIZE) * n_steps\n    i2 = n_rays * n_steps + i1\n    return int_to_randn(i1, i2, seed_buffer)\n"
    },
    {
      "input": "@triton.jit\ndef _contract_pi_one(x, n):\n    x_c = tl.where(n <= 1.0, x, tl.where(tl.abs(tl.abs(x) - n) <= 1e-08, (2 -\n        1 / tl.abs(x)) * (x / tl.abs(x)), x / n))\n    x_c = x_c * 0.5\n    return x_c\n"
    },
    {
      "input": "@triton.jit\ndef contract_pi(x, y, z):\n    n = tl.maximum(tl.maximum(tl.abs(x), tl.abs(y)), tl.abs(z))\n    x_c = _contract_pi_one(x, n)\n    y_c = _contract_pi_one(y, n)\n    z_c = _contract_pi_one(z, n)\n    return x_c, y_c, z_c\n"
    },
    {
      "input": "@triton.jit\ndef depth_inv_sphere(far, disparity_at_inf, n, step):\n    frac_step = (step + 1) / n\n    n_disp = (disparity_at_inf - 1) * frac_step + 1\n    return far * (1 / n_disp)\n"
    },
    {
      "input": "@triton.jit\ndef depth_lin(near, far, n, step):\n    frac_step = step / (n - 1)\n    return (far - near) * frac_step + near\n"
    },
    {
      "input": "@triton.jit\ndef bw_kernel(grad_feature_grid, grad_feature_grid_sizes, directions,\n    origins, grid_idx, near, far, splatting_feature, mask, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', num_rays:\n    'tl.constexpr', grid_channel: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr',\n    feature_channel: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    mask_out_of_bounds_samples: 'tl.constexpr', contract_coords:\n    'tl.constexpr', disparity_at_inf: 'tl.constexpr', grad_splatting_feature):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    depth = near_buffer\n    grad_splatting_feature_buffer = tl.zeros((BLOCK_SIZE, feature_channel),\n        dtype=tl.float32)\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        grad_vec = sample_grid_rep(grad_feature_grid,\n            grad_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, grid_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_vec = grad_vec * mask_buffer\n        grad_splatting_feature_buffer += grad_vec\n    tl.store(grad_splatting_feature + offs_features,\n        grad_splatting_feature_buffer, mask=offs_features_mask)\n"
    },
    {
      "input": "@triton.jit\ndef fw_kernel(feature_grid, feature_grid_sizes, directions, origins,\n    grid_idx, near, far, splatting_feature, mask, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', num_rays:\n    'tl.constexpr', grid_channel: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr',\n    feature_channel: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    mask_out_of_bounds_samples: 'tl.constexpr', contract_coords:\n    'tl.constexpr', disparity_at_inf: 'tl.constexpr'):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    feature_buffer = feature_buffer * mask_buffer\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        splat_grid_rep(feature_buffer, feature_grid, feature_grid_sizes,\n            grid_idx_buffer, sample_x, sample_y, sample_z, grid_channel,\n            NUM_GRIDS, BLOCK_SIZE, mask_out_of_bounds_samples)\n"
    },
    {
      "input": "@triton.jit\ndef bw_kernel_wMLP(grad_feature_grid, grad_feature_grid_sizes, feature_grid,\n    feature_grid_sizes, input_feature_grid, input_feature_grid_sizes,\n    directions, origins, grid_idx, near, far, splatting_feature, mask,\n    mlp_params, DIM_HIDDEN: 'tl.constexpr', DIM_IN: 'tl.constexpr', DIM_OUT:\n    'tl.constexpr', num_samples: 'tl.constexpr', num_samples_inf:\n    'tl.constexpr', num_rays: 'tl.constexpr', grid_channel: 'tl.constexpr',\n    NUM_GRIDS: 'tl.constexpr', feature_channel: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr',\n    contract_coords: 'tl.constexpr', disparity_at_inf: 'tl.constexpr',\n    grad_splatting_feature, grad_mlp_params, grad_input_feature_grid):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    depth = near_buffer\n    grad_splatting_feature_buffer = tl.zeros((BLOCK_SIZE, feature_channel),\n        dtype=tl.float32)\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        prev_vec = sample_grid_rep(input_feature_grid,\n            input_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, feature_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_vec = sample_grid_rep(grad_feature_grid,\n            grad_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, grid_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_vec = grad_vec * mask_buffer\n        fused_feature = feature_buffer + prev_vec\n        splat_grid_rep(grad_splatting, grad_input_feature_grid,\n            input_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, feature_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_splatting_feature_buffer += grad_splatting\n    tl.store(grad_splatting_feature + offs_features,\n        grad_splatting_feature_buffer, mask=offs_features_mask)\n    update_mlp_params(grad_mlp_params, DIM_IN, DIM_HIDDEN)\n"
    },
    {
      "input": "@triton.jit\ndef fw_kernel_wMLP(feature_grid, feature_grid_sizes, input_feature_grid,\n    input_feature_grid_sizes, directions, origins, grid_idx, near, far,\n    splatting_feature, mask, mlp_params, DIM_HIDDEN: 'tl.constexpr', DIM_IN:\n    'tl.constexpr', DIM_OUT: 'tl.constexpr', num_samples: 'tl.constexpr',\n    num_samples_inf: 'tl.constexpr', num_rays: 'tl.constexpr', grid_channel:\n    'tl.constexpr', NUM_GRIDS: 'tl.constexpr', feature_channel:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr', contract_coords: 'tl.constexpr', disparity_at_inf:\n    'tl.constexpr'):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        prev_vec = sample_grid_rep(input_feature_grid,\n            input_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, feature_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        fused_feature = feature_buffer + prev_vec\n        fused_feature = fused_feature * mask_buffer\n        splat_grid_rep(fused_feature, feature_grid, feature_grid_sizes,\n            grid_idx_buffer, sample_x, sample_y, sample_z, grid_channel,\n            NUM_GRIDS, BLOCK_SIZE, mask_out_of_bounds_samples)\n"
    },
    {
      "input": "@triton.jit\ndef index_select_cat_fwd_kernel(output_ptr, source_ptr, index_ptr,\n    num_indices, num_cols, stride0, stride1, BLOCK_SIZE_INDEX:\n    'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    rows = tl.load(index_ptr + indices, mask=indices < num_indices)\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_offsets = source_ptr + rows[:, None] * stride0 + cols[None, :\n        ] * stride1\n    mask = (indices[:, None] < num_indices) & (cols[None, :] < num_cols)\n    output = tl.load(source_offsets, mask=mask)\n    output_offsets = output_ptr + indices[:, None] * stride0 + cols[None, :\n        ] * stride1\n    tl.store(output_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef index_select_cat_bwd_kernel(grad_source_ptr, index_ptr, grad_output_ptr,\n    num_rows, num_indices, num_cols, stride0, stride1, BLOCK_SIZE_INDEX:\n    'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    grad_output_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0,\n        BLOCK_SIZE_INDEX)\n    grad_output_offsets = grad_output_ptr + grad_output_indices[:, None\n        ] * stride0 + cols[None, :] * stride1\n    grad_output_mask = (grad_output_indices[:, None] < num_indices) & (cols\n        [None, :] < num_cols)\n    grad_output = tl.load(grad_output_offsets, mask=grad_output_mask)\n    grad_source_indices = tl.load(index_ptr + grad_output_indices, mask=\n        grad_output_indices < num_indices)\n    grad_source_offsets = grad_source_ptr + grad_source_indices[:, None\n        ] * stride0 + cols[None, :] * stride1\n    tl.store(grad_source_offsets, grad_output, mask=grad_output_mask)\n"
    },
    {
      "input": "@triton.jit\ndef scaled_index_add_fwd_kernel(input_ptr, index_ptr, source_ptr,\n    scaling_ptr, alpha, num_inp_indices, num_src_indices, num_rows,\n    num_cols, stride0, stride1, stride2, BLOCK_SIZE_INDEX: 'tl.constexpr',\n    BLOCK_SIZE_ROW: 'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr',\n    HAS_SCALING: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    pid2 = tl.program_id(axis=2)\n    rows = pid1 * BLOCK_SIZE_ROW + tl.arange(0, BLOCK_SIZE_ROW)\n    cols = pid2 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    source_offsets = source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    source_mask = (source_indices[:, None, None] < num_src_indices) & (rows\n        [None, :, None] < num_rows) & (cols[None, None, :] < num_cols)\n    source = tl.load(source_offsets, mask=source_mask)\n    input_indices = tl.load(index_ptr + source_indices, mask=source_indices <\n        num_src_indices)\n    input_offsets = input_ptr + input_indices[:, None, None] * stride0 + rows[\n        None, :, None] * stride1 + cols[None, None, :] * stride2\n    x = tl.load(input_offsets, mask=source_mask)\n    if HAS_SCALING:\n        scaling = tl.load(scaling_ptr + cols[None, None, :] * stride2, mask\n            =cols[None, None, :] < num_cols)\n        tl.store(input_offsets, x + alpha * scaling * source, mask=source_mask)\n    else:\n        tl.store(input_offsets, x + alpha * source, mask=source_mask)\n"
    },
    {
      "input": "@triton.jit\ndef scaled_index_add_bwd_kernel(grad_output_ptr, grad_source_ptr,\n    grad_scaling_ptr, source_ptr, scaling_ptr, index_ptr, alpha,\n    num_inp_indices, num_src_indices, num_rows, num_cols, stride0, stride1,\n    stride2, BLOCK_SIZE_INDEX: 'tl.constexpr', BLOCK_SIZE_ROW:\n    'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr', HAS_SCALING: 'tl.constexpr'\n    ):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    pid2 = tl.program_id(axis=2)\n    rows = pid1 * BLOCK_SIZE_ROW + tl.arange(0, BLOCK_SIZE_ROW)\n    cols = pid2 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    source_offsets = source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    source_mask = (source_indices[:, None, None] < num_src_indices) & (rows\n        [None, :, None] < num_rows) & (cols[None, None, :] < num_cols)\n    source = tl.load(source_offsets, mask=source_mask)\n    grad_output_indices = tl.load(index_ptr + source_indices, mask=\n        source_indices < num_src_indices)\n    grad_output_offsets = (grad_output_ptr + grad_output_indices * stride0 +\n        rows[None, :, None] * stride1 + cols[None, None, :] * stride2)\n    grad_output = tl.load(grad_output_offsets, mask=source_mask)\n    grad_source_offsets = grad_source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    if HAS_SCALING:\n        scaling = tl.load(scaling_ptr + cols[None, None, :] * stride2, mask\n            =cols[None, None, :] < num_cols)\n        tl.store(grad_source_offsets, alpha * grad_output * scaling, mask=\n            source_mask)\n        grad_scaling_offsets = grad_scaling_ptr + source_indices[:, None, None\n            ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n            ] * stride2\n        tl.store(grad_scaling_offsets, alpha * grad_output * source, mask=\n            source_mask)\n    else:\n        tl.store(grad_source_offsets, alpha * grad_output, mask=source_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_kernel(x_ptr, h1_ptr, w_ptr, eps, stride, N_COLS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', INCLUDE_WEIGHT: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    x_ptr += row * stride\n    h1_ptr += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(x_ptr + cols, mask=cols < N_COLS, other=0.0,\n            eviction_policy='evict_last')\n        _mean += a * a\n    rstd = rsqrt(tl.sum(_mean, axis=0) / N_COLS + eps)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        a = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first')\n        if INCLUDE_WEIGHT:\n            w = tl.load(w_ptr + cols, mask=mask)\n            tl.store(h1_ptr + cols, a * rstd * w, mask=mask)\n        else:\n            tl.store(h1_ptr + cols, a * rstd, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_add_kernel(x_ptr, y_ptr, h1_ptr, w_ptr, eps, stride, N_COLS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', INCLUDE_WEIGHT: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    x_ptr += row * stride\n    y_ptr += row * stride\n    h1_ptr += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        ax = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_last')\n        ay = tl.load(y_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first')\n        a = ax + ay\n        tl.store(x_ptr + cols, a, mask=mask)\n        _mean += a * a\n    rstd = rsqrt(tl.sum(_mean, axis=0) / N_COLS + eps)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        a = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first')\n        if INCLUDE_WEIGHT:\n            w = tl.load(w_ptr + cols, mask=mask)\n            tl.store(h1_ptr + cols, a * rstd * w, mask=mask)\n        else:\n            tl.store(h1_ptr + cols, a * rstd, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rope_padded_kernel(xq, xk, xv, out_q, cache_k, cache_v, seqstartq,\n    seqstartk, seqlenk, theta, linear_scale, use_dynamic_scaling:\n    'tl.constexpr', dynamic_old_context_len: 'tl.constexpr',\n    dynamic_scale_factor: 'tl.constexpr', dynamic_low_freq_factor:\n    'tl.constexpr', dynamic_high_freq_factor: 'tl.constexpr', first_seqpos,\n    seqpos, k_start: 'tl.constexpr', v_start: 'tl.constexpr', n_groups, dim:\n    'tl.constexpr', stride_xqM, stride_xqG, stride_xqH, stride_xkM,\n    stride_xkG, stride_xkH, stride_xvM, stride_xvG, stride_xvH,\n    stride_cachekM, stride_cachekG, stride_cachekH, stride_cachevM,\n    stride_cachevG, stride_cachevH, stride_seqstartq, stride_seqstartk,\n    stride_seqlenk, stride_outqM, stride_outqG, stride_outqH, stride_seqpos,\n    internal_dtype: 'tl.constexpr', const_batch_strides: 'tl.constexpr',\n    cache_padding_length, seqlenk_shift: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', adjacents: 'tl.constexpr'):\n    \"\"\"\n    Each letter in this diagram is a whole row of length dim.\n\n     INPUT      xq        xk       xv\n\n        head_dim \u2500\u25ba\n\n      batch   qqqqqq      kk       vv\n        \u2502     qqqqqq      kk       vv\n        \u25bc     qqqqqq      kk       vv\n\n    head_idx:  (goes across all heads of all 3 inputs)\n              \u25b2     \u25b2     \u25b2 \u25b2      \u25b2 \u25b2\n              \u2502     \u2502     \u2502 \u2502      \u2502 \u2502\n                          \u2502        \u2502\n              0  k_start  \u2502v_start \u2502n_total_heads\n                          \u2502        \u2502\n                          \u2502        \u2502\n                      k_start    v_start\n\n    Output is to out_q (same shape as xq), an xk-shaped part\n    of cache_k and an xv-shaped part of cache_v\n    \"\"\"\n    query_pos_in_batch_elt = tl.program_id(0)\n    batch_elt = tl.program_id(1)\n    group_head_idx = tl.program_id(2)\n    group_idx = group_head_idx % n_groups\n    head_idx = group_head_idx // n_groups\n    if internal_dtype == 'f32':\n        theta = theta\n    elif internal_dtype == 'f64':\n        theta = theta\n    if const_batch_strides:\n        query_pos = query_pos_in_batch_elt + tl.num_programs(1) * batch_elt\n        end_query_pos = tl.num_programs(1) * (batch_elt + 1)\n    else:\n        query_pos = query_pos_in_batch_elt + tl.load(seqstartq + batch_elt *\n            stride_seqstartq)\n        end_query_pos = tl.load(seqstartq + (batch_elt + 1) * stride_seqstartq)\n        if query_pos >= end_query_pos:\n            return\n    is_q = head_idx < k_start\n    is_v = head_idx >= v_start\n    xq += (query_pos * stride_xqM + head_idx * stride_xqH + group_idx *\n        stride_xqG)\n    out_q += (query_pos * stride_outqM + head_idx * stride_outqH + \n        group_idx * stride_outqG)\n    if const_batch_strides:\n        cache_start = cache_padding_length * batch_elt\n    else:\n        cache_start = tl.load(seqstartk + batch_elt * stride_seqstartk)\n    end_of_batch_elt_cache = cache_start + tl.load(seqlenk + batch_elt *\n        stride_seqlenk) + seqlenk_shift\n    cache_pos = end_of_batch_elt_cache - (end_query_pos - query_pos)\n    if seqpos is not None:\n        seq_pos = tl.load(seqpos + query_pos * stride_seqpos)\n    else:\n        seq_pos = cache_pos - cache_start\n        if first_seqpos is not None:\n            seq_pos += tl.load(first_seqpos + batch_elt * stride_seqpos)\n    cache_k += ((head_idx - k_start) * stride_cachekH + cache_pos *\n        stride_cachekM + group_idx * stride_cachekG)\n    xk += query_pos * stride_xkM + (head_idx - k_start\n        ) * stride_xkH + group_idx * stride_xkG\n    in_qk = tl.where(is_q, xq, xk)\n    out_qk = tl.where(is_q, out_q, cache_k)\n    cache_v += ((head_idx - v_start) * stride_cachevH + cache_pos *\n        stride_cachevM + group_idx * stride_cachevG)\n    xv += query_pos * stride_xvM + (head_idx - v_start\n        ) * stride_xvH + group_idx * stride_xvG\n    out = tl.where(is_v, cache_v, out_qk)\n    x_in = tl.where(is_v, xv, in_qk)\n    for offset in range(0, dim // 2, BLOCK_SIZE // 2):\n        c = tl.arange(0, BLOCK_SIZE // 2)\n        powers = (offset + c) * 2.0\n        if adjacents:\n            cols_re = (offset + c) * 2\n            cols_im = cols_re + 1\n        else:\n            cols_re = offset + c\n            cols_im = cols_re + dim // 2\n        mask = cols_im < dim\n        re_x = tl.load(x_in + cols_re, mask=mask)\n        im_x = tl.load(x_in + cols_im, mask=mask)\n        freqs = pow(theta, powers / -dim)\n        if use_dynamic_scaling:\n            lo_freq_wavelen = dynamic_old_context_len / dynamic_low_freq_factor\n            hi_freq_wavelen = (dynamic_old_context_len /\n                dynamic_high_freq_factor)\n            wavelens = 6.28318530718 / freqs\n            is_low_freq = wavelens > lo_freq_wavelen\n            freqs = tl.where(is_low_freq, freqs / dynamic_scale_factor, freqs)\n            is_mid_freq = (hi_freq_wavelen <= wavelens and wavelens <=\n                lo_freq_wavelen)\n            smooth = (dynamic_old_context_len / wavelens -\n                dynamic_low_freq_factor) / (dynamic_high_freq_factor -\n                dynamic_low_freq_factor)\n            freqs = tl.where(is_mid_freq, (1 - smooth) * freqs /\n                dynamic_scale_factor + smooth * freqs, freqs)\n        freqs = seq_pos * freqs / linear_scale\n        sines = tl.sin(freqs)\n        cosines = tl.cos(freqs)\n        re_out = re_x * cosines - im_x * sines\n        im_out = im_x * cosines + re_x * sines\n        re_out_ = tl.where(is_v, re_x, re_out)\n        im_out_ = tl.where(is_v, im_x, im_out)\n        if internal_dtype == 'f64':\n            if re_x.dtype == tl.bfloat16:\n                re_out_ = re_out_\n                im_out_ = im_out_\n        tl.store(out + cols_re, re_out_, mask=mask)\n        tl.store(out + cols_im, im_out_, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef cast_uint32_to_half2(scale_shift):\n    \"\"\"Extract two float16 packed into one int32\"\"\"\n    scale = scale_shift & 65535\n    shift = scale_shift >> 16\n    scale = scale.to(tl.uint16)\n    shift = shift.to(tl.uint16)\n    return scale, shift\n"
    },
    {
      "input": "@triton.jit\ndef dequantize(x_, scale, shift, PACKED_PER_VAL: 'tl.constexpr'):\n    \"\"\"PACKED_PER_VAL is the number of values packed into each element x_.\n    For example, for int4 quantization and x_ of type int32, PACKED_PER_VAL is 8.\n    \"\"\"\n    BLOCK_N: 'tl.constexpr' = x_.shape[0]\n    BLOCK_DMODEL_PACKED: 'tl.constexpr' = x_.shape[1]\n    offsets = tl.arange(0, PACKED_PER_VAL) * (32 // PACKED_PER_VAL)\n    quant_offset = x_[:, :, None, :] >> offsets\n    quant_offset = tl.reshape(quant_offset, (BLOCK_N, BLOCK_DMODEL_PACKED *\n        PACKED_PER_VAL))\n    if PACKED_PER_VAL == 4:\n        fp8_type = (tl.float8e4b8 if torch.version.hip is not None else tl.\n            float8e4nv)\n        dequant = quant_offset.to(tl.uint8).to(fp8_type, bitcast=True\n            ) * scale + shift\n    else:\n        quant_offset = (quant_offset & 15).to(tl.uint16)\n        quant_offset = quant_offset * 32768.0\n        scale_512 = scale * 512\n        dequant = quant_offset * scale_512 + shift\n    return dequant\n"
    },
    {
      "input": "@triton.jit\ndef load_dequantize_k_v_group(K_block_ptr, V_block_ptr,\n    K_scale_shift_block_ptr, V_scale_shift_block_ptr, BOUNDS_CHECKS_N:\n    'tl.constexpr', PACKED_PER_VAL: 'tl.constexpr', PACKED_D_PER_GROUP:\n    'tl.constexpr', FP8_QUANTIZED: 'tl.constexpr', dtype: 'tl.constexpr',\n    group_id: 'tl.constexpr'):\n    \"\"\"Load K/V for a given block. In case of int4/fp8-quantized K/V, dequantize them after loading.\n    If quantization is group-wise, use group_id to advance the pointers to the current group.\n    \"\"\"\n    K_block_ptr = tl.advance(K_block_ptr, (PACKED_D_PER_GROUP * group_id, 0))\n    V_block_ptr = tl.advance(V_block_ptr, (0, PACKED_D_PER_GROUP * group_id))\n    k = tl.load(K_block_ptr, boundary_check=(1,) if BOUNDS_CHECKS_N else ())\n    v = tl.load(V_block_ptr, boundary_check=(0,) if BOUNDS_CHECKS_N else ())\n    if FP8_QUANTIZED:\n        v_scale_shift = tl.load(V_scale_shift_block_ptr, boundary_check=(0,\n            ) if BOUNDS_CHECKS_N else ())\n        v_scale, v_shift = cast_uint32_to_half2(v_scale_shift)\n        v = dequantize(v, v_scale, v_shift, PACKED_PER_VAL)\n        k_scale_shift = tl.load(K_scale_shift_block_ptr, boundary_check=(1,\n            ) if BOUNDS_CHECKS_N else ())\n        k_scale, k_shift = cast_uint32_to_half2(k_scale_shift)\n        k_t = dequantize(tl.trans(k), tl.trans(k_scale), tl.trans(k_shift),\n            PACKED_PER_VAL)\n        k = tl.trans(k_t)\n    elif PACKED_PER_VAL > 1:\n        K_scale_shift_block_ptr = tl.advance(K_scale_shift_block_ptr, (\n            group_id, 0))\n        V_scale_shift_block_ptr = tl.advance(V_scale_shift_block_ptr, (0,\n            group_id))\n        k_scale_shift = tl.load(K_scale_shift_block_ptr, boundary_check=(1,\n            ) if BOUNDS_CHECKS_N else ())\n        v_scale_shift = tl.load(V_scale_shift_block_ptr, boundary_check=(0,\n            ) if BOUNDS_CHECKS_N else ())\n        k_scale, k_shift = cast_uint32_to_half2(k_scale_shift)\n        v_scale, v_shift = cast_uint32_to_half2(v_scale_shift)\n        v = dequantize(v, v_scale, v_shift, PACKED_PER_VAL)\n        k_t = dequantize(tl.trans(k), tl.trans(k_scale), tl.trans(k_shift),\n            PACKED_PER_VAL)\n        k = tl.trans(k_t)\n    return k, v\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_splitK(Q, K, V, sm_scale, Out_splitK, LSE_splitk,\n    block_tables, Seq_len, Seq_starts_k, Seq_starts_q,\n    Seq_starts_q_multiplier, additive_bias, K_fp8_scale_shift,\n    V_fp8_scale_shift, stride_qz, stride_qm, stride_qg, stride_qh,\n    stride_qk, stride_kz, stride_kn, stride_kg, stride_kh, stride_kk,\n    stride_vz, stride_vn, stride_vg, stride_vh, stride_vk, stride_osk_z,\n    stride_osk_g, stride_osk_h, stride_osk_s, stride_osk_m, stride_osk_k,\n    stride_lsek_z, stride_lsek_g, stride_lsek_h, stride_lsek_s,\n    stride_lsek_m, stride_blocktablesz, stride_blocktablesl, stride_bias_b,\n    stride_bias_g, stride_bias_h, stride_bias_qm, stride_bias_km,\n    stride_k_fp8_scale_shift_z: 'tl.constexpr', stride_k_fp8_scale_shift_n:\n    'tl.constexpr', stride_k_fp8_scale_shift_g: 'tl.constexpr',\n    stride_k_fp8_scale_shift_h: 'tl.constexpr', stride_v_fp8_scale_shift_z:\n    'tl.constexpr', stride_v_fp8_scale_shift_n: 'tl.constexpr',\n    stride_v_fp8_scale_shift_g: 'tl.constexpr', stride_v_fp8_scale_shift_h:\n    'tl.constexpr', kv_cache_blocks_per_row: 'tl.constexpr', Z:\n    'tl.constexpr', N_CTX_Q: 'tl.constexpr', N_CTX_K: 'tl.constexpr',\n    BLOCK_N_PER_SPLIT: 'tl.constexpr', H: 'tl.constexpr', G: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', USE_SEQ_LEN: 'tl.constexpr',\n    PACKED_PER_VAL: 'tl.constexpr', N_GROUPS: 'tl.constexpr',\n    BOUNDS_CHECKS_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', IS_SPLITK: 'tl.constexpr', SPLIT_K_EARLY_EXIT:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', NUM_QUERIES_CAUSAL:\n    'tl.constexpr', USE_PAGED_ATTENTION: 'tl.constexpr', PAGE_SIZE:\n    'tl.constexpr', WRITE_LSE: 'tl.constexpr', HAS_ADDITIVE_BIAS:\n    'tl.constexpr'):\n    \"\"\"This kernel can accept non-quantized or int4-quantized keys/values.\n    PACKED_PER_VAL determines the quantization type:\n        - PACKED_PER_VAL == 1 means no quantization\n        - PACKED_PER_VAL == 8 means 4-bit quantization (8 packed quantized values inside one int32)\n    For the quantized case K/V should be int32 tensors.\n    Quantization can be row-wise (when N_GROUPS = 1) or group-wise with N_GROUPS = 2, 4, or 8.\n    Quantization coefficients are stored at the beginning of the row along the last dimension of K/V\n    So K[B, H, M, :] has a form\n    [   quant_coef0, quant_coef1, ...|\n        group0_quant_value0, group0_quant_value1,... |\n        group1_quant_value0, group1_quant_value1,...]\n    where each quant_coef is an int32 which should be interpreted as 2 packed float16: scale and offset.\n\n    Note: this kernel needs to be processed by xformers.triton.vararg_kernel.unroll_varargs\n    before compilation. That will unroll variables marked with \"VAR_ARGS_ARRAY\" into lists.\n    See how FwOp.apply does it below.\n\n    Set IS_SPLITK=False to indicate the MHA result should be written directly.\n    No metadata will be written.\n    \"\"\"\n    internal_dtype = (tl.float64 if Out_splitK.dtype.element_ty is tl.\n        float64 else tl.float32)\n    tl.static_assert(PACKED_PER_VAL == 1 and tl.constexpr(K.dtype.\n        element_ty != tl.int32) or (PACKED_PER_VAL == 4 or PACKED_PER_VAL ==\n        8) and tl.constexpr(K.dtype.element_ty == tl.int32),\n        f'Only int4 and fp8 quantization is supported, K/V should have dtype int32 in the quantized case: PACKED_PER_VAL={PACKED_PER_VAL!r} tl.constexpr(K.dtype)={tl.constexpr(K.dtype)!r} tl.constexpr(K.dtype.element_ty)={tl.constexpr(K.dtype.element_ty)!r}'\n        )\n    tl.static_assert(((N_GROUPS == 1 or N_GROUPS == 2) or N_GROUPS == 4) or\n        N_GROUPS == 8,\n        'Number of quantization groups can be 1 (row-wise quantization), 2, 4, or 8.'\n        )\n    tl.static_assert(N_GROUPS == 1 or K_fp8_scale_shift is None,\n        f'Only row-wise fp8 quantization is supported, but got N_GROUPS={N_GROUPS!r} > 1.'\n        )\n    FP8_QUANTIZED: 'tl.constexpr' = K_fp8_scale_shift is not None\n    INT4_QUANTIZED: 'tl.constexpr' = PACKED_PER_VAL > 1 and not FP8_QUANTIZED\n    PACKED_D_PER_GROUP: 'tl.constexpr' = (BLOCK_DMODEL // PACKED_PER_VAL //\n        N_GROUPS)\n    D_PER_GROUP: 'tl.constexpr' = BLOCK_DMODEL // N_GROUPS\n    start_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_hg = off_zhg % (H * G)\n    off_h = off_hg // G\n    off_g = off_hg % G\n    splitk_idx = tl.program_id(2)\n    if USE_SEQ_LEN:\n        kv_len = tl.load(Seq_len + off_z)\n        if SPLIT_K_EARLY_EXIT and kv_len == 0:\n            return\n    else:\n        kv_len = N_CTX_K\n    if Seq_starts_k is None:\n        start_kv_idx = 0\n    else:\n        start_kv_idx = tl.load(Seq_starts_k + off_z)\n    if Seq_starts_q is None:\n        q_len = N_CTX_Q\n        queries_use_batch_dim = 1\n        off_m = 0\n    else:\n        queries_use_batch_dim = 0\n        off_m = tl.load(Seq_starts_q + off_z) * Seq_starts_q_multiplier\n        q_len = tl.load(Seq_starts_q + off_z + 1\n            ) * Seq_starts_q_multiplier - off_m\n        if q_len == 0:\n            return\n    k_base = K + off_h * stride_kh + off_g * stride_kg\n    v_base = V + off_h * stride_vh + off_g * stride_vg\n    if FP8_QUANTIZED:\n        k_fp8_scale_shift_base = (K_fp8_scale_shift + off_h *\n            stride_k_fp8_scale_shift_h + off_g * stride_k_fp8_scale_shift_g)\n        v_fp8_scale_shift_base = (V_fp8_scale_shift + off_h *\n            stride_v_fp8_scale_shift_h + off_g * stride_v_fp8_scale_shift_g)\n    else:\n        k_fp8_scale_shift_base = None\n        v_fp8_scale_shift_base = None\n    chunk_hi = (splitk_idx + 1) * BLOCK_N_PER_SPLIT\n    chunk_lo = splitk_idx * BLOCK_N_PER_SPLIT\n    ignore_in_first_block = 0\n    if PAGE_SIZE > 0:\n        BLOCKS_IN_PAGE: 'tl.constexpr' = PAGE_SIZE // BLOCK_N\n        is_last_chunk = splitk_idx == tl.num_programs(2) - 1\n        shift = BLOCK_N - 1 if is_last_chunk else 0\n        lo = tl.maximum(chunk_lo, start_kv_idx) // BLOCK_N * BLOCK_N\n        ignore_in_first_block = tl.maximum(0, start_kv_idx - lo)\n        hi = (chunk_hi + shift) // BLOCK_N * BLOCK_N\n        hi = tl.minimum(hi, kv_len + start_kv_idx)\n        block_table = block_tables + stride_blocktablesz * off_z\n        logical_block_idx = lo // BLOCK_N\n    else:\n        lo = chunk_lo\n        hi = tl.minimum(chunk_hi, kv_len)\n        if Seq_starts_k is not None:\n            k_base += start_kv_idx * stride_kn\n            v_base += start_kv_idx * stride_vn\n        else:\n            k_base += off_z * stride_kz\n            v_base += off_z * stride_vz\n        K_block_ptr = tl.make_block_ptr(base=k_base + stride_kk *\n            INT4_QUANTIZED * N_GROUPS, shape=(PACKED_D_PER_GROUP, hi),\n            strides=(stride_kk, stride_kn), offsets=(0, lo), block_shape=(\n            PACKED_D_PER_GROUP, BLOCK_N), order=(0, 1))\n        V_block_ptr = tl.make_block_ptr(base=v_base + stride_vk *\n            INT4_QUANTIZED * N_GROUPS, shape=(hi, PACKED_D_PER_GROUP),\n            strides=(stride_vn, stride_vk), offsets=(lo, 0), block_shape=(\n            BLOCK_N, PACKED_D_PER_GROUP), order=(1, 0))\n        if INT4_QUANTIZED:\n            K_scale_shift_block_ptr = tl.make_block_ptr(base=k_base, shape=\n                (1, hi), strides=(stride_kk, stride_kn), offsets=(0, lo),\n                block_shape=(1, BLOCK_N), order=(0, 1))\n            V_scale_shift_block_ptr = tl.make_block_ptr(base=v_base, shape=\n                (hi, 1), strides=(stride_vn, stride_vk), offsets=(lo, 0),\n                block_shape=(BLOCK_N, 1), order=(1, 0))\n        elif FP8_QUANTIZED:\n            if Seq_starts_k is not None:\n                k_fp8_scale_shift_base += (start_kv_idx *\n                    stride_k_fp8_scale_shift_n)\n                v_fp8_scale_shift_base += (start_kv_idx *\n                    stride_v_fp8_scale_shift_n)\n            else:\n                k_fp8_scale_shift_base += off_z * stride_k_fp8_scale_shift_z\n                v_fp8_scale_shift_base += off_z * stride_v_fp8_scale_shift_z\n            K_scale_shift_block_ptr = tl.make_block_ptr(base=\n                k_fp8_scale_shift_base, shape=(1, hi), strides=(1,\n                stride_k_fp8_scale_shift_n), offsets=(0, lo), block_shape=(\n                1, BLOCK_N), order=(0, 1))\n            V_scale_shift_block_ptr = tl.make_block_ptr(base=\n                v_fp8_scale_shift_base, shape=(hi, 1), strides=(\n                stride_v_fp8_scale_shift_n, 1), offsets=(lo, 0),\n                block_shape=(BLOCK_N, 1), order=(1, 0))\n        else:\n            K_scale_shift_block_ptr = None\n            V_scale_shift_block_ptr = None\n        if HAS_ADDITIVE_BIAS:\n            additive_bias_block_ptr = tl.make_block_ptr(base=additive_bias +\n                off_z * stride_bias_b + off_g * stride_bias_g + off_h *\n                stride_bias_h, shape=(N_CTX_Q, hi), strides=(stride_bias_qm,\n                stride_bias_km), offsets=(start_m * BLOCK_M, lo),\n                block_shape=(BLOCK_M, BLOCK_N), order=(0, 1))\n    if SPLIT_K_EARLY_EXIT and lo >= hi:\n        return\n    Q_block_ptr = tl.make_block_ptr(base=Q + off_m * stride_qm + off_h *\n        stride_qh + off_z * stride_qz * queries_use_batch_dim + off_g *\n        stride_qg, shape=(q_len, BLOCK_DMODEL), strides=(stride_qm,\n        stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M,\n        D_PER_GROUP), order=(1, 0))\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(acc)):\n        acc[i] = tl.zeros([BLOCK_M, D_PER_GROUP], dtype=internal_dtype)\n    qk_scale = sm_scale * 1.44269504\n    q: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(acc)):\n        q[i] = tl.load(tl.advance(Q_block_ptr, (0, i * D_PER_GROUP)),\n            boundary_check=(0,))\n    if IS_CAUSAL:\n        q_offset = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        diag_idx = q_offset[:, None] % NUM_QUERIES_CAUSAL - tl.arange(0,\n            BLOCK_N)[None, :]\n        diag_idx_shifted = tl.constexpr(diag_idx - NUM_QUERIES_CAUSAL + kv_len)\n    for start_n in range(lo, hi, BLOCK_N):\n        if PAGE_SIZE > 0:\n            block_offset_in_page = logical_block_idx % BLOCKS_IN_PAGE\n            logical_page_idx = logical_block_idx // BLOCKS_IN_PAGE\n            physical_page_idx = tl.load(block_table + stride_blocktablesl *\n                logical_page_idx)\n            offset = (physical_page_idx * PAGE_SIZE + block_offset_in_page *\n                BLOCK_N)\n            current_block_size = min(hi - start_n, BLOCK_N)\n            K_block_ptr = tl.make_block_ptr(base=k_base + stride_kk *\n                INT4_QUANTIZED * N_GROUPS, shape=(PACKED_D_PER_GROUP, \n                offset + current_block_size), strides=(stride_kk, stride_kn\n                ), offsets=(0, offset), block_shape=(PACKED_D_PER_GROUP,\n                BLOCK_N), order=(0, 1))\n            V_block_ptr = tl.make_block_ptr(base=v_base + stride_vk *\n                INT4_QUANTIZED * N_GROUPS, shape=(offset +\n                current_block_size, PACKED_D_PER_GROUP), strides=(stride_vn,\n                stride_vk), offsets=(offset, 0), block_shape=(BLOCK_N,\n                PACKED_D_PER_GROUP), order=(1, 0))\n            if INT4_QUANTIZED:\n                K_scale_shift_block_ptr = tl.make_block_ptr(base=k_base,\n                    shape=(1, offset + current_block_size), strides=(\n                    stride_kk, stride_kn), offsets=(0, offset), block_shape\n                    =(1, BLOCK_N), order=(0, 1))\n                V_scale_shift_block_ptr = tl.make_block_ptr(base=v_base,\n                    shape=(offset + current_block_size, 1), strides=(\n                    stride_vn, stride_vk), offsets=(offset, 0), block_shape\n                    =(BLOCK_N, 1), order=(1, 0))\n            elif FP8_QUANTIZED:\n                K_scale_shift_block_ptr = tl.make_block_ptr(base=\n                    k_fp8_scale_shift_base, shape=(1, offset +\n                    current_block_size), strides=(1,\n                    stride_k_fp8_scale_shift_n), offsets=(0, offset),\n                    block_shape=(1, BLOCK_N), order=(0, 1))\n                V_scale_shift_block_ptr = tl.make_block_ptr(base=\n                    v_fp8_scale_shift_base, shape=(offset +\n                    current_block_size, 1), strides=(\n                    stride_v_fp8_scale_shift_n, 1), offsets=(offset, 0),\n                    block_shape=(BLOCK_N, 1), order=(1, 0))\n            else:\n                K_scale_shift_block_ptr = None\n                V_scale_shift_block_ptr = None\n            logical_block_idx += 1\n        k: \"'VAR_ARGS_ARRAY'\"\n        v: \"'VAR_ARGS_ARRAY'\"\n        for i in range(len(acc)):\n            k[i], v[i] = load_dequantize_k_v_group(K_block_ptr, V_block_ptr,\n                K_scale_shift_block_ptr, V_scale_shift_block_ptr,\n                BOUNDS_CHECKS_N, PACKED_PER_VAL, PACKED_D_PER_GROUP,\n                FP8_QUANTIZED, Q.dtype.element_ty, i)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        for i in range(len(acc)):\n            qk += tl.dot(q[i], k[i])\n        qk *= qk_scale\n        if start_n == lo and ignore_in_first_block > 0:\n            qk = tl.where(tl.arange(0, BLOCK_N) < ignore_in_first_block,\n                float('-inf'), qk)\n        if HAS_ADDITIVE_BIAS:\n            loaded_bias = tl.load(additive_bias_block_ptr, boundary_check=(\n                0, 1) if BOUNDS_CHECKS_N else (0,))\n            qk += loaded_bias * 1.44269504\n            additive_bias_block_ptr = tl.advance(additive_bias_block_ptr, (\n                0, BLOCK_N))\n        if BOUNDS_CHECKS_N:\n            qk = tl.where(tl.arange(0, BLOCK_N) < hi - start_n, qk, float(\n                '-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(diag_idx_shifted >= start_n, qk, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        if HAS_ADDITIVE_BIAS or IS_CAUSAL:\n            alpha = tl.where(m_i_new == float('-inf'), 0, alpha)\n            p = tl.where(m_i_new[:, None] == float('-inf'), 0, p)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        p = p\n        for i in range(len(acc)):\n            acc[i] *= alpha[:, None]\n            acc[i] += tl.dot(p, v[i])\n        if not PAGE_SIZE:\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n            if PACKED_PER_VAL > 1:\n                K_scale_shift_block_ptr = tl.advance(K_scale_shift_block_ptr,\n                    (0, BLOCK_N))\n                V_scale_shift_block_ptr = tl.advance(V_scale_shift_block_ptr,\n                    (BLOCK_N, 0))\n    O_block_ptr = tl.make_block_ptr(base=Out_splitK + off_z * stride_osk_z *\n        queries_use_batch_dim + off_m * stride_osk_m + off_g * stride_osk_g +\n        off_h * stride_osk_h + splitk_idx * stride_osk_s, shape=(q_len,\n        D_PER_GROUP), strides=(stride_osk_m, 1), offsets=(start_m * BLOCK_M,\n        0), block_shape=(BLOCK_M, D_PER_GROUP), order=(1, 0))\n    for i in range(len(acc)):\n        attn_out = tl.where(l_i[:, None] == 0, 0.0, acc[i] / l_i[:, None])\n        tl.store(tl.advance(O_block_ptr, (0, i * D_PER_GROUP)), attn_out,\n            boundary_check=(0,))\n    if WRITE_LSE:\n        LSE_splitk_ptr = (LSE_splitk + off_z * stride_lsek_z *\n            queries_use_batch_dim + off_m * stride_lsek_m + off_g *\n            stride_lsek_g + off_h * stride_lsek_h + splitk_idx *\n            stride_lsek_s + (start_m * BLOCK_M + tl.arange(0, BLOCK_M)) *\n            stride_lsek_m)\n        mask = start_m * BLOCK_M + tl.arange(0, BLOCK_M) < q_len\n        lse_dtype = LSE_splitk.dtype.element_ty\n        tl.store(LSE_splitk_ptr, (tl.math.log2(l_i) + m_i) / 1.44269504,\n            mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _splitK_reduce(Out_splitK, LSE_splitK, Out, LSE, split_k:\n    'tl.constexpr', splitK_pow2: 'tl.constexpr', stride_osk_z:\n    'tl.constexpr', stride_osk_g: 'tl.constexpr', stride_osk_h:\n    'tl.constexpr', stride_osk_s: 'tl.constexpr', stride_osk_m:\n    'tl.constexpr', stride_osk_k: 'tl.constexpr', stride_lsek_z:\n    'tl.constexpr', stride_lsek_g: 'tl.constexpr', stride_lsek_h:\n    'tl.constexpr', stride_lsek_s: 'tl.constexpr', stride_lsek_m:\n    'tl.constexpr', stride_oz: 'tl.constexpr', stride_og: 'tl.constexpr',\n    stride_oh: 'tl.constexpr', stride_om: 'tl.constexpr', stride_ok:\n    'tl.constexpr', stride_lse_z: 'tl.constexpr', stride_lse_g:\n    'tl.constexpr', stride_lse_h: 'tl.constexpr', stride_lse_m:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', H: 'tl.constexpr', G:\n    'tl.constexpr', WRITE_LSE: 'tl.constexpr'):\n    off_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    Out_splitK_ptr = (Out_splitK + stride_osk_z * off_z + stride_osk_g *\n        off_g + stride_osk_h * off_h + stride_osk_m * off_m + tl.arange(0,\n        BLOCK_SIZE)[None, :] + stride_osk_s * tl.arange(0, splitK_pow2)[:,\n        None])\n    LSE_splitK_ptr0 = (LSE_splitK + stride_lsek_z * off_z + stride_lsek_g *\n        off_g + stride_lsek_h * off_h + stride_lsek_m * off_m + \n        stride_lsek_s * tl.arange(0, splitK_pow2))\n    if splitK_pow2 > split_k:\n        mask_1d = tl.arange(0, splitK_pow2) < split_k\n        mask_2d = mask_1d[:, None]\n        lse_splitk = tl.load(LSE_splitK_ptr0, mask=mask_1d, other=float('-inf')\n            )\n        lse_max = tl.max(lse_splitk)\n        out_splitk = tl.load(Out_splitK_ptr, mask=mask_2d, other=0)\n        lse_splitk = tl.load(LSE_splitK_ptr0, mask=mask_1d, other=float('-inf')\n            )\n    else:\n        lse_splitk = tl.load(LSE_splitK_ptr0)\n        lse_max = tl.max(lse_splitk)\n        out_splitk = tl.load(Out_splitK_ptr)\n        lse_splitk = tl.load(LSE_splitK_ptr0)\n    sumexp_normalized_splitk = tl.math.exp2((lse_splitk - lse_max) * 1.44269504\n        )\n    sumexp_normalized = tl.sum(sumexp_normalized_splitk, axis=0)\n    numerator_normalized = tl.sum(out_splitk * sumexp_normalized_splitk[:,\n        None], axis=0)\n    acc = numerator_normalized / sumexp_normalized\n    acc = tl.where(lse_max == float('-inf'), 0.0, acc)\n    Out_ptr = (Out + stride_oz * off_z + stride_oh * off_h + stride_og *\n        off_g + stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    if acc.dtype is tl.float64 and Out.dtype.element_ty is not tl.float64:\n        acc = acc\n    tl.store(Out_ptr, acc)\n    if WRITE_LSE:\n        l_ptrs = (LSE + off_z * stride_lse_z + off_g * stride_lse_g + off_h *\n            stride_lse_h + off_m * stride_lse_m)\n        to_store = lse_max + tl.math.log2(sumexp_normalized) / 1.44269504\n        to_store = tl.where(lse_max == float('-inf'), lse_max, to_store)\n        tl.store(l_ptrs, to_store)\n"
    },
    {
      "input": "@triton.jit\ndef _splitK_reduce_varargs(Out_splitK: \"'VAR_ARGS_ARRAY'\", LSE_splitK:\n    \"'VAR_ARGS_ARRAY'\", Out, LSE, stride_osk_z: \"'VAR_ARGS_ARRAY'\",\n    stride_osk_g: \"'VAR_ARGS_ARRAY'\", stride_osk_h: \"'VAR_ARGS_ARRAY'\",\n    stride_osk_m: \"'VAR_ARGS_ARRAY'\", stride_osk_k: \"'VAR_ARGS_ARRAY'\",\n    stride_lsek_z: \"'VAR_ARGS_ARRAY'\", stride_lsek_g: \"'VAR_ARGS_ARRAY'\",\n    stride_lsek_h: \"'VAR_ARGS_ARRAY'\", stride_lsek_m: \"'VAR_ARGS_ARRAY'\",\n    stride_oz, stride_og, stride_oh, stride_om, stride_ok, stride_lse_z,\n    stride_lse_g, stride_lse_h, stride_lse_m, BLOCK_SIZE: 'tl.constexpr', H:\n    'tl.constexpr', G: 'tl.constexpr', WRITE_LSE: 'tl.constexpr'):\n    \"\"\"\n    This version of reduce kernel takes attention and LSE of chunks as lists of tensors,\n    as opposed to _splitK_reduce, which takes each as a stacked tensor.\n    \"\"\"\n    off_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    out_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        out_splitk_offset[i] = stride_osk_z[i] * off_z + stride_osk_g[i\n            ] * off_g + stride_osk_h[i] * off_h + stride_osk_m[i\n            ] * off_m + tl.arange(0, BLOCK_SIZE)\n    lse_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        lse_splitk_offset[i] = stride_lsek_z[i] * off_z + stride_lsek_g[i\n            ] * off_g + stride_lsek_h[i] * off_h + stride_lsek_m[i] * off_m\n    lse_max = float('-inf')\n    for split_k_idx in range(len(Out_splitK)):\n        LSE_splitK_ptr = LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        lse_splitk = tl.load(LSE_splitK_ptr)\n        lse_max = tl.maximum(lse_max, lse_splitk)\n    sumexp_normalized = 0.0\n    numerator_normalized = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for split_k_idx in range(len(Out_splitK)):\n        out_splitk = tl.load(Out_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx])\n        lse_splitk = tl.load(LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx])\n        sumexp_normalized_splitk = tl.math.exp2((lse_splitk - lse_max) * \n            1.44269504)\n        sumexp_normalized += sumexp_normalized_splitk\n        numerator_normalized += out_splitk * sumexp_normalized_splitk\n    acc = numerator_normalized / sumexp_normalized\n    acc = tl.where(lse_max == float('-inf'), 0.0, acc)\n    Out_ptr = (Out + stride_oz * off_z + stride_oh * off_h + stride_og *\n        off_g + stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    if acc.dtype is tl.float64 and Out.dtype.element_ty is not tl.float64:\n        acc = acc\n    tl.store(Out_ptr, acc)\n    if WRITE_LSE:\n        l_ptrs = (LSE + off_z * stride_lse_z + off_g * stride_lse_g + off_h *\n            stride_lse_h + off_m * stride_lse_m)\n        to_store = lse_max + tl.math.log2(sumexp_normalized) / 1.44269504\n        to_store = tl.where(lse_max == float('-inf'), lse_max, to_store)\n        tl.store(l_ptrs, to_store)\n"
    },
    {
      "input": "@triton.jit\ndef _splitK_reduce_varargs_backward(Out_splitK: \"'VAR_ARGS_ARRAY'\",\n    LSE_splitK: \"'VAR_ARGS_ARRAY'\", Dout_splitK: \"'VAR_ARGS_ARRAY'\",\n    DLSE_splitK: \"'VAR_ARGS_ARRAY'\", Out, LSE, DOut, DLSE, stride_osk_z:\n    \"'VAR_ARGS_ARRAY'\", stride_osk_g: \"'VAR_ARGS_ARRAY'\", stride_osk_h:\n    \"'VAR_ARGS_ARRAY'\", stride_osk_m: \"'VAR_ARGS_ARRAY'\", stride_osk_k:\n    \"'VAR_ARGS_ARRAY'\", stride_lsek_z: \"'VAR_ARGS_ARRAY'\", stride_lsek_g:\n    \"'VAR_ARGS_ARRAY'\", stride_lsek_h: \"'VAR_ARGS_ARRAY'\", stride_lsek_m:\n    \"'VAR_ARGS_ARRAY'\", stride_oz, stride_og, stride_oh, stride_om,\n    stride_ok, stride_lse_z, stride_lse_g, stride_lse_h, stride_lse_m,\n    stride_doz, stride_dog, stride_doh, stride_dom, stride_dok,\n    stride_dlse_z, stride_dlse_g, stride_dlse_h, stride_dlse_m, BLOCK_SIZE:\n    'tl.constexpr', H: 'tl.constexpr', G: 'tl.constexpr'):\n    \"\"\"\n    Backward for _splitK_reduce_varargs. Similar to forward, it takes\n    attention and LSE of chunks as lists of tensors,\n    and outputs the corresponding gradients in the same format.\n    \"\"\"\n    off_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    out_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        out_splitk_offset[i] = stride_osk_z[i] * off_z + stride_osk_g[i\n            ] * off_g + stride_osk_h[i] * off_h + stride_osk_m[i\n            ] * off_m + tl.arange(0, BLOCK_SIZE)\n    lse_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        lse_splitk_offset[i] = stride_lsek_z[i] * off_z + stride_lsek_g[i\n            ] * off_g + stride_lsek_h[i] * off_h + stride_lsek_m[i] * off_m\n    lse_max = float('-inf')\n    for split_k_idx in range(len(Out_splitK)):\n        LSE_splitK_ptr = LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        lse_splitk = tl.load(LSE_splitK_ptr)\n        lse_max = tl.maximum(lse_max, lse_splitk)\n    offset_out = (stride_oz * off_z + stride_oh * off_h + stride_og * off_g +\n        stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    offset_dout = (stride_doz * off_z + stride_doh * off_h + stride_dog *\n        off_g + stride_dom * off_m + tl.arange(0, BLOCK_SIZE))\n    out = tl.load(Out + offset_out)\n    dattn = tl.load(DOut + offset_dout)\n    offset_lse = (stride_lse_z * off_z + stride_lse_h * off_h + \n        stride_lse_g * off_g + stride_lse_m * off_m)\n    offset_dlse = (stride_dlse_z * off_z + stride_dlse_h * off_h + \n        stride_dlse_g * off_g + stride_dlse_m * off_m)\n    lse = tl.load(LSE + offset_lse)\n    dlse = tl.load(DLSE + offset_dlse)\n    for split_k_idx in range(len(Out_splitK)):\n        out_splitk = tl.load(Out_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx])\n        lse_splitk = tl.load(LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx])\n        dout_splitk_ptr = Dout_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx]\n        dlse_splitk_ptr = DLSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        dattn_dattn_i = tl.exp(lse_splitk - lse_max) / tl.exp(lse - lse_max)\n        dX_dattn_i = dattn_dattn_i * dattn\n        tl.store(dout_splitk_ptr, dX_dattn_i)\n        dattn_dlse_i = (out_splitk - out) * dattn_dattn_i\n        dlse_dlse_i = dattn_dattn_i\n        dX_dlse_i = dlse_dlse_i * dlse + tl.sum(dattn_dlse_i * dattn)\n        tl.store(dlse_splitk_ptr, dX_dlse_i)\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.jit\ndef _attention_core(Q, K, V, mask, bias, sm_scale, TMP, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_on, Z, H, N_CTX, BATCH, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    use_mask: 'tl.constexpr', use_bias: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    if use_bias:\n        batch_2 = Z // BATCH\n        off_hz_bias = off_hz // (batch_2 * H) * H + off_hz % H\n        offs_base_bias = off_hz_bias * (N_CTX * N_CTX) + offs_m[:, None\n            ] * N_CTX + offs_n[None, :]\n    if use_mask:\n        off_hz_mask = off_hz // H\n        offs_base_mask = off_hz_mask * N_CTX\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q_load_mask = offs_m[:, None] < N_CTX\n    q = tl.load(q_ptrs, mask=q_load_mask, other=0.0)\n    for start_n in range(0, N_CTX, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        load_mask = (start_n + offs_n)[:, None] < N_CTX\n        k = tl.load(k_ptrs + start_n * stride_kn, mask=load_mask, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= N_CTX, float('-1e20'), qk)\n        qk = tl.where((start_n + offs_n)[None, :] >= N_CTX, float('-1e20'), qk)\n        if use_bias:\n            bias_load_mask = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            bias_load_mask = tl.where(offs_m[:, None] >= N_CTX, 1.0,\n                bias_load_mask)\n            bias_load_mask = tl.where((start_n + offs_n)[None, :] >= N_CTX,\n                1.0, bias_load_mask)\n            bias_data = tl.load(bias + offs_base_bias + start_n, mask=\n                bias_load_mask == 0.0, other=0.0)\n            qk += bias_data\n        if use_mask:\n            mask_data = tl.load(mask + offs_base_mask + offs_n + start_n,\n                mask=start_n + offs_n < N_CTX, other=0.0)\n            qk = tl.where(mask_data[None, :] == 0.0, float('-1e20'), qk)\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale, mask=offs_m < N_CTX)\n        acc_scale = tl.load(TMP + off_hz * N_CTX + start_m * BLOCK_M + tl.\n            arange(0, BLOCK_M), mask=start_m * BLOCK_M + tl.arange(0,\n            BLOCK_M) < N_CTX, other=float(0.0))\n        acc = acc * acc_scale[:, None]\n        load_mask = (start_n + offs_n)[:, None] < N_CTX\n        v = tl.load(v_ptrs + start_n * stride_vn, mask=load_mask, other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    out_store_mask = offs_m[:, None] < N_CTX\n    tl.store(out_ptrs, acc, mask=out_store_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(Out, A, Weight, Bias, Mean, Rstd, stride, N, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Out += row * stride\n    A += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0)\n        a = tl.where(cols < N, a - mean, 0.0)\n        _var += a * a\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        weight = tl.load(Weight + cols, mask=mask)\n        bias = tl.load(Bias + cols, mask=mask)\n        a = tl.load(A + cols, mask=mask, other=0.0)\n        a_hat = (a - mean) * rstd\n        out = a_hat * weight + bias\n        tl.store(Out + cols, out, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(_DA, _DOut, _A, Weight, Mean, Rstd, stride,\n    NumRows, NumCols, eps, BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    row = pid\n    A = _A + row * stride\n    DOut = _DOut + row * stride\n    DA = _DA + row * stride\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    _mean1 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n    _mean2 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n    for off in range(0, NumCols, BLOCK_SIZE_N):\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\n        mask = cols < NumCols\n        a = tl.load(A + cols, mask=mask, other=0)\n        dout = tl.load(DOut + cols, mask=mask, other=0)\n        weight = tl.load(Weight + cols, mask=mask, other=0)\n        a_hat = (a - mean) * rstd\n        wdout = weight * dout\n        _mean1 += a_hat * wdout\n        _mean2 += wdout\n    mean1 = tl.sum(_mean1, axis=0) / NumCols\n    mean2 = 0.0\n    mean2 = tl.sum(_mean2, axis=0) / NumCols\n    for off in range(0, NumCols, BLOCK_SIZE_N):\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\n        mask = cols < NumCols\n        a = tl.load(A + cols, mask=mask, other=0)\n        dout = tl.load(DOut + cols, mask=mask, other=0)\n        weight = tl.load(Weight + cols, mask=mask, other=0)\n        a_hat = (a - mean) * rstd\n        wdout = weight * dout\n        da = (wdout - (a_hat * mean1 + mean2)) * rstd\n        tl.store(DA + cols, da, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dwdb(A, DOut, Mean, Var, DW, DB, M, N, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    UNROLL: 'tl.constexpr' = 4\n    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n        for j in range(UNROLL):\n            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            mask = (rows[:, None] < M) & (cols[None, :] < N)\n            offs = rows[:, None] * N + cols[None, :]\n            a = tl.load(A + offs, mask=mask, other=0.0)\n            dout = tl.load(DOut + offs, mask=mask, other=0.0)\n            mean = tl.load(Mean + rows, mask=rows < M, other=0.0)\n            rstd = tl.load(Var + rows, mask=rows < M, other=0.0)\n            a_hat = (a - mean[:, None]) * rstd[:, None]\n            dw += dout * a_hat\n            db += dout\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(DW + cols, sum_dw, mask=cols < N)\n    tl.store(DB + cols, sum_db, mask=cols < N)\n"
    },
    {
      "input": "@triton.jit\ndef _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n    col_offsets, n_cols, use_mask: 'tl.constexpr', use_bias: 'tl.constexpr'):\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    if use_bias:\n        bias = tl.load(bias_ptrs, mask=col_offsets < n_cols, other=float(\n            '-inf'))\n        row += bias\n    if use_mask:\n        mask = tl.load(mask_ptrs, mask=col_offsets < n_cols, other=float(\n            '-inf'))\n        row = tl.where(mask == 0, float('-1e20'), row)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n    col_offsets, n_cols, is_bf16: 'tl.constexpr'):\n    output_row = tl.load(output_ptrs, mask=col_offsets < n_cols, other=float(0)\n        )\n    d_output_row = tl.load(d_output_ptrs, mask=col_offsets < n_cols, other=\n        float(0))\n    if is_bf16:\n        output_row = output_row\n        d_output_row = d_output_row\n    row_sum = tl.sum(output_row * d_output_row, axis=0)\n    d_softmax_output = (d_output_row - row_sum) * output_row\n    tl.store(d_input_ptrs, d_softmax_output, mask=col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_mask_bias_kernel(output_ptr, input_ptr, mask_ptr, bias_ptr,\n    input_row_stride, output_row_stride, n_cols, n_heads, BLOCK_SIZE:\n    'tl.constexpr', use_mask: 'tl.constexpr', use_bias: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_row_ptr = input_ptr + row_idx * input_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n    input_ptrs = input_row_ptr + col_offsets\n    output_ptrs = output_row_ptr + col_offsets\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + row_idx // (n_heads * n_cols) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + row_idx % (n_heads * n_cols) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n        col_offsets, n_cols, use_mask, use_bias)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_mask_bias_kernel_two_rows(output_ptr, input_ptr, mask_ptr,\n    bias_ptr, input_row_stride, output_row_stride, n_cols, n_heads,\n    BLOCK_SIZE: 'tl.constexpr', use_mask: 'tl.constexpr', use_bias:\n    'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_row_ptr = input_ptr + 2 * row_idx * input_row_stride\n    output_row_ptr = output_ptr + 2 * row_idx * output_row_stride\n    input_ptrs = input_row_ptr + col_offsets\n    output_ptrs = output_row_ptr + col_offsets\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + 2 * row_idx // (n_heads * n_cols) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + 2 * row_idx % (n_heads * n_cols) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n        col_offsets, n_cols, use_mask, use_bias)\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + (2 * row_idx + 1) // (n_heads * n_cols\n            ) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + (2 * row_idx + 1) % (n_heads * n_cols\n            ) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs + n_cols, output_ptrs + n_cols, mask_ptrs,\n        bias_ptrs, col_offsets, n_cols, use_mask, use_bias)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_grad_kernel(d_output_ptr, output_ptr, d_input_ptr,\n    d_output_row_stride, output_row_stride, d_input_row_stride, n_cols,\n    BLOCK_SIZE: 'tl.constexpr', is_bf16: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n    d_output_row_ptr = d_output_ptr + row_idx * d_output_row_stride\n    d_input_row_ptr = d_input_ptr + row_idx * d_input_row_stride\n    output_ptrs = output_row_ptr + col_offsets\n    d_output_ptrs = d_output_row_ptr + col_offsets\n    d_input_ptrs = d_input_row_ptr + col_offsets\n    _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n        col_offsets, n_cols, is_bf16)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_grad_kernel_two_rows(d_output_ptr, output_ptr, d_input_ptr,\n    d_output_row_stride, output_row_stride, d_input_row_stride, n_cols,\n    BLOCK_SIZE: 'tl.constexpr', is_bf16: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    output_row_ptr = output_ptr + 2 * row_idx * output_row_stride\n    d_output_row_ptr = d_output_ptr + 2 * row_idx * d_output_row_stride\n    d_input_row_ptr = d_input_ptr + 2 * row_idx * d_input_row_stride\n    output_ptrs = output_row_ptr + col_offsets\n    d_output_ptrs = d_output_row_ptr + col_offsets\n    d_input_ptrs = d_input_row_ptr + col_offsets\n    _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n        col_offsets, n_cols, is_bf16)\n    _softmax_grad_core(output_ptrs + n_cols, d_output_ptrs + n_cols, \n        d_input_ptrs + n_cols, col_offsets, n_cols, is_bf16)\n"
    },
    {
      "input": "@triton.jit\ndef triton_local_scan(x, y, K: 'tl.constexpr', flip: 'tl.constexpr', BC:\n    'tl.constexpr', BH: 'tl.constexpr', BW: 'tl.constexpr', DC:\n    'tl.constexpr', DH: 'tl.constexpr', DW: 'tl.constexpr', NH:\n    'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    _i = (tl.arange(0, BH) + BH * i_h)[:, None]\n    _j = (tl.arange(0, BW) + BW * i_w)[None, :]\n    _c_offset = (DW // K * (_i // K) + _j // K) * K * K + _i % K * K + _j % K\n    if flip:\n        _c_offset = DH * DW - _c_offset - 1\n    p_y = y + i_b * _tmp1 + _tmp0 + _c_offset\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y + _idx, _x, mask=_mask_hw)\n    tl.debug_barrier()\n"
    },
    {
      "input": "@triton.jit\ndef triton_local_reverse(x, y, K: 'tl.constexpr', flip: 'tl.constexpr', BC:\n    'tl.constexpr', BH: 'tl.constexpr', BW: 'tl.constexpr', DC:\n    'tl.constexpr', DH: 'tl.constexpr', DW: 'tl.constexpr', NH:\n    'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    _i = (tl.arange(0, BH) + BH * i_h)[:, None]\n    _j = (tl.arange(0, BW) + BW * i_w)[None, :]\n    _o = _i * DW + _j\n    _i = _o // (K * K) // (DW // K) * K + _o % (K * K) // K\n    _j = _o // (K * K) % (DW // K) * K + _o % (K * K) % K\n    _c_offset = _i * DW + _j\n    if flip:\n        _c_offset = DH * DW - _c_offset - 1\n    p_y = y + i_b * _tmp1 + _tmp0 + _c_offset\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y + _idx, _x, mask=_mask_hw)\n    tl.debug_barrier()\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate,\n    stride_state_batch, stride_state_dim, stride_state_dstate,\n    stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim,\n    stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch,\n    stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim,\n    stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim,\n    DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS:\n    'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    state_ptr += pid_b * stride_state_batch\n    x_ptr += pid_b * stride_x_batch\n    dt_ptr += pid_b * stride_dt_batch\n    B_ptr += pid_b * stride_B_batch\n    C_ptr += pid_b * stride_C_batch\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch\n    out_ptr += pid_b * stride_out_batch\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n    if DT_SOFTPLUS:\n        dt = tl.log(1.0 + tl.exp(dt))\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n        dstate), other=0.0)\n    dA = tl.exp(A * dt[:, None])\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    dB = B[None, :] * dt[:, None]\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n"
    },
    {
      "input": "@triton.jit\ndef add_kernel(x_ptr, y_ptr, length, output_ptr, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < length\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef fused_attention_kernel(Q, K, V, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, L, M, Out, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(q_ptrs)\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        k = tl.load(k_ptrs)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(qk - m_curr[:, None])\n        l_curr = tl.sum(p, 1) + l_prev\n        l_rcp = 1.0 / l_curr\n        p *= l_rcp\n        acc *= (l_prev * l_rcp)[:, None]\n        p = p\n        v = tl.load(v_ptrs)\n        acc += tl.dot(p, v)\n        l_prev = l_curr\n        m_prev = m_curr\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vk\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_prev)\n    tl.store(m_ptrs, m_prev)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n"
    },
    {
      "input": "@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, c_ptr, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr', K_EXACTLY_DIVISIBLE_BY_BLOCK: 'tl.constexpr'\n    ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k_remaining in range(K, 0, -BLOCK_SIZE_K):\n        if K_EXACTLY_DIVISIBLE_BY_BLOCK:\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n        else:\n            mask = tl.arange(0, BLOCK_SIZE_K) < k_remaining\n            a = tl.load(a_ptrs, mask=mask[None, :], other=0.0)\n            b = tl.load(b_ptrs, mask=mask[:, None], other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef relu(x):\n    return tl.where(x >= 0, x, 0)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_kernel(input_ptr, output_ptr, input_row_stride: 'tl.constexpr',\n    output_row_stride: 'tl.constexpr', n_cols: 'tl.constexpr', block_size:\n    'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, block_size)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef tanh_kernel(x_ptr, length, output_ptr, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < length\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = libdevice.tanh(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef liger_cross_entropy_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr,\n    loss_stride, n_cols, n_non_ignore, ignore_index, label_smoothing:\n    'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    This kernel computes both cross entropy loss and the gradient of the input.\n    We only consider hard label + mean reduction for now. Please refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for the math.\n\n    Parameters:\n    X_ptr: Pointer to input tensor.\n    X_stride (int): The stride of the input tensor.\n    Y_ptr: Pointer to target tensor.\n    Y_stride (int): The stride of the target tensor.\n    loss_ptr: Pointer to tensor to store the loss.\n    loss_stride (int): The stride of the loss tensor.\n    n_cols (int): The number of columns in the input tensor.\n    n_non_ignore (int): The number of non-ignored elements in the batch.\n    ignore_index (int): The index to ignore in the target.\n    label_smoothing (float): The amount of smoothing when computing the loss, where 0.0 means no smoothing.\n    reduction (str): The string for the reduction to apply\n    BLOCK_SIZE (int): The block size for Triton operations.\n    \"\"\"\n    program_id = tl.program_id(0)\n    Y_ptr += program_id * Y_stride\n    y = tl.load(Y_ptr)\n    X_ptr += program_id * X_stride\n    if y == ignore_index:\n        for i in range(0, n_cols, BLOCK_SIZE):\n            X_offsets = i + tl.arange(0, BLOCK_SIZE)\n            tl.store(X_ptr + X_offsets, 0.0, mask=X_offsets < n_cols)\n        return\n    loss_ptr += program_id * loss_stride\n    m = float('-inf')\n    d = 0.0\n    ori_X_y = tl.load(X_ptr + y)\n    scaled_x_sum = 0.0\n    eps = label_smoothing / n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        block_max = tl.max(X_block)\n        if label_smoothing > 0:\n            scaled_x_sum += tl.sum(tl.where(X_offsets < n_cols, -eps *\n                X_block, 0.0))\n        m_new = tl.maximum(m, block_max)\n        d = d * tl.exp(m - m_new) + tl.sum(tl.exp(X_block - m_new))\n        m = m_new\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        if reduction == 'mean':\n            X_block = (tl.exp(X_block - m) / d - eps) / n_non_ignore\n        else:\n            X_block = tl.exp(X_block - m) / d - eps\n        tl.store(X_ptr + X_offsets, X_block, mask=X_offsets < n_cols)\n    tl.debug_barrier()\n    loss = -(ori_X_y - m - tl.log(d))\n    if label_smoothing > 0:\n        smooth_loss = scaled_x_sum + label_smoothing * (m + tl.log(d))\n        loss = loss * (1 - label_smoothing) + smooth_loss\n    if reduction == 'mean':\n        loss = loss / n_non_ignore\n    X_y = tl.load(X_ptr + y)\n    if reduction == 'mean':\n        X_y += -(1 - label_smoothing) / n_non_ignore\n    else:\n        X_y += -(1 - label_smoothing)\n    tl.store(loss_ptr, loss)\n    tl.store(X_ptr + y, X_y)\n"
    },
    {
      "input": "@triton.jit\ndef embedding_forward_kernel(embeddings_ptr, indices_ptr, output_ptr,\n    n_elements, embedding_dim: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    embedding_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    embeddings = tl.load(embeddings_ptr + embedding_offsets, mask=mask_m[:,\n        None] & mask_n[None, :], other=0.0)\n    output_offsets = offsets_m[:, None] * embedding_dim + offsets_n[None, :]\n    tl.store(output_ptr + output_offsets, embeddings, mask=mask_m[:, None] &\n        mask_n[None, :])\n"
    },
    {
      "input": "@triton.jit\ndef embedding_backward_kernel(grad_output_ptr, grad_weight_ptr, indices_ptr,\n    n_elements, embedding_dim: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    grad_output = tl.load(grad_output_ptr + offsets_m[:, None] *\n        embedding_dim + offsets_n[None, :], mask=mask_m[:, None] & mask_n[\n        None, :], other=0.0)\n    grad_weight_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    tl.atomic_add(grad_weight_ptr + grad_weight_offsets, grad_output, mask=\n        mask_m[:, None] & mask_n[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K: 'tl.constexpr', stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    tl.static_assert(K % (4 * BLOCK_SIZE_K) == 0,\n        'K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K'\n        )\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \"\"\"\n        This part of the code generates pointers to the specific blocks of matrices A and B that the current thread block will process.\n\n        As described in the PyTorch documentation, a stride refers to the step size needed to move from one element to the next along a given dimension:\n\n        For matrix A: stride_am = A.stride(0) = K (stride along the rows), and stride_ak = A.stride(1) = 1 (stride along the columns).\n        For matrix B: stride_bk = B.stride(0) = N (stride along the rows), and stride_bn = B.stride(1) = 1 (stride along the columns).\n        Now, let's break down the pointer generation:\n\n        offs_am[:, None] creates a column of shape [BLOCK_SIZE_M, 1], which represents the row indices of matrix A that this block is processing. It is multiplied by K (the number of columns in matrix A) since A is stored in row-major order. So, the element at position (i, j) in A is located at index i*K + j in memory.\n        offs_k[None, BLOCK_SIZE_K] creates a row vector representing the column indices of the block, i.e., a range from 0 to BLOCK_SIZE_K. This is used to compute the positions of the columns within the block.\n        When combined, the result has the shape [BLOCK_SIZE_M, BLOCK_SIZE_K], where each entry (i, j) points to the element in matrix A at position (i, j) for the current block.\n\n        The same logic is applied to matrix B, but the resulting shape is [BLOCK_SIZE_K, BLOCK_SIZE_N], representing the block of matrix B that the thread block will work on.\n    \"\"\"\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    \"\"\"\n        We split the loop into two layers. The outer loop runs 4 times, and each iteration focuses on a specific portion of matrix A.\n\n        For example, when i = 0, we\u2019re only concerned with the blocks of matrix A that cover the range from 0 to K // (4 * BLOCK_SIZE_K).\n        Since matrix B is packed, its first dimension is effectively divided by 4. So, while we process the first segment of matrix A,\n        we still iterate over the entire first dimension of matrix B.\n\n        In each of the 4 iterations of the outer loop, we go through the full blocks of matrix B, but what changes is the data we extract.\n        Matrix B elements contain 4 weights, all packed into an int8 format, and during each iteration of the outer loop,\n        we extract a different weight by using bitwise shifting operations. This way, we access a unique weight on each pass.\n    \"\"\"\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n            stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n                other=0)\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n            mask = 3 << 2 * i\n            b = (b_uint8 & mask) >> 2 * i\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n            accumulator += tl.dot(a, b - tensor_full, out_dtype=tl.int32)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_forward_kernel(a, b, c, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_backward_kernel(dc, a, b, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    db_row = dc_row * geglu_a\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = 0.5 * a_row * (1 - tanh_sq) * (sqrt_2_over_pi * (1 + 3 * \n        0.044715 * a_row * a_row))\n    da_row = dc_row * b_row * (term1 + term2)\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _jsd_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr, loss_stride,\n    dX_ptr, dX_stride, label_ptr, beta, n_non_ignore, ignore_index:\n    'tl.constexpr', n_cols, BLOCK_SIZE: 'tl.constexpr', HAS_LABEL:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    X_ptr += pid * X_stride\n    dX_ptr += pid * dX_stride\n    Y_ptr += pid * Y_stride\n    loss_ptr += pid * loss_stride\n    label_ptr += pid\n    if HAS_LABEL:\n        label = tl.load(label_ptr)\n        if label == ignore_index:\n            for i in range(0, n_cols, BLOCK_SIZE):\n                offsets = i + tl.arange(0, BLOCK_SIZE)\n                tl.store(dX_ptr + offsets, 0.0, mask=offsets < n_cols)\n            return\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        X = tl.load(X_ptr + offsets, mask=mask, other=float('-inf'))\n        Y = tl.load(Y_ptr + offsets, mask=mask, other=float('-inf'))\n        Q = tl.exp(X)\n        P = tl.exp(Y)\n        M = beta * P + (1 - beta) * Q\n        log_M = tl.log(M)\n        loss = beta * P * Y + (1 - beta) * Q * X - M * log_M\n        loss = loss / n_non_ignore\n        tl.store(loss_ptr + offsets, loss, mask=mask)\n        dX = (1 - beta) * Q * (X - log_M) / n_non_ignore\n        tl.store(dX_ptr + offsets, dX, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _kldiv_kernel_forward(y_ptr, y_stride, gt_ptr, gt_stride, loss_ptr,\n    loss_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr', log_target:\n    'tl.constexpr'=False, reduction: 'tl.constexpr'=_REDUCTION_MODE_BATCHMEAN):\n    pid = tl.program_id(0)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n    loss_sum = 0.0\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            loss = y_true * (tl.log(tl.maximum(y_true, eps)) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n        if reduction == _REDUCTION_MODE_NONE:\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss_sum += tl.sum(loss, axis=0)\n    if reduction != _REDUCTION_MODE_NONE:\n        tl.store(loss_ptr, loss_sum)\n"
    },
    {
      "input": "@triton.jit\ndef _kldiv_kernel_backward(target_ptr, target_stride, new_grads_ptr,\n    new_grads_stride, n_cols, BLOCK_SIZE: 'tl.constexpr', log_target:\n    'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    target_ptr += pid * target_stride\n    new_grads_ptr += pid * new_grads_stride\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n        tl.store(new_grads_ptr + offsets, res, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, B_ptr, B_row_stride, Mean_ptr, Mean_row_stride,\n    RSTD_ptr, RSTD_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    Mean_ptr += row_idx * Mean_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    B_row = tl.load(B_ptr + col_offsets, mask=mask, other=0)\n    mean = tl.sum(X_row, axis=0) / n_cols\n    var = tl.sum((X_row - mean) * (X_row - mean), axis=0) / n_cols\n    rstd = rsqrt(var + eps)\n    tl.store(Mean_ptr, mean)\n    tl.store(RSTD_ptr, rstd)\n    Y_row = (X_row - mean) * rstd * W_row + B_row\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_backward_kernel(X_ptr, W_ptr, Mean_ptr, RSTD_ptr, DX_ptr,\n    DW_ptr, DB_ptr, DY_ptr, stride_x, stride_dx, stride_dw, stride_db,\n    stride_dy, n_rows, n_cols, rows_per_program: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', dtype: 'tl.constexpr'):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/ops/triton/layer_norm.py\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < n_cols\n    dw_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    db_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    X_ptr += row_start * stride_x\n    Mean_ptr += row_start\n    RSTD_ptr += row_start\n    DX_ptr += row_start * stride_dx\n    DY_ptr += row_start * stride_dy\n    for _ in range(row_start, row_end):\n        x = tl.load(X_ptr + cols, mask=mask, other=0.0)\n        w = tl.load(W_ptr + cols, mask=mask, other=0.0)\n        dy = tl.load(DY_ptr + cols, mask=mask, other=0.0)\n        mean = tl.load(Mean_ptr)\n        rstd = tl.load(RSTD_ptr)\n        x_hat = (x - mean) * rstd\n        wdy = w * dy\n        c1 = tl.sum(x_hat * wdy, axis=0) / n_cols\n        c2 = tl.sum(wdy, axis=0) / n_cols\n        dx = (wdy - (x_hat * c1 + c2)) * rstd\n        tl.store(DX_ptr + cols, dx, mask=mask)\n        dw_row += dy * x_hat\n        db_row += dy\n        X_ptr += stride_x\n        Mean_ptr += 1\n        RSTD_ptr += 1\n        DX_ptr += stride_dx\n        DY_ptr += stride_dy\n    tl.store(DW_ptr + row_block_id * stride_dw + cols, dw_row, mask=mask)\n    tl.store(DB_ptr + row_block_id * stride_db + cols, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, RSTD_ptr, RSTD_row_stride, n_cols, eps, offset,\n    casting_mode: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    y_i = (x_i / (RMS)) * (offset + wi), RMS = sqrt(sum(x_i^2) / N)\n\n    Reference:\n    1. https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    2. https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/rms_layernorm.py#L22\n    3. https://arxiv.org/pdf/1910.07467\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    X_row_dtype = X_row.dtype\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row\n    if casting_mode == _CASTING_MODE_GEMMA:\n        W_row = W_row\n        X_row = X_row\n    if casting_mode == _CASTING_MODE_NONE:\n        eps = eps\n        offset = offset\n    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols\n    rstd = rsqrt(mean_square + eps)\n    tl.store(RSTD_ptr, rstd)\n    X_row = X_row * rstd\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row\n    Y_row = X_row * (offset + W_row)\n    if casting_mode == _CASTING_MODE_GEMMA:\n        Y_row = Y_row\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_norm_backward_kernel(dY_ptr, dY_row_stride, X_ptr, X_row_stride,\n    X_dtype: 'tl.constexpr', W_ptr, W_row_stride, RSTD_ptr, RSTD_row_stride,\n    dW_ptr, dW_row_stride, n_rows, n_cols, offset, rows_per_program:\n    'tl.constexpr', casting_mode: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    dx = (1 / RMS) * [dy * (w + offset - (1 / N) * (1 / RMS^2) * ((dy * (w + offset)) dot x) * x]. * means element-wise multiplication, whileas dot means dot product\n    dw = sum(dy * (x / RMS)). summation over BxT dimension\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dW_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    dY_ptr += row_start * dY_row_stride\n    X_ptr += row_start * X_row_stride\n    RSTD_ptr += row_start\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0.0)\n    W_row = W_row + offset\n    for _ in range(row_start, row_end):\n        dY_row = tl.load(dY_ptr + col_offsets, mask=mask, other=0.0)\n        X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0.0)\n        rstd_row = tl.load(RSTD_ptr)\n        X_row = X_row\n        if casting_mode == _CASTING_MODE_LLAMA:\n            m = dY_row * W_row\n        elif casting_mode == _CASTING_MODE_GEMMA:\n            dY_row = dY_row\n            m = dY_row * W_row\n        else:\n            m = dY_row * W_row\n        dX_row = rstd_row * m\n        dX_row += rstd_row * (-(1 / n_cols) * rstd_row * rstd_row * tl.sum(\n            m * X_row, axis=0) * X_row)\n        if casting_mode == _CASTING_MODE_LLAMA:\n            dW_row += dY_row * (X_row * rstd_row)\n        else:\n            dW_row += dY_row * (X_row * rstd_row)\n        tl.store(dY_ptr + col_offsets, dX_row, mask=mask)\n        dY_ptr += dY_row_stride\n        X_ptr += X_row_stride\n        RSTD_ptr += RSTD_row_stride\n    tl.store(dW_ptr + row_block_id * dW_row_stride + col_offsets, dW_row,\n        mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_rope(q_ptr, q_row_stride, k_ptr, k_row_stride, cos,\n    cos_row_stride, sin, sin_row_stride, sl, bs: 'tl.constexpr', n_qh:\n    'tl.constexpr', n_kh: 'tl.constexpr', hd: 'tl.constexpr', pad_n_qh:\n    'tl.constexpr', pad_n_kh: 'tl.constexpr', pad_hd: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', BACKWARD_PASS: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n    cos_row_idx = pid % sl\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        )\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        )\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n"
    },
    {
      "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n"
    },
    {
      "input": "@triton.jit\ndef _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef element_mul_kernel(X_ptr, X_stride, grad_output_ptr, n_cols, BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by X_ptr with the value pointed by grad_output_ptr.\n    The multiplication is performed in-place on the tensor pointed by X_ptr.\n\n    Parameters:\n    X_ptr: Pointer to the input tensor.\n    X_stride (int): The stride of the input tensor.\n    grad_output_ptr: Pointer to the gradient output value.\n    n_cols (int): The number of columns in the input tensor.\n    BLOCK_SIZE (int): The block size for Triton operations.\n    \"\"\"\n    program_id = tl.program_id(0)\n    X_ptr += program_id * X_stride\n    grad_output = tl.load(grad_output_ptr)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols)\n        tl.store(X_ptr + X_offsets, X_block * grad_output, mask=X_offsets <\n            n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef coor_descent_kernel_forward(a_ptr, b_ptr, input_ptr, mask_ptr, k_ptr,\n    a_iter_stride, b_row_stride, b_iter_stride, input_row_stride,\n    mask_row_stride, n_iters, current_eps, eps_decay, eps, n_cols,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    mask_start_ptr = mask_ptr + row_idx * mask_row_stride\n    mask_ptrs = mask_start_ptr + col_offsets\n    mask_ints = tl.load(mask_ptrs, mask=col_mask, other=0)\n    mask = mask_ints == 1\n    a_ptr = a_ptr + row_idx\n    a = tl.load(a_ptr)\n    b_start_ptr = b_ptr + row_idx * b_row_stride\n    b_ptrs = b_start_ptr + col_offsets\n    b = tl.load(b_ptrs, mask=col_mask, other=0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    input_ptrs = row_start_ptr + col_offsets\n    s = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    k_ptr = k_ptr + row_idx\n    k = tl.load(k_ptr)\n    logk = tl.log(k)\n    for _ in range(n_iters):\n        a = (s + b) / current_eps\n        a = tl.where(mask, a, -float('inf'))\n        a_max = tl.max(a, axis=0)\n        a_minus_max = tl.where(mask, a - a_max, -float('inf'))\n        exp = tl.exp(a_minus_max)\n        sum_exp = tl.sum(exp, axis=0)\n        log_sum_exp = tl.log(sum_exp) + a_max\n        a = current_eps * (logk - log_sum_exp)\n        b = s + a\n        b = tl.where(b >= 0.0, -b, 0.0)\n        current_eps *= eps_decay\n        if current_eps < eps:\n            current_eps = eps\n    next_a_ptrs = a_ptr + a_iter_stride\n    next_b_ptrs = b_ptrs + b_iter_stride\n    tl.store(next_a_ptrs, a)\n    tl.store(next_b_ptrs, b, mask=col_mask)\n"
    },
    {
      "input": "@triton.jit\ndef coor_descent_kernel_backward(dk_ptr, input_ptr, a_ptr, b_ptr, mask_ptr,\n    ds_ptr, db_ptr, k_ptr, last_da_ptr, input_row_stride, b_row_stride,\n    mask_row_stride, ds_row_stride, db_row_stride, n_iters, eps_init,\n    eps_decay, eps, n_cols, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    mask_start_ptr = mask_ptr + row_idx * mask_row_stride\n    mask_ptrs = mask_start_ptr + col_offsets\n    mask_ints = tl.load(mask_ptrs, mask=col_mask, other=0)\n    mask = mask_ints == 1\n    a_ptr = a_ptr + row_idx\n    init_a = tl.load(a_ptr)\n    b_start_ptr = b_ptr + row_idx * b_row_stride\n    b_ptrs = b_start_ptr + col_offsets\n    init_b = tl.load(b_ptrs, mask=mask, other=0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    input_ptrs = row_start_ptr + col_offsets\n    s = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    k_ptr = k_ptr + row_idx\n    k = tl.load(k_ptr)\n    logk = tl.log(k)\n    last_da_ptr = last_da_ptr + row_idx\n    last_da = tl.load(last_da_ptr)\n    ds_row_start_ptr = ds_ptr + row_idx * ds_row_stride\n    ds_ptrs = ds_row_start_ptr + col_offsets\n    ds = tl.load(ds_ptrs, mask=mask, other=0.0)\n    db_row_start_ptr = db_ptr + row_idx * db_row_stride\n    db_ptrs = db_row_start_ptr + col_offsets\n    db = tl.load(db_ptrs, mask=mask, other=0.0)\n    dk_ptr = dk_ptr + row_idx\n    dk = tl.load(dk_ptr)\n    for ind in range(n_iters):\n        a = init_a\n        b = init_b\n        sa = s * 0\n        softmax = s * 0\n        current_eps = eps_init / eps_decay\n        for _ in range(n_iters - ind):\n            current_eps *= eps_decay\n            if current_eps < eps:\n                current_eps = eps\n            sb = (s + b) / current_eps\n            sb = tl.where(mask, sb, -float('inf'))\n            sb_max = tl.max(sb, axis=0)\n            sb_minus_max = tl.where(mask, sb - sb_max, -float('inf'))\n            exp = tl.exp(sb_minus_max)\n            sum_exp = tl.sum(exp, axis=0)\n            softmax = exp / sum_exp\n            log_sum_exp = tl.log(sum_exp) + sb_max\n            a = current_eps * (logk - log_sum_exp)\n            sa = s + a\n            b = tl.where(sa > 0.0, -sa, 0.0)\n        dsa = db * tl.where(sa > 0, -1.0, 0.0)\n        ds += dsa\n        da = tl.sum(dsa, axis=0) + last_da\n        dk += da * current_eps\n        dsb = da * -softmax\n        ds += dsb\n        db = dsb\n        last_da *= 0.0\n    tl.store(dk_ptr, dk)\n    tl.store(ds_ptrs, ds, mask=col_mask)\n    tl.store(db_ptrs, db, mask=col_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, M, V, stride, N, BLOCK_SIZE: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    X += row * stride\n    Y += row * stride\n    x = tl.load(X + cols, mask=mask, other=0)\n    mean = tl.sum(x, axis=0) / N\n    xmean = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(xmean * xmean, axis=0) / N\n    rstd = 1 / tl.sqrt(var + 1e-05)\n    xhat = xmean * rstd\n    tl.store(M + row, mean)\n    tl.store(V + row, rstd)\n    w = tl.load(W + cols, mask=mask)\n    y = xhat * w\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(DX, DY, DW, X, W, M, V, Lock, stride, N,\n    GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    mean = tl.load(M + row)\n    rstd = tl.load(V + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    mean1 = tl.sum(xhat * wdy, axis=0) / N\n    mean2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * mean1 + mean2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n"
    },
    {
      "input": "@triton.jit\ndef _layer_norm_bwd_dw(DW, FINAL_DW, M, N, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_kernel_forward(output_ptr, input_ptr, input_row_stride,\n    output_row_stride, n_cols, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    mask = col_offsets < n_cols\n    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    causal_mask = col_offsets > row_idx % n_cols\n    row = row + tl.where(causal_mask, -float('inf'), 0.0)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef softmax_kernel_backward(output_ptr, input_ptr, grad_ptr,\n    grad_row_stride, input_row_stride, output_row_stride, n_cols,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    grad_row_start_ptr = grad_ptr + row_idx * grad_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    grad_ptrs = grad_row_start_ptr + col_offsets\n    mask = col_offsets < n_cols\n    probs_row = tl.load(input_ptrs, mask=mask, other=0.0)\n    grad_row = tl.load(grad_ptrs, mask=mask, other=0.0)\n    dxhat = probs_row * grad_row\n    softmax_grad_output = dxhat - probs_row * tl.sum(dxhat, axis=0)\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_grad_output, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n    triton.Config({'BLOCK_SIZE': 1024}, num_warps=8)], key=['n_elements'],\n    restore_value=['p_ptr', 'exp_avg_ptr'])\n@triton.jit\ndef update_fn_kernel(p_ptr, grad_ptr, exp_avg_ptr, lr, wd, beta1, beta2,\n    n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    offset_p_ptr = p_ptr + offsets\n    offset_grad_ptr = grad_ptr + offsets\n    offset_exp_avg_ptr = exp_avg_ptr + offsets\n    p = tl.load(offset_p_ptr, mask=mask)\n    grad = tl.load(offset_grad_ptr, mask=mask)\n    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)\n    p = p * (1 - lr * wd)\n    diff = exp_avg - grad\n    update = diff * beta1 + grad\n    can_update = update != 0\n    update_sign = tl.where(update > 0, -lr, lr)\n    p = p + update_sign * can_update\n    exp_avg = diff * beta2 + grad\n    tl.store(offset_p_ptr, p, mask=mask)\n    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_block_sparse_attn_fwd_kernel(Q, K, V, seqlens, sm_scale,\n    block_index, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vn,\n    stride_vk, stride_oz, stride_oh, stride_om, stride_ok, Z, H, N_CTX,\n    NUM_ROWS, MAX_BLOCKS_PRE_ROW, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', dtype: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    blocks_ptr = block_index + (off_hz * NUM_ROWS + start_m\n        ) * MAX_BLOCKS_PRE_ROW\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = q * qk_scale\n    m_mask = offs_m[:, None] < seqlen\n    block_count = tl.minimum((start_m + 1) * BLOCK_M // BLOCK_N,\n        MAX_BLOCKS_PRE_ROW)\n    for sparse_block_idx in range(block_count):\n        real_block_idx = tl.load(blocks_ptr + sparse_block_idx)\n        start_n = real_block_idx * BLOCK_N\n        cols = start_n + offs_n\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc, mask=m_mask)\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_stages=1, num_warps=4),\n    triton.Config({}, num_stages=1, num_warps=8), triton.Config({},\n    num_stages=2, num_warps=4), triton.Config({}, num_stages=2, num_warps=8\n    ), triton.Config({}, num_stages=3, num_warps=4), triton.Config({},\n    num_stages=3, num_warps=8), triton.Config({}, num_stages=4, num_warps=4\n    ), triton.Config({}, num_stages=4, num_warps=8), triton.Config({},\n    num_stages=5, num_warps=4), triton.Config({}, num_stages=5, num_warps=8\n    )], key=['N_CTX'])\n@triton.jit\ndef triton_sparse_fwd_kernel(Q, K, V, seqlens, sm_scale, col_count,\n    col_index, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vn,\n    stride_vk, stride_oz, stride_oh, stride_om, stride_ok, Z, H, N_CTX,\n    NUM_ROWS, MAX_COLS_PRE_ROW, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', dtype: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    num_cols = tl.load(col_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = col_index + (off_hz * NUM_ROWS + start_m) * MAX_COLS_PRE_ROW\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = q * qk_scale\n    m_mask = offs_m[:, None] < seqlen\n    split = tl.maximum(num_cols - BLOCK_N, 0) & ~(BLOCK_N - 1)\n    for start_n in range(0, split, BLOCK_N):\n        cols = tl.load(cols_ptr + start_n + offs_n)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    for start_n in range(split, num_cols, BLOCK_N):\n        n_mask = start_n + offs_n < num_cols\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=n_mask, other=\n            N_CTX - 1)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc = tl.where(m_mask, acc / l_i[:, None], 0.0)\n    tl.store(o_ptrs, acc, mask=m_mask)\n"
    },
    {
      "input": "@triton.jit\ndef triton_dense_fwd_kernel(Q, K, V, seqlens, sm_scale, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_ok, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', dtype:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qo_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + kv_offset, shape=(BLOCK_DMODEL,\n        N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape\n        =(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + kv_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vn, stride_vk), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M\n    m_mask = offs_m[:, None] < seqlen\n    for start_n in range(lo, hi, BLOCK_N):\n        n_mask = start_n + offs_n[None, :] <= offs_m[:, None]\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = tl.where(m_mask, acc / l_i[:, None], 0.0)\n    O_block_ptr = tl.make_block_ptr(base=Out + qo_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_ok), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc, mask=m_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_mixed_sparse_attn_fwd_kernel(Q, K, V, seqlens, sm_scale,\n    block_count, block_offset, column_count, column_index, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_ok, Z, H, N_CTX, NUM_ROWS, NNZ_S, NNZ_V,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', dtype: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    num_blks = tl.load(block_count + off_hz * NUM_ROWS + start_m)\n    blks_ptr = block_offset + (off_hz * NUM_ROWS + start_m) * NNZ_S\n    num_cols = tl.load(column_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = column_index + (off_hz * NUM_ROWS + start_m) * NNZ_V\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = q * qk_scale\n    m_mask = offs_m[:, None] < seqlen\n    for block_index in range(num_blks):\n        start_n = tl.load(blks_ptr + block_index)\n        cols = start_n + offs_n\n        n_mask = cols < seqlen\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :\n            ], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None\n            ], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    for start_n in range(0, num_cols, BLOCK_N):\n        n_mask = start_n + offs_n < num_cols\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=n_mask, other=0)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :\n            ], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None\n            ], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc, mask=m_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, N_CTX, sliding_window_offset, sliding_window_size, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    if SLIDING_WINDOW and not COMPLEMENT_SLIDING_WINDOW:\n        if COMPLEMENT_SLIDING_WINDOW:\n            lo = 0\n            hi = ((start_m + 1) * BLOCK_M + sliding_window_offset -\n                sliding_window_size + BLOCK_N - 1) // BLOCK_N * BLOCK_N\n        else:\n            lo = (start_m * BLOCK_M + sliding_window_offset -\n                sliding_window_size + 1) // BLOCK_N * BLOCK_N\n            hi = ((start_m + 1) * BLOCK_M - 1 + sliding_window_offset + BLOCK_N\n                ) // BLOCK_N * BLOCK_N\n            if lo < 0:\n                lo = 0\n            if hi > N_CTX:\n                hi = N_CTX\n            lo = tl.multiple_of(lo, BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n            V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    else:\n        lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if IS_EVEN_N:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m * BLOCK_M - start_n + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n            qk = tl.where(mask, qk, float('-inf'))\n        if not IS_EVEN_N:\n            qk = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :\n                ], qk, float('-inf'))\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :],\n                p, 0)\n        l_ij = tl.sum(p, 1)\n        tmp = m_i - m_ij\n        alpha_mask = tmp != tmp\n        alpha = tl.math.exp2(tmp)\n        alpha = tl.where(alpha_mask, 1.0, alpha)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        if IS_EVEN_N:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        acc += tl.dot(p, v)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n"
    },
    {
      "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, L, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX, sliding_window_offset,\n    sliding_window_size, IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', END: 'tl.constexpr', INIT: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    v_offset = off_z * stride_vz + off_hkv * stride_vh\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + v_offset, shape=(NKV_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(ROUND_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n        m_i = tl.load(m_ptrs)\n        l_i = tl.load(l_ptrs)\n        acc = tl.load(O_block_ptr)\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n        V_block_ptr, start_m, qk_scale, NKV_CTX, sliding_window_offset,\n        sliding_window_size, BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW,\n        IS_EVEN_M, IS_EVEN_N, COMPLEMENT_SLIDING_WINDOW)\n    if END:\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n"
    },
    {
      "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _score_kernel(Q, K, M, sm_scale, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_oz,\n    stride_oh, stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX,\n    sliding_window_offset, sliding_window_size, SLIDING_WINDOW:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M:\n    'tl.constexpr', IS_EVEN_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, start_n *\n        BLOCK_N), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        m = tl.load(m_ptrs)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m - start_n * BLOCK_N + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_M) + start_m < N_CTX)[:, None],\n                p, 0)\n        o += tl.sum(p, axis=0)\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o, mask=o_range < NKV_CTX)\n"
    },
    {
      "input": "@triton.jit\ndef unpack64(merged):\n    tl.static_assert(merged.dtype == tl.uint64)\n    b = (merged & 4294967295).to(tl.uint32)\n    a = (merged >> 32).to(tl.uint32)\n    return a, b\n"
    },
    {
      "input": "@triton.jit\ndef pack64(a, b):\n    tl.static_assert(a.dtype == tl.float32)\n    tl.static_assert(b.dtype == tl.float32)\n    a = a.to(dtype=tl.uint32, bitcast=True)\n    a = a << 32\n    b = b.to(dtype=tl.uint32, bitcast=True)\n    return a | b\n"
    },
    {
      "input": "@triton.jit\ndef forward_scan(gates, tokens, outputs, SEQUENCE_LENGTH: 'tl.constexpr'):\n    sequence_id = tl.num_programs(axis=1) * tl.program_id(axis=0\n        ) + tl.program_id(axis=1)\n    strides = tl.arange(0, SEQUENCE_LENGTH) + sequence_id * SEQUENCE_LENGTH\n    tokens_ = tl.load(tokens + strides)\n    gates_ = tl.load(gates + strides)\n    tuples = pack64(tokens_, gates_)\n    output_tuples_ = tl.associative_scan(tuples, axis=0, combine_fn=\n        first_order_op)\n    output_tokens_, output_gates_ = unpack64(output_tuples_)\n    tl.store(outputs + strides, output_tokens_)\n"
    },
    {
      "input": "@triton.jit\ndef backward_scan(gates, tokens, outputs, SEQUENCE_LENGTH: 'tl.constexpr'):\n    sequence_id = tl.num_programs(axis=1) * tl.program_id(axis=0\n        ) + tl.program_id(axis=1)\n    forward_strides = tl.arange(0, SEQUENCE_LENGTH\n        ) + sequence_id * SEQUENCE_LENGTH\n    reverse_strides = tl.num_programs(axis=0) * tl.num_programs(axis=1\n        ) * SEQUENCE_LENGTH - 1 - forward_strides\n    tokens_ = tl.load(tokens + reverse_strides)\n    gates_ = tl.load(gates + reverse_strides)\n    tuples = pack64(tokens_, gates_)\n    output_tuples_ = tl.associative_scan(tuples, axis=0, combine_fn=\n        first_order_op)\n    output_tokens_, output_gates_ = unpack64(output_tuples_)\n    tl.store(outputs + reverse_strides, output_tokens_)\n"
    },
    {
      "input": "@triton.jit\ndef _kernel_matmul_fp8_row_tma_persistent(A_ptr, B_ptr, C_ptr, M, N, K,\n    A_scale, B_scale, stride_am, stride_ak, stride_bn, stride_bk, stride_cm,\n    stride_cn, dot_out_dtype: 'tl.constexpr', allow_tf32: 'tl.constexpr',\n    fp8_fast_accum: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr',\n    AB_DTYPE: 'tl.constexpr', NUM_SMS: 'tl.constexpr') ->None:\n    \"\"\"Matmul kernel of [M, K] @ [N, K] with row-wise scales\n\n    performs swizzled matmul in [BLOCK_M, BLOCK_K] with [BLOCK_K, BLOCK_N] tiles.\n\n    Args:\n        A (TensorWrapper): [M, K] input tensor.\n        B (TensorWrapper): [N, K] input tensor.\n        C (TensorWrapper): [M, N] output tensor.\n        M (int): M dimension of input tensor.\n        N (int): N dimension of input tensor.\n        K (int): K dimension of input tensor.\n        A_scale (TensorWrapper): [M] reciprocal scale tensor per row. A * A_scale = original A\n        B_scale (TensorWrapper): [N] reciprocal scale tensor per row. B * B_scale = original B\n        stride_am (int): Stride of M dimension of A.\n        stride_ak (int): Stride of K dimension of A.\n        stride_bn (int): Stride of N dimension of B.\n        stride_bk (int): Stride of K dimension of B.\n        stride_cm (int): Stride of M dimension of C.\n        stride_cn (int): Stride of N dimension of C.\n        dot_out_dtype (torch.dtype): Output type of tensor core.\n        allow_tf32 (bool): Whether to use TF32 for tensor core.\n        fp8_fast_accum (bool): Whether to use fast accumulation for tensor core.\n        BLOCK_M (int): Block size for M dimension.\n        BLOCK_N (int): Block size for N dimension.\n        BLOCK_K (int): Block size for K dimension.\n        GROUP_M (int): Number of groups for M dimension swizzle.\n        SPLIT_K (int): Number of SM's to launch per row.\n        EVEN_K (bool): Whether K is evenly divisible by BLOCK_K * SPLIT_K.\n        AB_DTYPE (bool): Wether to cast A and B to C.dtype before tensor core.\n    \"\"\"\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    k_tiles = tl.cdiv(K, BLOCK_K)\n    num_tiles = num_pid_m * num_pid_n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n    pid_m = 0\n    pid_n = 0\n    offs_am = 0\n    offs_bn = 0\n    num_pid_in_group = GROUP_M * num_pid_n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n    dtype_fp8 = tl.float8e4nv\n    scale_dtype = tl.float32\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n            pid_m = first_pid_m + tile_id % group_size_m\n            pid_n = tile_id % num_pid_in_group // group_size_m\n            offs_am = pid_m * BLOCK_M\n            offs_bn = pid_n * BLOCK_N\n            offs_am = tl.multiple_of(offs_am, BLOCK_M)\n            offs_bn = tl.multiple_of(offs_bn, BLOCK_N)\n        offs_k = ki * BLOCK_K\n        a = tl._experimental_descriptor_load(A_ptr, [offs_am, offs_k], [\n            BLOCK_M, BLOCK_K], dtype_fp8)\n        b = tl._experimental_descriptor_load(B_ptr, [offs_bn, offs_k], [\n            BLOCK_N, BLOCK_K], dtype_fp8)\n        acc = tl.dot(a, b.T, acc, out_dtype=dot_out_dtype, allow_tf32=\n            allow_tf32)\n        if ki == k_tiles - 1:\n            rm = pid_m * BLOCK_M\n            rn = pid_n * BLOCK_N\n            a_scale = tl._experimental_descriptor_load(A_scale, [rm], [\n                BLOCK_M], scale_dtype)\n            b_scale = tl._experimental_descriptor_load(B_scale, [rn], [\n                BLOCK_N], scale_dtype)\n            scale = a_scale[:, None] * b_scale[None, :]\n            acc *= scale\n            acc = acc\n            tl._experimental_descriptor_store(C_ptr, acc, [rm, rn])\n            acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n"
    },
    {
      "input": "@triton.jit\ndef fused_moe_kernel(a_ptr, b_ptr, c_ptr, topk_weights_ptr,\n    sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K,\n    EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk,\n    stride_bn, stride_cm, stride_cn, stride_weight, stride_token_id,\n    block_m: 'tl.constexpr', block_n: 'tl.constexpr', block_k:\n    'tl.constexpr', MUL_ROUTED_WEIGHT: 'tl.constexpr', top_k:\n    'tl.constexpr', compute_type: 'tl.constexpr'):\n    \"\"\"\n    Implements the fused computation for a Mixture of Experts (MOE) using token and expert matrices.\n\n    Key Parameters:\n    - A: The input tensor representing tokens with shape (*, K), where '*' can be any shape representing batches and K is the feature dimension of each token.\n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is the number of experts, K is the input feature dimension, and N is the output feature dimension.\n    - C: The output cache tensor with shape (M, topk, N), where M is the total number of tokens post padding, topk is the number of times each token is repeated,\n        and N is the output feature dimension.\n    - sorted_token_ids: A tensor containing the sorted indices of tokens, repeated topk times and arranged by the expert index they are assigned to.\n    - expert_ids: A tensor containing the indices of the expert for each block. It determines which expert matrix from B should be used for each block in A.\n    This kernel performs the multiplication of a token by its corresponding expert matrix as determined by `expert_ids`. The sorting of `sorted_token_ids`\n    by expert index and padding ensures divisibility by block_m, which is necessary to maintain consistency in block matrix multiplication across different blocks processed by the same expert.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_m, pid_n = col_major(pid, EM, N, block_m, block_n)\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * block_m >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * block_m + tl.arange(0, block_m)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * block_n + tl.arange(0, block_n)) % N\n    offs_k = tl.arange(0, block_k)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am + offs_k[\n        None, :] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    accumulator = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, block_k)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * block_k), other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * block_k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += block_k * stride_ak\n        b_ptrs += block_k * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token * stride_weight,\n            mask=token_mask, other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    accumulator = accumulator\n    offs_cn = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.jit\ndef scaled_gemm_splitk(a_ptr, b_ptr, c_ptr, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, scale_a, scale_b, m, n, k, block_m:\n    'tl.constexpr', block_n: 'tl.constexpr', block_k: 'tl.constexpr',\n    split_k: 'tl.constexpr', group_m: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    grid_k = tl.cdiv(k, block_k * split_k)\n    pid_m, pid_n = column_major(pid, m, n, block_m, block_n)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_k = pid_k * block_k + tl.arange(0, block_k)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, block_m), block_m)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, block_n), block_n)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k_ in range(0, grid_k):\n        k_remaining = k - k_ * (block_k * split_k)\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < k_remaining, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < k_remaining, other=0.0)\n        acc = tl.dot(a, b, acc, out_dtype=tl.float32)\n        a_ptrs += block_k * split_k * stride_ak\n        b_ptrs += block_k * split_k * stride_bk\n    acc = scale_a * scale_b * acc\n    acc\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    mask = (offs_m < m)[:, None] & (offs_n < n)[None, :]\n    tl.atomic_add(c_ptrs, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef gemm_split_k_kernel(a_ptr, b_ptr, c_ptr, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, m, n, k, block_m:\n    'tl.constexpr', block_n: 'tl.constexpr', block_k: 'tl.constexpr',\n    split_k: 'tl.constexpr', group_m: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    grid_k = tl.cdiv(k, block_k * split_k)\n    pid_m, pid_n = grouped_launch(pid, m, n, block_m, block_n, group_m)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_k = pid_k * block_k + tl.arange(0, block_k)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, block_m), block_m)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, block_n), block_n)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k_ in range(0, grid_k):\n        k_remaining = k - k_ * (block_k * split_k)\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < k_remaining, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < k_remaining, other=0.0)\n        acc = tl.dot(a, b, acc, out_dtype=tl.float32)\n        a_ptrs += block_k * split_k * stride_ak\n        b_ptrs += block_k * split_k * stride_bk\n    acc\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    mask = (offs_m < m)[:, None] & (offs_n < n)[None, :]\n    tl.atomic_add(c_ptrs, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef gemm_kernel_tma(a_desc_ptr, b_desc_ptr, c_desc_ptr, prob_m, prob_n,\n    prob_k, block_m: 'tl.constexpr', block_n: 'tl.constexpr', block_k:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(prob_m, block_m)\n    num_pid_k = tl.cdiv(prob_k, block_k)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n    offs_am = pid_m * block_m\n    offs_bn = pid_n * block_n\n    offs_k = 0\n    accumulator = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for kk in range(0, num_pid_k):\n        a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k],\n            [block_m, block_k], tl.float8e4nv)\n        b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k],\n            [block_n, block_k], tl.float8e4nv)\n        accumulator = tl.dot(a, b.T, acc=accumulator, out_dtype=tl.float32)\n        offs_k += block_k\n    accumulator = accumulator\n    tl._experimental_descriptor_store(c_desc_ptr, accumulator, [offs_am,\n        offs_bn])\n"
    },
    {
      "input": "@triton.jit\ndef print_tensor_dim(tensor, str_name):\n    if tl.program_id(0) == 0 and tl.program_id(1) == 0:\n        tl.static_print(str_name, ' ', tensor.shape, ' ', tensor.dtype)\n"
    },
    {
      "input": "@triton.jit\ndef print_value(value):\n    if tl.program_id(0) == 0 and tl.program_id(1) == 0:\n        tl.device_print(str(value))\n"
    },
    {
      "input": "@triton.jit\ndef print_line(str_line):\n    if tl.program_id(0) == 0 and tl.program_id(1) == 0:\n        None\n"
    },
    {
      "input": "@triton.jit\ndef paged_attention_v1(scratchpad_key_ptr, scratchpad_value_ptr, output_ptr,\n    query_ptr, key_cache_ptr, value_cache_ptr, block_tables_ptr,\n    context_lens_ptr, scale, num_seqs, num_heads, cache_block_stride,\n    MAX_SEQ_LEN: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', HEAD_SIZE:\n    'tl.constexpr', MAX_NUM_BLOCKS_PER_SEQ: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    query_offset = seq_idx * num_seqs + head_idx * HEAD_SIZE\n    query_head = tl.load(query_ptr + query_offset + tl.arange(0, HEAD_SIZE))\n    block_table_offset = seq_idx * MAX_NUM_BLOCKS_PER_SEQ\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    for tok_idx in range(0, context_len):\n        logical_block_idx = tok_idx // BLOCK_SIZE\n        physical_block_idx = tl.load(block_tables_ptr + block_table_offset +\n            logical_block_idx)\n        start_of_block_offset = (physical_block_idx * cache_block_stride + \n            head_idx * HEAD_SIZE * BLOCK_SIZE)\n        tok_idx_within_block = tok_idx % BLOCK_SIZE\n        tok_offsets = start_of_block_offset + BLOCK_SIZE * tl.arange(0,\n            HEAD_SIZE) + tok_idx_within_block\n        tok_key = tl.load(key_cache_ptr + tok_offsets)\n        tok_value = tl.load(value_cache_ptr + tok_offsets)\n        scratchpad_offset = seq_idx * (MAX_SEQ_LEN * num_heads * HEAD_SIZE\n            ) + tok_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n        tl.store(scratchpad_key_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_key)\n        tl.store(scratchpad_value_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_value)\n    tl.debug_barrier()\n    start_seq_offset = MAX_SEQ_LEN * num_heads * HEAD_SIZE * seq_idx\n    start_tok_offset = start_seq_offset + tl.arange(0, MAX_SEQ_LEN) * (\n        num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    mask = tl.arange(0, MAX_SEQ_LEN)[:, None] < context_len\n    kv_offs = start_tok_offset[:, None] + tl.arange(0, HEAD_SIZE)[None, :]\n    print_tensor_dim(kv_offs, 'kv_offs_v1')\n    keys = tl.load(scratchpad_key_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(keys, 'keys_v1')\n    values = tl.load(scratchpad_value_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(values, 'values_v1')\n    scores = tl.sum(scale * keys * query_head[None, :], axis=1)\n    mask = tl.full([MAX_SEQ_LEN], -float('inf'), dtype=tl.float32)\n    cond = tl.arange(0, MAX_SEQ_LEN) < context_len\n    scores_masked = tl.where(cond, scores, mask)\n    scores_minus_max = scores_masked - tl.max(scores_masked, axis=0)\n    numerator = tl.exp(scores_minus_max)\n    denominator = tl.sum(numerator, axis=0) + float(1e-06)\n    logits = numerator / denominator\n    print_tensor_dim(logits, 'logits_v1')\n    weighted_values = tl.sum(values * logits[:, None], axis=0)\n    print_tensor_dim(weighted_values, 'weighted_values_v1')\n    output_offset = seq_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    tl.store(output_ptr + output_offset + tl.arange(0, HEAD_SIZE),\n        weighted_values)\n"
    },
    {
      "input": "@triton.jit\ndef paged_attention_v2(scratchpad_key_ptr, scratchpad_value_ptr,\n    partition_buf_ptr, output_ptr, query_ptr, key_cache_ptr,\n    value_cache_ptr, block_tables_ptr, context_lens_ptr, scale, num_seqs,\n    num_heads, cache_block_stride, num_partitions, PARTITION_SIZE:\n    'tl.constexpr', MAX_SEQ_LEN: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    HEAD_SIZE: 'tl.constexpr', MAX_NUM_BLOCKS_PER_SEQ: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    partition_idx = tl.program_id(2)\n    query_offset = seq_idx * num_seqs + head_idx * HEAD_SIZE\n    query_head = tl.load(query_ptr + query_offset + tl.arange(0, HEAD_SIZE))\n    print_tensor_dim(query_head, 'query_head')\n    block_table_offset = seq_idx * MAX_NUM_BLOCKS_PER_SEQ\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    assert context_len <= MAX_SEQ_LEN\n    token_start_idx = partition_idx * PARTITION_SIZE\n    token_end_idx = min((partition_idx + 1) * PARTITION_SIZE, context_len)\n    for tok_idx in range(token_start_idx, token_end_idx):\n        logical_block_offset = tok_idx // BLOCK_SIZE\n        physical_block_idx = tl.load(block_tables_ptr + block_table_offset +\n            logical_block_offset)\n        start_of_block_offset = (physical_block_idx * cache_block_stride + \n            head_idx * HEAD_SIZE * BLOCK_SIZE)\n        tok_idx_within_block = tok_idx % BLOCK_SIZE\n        tok_offsets = start_of_block_offset + BLOCK_SIZE * tl.arange(0,\n            HEAD_SIZE) + tok_idx_within_block\n        tok_key = tl.load(key_cache_ptr + tok_offsets)\n        tok_value = tl.load(value_cache_ptr + tok_offsets)\n        scratchpad_offset = seq_idx * (MAX_SEQ_LEN * num_heads * HEAD_SIZE\n            ) + tok_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n        print_tensor_dim(scratchpad_key_ptr, 'scratchpad_key_ptr')\n        mask = tl.full([HEAD_SIZE], 1, dtype=tl.float32) > 0\n        tl.store(scratchpad_key_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_key, mask)\n        tl.store(scratchpad_value_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_value, mask)\n    tl.debug_barrier()\n    start_seq_offset = MAX_SEQ_LEN * num_heads * HEAD_SIZE * seq_idx\n    start_tok_offsets = start_seq_offset + tl.arange(0, PARTITION_SIZE) * (\n        num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    mask = tl.arange(0, PARTITION_SIZE)[:, None] < context_len\n    kv_offs = start_tok_offsets[:, None] + tl.arange(0, HEAD_SIZE)[None, :]\n    print_tensor_dim(kv_offs, 'kv_offs_v2')\n    keys = tl.load(scratchpad_key_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(keys, 'keys_v2')\n    scores = tl.sum(scale * keys * query_head[None, :], axis=1)\n    print_tensor_dim(keys, 'scores_v2')\n    partition_buf_offset = (start_seq_offset + head_idx * HEAD_SIZE + \n        partition_idx * PARTITION_SIZE)\n    print_tensor_dim(partition_buf_offset, 'partition_buf_offset_v2')\n    tl.store(partition_buf_ptr + partition_buf_offset + tl.arange(0,\n        PARTITION_SIZE), scores)\n    mask = tl.full([PARTITION_SIZE], -float('inf'), dtype=tl.float32)\n    cond = tl.arange(0, PARTITION_SIZE) < context_len\n    scores_masked = tl.where(cond, scores, mask)\n    scores_minus_max = scores_masked - tl.max(scores_masked, axis=0)\n    numerator = tl.exp(scores_minus_max)\n    denominator = tl.sum(numerator, axis=0) + float(1e-06)\n    logits = numerator / denominator\n    print_tensor_dim(logits, 'logits_v2')\n    values = tl.load(scratchpad_value_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(values, 'values_v2')\n    weighted_values += tl.sum(values * logits[:, None], axis=0)\n    print_tensor_dim(weighted_values, 'weighed_values_v2')\n    output_offset = seq_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    tl.store(output_ptr + output_offset + tl.arange(0, HEAD_SIZE),\n        weighted_values)\n"
    },
    {
      "input": "@triton.jit\ndef _softmax_kernel_fwd(output_ptr, output_row_stride, input_ptr,\n    input_row_stride, n_cols, block_size: 'tl.constexpr'):\n    row_index = tl.program_id(0)\n    input_row_ptr = input_ptr + row_index * input_row_stride\n    col_offsets = tl.arange(0, block_size)\n    input_ptrs = input_row_ptr + col_offsets\n    rw_mask = col_offsets < n_cols\n    row = tl.load(input_ptrs, mask=rw_mask, other=float('-inf'))\n    safe_row = row - tl.max(row, axis=0)\n    numerator = tl.exp(safe_row)\n    denom = tl.sum(numerator, axis=0)\n    sm_out = numerator / denom\n    out_row_ptr = output_ptr + row_index * output_row_stride\n    out_row_ptrs = out_row_ptr + col_offsets\n    tl.store(out_row_ptrs, sm_out, mask=rw_mask)\n"
    },
    {
      "input": "@triton.jit\ndef _softmax_kernel_bwd(output_ptr, stride_output_row, grad_ptr,\n    stride_grad_row, input_ptr, stride_input_row, n_cols, block_size:\n    'tl.constexpr'):\n    row_index = tl.program_id(0)\n    input_row_ptr = input_ptr + row_index * stride_input_row\n    grad_row_ptr = grad_ptr + row_index * stride_grad_row\n    col_offsets = tl.arange(0, block_size)\n    rw_mask = col_offsets < n_cols\n    input_row_ptrs = input_row_ptr + col_offsets\n    grad_row_ptrs = grad_row_ptr + col_offsets\n    probs_row = tl.load(input_row_ptrs, mask=rw_mask, other=0)\n    grads_row = tl.load(grad_row_ptrs, mask=rw_mask, other=0)\n    dx = probs_row * grads_row\n    dsm_out = dx - probs_row * tl.sum(dx, axis=0)\n    output_row_ptr = output_ptr + row_index * stride_output_row\n    output_ptrs = output_row_ptr + col_offsets\n    tl.store(output_ptrs, dsm_out, mask=rw_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _rms_norm_fwd_kernel(X, stride_x, Y, stride_y, W, Rstd, eps, M, N,\n    block_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, block_N)\n    mask = cols < N\n    x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0)\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    x_hat = x * rstd\n    y = x_hat * w\n    tl.store(Y + row * stride_y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _rms_norm_bwd_kernel_sm(X, stride_x, W, DY, stride_dy, DX, stride_dx,\n    Rstd, DW, eps, M, N, rows_per_program, block_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, block_N)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    dw = tl.zeros((block_N,), dtype=tl.float32)\n    row_end = min(row_start + rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0)\n        dy = tl.load(DY + row * stride_dy + cols, mask=mask, other=0.0)\n        rstd = tl.load(Rstd + row)\n        x_hat = x * rstd\n        wdy = w * dy\n        dw += dy * x_hat\n        c1 = tl.sum(x_hat * wdy, axis=0) / N\n        dx = (wdy - x_hat * c1) * rstd\n        tl.store(DX + row * stride_dx + cols, dx, mask=mask)\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef kernel_vector_addition(a_ptr, b_ptr, out_ptr, num_elems: 'tl.constexpr',\n    block_size: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * block_size\n    thread_offsets = block_start + tl.arange(0, block_size)\n    mask = thread_offsets < num_elems\n    a_pointers = tl.load(a_ptr + thread_offsets, mask=mask)\n    b_pointers = tl.load(b_ptr + thread_offsets, mask=mask)\n    res = a_pointers + b_pointers\n    tl.store(out_ptr + thread_offsets, res, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_aligned(Q, K, V, B0, sm_scale, Out, stride_qh, stride_qm,\n    stride_qk, stride_kh, stride_kn, stride_kk, stride_vh, stride_vk,\n    stride_vn, stride_oh, stride_om, stride_on, stride_b0h, stride_b0m, Z,\n    H, N_CTX, P_SEQ, OUT_DTYPE: 'tl.constexpr', BIAS_LAST_SIZE:\n    'tl.constexpr', B0_NUMEL: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + kv_offset, shape=(BLOCK_DMODEL,\n        N_CTX + P_SEQ), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + kv_offset, shape=(N_CTX +\n        P_SEQ, BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0\n        ), block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = N_CTX + P_SEQ\n    b_ptr_offsets_m = tl.arange(0, BLOCK_M)\n    b_offset = off_hz * stride_b0h\n    b_ptr_offsets_n_1 = tl.arange(0, BLOCK_N) % BIAS_LAST_SIZE + BIAS_LAST_SIZE\n    b1 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m) *\n        stride_b0m)[:, None] + b_ptr_offsets_n_1[None, :])\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=OUT_DTYPE)\n        qk += tl.dot(q, k)\n        b0 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m) *\n            stride_b0m)[:, None] + start_n // BLOCK_N)\n        qk += (b0 + b1) * 1.44269504\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    O_block_ptr = tl.make_block_ptr(base=Out + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc)\n"
    },
    {
      "input": "@triton.jit\ndef matmul_kernel_with_block_pointers(a_ptr, b_ptr, c_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K:\n    'tl.constexpr', GROUP_M: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    GROUP_M = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % GROUP_M\n    pid_n = pid % num_pid_in_group // GROUP_M\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(\n        stride_am, stride_ak), offsets=(pid_m * BLOCK_M, 0), block_shape=(\n        BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(\n        stride_bk, stride_bn), offsets=(0, pid_n * BLOCK_N), block_shape=(\n        BLOCK_K, BLOCK_N), order=(1, 0))\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n        accumulator += tl.dot(a, b)\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    c = accumulator\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(\n        stride_cm, stride_cn), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef scaled_matmul_kernel_with_block_pointers(a_ptr, b_ptr, c_ptr, s1_ptr, M,\n    N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_s1m, stride_s1n, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', ACC_TYPE: 'tl.constexpr'=tl.int32):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = a_ptr + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = b_ptr + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    idx_m = rm[:, None]\n    idx_n = rn[None, :]\n    mask = (idx_m < M) & (idx_n < N)\n    xindex = idx_n + N * idx_m\n    tmp0 = tl.load(s1_ptr + tl.broadcast_to(idx_m, mask.shape), mask,\n        eviction_policy='evict_last')\n    tl.store(c_ptr + tl.broadcast_to(xindex, mask.shape), acc * tmp0, mask)\n"
    },
    {
      "input": "@triton.jit\ndef _matmul_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', GROUP_M: 'tl.constexpr', epilogue_alpha=None,\n    epilogue_beta=None, epilogue_source=None, acc_dtype: 'tl.constexpr'=tl.\n    float32, allow_tf32: 'tl.constexpr'=True, fp8_fast_accum:\n    'tl.constexpr'=True, AB_DTYPE: 'tl.constexpr'=None, EPILOGUE:\n    'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        if AB_DTYPE is not None:\n            a = a\n            b = b\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        else:\n            acc += tl.dot(a, b, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    if EPILOGUE:\n        if epilogue_alpha is not None:\n            acc = epilogue_alpha * acc\n        if epilogue_source is not None:\n            epilogue_src = tl.load(epilogue_source + rm[:, None] *\n                stride_cm + rn[None, :] * stride_cn)\n            if epilogue_beta is not None:\n                epilogue_src = epilogue_src * epilogue_beta\n            acc = acc + epilogue_src\n    acc = acc\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n"
    },
    {
      "input": "@autotune(get_small_k_configs(), key=['M', 'N', 'K'], prune_configs_by={\n    'early_config_prune': small_k_early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': _AUTOTUNE_TOPK})\n@triton.jit\ndef _mm_small_k_kernel(A, B, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, acc_dtype: 'tl.constexpr', input_precision: 'tl.constexpr',\n    fp8_fast_accum: 'tl.constexpr', BLOCK_K: 'tl.constexpr', AB_DTYPE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'=256, BLOCK_N: 'tl.constexpr'=64,\n    C=None, stride_cm=None, stride_cn=None, Norm2=None, Source=None,\n    stride_sourcem=None, stride_sourcen=None, Magnitude=None, ADD_SOURCE:\n    'tl.constexpr'=False, EPILOGUE_NORM: 'tl.constexpr'=False,\n    EPILOGUE_MAGNITUDE: 'tl.constexpr'=False, STORE_ACC: 'tl.constexpr'=False):\n    pid_m = tl.program_id(0)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    a = tl.load(A)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    rn = tl.arange(0, BLOCK_N)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    if STORE_ACC:\n        C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    if ADD_SOURCE:\n        Source = Source + (rm[:, None] * stride_sourcem + rn[None, :] *\n            stride_sourcen)\n    if EPILOGUE_NORM:\n        norm_vec = tl.zeros((BLOCK_M,), dtype=acc_dtype)\n    if EPILOGUE_MAGNITUDE:\n        Magnitude = Magnitude + ram\n    mask_m = rm < M\n    for n in range(0, tl.cdiv(N, BLOCK_N)):\n        b = tl.load(B)\n        if AB_DTYPE is not None:\n            a = a\n            b = b\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        else:\n            acc = tl.dot(a, b, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        if ADD_SOURCE:\n            mask_n = (n * BLOCK_N + rn < N)[None, :]\n            source = tl.load(Source, mask=mask_m[:, None] & mask_n)\n            acc += source\n            Source += BLOCK_N * stride_sourcen\n        if EPILOGUE_NORM:\n            norm_vec += tl.sum(acc * acc, axis=1)\n        if STORE_ACC:\n            mask_n = (n * BLOCK_N + rn < N)[None, :]\n            tl.store(C, acc, mask=mask_m[:, None] & mask_n)\n            C += BLOCK_N * stride_cn\n        B += BLOCK_N * stride_bn\n    if EPILOGUE_NORM:\n        Norm2 = Norm2 + rm\n        norm_vec = tl.rsqrt(norm_vec)\n        if EPILOGUE_MAGNITUDE:\n            magnitude = tl.load(Magnitude, mask=mask_m)\n            norm_vec *= magnitude\n        tl.store(Norm2, norm_vec, mask=mask_m)\n"
    },
    {
      "input": "@triton.jit\ndef _dequant_kernel(q_idx_ptr, absmax_ptr, qmap_ptr, dq_ptr, stride_qm,\n    stride_qn, M, N, GROUP_SIZE: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offsets = rm[:, None] * stride_qm + rn[None, :] * stride_qn\n    mask = (rm[:, None] < M) & (rn[None, :] < N)\n    tl.static_print(offsets)\n    group_offsets = offsets // GROUP_SIZE\n    tl.static_print('group_offsets', group_offsets)\n    q_idx = tl.load(q_idx_ptr + offsets, mask=mask)\n    tl.static_print(q_idx)\n    q_vals = tl.load(qmap_ptr + q_idx)\n    absmax = tl.load(absmax_ptr + group_offsets, mask=group_offsets < M * N //\n        GROUP_SIZE)\n    dq = q_vals * absmax\n    tl.store(dq_ptr + offsets, dq, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics(values={'USE_MASK': lambda args: args['numels'] % args[\n    'BLOCK_SIZE'] != 0, 'NUM_GROUPS': lambda args: triton.cdiv(args[\n    'numels'], args['BLOCK_SIZE'])})\n@triton.jit\ndef _quantize_blockwise_kernel(t_ptr, cutoffs_ptr, q_ptr, absmax_ptr,\n    norm_ptr, numels, BLOCK_SIZE: 'tl.constexpr', NUM_BUCKETS:\n    'tl.constexpr', USE_MASK: 'tl.constexpr', NUM_GROUPS: 'tl.constexpr',\n    RETURN_NORM: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = None\n    absmax_mask = None\n    if USE_MASK:\n        mask = offsets < numels\n        absmax_mask = pid < NUM_GROUPS\n    t = tl.load(t_ptr + offsets, mask=mask)\n    absmax = tl.max(tl.abs(t), axis=0)\n    normalized = t / absmax\n    cutoffs = tl.load(cutoffs_ptr + tl.arange(0, NUM_BUCKETS))\n    q = tl.reshape(normalized, (BLOCK_SIZE, 1)) > cutoffs\n    q = q\n    q = tl.sum(q, axis=1)\n    tl.store(q_ptr + offsets, q, mask=mask)\n    tl.store(absmax_ptr + pid, absmax, mask=absmax_mask)\n    if RETURN_NORM:\n        tl.store(norm_ptr + offsets, normalized, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _mixed_mm_kernel(A, B, scales_ptr, zeros_ptr, C, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scale_k,\n    stride_scale_n, IS_BFLOAT16: 'tl.constexpr', QGROUP_SIZE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr', EVEN_K:\n    'tl.constexpr', TRANSPOSED: 'tl.constexpr'=False, GROUP_M:\n    'tl.constexpr'=8, acc_dtype: 'tl.constexpr'=tl.float32, input_precision:\n    'tl.constexpr'='ieee', fp8_fast_accum: 'tl.constexpr'=False, DEBUG:\n    'tl.constexpr'=False):\n    \"\"\"Mixed matmul kernel\n\n    A has shape (M, K) and is float16, bfloat16, or float32\n\n    B is i4 / s4 and has shape (K // 2, N) and is packed as uint8 / int8. See `packed_2xint4` for details.\n\n    Scales and zeros are of shape (NUM_GROUPS, N) and are same dtype as A, where NUM_GROUPS = (K // QGROUP_SIZE)\n    QGROUP_SIZE should be a multiple of BLOCK_K such that a vector of scales / zeros is loaded and broadcasted to block shape\n    per mainloop iteration.\n\n    In the transposed case, A is M x N and B is K x N, and we reduce along \"N\":\n    - TLDR: we are loading rows of A and B blocks at a time, dequantizing and transposing each block of B to achieve the overall\n    effect of a transposed matmul. This is necessary to perform a transposed matmul without unpacking and repacking the B matrix.\n        - Indexing remains the same for A (the reduction dim (BLK_K / K) corresponds to axis 1 of A -- \"N\" above)\n            - We load a BLK_M x BLK_K block of A\n        - Indexing for B is now flipped: N <-> K\n            - We load BLK_N x BLK_K block of B (remembering that the reduction dimension is axis 1 of B)\n            - We dequantize and transpose to BLK_K x BLK_N\n            - scale / zero indexing also change, since we are now iterating along the non-grouping dim within the mac loop and along\n            the grouping dim across blocks.\n        - Each mac loop calculates BLK_M x BLK_N -> M x \"N\"(= K)\n        - Within the mac loop for each block, we iterate along axis=1 for **both** A and B since axis = 1 is now the reduction dim for B.\n\n    NOTE: Assumes that the quantization grouping was done along the K dimension originally (i.e., QGROUP_SIZE consecutive elements\n    of original weight matrix in the K dimension were grouped together when calculating min / max scaling factors).\n    \"\"\"\n    if not TRANSPOSED:\n        tl.static_assert(QGROUP_SIZE % BLOCK_K == 0)\n    else:\n        tl.static_assert(QGROUP_SIZE % BLOCK_N == 0)\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    if not DEBUG:\n        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    else:\n        ram = rm\n    rak = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    if not TRANSPOSED:\n        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n        if not DEBUG:\n            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n        else:\n            rbn = rn\n        rbk = pid_z * BLOCK_K // 2 + tl.arange(0, BLOCK_K // 2)\n    else:\n        rn = (pid_n * BLOCK_N // 2 + tl.arange(0, BLOCK_N // 2)) % N\n        if not DEBUG:\n            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N // 2), \n                BLOCK_N // 2)\n        else:\n            rbn = rn\n        rbk = rak\n    A = A + (ram[:, None] * stride_am + rak[None, :] * stride_ak)\n    if not TRANSPOSED:\n        B = B + (rbk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    else:\n        B = B + (rbn[:, None] * stride_bk + rbk[None, :] * stride_bn)\n    if not TRANSPOSED:\n        offsets_scale_n = pid_n * stride_scale_n * BLOCK_N + tl.arange(0,\n            BLOCK_N) * stride_scale_n\n    else:\n        scale_offset_k = pid_n * BLOCK_N * stride_scale_k // QGROUP_SIZE\n        offsets_scale_n = tl.arange(0, BLOCK_K) * stride_scale_n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            qb = tl.load(B)\n        else:\n            k_remaining_a = K - k * (BLOCK_K * SPLIT_K)\n            if not TRANSPOSED:\n                k_remaining_b = K - k * (BLOCK_K * SPLIT_K) // 2\n            else:\n                k_remaining_b = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rak[None, :] < k_remaining_a, other=_0)\n            qb = tl.load(B, mask=rbk[:, None] < k_remaining_b, other=_0)\n        if not TRANSPOSED:\n            scale_offset_k = (k * BLOCK_K * SPLIT_K * stride_scale_k //\n                QGROUP_SIZE)\n        else:\n            offsets_scale_n = k * stride_scale_n * BLOCK_K + tl.arange(0,\n                BLOCK_K) * stride_scale_n\n        scales = tl.load(scales_ptr + offsets_scale_n + scale_offset_k)\n        zeros = tl.load(zeros_ptr + offsets_scale_n + scale_offset_k)\n        _4_i8 = tl.full((1,), 4, dtype=tl.int8)\n        qb_lo = qb << _4_i8 >> _4_i8\n        qb_hi = qb >> _4_i8\n        if IS_BFLOAT16:\n            dq_b = tl.join(qb_lo.to(tl.float16), qb_hi.to(tl.float16)).permute(\n                0, 2, 1)\n        else:\n            dq_b = tl.join(qb_lo, qb_hi).permute(0, 2, 1)\n        if not TRANSPOSED:\n            dq_b = dq_b.reshape(BLOCK_K, BLOCK_N)\n        else:\n            dq_b = dq_b.reshape(BLOCK_N, BLOCK_K)\n        zeros = zeros[None, :]\n        scales = scales[None, :]\n        dq_b = (dq_b - zeros) * scales\n        if TRANSPOSED:\n            dq_b = tl.trans(dq_b)\n        if fp8_fast_accum:\n            acc = tl.dot(a, dq_b, acc, out_dtype=acc_dtype, input_precision\n                =input_precision)\n        else:\n            acc += tl.dot(a, dq_b, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        if not TRANSPOSED:\n            B += BLOCK_K * SPLIT_K * stride_bk // 2\n        else:\n            B += BLOCK_K * SPLIT_K * stride_bn\n    acc = acc\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef grouped_launch(pid, m, n, block_m: 'tl.constexpr', block_n:\n    'tl.constexpr', group_m: 'tl.constexpr'):\n    grid_m = tl.cdiv(m, block_m)\n    grid_n = tl.cdiv(n, block_n)\n    width = group_m * grid_n\n    group_id = pid // width\n    group_size = tl.minimum(grid_m - group_id * group_m, group_m)\n    pid_m = group_id * group_m + pid % group_size\n    pid_n = pid % width // group_size\n    return pid_m, pid_n\n"
    },
    {
      "input": "@triton.jit\ndef gemm_split_k_kernel(a_ptr, b_ptr, c_ptr, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, scale_a, scale_b, m, n, k,\n    block_m: 'tl.constexpr', block_n: 'tl.constexpr', block_k:\n    'tl.constexpr', split_k: 'tl.constexpr', group_m: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    grid_k = tl.cdiv(k, block_k * split_k)\n    pid_m, pid_n = grouped_launch(pid, m, n, block_m, block_n, group_m)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_k = pid_k * block_k + tl.arange(0, block_k)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, block_m), block_m)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, block_n), block_n)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k_ in range(0, grid_k):\n        k_remaining = k - k_ * (block_k * split_k)\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < k_remaining, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < k_remaining, other=0.0)\n        acc = tl.dot(a, b, acc, out_dtype=tl.float32)\n        a_ptrs += block_k * split_k * stride_ak\n        b_ptrs += block_k * split_k * stride_bk\n    acc = scale_a * scale_b * acc\n    acc\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    mask = (offs_m < m)[:, None] & (offs_n < n)[None, :]\n    tl.atomic_add(c_ptrs, acc, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n"
    },
    {
      "input": "@triton.jit\ndef create_flashinfer_kv_indices_triton(req_to_token_ptr,\n    req_pool_indices_ptr, page_kernel_lens_ptr, kv_indptr, kv_start_idx,\n    kv_indices_ptr, max_context_len: 'tl.constexpr'):\n    BLOCK_SIZE: 'tl.constexpr' = 512\n    pid = tl.program_id(axis=0)\n    req_pool_index = tl.load(req_pool_indices_ptr + pid)\n    kv_indices_offset = tl.load(kv_indptr + pid)\n    kv_start = 0\n    kv_end = 0\n    if kv_start_idx:\n        kv_start = tl.load(kv_start_idx + pid)\n        kv_end = kv_start\n    kv_end += tl.load(page_kernel_lens_ptr + pid)\n    req_to_token_ptr += req_pool_index * max_context_len\n    kv_indices_ptr += kv_indices_offset\n    ld_offset = kv_start + tl.arange(0, BLOCK_SIZE)\n    st_offset = tl.arange(0, BLOCK_SIZE)\n    num_loop = tl.cdiv(kv_end - kv_start, BLOCK_SIZE)\n    for _ in range(num_loop):\n        mask = ld_offset < kv_end\n        data = tl.load(req_to_token_ptr + ld_offset, mask=mask)\n        tl.store(kv_indices_ptr + st_offset, data, mask=mask)\n        ld_offset += BLOCK_SIZE\n        st_offset += BLOCK_SIZE\n"
    },
    {
      "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_stage1(Q, K_Buffer, sm_scale, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, Att_Out, stride_req_to_tokens_b, stride_qbs,\n    stride_qh, stride_buf_kbs, stride_buf_kh, att_stride_h, kv_group_num:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    logit_cap: 'tl.constexpr', Lk: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    reduce_dtype = Att_Out.dtype.element_ty\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        offs_buf_k = k_loc[:, None\n            ] * stride_buf_kbs + cur_kv_head * stride_buf_kh + offs_d[None, :]\n        k = tl.load(K_Buffer + offs_buf_k, mask=(offs_n_new[:, None] <\n            cur_batch_end_index) & (offs_d[None, :] < Lk), other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        if logit_cap > 0:\n            att_value = logit_cap * tanh(att_value / logit_cap)\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n)\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_stage2(logits, V_Buffer, Out, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, stride_logic_h, stride_buf_vbs, stride_buf_vh,\n    stride_obs, stride_oh, stride_req_to_token_b, kv_group_num:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    Lv: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_buf_v = cur_kv_head * stride_buf_vh + offs_d[None, :]\n    v_ptrs = V_Buffer + offs_buf_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_token_b + (start_n + offs_n), mask=start_n +\n            offs_n < cur_batch_seq_len, other=0)\n        qk = tl.load(logits + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n), mask=start_n + offs_n <\n            cur_batch_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_buf_vbs, mask=offs_d\n            [None, :] < Lv)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_d < Lv)\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_grouped_kernel_stage1(Q, K_Buffer, sm_scale, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, Att_Out, stride_req_to_tokens_b,\n    stride_qbs, stride_qh, stride_buf_kbs, stride_buf_kh, att_stride_h,\n    kv_group_num: 'tl.constexpr', q_head_num: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_DPE: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_H: 'tl.constexpr', logit_cap: 'tl.constexpr', Lk: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head_id = tl.program_id(1)\n    cur_kv_head = cur_head_id // tl.cdiv(kv_group_num, BLOCK_H)\n    start_n = tl.program_id(2)\n    reduce_dtype = Att_Out.dtype.element_ty\n    if BLOCK_H < kv_group_num:\n        VALID_BLOCK_H: 'tl.constexpr' = BLOCK_H\n    else:\n        VALID_BLOCK_H: 'tl.constexpr' = kv_group_num\n    cur_head = cur_head_id * VALID_BLOCK_H + tl.arange(0, BLOCK_H)\n    mask_h = cur_head < (cur_head_id + 1) * VALID_BLOCK_H\n    mask_h = mask_h & (cur_head < q_head_num)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    offs_q = cur_batch * stride_qbs + cur_head[:, None] * stride_qh + offs_d[\n        None, :]\n    if BLOCK_DPE > 0:\n        offs_dpe = BLOCK_DMODEL + tl.arange(0, BLOCK_DPE)\n        off_qpe = cur_batch * stride_qbs + cur_head[:, None\n            ] * stride_qh + offs_dpe[None, :]\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + offs_q + start_mark, mask=mask_h[:, None] & (offs_d\n            [None, :] < Lk))\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        offs_buf_k = k_loc[None, :\n            ] * stride_buf_kbs + cur_kv_head * stride_buf_kh + offs_d[:, None]\n        k = tl.load(K_Buffer + offs_buf_k, mask=(offs_n_new[None, :] <\n            cur_batch_end_index) & (offs_d[:, None] < Lk), other=0.0)\n        qk = tl.dot(q, k)\n        if BLOCK_DPE > 0:\n            qpe = tl.load(Q + off_qpe + start_mark, mask=mask_h[:, None])\n            offs_buf_kpe = k_loc[None, :\n                ] * stride_buf_kbs + cur_kv_head * stride_buf_kh + offs_dpe[\n                :, None]\n            kpe = tl.load(K_Buffer + offs_buf_kpe, mask=offs_n_new[None, :] <\n                cur_batch_end_index, other=0.0)\n            qk += tl.dot(qpe, kpe)\n        qk *= sm_scale\n        if logit_cap > 0:\n            qk = logit_cap * tanh(qk / logit_cap)\n        offs_o = cur_head[:, None] * att_stride_h + (\n            cur_batch_in_all_start_index + offs_n[None, :])\n        tl.store(Att_Out + offs_o, qk, mask=mask_h[:, None] & (offs_n_new[\n            None, :] < cur_batch_end_index))\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_grouped_kernel_stage2(logits, V_Buffer, Out, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, stride_logic_h, stride_buf_vbs,\n    stride_buf_vh, stride_obs, stride_oh, stride_req_to_token_b,\n    kv_group_num: 'tl.constexpr', q_head_num: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_H: 'tl.constexpr', Lv:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head_id = tl.program_id(1)\n    cur_kv_head = cur_head_id // tl.cdiv(kv_group_num, BLOCK_H)\n    if BLOCK_H < kv_group_num:\n        VALID_BLOCK_H: 'tl.constexpr' = BLOCK_H\n    else:\n        VALID_BLOCK_H: 'tl.constexpr' = kv_group_num\n    cur_head = cur_head_id * VALID_BLOCK_H + tl.arange(0, BLOCK_H)\n    mask_h = cur_head < (cur_head_id + 1) * VALID_BLOCK_H\n    mask_h = mask_h & (cur_head < q_head_num)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_buf_v = cur_kv_head * stride_buf_vh + offs_d[None, :]\n    v_ptrs = V_Buffer + offs_buf_v\n    e_max = tl.zeros([BLOCK_H], dtype=tl.float32) - float('inf')\n    e_sum = tl.zeros([BLOCK_H], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_H, BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_token_b + (start_n + offs_n), mask=start_n +\n            offs_n < cur_batch_seq_len, other=0)\n        offs_qk = cur_head[:, None] * stride_logic_h + (cur_batch_start_loc +\n            start_n + offs_n[None, :])\n        qk = tl.load(logits + offs_qk, mask=mask_h[:, None] & (start_n +\n            offs_n[None, :] < cur_batch_seq_len), other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 1), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max[:, None])\n        e_sum = e_sum * old_scale + tl.sum(p, 1)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_buf_vbs, mask=offs_d\n            [None, :] < Lv)\n        p = p\n        acc = acc * old_scale[:, None] + tl.dot(p, v)\n        e_max = n_e_max\n    acc = acc / e_sum[:, None]\n    off_o = cur_batch * stride_obs + cur_head[:, None] * stride_oh + offs_d[\n        None, :]\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=mask_h[:, None] & (offs_d[None, :] < Lv))\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage1(Q, K, V, sm_scale, Req_to_tokens,\n    B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_mid_ob,\n    stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb,\n    stride_mid_o_eh, stride_mid_o_es, gqa_group_size, BLOCK_SEQ:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    seq_start_block = tl.program_id(2)\n    cur_kv_head = cur_head // gqa_group_size\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(cur_batch_seq_len, \n        cur_batch_start_index + BLOCK_SEQ)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = cur_batch_start_index + tl.arange(0, BLOCK_N)\n    q = tl.load(Q + off_q)\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, block_n_size, 1):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :]\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new < cur_batch_end_index, att_value,\n            float('-inf'))\n        v = tl.load(V + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        cur_max_logic = tl.max(att_value, axis=0)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic)\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale\n        acc += tl.sum(exp_logic[:, None] * v, axis=0)\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=0)\n        max_logic = new_max_logic\n    need_store = tl.where(block_n_size == 0, 0, 1)\n    for _ in range(0, need_store, 1):\n        off_mid_o = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +\n            seq_start_block * stride_mid_os + offs_d)\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + cur_head *\n            stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp)\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, O,\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs,\n    stride_oh, stride_od, BLOCK_SEQ: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len +\n        BLOCK_SEQ - 1) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(O + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _sparse_fwd_kernel_flash_decode_stage1(Q_Label, K_Label_Buffer,\n    sm_scale, Req_to_tokens, B_Seqlen, Att_Out, stride_req_to_tokens_b,\n    stride_qbs, stride_qh, stride_buf_kbs, stride_buf_kh, att_stride_h,\n    att_stride_b, kv_group_num: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', logit_cap: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    min_val = -float('inf')\n    att_value = tl.full([BLOCK_N], min_val, dtype=tl.float32)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_index = start_n * BLOCK_N\n    block_mask = tl.where(block_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q_Label + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b * cur_batch +\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        offs_buf_k = k_loc[:, None\n            ] * stride_buf_kbs + cur_kv_head * stride_buf_kh + offs_d[None, :]\n        k = tl.load(K_Label_Buffer + offs_buf_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        if logit_cap > 0:\n            att_value = logit_cap * tanh(att_value / logit_cap)\n    att_value = tl.where(offs_n < cur_batch_end_index, att_value, min_val)\n    off_o = cur_head * att_stride_h + (cur_batch * att_stride_b + offs_n)\n    tl.store(Att_Out + off_o, att_value)\n"
    },
    {
      "input": "@triton.jit\ndef _sparse_fwd_kernel_flash_decode_stage2(Q, K, V, sm_scale, Req_to_tokens,\n    Topk_token_indices, Mid_O, Mid_O_LogExpSum, Heavy_token_num,\n    stride_req_to_tokens_b, stride_topk_token_indices_h,\n    stride_topk_token_indices_b, stride_qbs, stride_qh, stride_kbs,\n    stride_kh, stride_vbs, stride_vh, stride_mid_ob, stride_mid_oh,\n    stride_mid_os, stride_mid_o_eb, stride_mid_o_eh, gqa_group_size,\n    BLOCK_SEQ: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    seq_start_block = tl.program_id(2)\n    cur_kv_head = cur_head // gqa_group_size\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(Heavy_token_num, cur_batch_start_index +\n        BLOCK_SEQ)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = tl.arange(0, BLOCK_N)\n    q = tl.load(Q + off_q)\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(cur_batch_start_index, cur_batch_end_index, BLOCK_N):\n        offs_n_new = start_n + offs_n\n        topk_token_indices = tl.load(Topk_token_indices + \n            stride_topk_token_indices_h * cur_head + \n            stride_topk_token_indices_b * cur_batch + offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b * cur_batch +\n            topk_token_indices, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :]\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new < cur_batch_end_index, att_value,\n            float('-inf'))\n        v = tl.load(V + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        cur_max_logic = tl.max(att_value, axis=0)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic)\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale\n        acc += tl.sum(exp_logic[:, None] * v, axis=0)\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=0)\n        max_logic = new_max_logic\n    need_store = 1\n    for _ in range(0, need_store, 1):\n        off_mid_o = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +\n            seq_start_block * stride_mid_os + offs_d)\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + cur_head *\n            stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp)\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _sparse_fwd_kernel_flash_decode_stage3(Mid_O, Mid_O_LogExpSum, O,\n    seq_len, stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_o_eb,\n    stride_mid_o_eh, stride_obs, stride_oh, BLOCK_SEQ: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    block_n_size = tl.where(seq_len <= 0, 0, seq_len + BLOCK_SEQ - 1\n        ) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(O + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp)\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out, stride_qbs,\n    stride_qh, stride_kbs, stride_kh, stride_vbs, stride_vh, stride_obs,\n    stride_oh, kv_group_num: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', IS_CAUSAL:\n    'tl.constexpr', Lk: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :]\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None]\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :]\n    mask_d = offs_d < Lk\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) &\n        mask_d[None, :], other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    end_n = cur_batch_seq_len if not IS_CAUSAL else tl.minimum((start_m + 1\n        ) * BLOCK_M, cur_batch_seq_len)\n    for start_n in range(0, block_mask * end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=(start_n + offs_n[None, :] < cur_batch_seq_len\n            ) & mask_d[:, None], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        if IS_CAUSAL:\n            qk += tl.where((start_n + offs_n[None, :] < cur_batch_seq_len) &\n                (offs_m[:, None] >= start_n + offs_n[None, :]), 0, float(\n                '-inf'))\n        else:\n            qk += tl.where(start_n + offs_n[None, :] < cur_batch_seq_len, 0,\n                float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=(start_n + offs_n[:, None] < cur_batch_seq_len\n            ) & mask_d[None, :], other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :]\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) &\n        mask_d[None, :])\n"
    },
    {
      "input": "@triton.jit\ndef fused_moe_kernel(a_ptr, b_ptr, c_ptr, a_scale_ptr, b_scale_ptr,\n    topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr,\n    num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am,\n    stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr',\n    MUL_ROUTED_WEIGHT: 'tl.constexpr', top_k: 'tl.constexpr', compute_type:\n    'tl.constexpr', use_fp8: 'tl.constexpr'):\n    \"\"\"\n    Implements the fused computation for a Mixture of Experts (MOE) using\n    token and expert matrices.\n\n    Key Parameters:\n    - A: The input tensor representing tokens with shape (*, K), where '*' can\n        be any shape representing batches and K is the feature dimension of\n        each token.\n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is\n        the number of experts, K is the input feature dimension, and N is\n        the output feature dimension.\n    - C: The output cache tensor with shape (M, topk, N), where M is the\n        total number of tokens post padding, topk is the number of times\n        each token is repeated, and N is the output feature dimension.\n    - sorted_token_ids: A tensor containing the sorted indices of tokens,\n        repeated topk times and arranged by the expert index they are\n        assigned to.\n    - expert_ids: A tensor containing the indices of the expert for each\n        block. It determines which expert matrix from B should be used for\n        each block in A.\n    This kernel performs the multiplication of a token by its corresponding\n    expert matrix as determined by `expert_ids`. The sorting of\n    `sorted_token_ids` by expert index and padding ensures divisibility by\n    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix\n    multiplication across different blocks processed by the same expert.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am + offs_k[\n        None, :] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    if use_fp8:\n        a_scale = tl.load(a_scale_ptr)\n        b_scale = tl.load(b_scale_ptr + off_experts)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        if use_fp8:\n            accumulator = tl.dot(a, b, acc=accumulator)\n        else:\n            accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    if use_fp8:\n        accumulator = accumulator * a_scale * b_scale\n    else:\n        accumulator = accumulator\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n"
    },
    {
      "input": "@triton.autotune(configs=_scatter2scatter_configs(), key=['M', 'N', 'K'])\n@triton.heuristics({'NO_K_MASK': lambda args: args['K'] % args['BLOCK_K'] ==\n    0, 'NO_N_MASK': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _scatter2scatter(X_ptr, stride_xm, stride_xk, W_ptr, stride_we,\n    stride_wk, stride_wn, Y_ptr, stride_ym, stride_yn, grouped_idx_ptr,\n    expert_idxs_ptr, block_start_idx_ptr, FAN_OUT: 'tl.constexpr', M, K:\n    'tl.constexpr', N: 'tl.constexpr', E: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr',\n    ACC_TYPE: 'tl.constexpr', OUT_M, allow_tf32: 'tl.constexpr', x_grouped:\n    'tl.constexpr', y_grouped: 'tl.constexpr', NO_K_MASK: 'tl.constexpr',\n    NO_N_MASK: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    N_BLOCK_COUNT = tl.cdiv(N, BLOCK_N)\n    M_block_id = pid // N_BLOCK_COUNT\n    N_block_id = pid % N_BLOCK_COUNT\n    M_range = tl.arange(0, BLOCK_M)\n    block_start_idx = tl.load(block_start_idx_ptr + M_block_id)\n    M_block = tl.max_contiguous(block_start_idx + M_range, BLOCK_M)\n    E_idxs = tl.load(expert_idxs_ptr + M_block, mask=M_block < FAN_OUT * M,\n        other=E)\n    E_idx = tl.min(E_idxs)\n    E_mask = E_idxs == E_idx\n    M_idx = tl.load(grouped_idx_ptr + M_block, mask=E_mask, other=0)\n    if x_grouped:\n        M_in_idx = M_block\n    else:\n        M_in_idx = M_idx // FAN_OUT\n    if y_grouped:\n        M_out_idx = M_block\n    else:\n        M_out_idx = M_idx\n    K_block = tl.arange(0, BLOCK_K)\n    N_block = N_block_id * BLOCK_N + tl.arange(0, BLOCK_N)\n    N_mask = N_block < N\n    X_blk_ptrs = X_ptr + M_in_idx[:, None] * stride_xm + K_block[None, :\n        ] * stride_xk\n    W_blk_ptrs = W_ptr + K_block[:, None] * stride_wk + N_block[None, :\n        ] * stride_wn + E_idx * stride_we\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    iters = tl.cdiv(K, BLOCK_K)\n    for K_block_id in range(0, iters):\n        if NO_K_MASK:\n            x = tl.load(X_blk_ptrs, mask=E_mask[:, None])\n            if NO_N_MASK or K_block_id < iters - 1:\n                w = tl.load(W_blk_ptrs)\n            else:\n                w = tl.load(W_blk_ptrs, mask=N_mask[None, :])\n        else:\n            K_mask = K_block_id * BLOCK_K + K_block < K\n            x = tl.load(X_blk_ptrs, mask=E_mask[:, None] & K_mask[None, :])\n            w = tl.load(W_blk_ptrs, mask=K_mask[:, None] & N_mask[None, :])\n        X_blk_ptrs += BLOCK_K * stride_xk\n        W_blk_ptrs += BLOCK_K * stride_wk\n        acc += tl.dot(x, w, allow_tf32=allow_tf32, out_dtype=ACC_TYPE)\n    Y_blk_ptrs = Y_ptr + (M_out_idx[:, None] * stride_ym + N_block[None, :] *\n        stride_yn)\n    tl.store(Y_blk_ptrs, acc, mask=E_mask[:, None] & N_mask[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=_config_XtY(), key=['M', 'N', 'K'])\n@triton.heuristics({'NO_K_MASK': lambda args: args['K'] % args['BLOCK_K'] ==\n    0, 'NO_N_MASK': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _groupXtY(DY_ptr, stride_dym, stride_dyk, X_ptr, stride_xm, stride_xn,\n    DW_ptr, stride_dwe, stride_dwk, stride_dwn, expert_offsets_ptr, M, K:\n    'tl.constexpr', N: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', ACC_TYPE: 'tl.constexpr',\n    allow_tf32: 'tl.constexpr', NO_K_MASK: 'tl.constexpr', NO_N_MASK:\n    'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    num0 = tl.num_programs(0)\n    num1 = tl.num_programs(1)\n    pid0, pid1 = tl.swizzle2d(pid0, pid1, num0, num1, 4)\n    K_BLOCK_COUNT = tl.cdiv(K, BLOCK_K)\n    E_idx = pid0 // K_BLOCK_COUNT\n    K_block_id = pid0 % K_BLOCK_COUNT\n    N_block_id = pid1\n    if E_idx == 0:\n        start_idx = 0\n    else:\n        start_idx = tl.load(expert_offsets_ptr + E_idx - 1)\n    end_idx = tl.load(expert_offsets_ptr + E_idx)\n    if end_idx > start_idx:\n        M_block = tl.max_contiguous(start_idx + tl.arange(0, BLOCK_M), BLOCK_M)\n        K_block = K_block_id * BLOCK_K + tl.arange(0, BLOCK_K)\n        K_mask = K_block < K\n        K_block = tl.max_contiguous(tl.multiple_of(K_block % K, BLOCK_K),\n            BLOCK_K)\n        N_block = N_block_id * BLOCK_N + tl.arange(0, BLOCK_N)\n        N_mask = N_block < N\n        N_block = tl.max_contiguous(tl.multiple_of(N_block % N, BLOCK_N),\n            BLOCK_N)\n        M_idxs = M_block\n        xt_blk_ptrs = X_ptr + K_block[:, None] * stride_xn + M_idxs[None, :\n            ] * stride_xm\n        dy_blk_ptrs = DY_ptr + M_idxs[:, None] * stride_dym + N_block[None, :\n            ] * stride_dyk\n        acc = tl.zeros((BLOCK_K, BLOCK_N), dtype=ACC_TYPE)\n        iters = tl.cdiv(end_idx - start_idx, BLOCK_M)\n        for i in range(0, iters):\n            M_mask = i * BLOCK_M + M_block < end_idx\n            if NO_K_MASK:\n                xt = tl.load(xt_blk_ptrs, mask=M_mask[None, :])\n            else:\n                xt = tl.load(xt_blk_ptrs, mask=K_mask[:, None] & M_mask[\n                    None, :])\n            if NO_N_MASK:\n                dy = tl.load(dy_blk_ptrs, mask=M_mask[:, None])\n            else:\n                dy = tl.load(dy_blk_ptrs, mask=M_mask[:, None] & N_mask[\n                    None, :])\n            xt_blk_ptrs += BLOCK_M * stride_xm\n            dy_blk_ptrs += BLOCK_M * stride_dym\n            acc += tl.dot(xt, dy, out_dtype=ACC_TYPE, allow_tf32=allow_tf32)\n        DW_blk_ptrs = DW_ptr + E_idx * stride_dwe + K_block[:, None\n            ] * stride_dwk + N_block[None, :] * stride_dwn\n        acc = acc\n        tl.store(DW_blk_ptrs, acc, mask=K_mask[:, None] & N_mask[None, :])\n"
    },
    {
      "input": "@triton.autotune(configs=_config_grouping(), key=['K'])\n@triton.heuristics({'NO_K_MASK': lambda args: args['K'] % args['BLOCK_K'] == 0}\n    )\n@triton.jit\ndef _group(src_ptr, stride_sn, stride_sk, has_coeff: 'tl.constexpr',\n    coeff_ptr, FAN_OUT: 'tl.constexpr', tgt_ptr, stride_tn, stride_ti,\n    grouped_idx_ptr, N, K: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K:\n    'tl.constexpr', NO_K_MASK: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    N_block_id = pid\n    N_blk = N_block_id * BLOCK_N + tl.arange(0, BLOCK_N)\n    N_mask = N_blk < N\n    N_blk = tl.max_contiguous(tl.multiple_of(N_blk % N, BLOCK_N), BLOCK_N)\n    N_idx = tl.load(grouped_idx_ptr + N_blk, mask=N_mask, other=0)\n    K_blk = tl.arange(0, BLOCK_K)\n    src_blk_ptrs = src_ptr + (N_idx // FAN_OUT)[:, None] * stride_sn + K_blk[\n        None, :] * stride_sk\n    tgt_blk_ptrs = tgt_ptr + N_blk[:, None] * stride_tn + K_blk[None, :\n        ] * stride_ti\n    if has_coeff:\n        c = tl.load(coeff_ptr + N_idx, mask=N_mask)[:, None]\n    iters = tl.cdiv(K, BLOCK_K)\n    for i in range(0, iters):\n        if NO_K_MASK or i < iters - 1:\n            block = tl.load(src_blk_ptrs, mask=N_mask[:, None])\n            if has_coeff:\n                block *= c\n            tl.store(tgt_blk_ptrs, block, mask=N_mask[:, None])\n        else:\n            K_mask = i * BLOCK_K + K_blk < K\n            mask = N_mask[:, None] & K_mask[None, :]\n            block = tl.load(src_blk_ptrs, mask=mask)\n            if has_coeff:\n                block *= c\n            tl.store(tgt_blk_ptrs, block, mask=mask)\n        src_blk_ptrs += BLOCK_K * stride_sk\n        tgt_blk_ptrs += BLOCK_K * stride_ti\n"
    },
    {
      "input": "@triton.jit\ndef _single2scatter(X_ptr, stride_xm, stride_xk, W_ptr, stride_we,\n    stride_wk, stride_wn, Y_ptr, stride_ym, stride_yn, expert_idxs_ptr,\n    FAN_OUT: 'tl.constexpr', K: 'tl.constexpr', N: 'tl.constexpr', E:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr',\n    ACC_TYPE: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    N_block_id = pid0\n    if FAN_OUT == 1:\n        in_idx = pid1\n    else:\n        in_idx = 0\n    out_idx = pid1\n    K_block = tl.arange(0, BLOCK_K)\n    N_block = tl.max_contiguous(tl.multiple_of((N_block_id * BLOCK_N + tl.\n        arange(0, BLOCK_N)) % N, BLOCK_N), BLOCK_N)\n    E_idx = tl.load(expert_idxs_ptr + pid1)\n    X_blk_ptrs = X_ptr + in_idx * stride_xm + K_block[:, None] * stride_xk\n    W_blk_ptrs = W_ptr + E_idx * stride_we + K_block[:, None\n        ] * stride_wk + N_block[None, :] * stride_wn\n    acc = tl.zeros((1, BLOCK_N), dtype=ACC_TYPE)\n    for K_block_id in range(0, tl.cdiv(K, BLOCK_K)):\n        x = tl.load(X_blk_ptrs)\n        w = tl.load(W_blk_ptrs)\n        acc += tl.sum(x * w, axis=0)[None, :]\n        X_blk_ptrs += BLOCK_K * stride_xk\n        W_blk_ptrs += BLOCK_K * stride_wk\n    Y_blk_ptrs = Y_ptr + out_idx * stride_ym + N_block[None, :] * stride_yn\n    tl.store(Y_blk_ptrs, acc)\n"
    },
    {
      "input": "@triton.jit\ndef maximum_path(path, value, t_x, t_y, B, T, S, max_neg_val, BLOCK_SIZE_X:\n    'tl.constexpr'):\n    batch = tl.program_id(axis=0)\n    path += batch * T * S\n    value += batch * T * S\n    x_length = tl.load(t_x + batch)\n    y_length = tl.load(t_y + batch)\n    offs_prev = tl.arange(0, BLOCK_SIZE_X)\n    init = tl.where(offs_prev == 0, tl.load(value), max_neg_val)\n    tl.store(value + offs_prev * S, init, mask=offs_prev < x_length)\n    for j in range(1, y_length, 1):\n        v_cur = tl.load(value + offs_prev * S + (j - 1), mask=offs_prev <\n            x_length, other=max_neg_val)\n        v_prev = tl.load(value + (offs_prev - 1) * S + (j - 1), mask=(0 <\n            offs_prev) & (offs_prev < x_length), other=max_neg_val)\n        v = tl.maximum(v_cur, v_prev) + tl.load(value + offs_prev * S + j,\n            mask=offs_prev < x_length)\n        tl.store(value + offs_prev * S + j, v, mask=offs_prev < x_length)\n    index = x_length - 1\n    for j in range(y_length - 1, -1, -1):\n        tl.store(path + index * S + j, 1)\n        if index > 0:\n            v_left = tl.load(value + index * S + j - 1)\n            v_leftdown = tl.load(value + (index - 1) * S + j - 1)\n            if v_left < v_leftdown:\n                index += -1\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8), triton.Config({'BT': 128}, num_warps=\n    2), triton.Config({'BT': 128}, num_warps=4), triton.Config({'BT': 128},\n    num_warps=8), triton.Config({'BT': 256}, num_warps=2), triton.Config({\n    'BT': 256}, num_warps=4), triton.Config({'BT': 256}, num_warps=8)], key\n    =['D'])\n@triton.jit\ndef logsigmoid_fwd_kernel(x, y, T: 'tl.constexpr', D: 'tl.constexpr', BT:\n    'tl.constexpr'):\n    i = tl.program_id(0)\n    o_i = i * BT + tl.arange(0, BT)\n    p_x = x + o_i\n    p_y = y + o_i\n    mask = o_i < T\n    b_x = tl.load(p_x, mask=mask, other=0.0)\n    b_m = tl.minimum(0.0, b_x)\n    b_z = 1.0 + tl.exp(-tl.abs(b_x))\n    b_y = b_m - tl.log(b_z)\n    tl.store(p_y, b_y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8), triton.Config({'BT': 128}, num_warps=\n    2), triton.Config({'BT': 128}, num_warps=4), triton.Config({'BT': 128},\n    num_warps=8), triton.Config({'BT': 256}, num_warps=2), triton.Config({\n    'BT': 256}, num_warps=4), triton.Config({'BT': 256}, num_warps=8)], key\n    =['D'])\n@triton.jit\ndef logsigmoid_bwd_kernel(x, dx, dy, T: 'tl.constexpr', D: 'tl.constexpr',\n    BT: 'tl.constexpr'):\n    i = tl.program_id(0)\n    o_i = i * BT + tl.arange(0, BT)\n    p_x = x + o_i\n    p_dx = dx + o_i\n    p_dy = dy + o_i\n    mask = o_i < T\n    b_x = tl.load(p_x, mask=mask, other=0.0)\n    b_dy = tl.load(p_dy, mask=mask, other=0.0)\n    b_dx = b_dy * (1.0 - tl.sigmoid(b_x))\n    tl.store(p_dx, b_dx, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_quant_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_WEIGHT:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w if HAS_WEIGHT else x_hat\n    if HAS_BIAS:\n        y = y + b\n    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-05)\n    y = tl.math.round(y * scale)\n    y = tl.maximum(tl.minimum(y, 127), -128) / scale\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd,\n    stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row,\n    stride_dz_row, stride_dw_row, stride_db_row, M, N, eps,\n    rows_per_program, NORM_BEFORE_GATE: 'tl.constexpr', IS_RMS_NORM:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    RECOMPUTE_OUTPUT: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        elif RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask\n        =mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db,\n            mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] >\n    0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, label_smoothing, logit_scale, lse_square_scale,\n    ignore_index, total_classes, class_start_idx, n_cols, n_rows,\n    logits_row_stride, BLOCK_SIZE: 'tl.constexpr', HAS_SMOOTHING:\n    'tl.constexpr', SPLIT: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf'))\n    logits = logits * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols,\n            (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - label_smoothing * sum_logits / total_classes - (1 -\n                    label_smoothing) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = label_smoothing * ((lse if not SPLIT else 0.0) - \n                sum_logits / total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] >\n    0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, label_smoothing, logit_scale, lse_square_scale,\n    ignore_index, total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: 'tl.constexpr',\n    HAS_SMOOTHING: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = label_smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 -\n            label_smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef kl_div_kernel(logits, target_logits, loss, s_logits, s_loss, reduction:\n    'tl.constexpr', N: 'tl.constexpr', V: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_n = tl.program_id(0)\n    logits += i_n * s_logits\n    target_logits += i_n * s_logits\n    sm, tm = float('-inf'), float('-inf')\n    sd, td = 0.0, 0.0\n    NV = tl.cdiv(V, BV)\n    for iv in range(0, NV):\n        o_x = iv * BV + tl.arange(0, BV)\n        b_sl = tl.load(logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_sm = tl.max(b_sl)\n        m_new = tl.maximum(sm, b_sm)\n        sd = sd * tl.exp(sm - m_new) + tl.sum(tl.exp(b_sl - m_new))\n        sm = m_new\n        b_tl = tl.load(target_logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_tm = tl.max(b_tl)\n        m_new = tl.maximum(tm, b_tm)\n        td = td * tl.exp(tm - m_new) + tl.sum(tl.exp(b_tl - m_new))\n        tm = m_new\n    b_loss = 0.0\n    for iv in range(0, NV):\n        o_x = iv * BV + tl.arange(0, BV)\n        b_sl = tl.load(logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_tl = tl.load(target_logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_sp_log = b_sl - sm - tl.log(sd)\n        b_tp_log = b_tl - tm - tl.log(td)\n        b_sp = tl.exp(b_sp_log)\n        b_tp = tl.exp(b_tp_log)\n        b_kl = tl.where(o_x < V, b_tp * (b_tp_log - b_sp_log), 0)\n        b_dl = -b_tp + b_sp\n        b_loss += tl.sum(b_kl)\n        if reduction == 'batchmean':\n            b_dl = b_dl / N\n        tl.store(logits + o_x, b_dl, mask=o_x < V)\n    if reduction == 'batchmean':\n        b_loss = b_loss / N\n    tl.store(loss + i_n * s_loss, b_loss)\n"
    },
    {
      "input": "@triton.jit\ndef elementwise_mul_kernel(x, g, N: 'tl.constexpr', B: 'tl.constexpr'):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by x with the value pointed by g.\n    The multiplication is performed in-place on the tensor pointed by x.\n\n    Parameters:\n    x:\n        Pointer to the input tensor.\n    g:\n        Pointer to the gradient output value.\n    N (int):\n        The number of columns in the input tensor.\n    B (int):\n        The block size for Triton operations.\n    \"\"\"\n    i_x = tl.program_id(0)\n    o_x = i_x * B + tl.arange(0, B)\n    b_g = tl.load(g)\n    b_x = tl.load(x + o_x, mask=o_x < N)\n    tl.store(x + o_x, b_x * b_g, mask=o_x < N)\n"
    },
    {
      "input": "@triton.jit\ndef cross_entropy_kernel(logits, lse, target, loss, total, ignore_index,\n    label_smoothing: 'tl.constexpr', logit_scale: 'tl.constexpr', reduction:\n    'tl.constexpr', V: 'tl.constexpr', BV: 'tl.constexpr'):\n    \"\"\"\n    This kernel computes both cross entropy loss and the gradient of the input.\n    We only consider hard label + mean reduction for now.\n    Please refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for the math.\n\n    Args:\n        logits:\n            Pointer to logits tensor.\n        lse:\n            Pointer to logsumexp tensor.\n        target: Pointer to target tensor.\n        loss:\n            Pointer to tensor to store the loss.\n        V (int):\n            The number of columns in the input tensor.\n        total (int):\n            The number of non-ignored classes.\n        ignore_index (int):\n            The index to ignore in the target.\n        label_smoothing (float):\n            The amount of smoothing when computing the loss, where 0.0 means no smoothing.\n        reduction (str):\n            The string for the reduction to apply\n        BV (int):\n            The block size for vocab.\n    \"\"\"\n    i_n = tl.program_id(0)\n    NV = tl.cdiv(V, BV)\n    b_y = tl.load(target + i_n)\n    logits += i_n * V\n    if b_y == ignore_index:\n        for i in range(0, V, BV):\n            o_v = i + tl.arange(0, BV)\n            tl.store(logits + o_v, 0.0, mask=o_v < V)\n        return\n    b_l = tl.load(logits + b_y) * logit_scale\n    b_lse = tl.load(lse + i_n)\n    b_loss = b_lse - b_l\n    b_z = 0.0\n    eps = label_smoothing / V\n    tl.debug_barrier()\n    for iv in range(0, NV):\n        o_v = iv * BV + tl.arange(0, BV)\n        b_logits = tl.load(logits + o_v, mask=o_v < V, other=float('-inf')\n            ) * logit_scale\n        if label_smoothing > 0:\n            b_z += tl.sum(tl.where(o_v < V, -eps * b_logits, 0.0))\n        b_p = (tl.exp(b_logits - b_lse) - eps) * logit_scale\n        if reduction == 'mean':\n            b_p = b_p / total\n        tl.store(logits + o_v, b_p, mask=o_v < V)\n        tl.debug_barrier()\n    if label_smoothing > 0:\n        b_loss = b_loss * (1 - label_smoothing) + (b_z + label_smoothing *\n            b_lse)\n    b_l = tl.load(logits + b_y)\n    if reduction == 'mean':\n        b_loss = b_loss / total\n        b_l += (label_smoothing - 1) / total * logit_scale\n    else:\n        b_l += (label_smoothing - 1) * logit_scale\n    tl.store(loss + i_n, b_loss)\n    tl.store(logits + b_y, b_l)\n"
    },
    {
      "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_z_row, M, N, eps, BLOCK_N: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr', NORM_BEFORE_GATE:\n    'tl.constexpr', IS_RMS_NORM: 'tl.constexpr'):\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask)\n        y *= z * tl.sigmoid(z)\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(X, Y, stride_x_row, N, eps, BLOCK_N:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0)\n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    y = x * rstd\n    tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _l2_norm_bwd_kernel(X, DY, DX, stride_x_row, N, eps, BLOCK_N:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    DX += row * stride_x_row\n    DY += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    x = tl.where(cols < N, x, 0.0)\n    var = tl.sum(x * x)\n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    dy = tl.load(DY + cols, mask=cols < N, other=0.0)\n    dy = tl.where(cols < N, dy, 0.0)\n    dx = dy * rstd - tl.sum(dy * x) * (1 / (var + eps)) * rstd * x\n    tl.store(DX + cols, dx, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_h(k, v, z, h, h0, ht, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NORMK: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    if NORMK:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_k *\n            BK,), (BK,), (0,))\n    else:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_v *\n            BV,), (BV,), (0,))\n    b_zp = tl.load(p_z0)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if NORMK:\n            p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,),\n                ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[:, None]\n            b_k = tl.exp(b_k - b_zc[:, None])\n        else:\n            p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,),\n                ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[None, :]\n            b_v = tl.exp(b_v - b_zc[None, :])\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_K(v, z, o, A, s_v_h, s_v_t, s_v_d, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, tl.exp(b_v - b_zn[None, :]), allow_tf32=False)\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_o *= tl.exp(b_zn[None, :] - b_z)\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n        b_v = tl.load(p_v, boundary_check=(0,))\n        m_i = o_i[:, None] >= j\n        b_o += tl.where(m_i, b_A[:, None] * tl.exp(b_v[None, :] - b_z), 0)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_K(q, k, z, h, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_A += tl.dot(b_q, b_k, allow_tf32=False)\n    p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    p_zp = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_p * V +\n        i_v * BV,), (BV,), (0,))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_zp[None, :] - b_z)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_V(q, k, z, A, s_k_h, s_k_t, s_k_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_q = b_q * tl.exp(b_zn[None, :] - b_z) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k = tl.exp(b_k - b_zn[:, None])\n        b_A = tl.dot(b_q, b_k, allow_tf32=False)\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_k * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_k = tl.load(p_k, boundary_check=(0,))\n            b_A = tl.sum(b_q * tl.exp(b_k[None, :] - b_z) * scale, 1)\n            b_A = tl.where(o_i >= j, b_A, 0.0)\n            tl.store(A + o_A + j, b_A, mask=m_A)\n            p_k = tl.advance(p_k, (K,))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_fwd_kernel_V(q, v, z, h, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_zp = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_p *\n            K + i_k * BK,), (BK,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_q = b_q * tl.exp(b_zp[None, :] - b_z)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dh(q, z, do, dh, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t,\n    s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NORMK: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    b_zp = tl.full([BK if NORMK else BV], float('inf'), dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        i_p = tl.maximum(i_t * BT - 1, 0)\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        if NORMK:\n            p_z = tl.make_block_ptr(z + i_bh * s_k_h, (K, T), (s_k_d, s_k_t\n                ), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,),\n                (i_p * K + i_k * BK,), (BK,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zc - b_zp), b_zc\n            b_z = tl.load(p_z, boundary_check=(0, 1))\n            b_q = b_q * tl.exp(b_zc[:, None] - b_z)\n            b_dh = b_dh * b_r[:, None]\n        else:\n            p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d\n                ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,),\n                (i_p * V + i_v * BV,), (BV,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zc - b_zp), b_zc\n            b_z = tl.load(p_z, boundary_check=(0,))\n            b_do = b_do * tl.exp(b_zc[None, :] - b_z)\n            b_dh = b_dh * b_r[None, :]\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_V(k, v, z, h, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    b_zc = tl.load(p_zc, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k = tl.exp(b_k - b_zc[None, :])\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n        if i_k == 0:\n            b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n        b_do = b_do * scale\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n    p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_zp = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_p * K +\n        i_k * BK,), (BK,), (0,))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_z = tl.exp(b_zp[None, :] - b_z)\n    b_dq = b_dq * b_z\n    b_dk = b_dk * b_k\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_dA = tl.where(m_s, b_dA, 0.0)\n    if i_k == 0:\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_V(q, k, z, dA, dq, dk, s_k_h, s_k_t, s_k_d,\n    T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_zq = tl.exp(b_zn[None, :] - b_z)\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kz = tl.exp(b_k - b_zn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kz, allow_tf32=False)\n    b_dq *= b_zq\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,))\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * tl.exp(b_kj[None, :] - b_z), 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kz = tl.exp(b_k - b_zn[None, :])\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_qz = b_q * tl.exp(b_zn[None, :] - b_z)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(tl.trans(b_dA), b_qz, allow_tf32=False)\n    b_dk *= b_kz\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, BC):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_zj = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        b_zj = tl.load(p_zj, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_k -\n            b_zj[None, :]), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_K(v, z, do, dA, s_v_h, s_v_t, s_v_d, scale,\n    T: 'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (i_bh + i_v * n_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_zn[None, :] - b_z) * scale\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = tl.exp(b_v - b_zn[:, None])\n        b_dA = tl.dot(b_do, b_v, allow_tf32=False)\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n            BT + i_j * BC) * V + i_v * BV,), (BV,), (0,))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_v * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_v = tl.load(p_v, boundary_check=(0,))\n            b_dA = tl.sum(b_do * tl.exp(b_v[None, :] - b_z), 1)\n            b_dA = tl.where(o_i >= j, b_dA, 0)\n            tl.store(dA + o_A + j, b_dA, mask=m_A)\n            p_v = tl.advance(p_v, (V,))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_K(q, k, v, z, h, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT,\n        1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A = tl.dot(b_q * scale, tl.trans(b_k), allow_tf32=False)\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_zp = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_p *\n            V + i_v * BV,), (BV,), (0,))\n        p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = tl.exp(b_v - b_zc[None, :])\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_z = tl.exp(b_zp[None, :] - b_z)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * b_z * scale\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = b_v * tl.dot(b_k, b_dh, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_k, allow_tf32=False)\n    b_dk += tl.dot(tl.trans(b_dA), b_q, allow_tf32=False)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_KV(v, z, A, do, dv, s_v_h, s_v_t, s_v_d, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * V + i_v * BV,), (BV,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (i_i *\n            BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_zn[None, :] - b_z)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n    b_dv *= tl.exp(b_v - b_zn[None, :])\n    o_i = tl.arange(0, BC)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T * BT,), (1,), ((i_t *\n            BT + i_i * BC + j) * BT + i_i * BC,), (BC,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(p_A, boundary_check=(0,))\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_do = tl.load(p_do, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(m_i, tl.exp(b_v - b_z[None, :]) * b_A[:, None] *\n            b_do[None, :], 0.0)\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_inter(s, z, ss, doo, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n    b_sp = tl.zeros([BS], dtype=tl.float32)\n    b_zp = tl.full([BS], float('inf'), dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_zc = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (s_s_d,), (i_t *\n            BT * S + i_m * BS,), (BS,), (0,))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_doo = tl.make_block_ptr(doo + i_bh * s_s_h, (T, S), (s_s_t, s_s_d\n            ), (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n        b_doo = tl.exp(b_s - b_zp[None, :]) * b_sp[None, :]\n        tl.store(p_doo, b_doo, boundary_check=(0, 1))\n        b_sp = b_sp * tl.exp(b_zc - b_zp) + tl.sum(b_ss * tl.exp(b_zc[None,\n            :] - b_z), 0)\n        b_zp = b_zc\n"
    },
    {
      "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_intra(s, z, ss, doo, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BS: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_s, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    o_i = tl.arange(0, BC)\n    m_o = tl.full([BC, BC], 1.0, dtype=tl.float32)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (s_s_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * S + i_s * BS,), (BS,), (0,))\n    p_doo = tl.make_block_ptr(doo + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_doo = tl.zeros([BC, BS], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n        b_doo += b_ss * tl.exp(b_zn[None, :] - b_z)\n    b_doo = tl.exp(b_s - b_zn[None, :]) * tl.dot(m_o, b_doo, allow_tf32=False)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_ss = tl.load(p_ss, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_doo += tl.where(m_i, tl.exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)\n    b_doo += tl.load(p_doo, boundary_check=(0, 1))\n    tl.store(p_doo, b_doo, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_0o = tl.zeros([BV], dtype=tl.float32)\n    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (0,\n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0,\n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_0o = 0\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT])\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1)) * scale\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_z = tl.zeros([BT], dtype=tl.float32)\n        b_o += b_h_0o\n        b_z += k_0o\n        b_o += tl.dot(b_q, b_h_1o, allow_tf32=False)\n        b_z += tl.sum(b_q * k_1o, axis=1)\n        b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK])\n        b_o += tl.dot(b_q_2o, b_h_2o, allow_tf32=False) * 0.5\n        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n        k_1o += tl.sum(b_k, axis=1)[None, :]\n        k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n        k_0o += BT\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        tl.store(p_z, b_z, mask=i * BT + tl.arange(0, BT) < T)\n        b_h_2o = b_h_2o + tl.dot(b_k_2o, b_v, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_z += BT\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_1o = tl.zeros([BV, BK], dtype=tl.float32)\n    b_h_2o = tl.zeros([BV, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t),\n            (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i * BT\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i * BT < T)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h_1o, allow_tf32=False)\n        if i_v == 0:\n            b_dq += b_dz[:, None] * k_1o\n        b_dq_2o = tl.dot(b_do, b_h_2o, allow_tf32=False) * 0.5\n        if i_v == 0:\n            b_dq_2o += b_dz[:, None] * k_2o * 0.5\n        b_dq_2o = tl.reshape(b_dq_2o, [BT, BK, BK])\n        b_dq += tl.sum(b_dq_2o * b_q[:, :, None], axis=1)\n        b_dq += tl.sum(b_dq_2o * b_q[:, None, :], axis=2)\n        b_dq *= scale\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_h_2o = b_h_2o + tl.dot(b_v, b_k_2o, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_v, b_k, allow_tf32=False)\n        if i_v == 0:\n            k_1o += tl.sum(b_k, axis=0)[None, :]\n            k_2o += tl.sum(b_k_2o, axis=0)[None, :]\n    tl.debug_barrier()\n    b_h_1o = None\n    b_h_2o = None\n    b_dh_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_dh_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    b_dh_0o = tl.zeros([BV], dtype=tl.float32)\n    m_s = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    dq_1o = tl.zeros([1, BK], dtype=tl.float32)\n    dq_2o = tl.zeros([BK * BK, 1], dtype=tl.float32)\n    for i in range(tl.cdiv(T, BT) * BT - BT, -BT, -BT):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (i, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i < T)\n        b_q = b_q * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds *= 1 + b_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s2, b_do, allow_tf32=False)\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_dv += tl.dot(b_k, b_dh_1o, allow_tf32=False)\n        b_dv += tl.dot(b_k_2o, b_dh_2o, allow_tf32=False)\n        b_dv += b_dh_0o\n        b_dk += tl.dot(b_v, tl.trans(b_dh_1o), allow_tf32=False)\n        if i_v == 0:\n            b_dk += dq_1o\n        b_dk_2o = tl.dot(b_dh_2o, tl.trans(b_v), allow_tf32=False)\n        if i_v == 0:\n            b_dk_2o += dq_2o\n        b_dk_2o = tl.reshape(b_dk_2o, [BK, BK, BT])\n        b_k_fp32 = tl.trans(b_k)\n        b_dk2 = tl.sum(b_dk_2o * b_k_fp32[:, None, :], axis=0)\n        b_dk2 += tl.sum(b_dk_2o * b_k_fp32[None, :, :], axis=1)\n        b_dk += tl.trans(b_dk2)\n        b_dh_0o += tl.sum(b_do, axis=0)\n        b_dh_1o = b_dh_1o + tl.dot(b_q, b_do, allow_tf32=False)\n        b_q_2o = b_q[None, :, :] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BK * BK, BT])\n        b_dh_2o = b_dh_2o + tl.dot(b_q_2o, b_do, allow_tf32=False) * 0.5\n        if i_v == 0:\n            dq_1o += tl.sum(b_dz[None, :] * b_q, axis=1)[None, :]\n            dq_2o += (tl.sum(b_dz[None, :] * b_q_2o, axis=1) * 0.5)[:, None]\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef parallel_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0,\n        i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (0,\n        i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds + b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, K), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (V, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (V, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, K), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, K=K, V=V)\n    tl.debug_barrier()\n    _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, K, V)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV', 'USE_G', 'USE_GK', 'USE_GV'])\n@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})\n@triton.jit\ndef chunk_fwd_kernel_h(k, v, h, g, gk, gv, h0, ht, s_k_h, s_k_t, s_v_h,\n    s_v_t, s_h_h, s_h_t, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', USE_G: 'tl.constexpr', USE_GK:\n    'tl.constexpr', USE_GV: 'tl.constexpr', USE_INITIAL_STATE:\n    'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        last_idx = min((i_t + 1) * BT, T) - 1\n        if USE_G:\n            b_g_last = tl.load(g + i_bh * T + last_idx)\n            b_h *= tl.exp(b_g_last)\n            p_g = g + i_bh * T + i_t * BT + tl.arange(0, BT)\n            p_g = tl.max_contiguous(tl.multiple_of(p_g, BT), BT)\n            b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n            b_v = b_v * tl.exp(b_g_last - b_g)[:, None]\n        if USE_GK:\n            p_gk_last = (gk + i_bh * s_k_h + last_idx * K + i_k * BK + tl.\n                arange(0, BK))\n            p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) <\n                K, other=0.0)\n            b_h *= tl.exp(b_gk_last)[:, None]\n            p_gk = tl.make_block_ptr(gk + i_bh * s_k_h, (K, T), (1, s_k_t),\n                (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_k = b_k * tl.exp(b_gk_last[:, None] - b_gk)\n        if USE_GV:\n            p_gv_last = (gv + i_bh * s_v_h + last_idx * V + i_v * BV + tl.\n                arange(0, BV))\n            p_gv_last = tl.max_contiguous(tl.multiple_of(p_gv_last, BV), BV)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) <\n                V, other=0.0)\n            b_h *= tl.exp(b_gv_last)[None, :]\n            p_gv = tl.make_block_ptr(gv + i_bh * s_v_h, (T, V), (s_v_t, 1),\n                (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_v = b_v * tl.exp(b_gv_last[None, :] - b_gv)\n        b_h += tl.dot(b_k, b_v)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV', 'USE_G', 'USE_GK', 'USE_GV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_bwd_kernel_dh(q, g, gk, gv, do, dh, dht, dh0, s_k_h, s_k_t, s_v_h,\n    s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NG: 'tl.constexpr', USE_G:\n    'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV: 'tl.constexpr',\n    STORE_INITIAL_STATE_GRADIENT: 'tl.constexpr', USE_FINAL_STATE_GRADIENT:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        if USE_G:\n            p_g = g + i_bg * T + i_t * BT + tl.arange(0, BT)\n            p_g = tl.max_contiguous(tl.multiple_of(p_g, BT), BT)\n            b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n            b_q = b_q * tl.exp(b_g)[None, :]\n            b_g_last = tl.load(g + i_bg * T + last_idx)\n            b_dh *= tl.exp(b_g_last)\n        if USE_GK:\n            p_gk = tl.make_block_ptr(gk + i_bg * s_k_h, (K, T), (1, s_k_t),\n                (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_q = b_q * tl.exp(b_gk)\n            p_gk_last = (gk + i_bg * s_k_h + last_idx * K + i_k * BK + tl.\n                arange(0, BK))\n            p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) <\n                K, other=0.0)\n            b_dh *= tl.exp(b_gk_last)[:, None]\n        if USE_GV:\n            p_gv = tl.make_block_ptr(gv + i_bg * s_v_h, (T, V), (s_v_t, 1),\n                (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_do = b_do * tl.exp(b_gv)\n            p_gv_last = (gv + i_bg * s_v_h + last_idx * V + i_v * BV + tl.\n                arange(0, BV))\n            p_gv_last = tl.max_contiguous(tl.multiple_of(p_gv_last, BV), BV)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) <\n                V, other=0.0)\n            b_dh *= tl.exp(b_gv_last)[None, :]\n        b_dh += tl.dot(b_q, b_do)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_fwd_kernel(q, k, v, alpha, beta, o, ha, h0, ht, s_qk_h,\n    s_vo_h, scale, B, H, T, K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_alpha = alpha + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_beta = beta + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_ha = ha + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_alpha = tl.load(p_alpha, mask=mask_bk, other=0)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0)\n        tmp = tl.sum(h * b_alpha[None, :], axis=1)\n        h += tmp[:, None] * b_beta[None, :] + b_k[None, :] * b_v[:, None]\n        _o = h * b_q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        tl.store(p_ha, tmp, mask=mask_bv)\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n        p_ha += V\n        p_alpha += K\n        p_beta += K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, h, mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_bwd_kernel(q, k, v, alpha, beta, ha, dht, dh0, do, dq,\n    dk, dv, dalpha, dbeta, dha, h0, s_qk_h, s_vo_h, NK, scale, B, H, T, K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', USE_DH0:\n    'tl.constexpr', USE_DHT: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_ha = ha + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_alpha = alpha + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_beta = beta + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * K\n    p_dbeta = dbeta + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(\n        0, BK) + (T - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * V\n    p_dha = dha + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV\n        ) + (T - 1) * V\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        d_h += tl.load(p_ht, mask=mask_bk[:, None] & mask_bv[None, :], other=0)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0)\n        b_alpha = tl.load(p_alpha, mask=mask_bk, other=0)\n        b_ha = tl.load(p_ha, mask=mask_bv, other=0)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * b_v[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        b_dha = tl.sum(d_h * b_beta[:, None], axis=0)\n        tl.store(p_dha, b_dha, mask=mask_bv)\n        b_dbeta = tl.sum(d_h * b_ha[None, :], axis=1)\n        tl.store(p_dbeta, b_dbeta, mask=mask_bk)\n        d_h += b_dha[None, :] * b_alpha[:, None]\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n        p_beta -= K\n        p_dbeta -= K\n        p_alpha -= K\n        p_dha -= V\n        p_ha -= V\n    if USE_DH0:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, d_h, mask=mask_bk[:, None] & mask_bv[None, :])\n    tl.debug_barrier()\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_beta = beta + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_ha = ha + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dha = dha + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_alpha = alpha + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(\n        0, BK)\n    p_dalpha = dalpha + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(\n        0, BK)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_h0, mask=mask_kv, other=0)\n    for i in range(0, T):\n        d_ha = tl.load(p_dha, mask=mask_bv, other=0)\n        d_alpha = tl.sum(d_ha[None, :] * h, axis=1)\n        tl.store(p_dalpha, d_alpha, mask=mask_bk)\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0)\n        b_ha = tl.load(p_ha, mask=mask_bv, other=0)\n        h += b_k[:, None] * b_v[None, :] + b_beta[:, None] * b_ha[None, :]\n        _d_q = h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dk += K\n        p_dalpha += K\n        p_dha += V\n        p_ha += V\n        p_dq += K\n        p_beta += K\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef fwd_prepare_dv_kernel(q, k, do, dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, T, K, V, scale, BT: 'tl.constexpr', BK: 'tl.constexpr',\n    BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_A += tl.dot(b_k, b_q, allow_tf32=False)\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_dv = tl.dot(b_A, b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(k, v, d, v_new, h, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h,\n    s_h_t, H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, \n            1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_h_cumsum = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC)):\n            p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d,\n                s_qk_t), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n            p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (T, K), (s_qk_t,\n                s_qk_d), (i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n            p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t,\n                s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            p_v_new = tl.make_block_ptr(v_new + i_bh * s_vo_h, (T, V), (\n                s_vo_t, s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV),\n                (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_v -= tl.dot(b_d, b_h)\n            tl.store(p_v_new, b_v, boundary_check=(0, 1))\n            b_h_cumsum += tl.dot(b_k, b_v, allow_tf32=False)\n        b_h += b_h_cumsum\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_bwd_kernel_dhu(q, k, d, dht, dh0, do, dh, dv, dv2,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BC: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', USE_DHT: 'tl.constexpr', USE_DH0:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_dh_tmp = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC) - 1, -1, -1):\n            p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d,\n                s_qk_t), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n            p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t,\n                s_qk_d), (i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n            p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (K, T), (s_qk_d,\n                s_qk_t), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n            p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, V), (s_vo_t,\n                s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n                s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_q = b_q * scale\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_dv = tl.load(p_dv, boundary_check=(0, 1))\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            p_dv2 = tl.make_block_ptr(dv2 + i_bh * s_vo_h, (T, V), (s_vo_t,\n                s_vo_d), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            tl.store(p_dv2, b_dv, boundary_check=(0, 1))\n            b_dh_tmp += tl.dot(b_q, b_do, allow_tf32=False)\n            b_dh_tmp -= tl.dot(b_d, b_dv, allow_tf32=False)\n        b_dh += b_dh_tmp\n    if USE_DH0:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_bwd_kernel_dqkw(q, k, v, w, h, do, dh, dq, dk, dv, dw,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dw = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, b_dh, allow_tf32=False)\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dw += tl.dot(b_dv, b_h, allow_tf32=False)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dq *= scale\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dw = tl.make_block_ptr(dw + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dw, -b_dw, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK'])\n@triton.jit\ndef fused_chunk_delta_rule_fwd_kernel(q, k, v, v_new, d, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new + i_bh * s_vo_h, (T, DV), (s_vo_t,\n        s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_v_prime = tl.dot(b_d, b_h, allow_tf32=False)\n        b_v = b_v - b_v_prime\n        tl.store(p_v_new, b_v, boundary_check=(0, 1))\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_v_new = tl.advance(p_v_new, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_d = tl.advance(p_d, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef fused_chunk_delta_rule_bwd_kernel(q, k, v, d, dht, dh0, do, dq, dk, dv,\n    dd, initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H,\n    T, scale, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    DK: 'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE:\n    'tl.constexpr', USE_DHT: 'tl.constexpr', USE_DHO: 'tl.constexpr', CHECK:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_dht = tl.make_block_ptr(dht + i_bh * DK * DV, (DK, DV), (DV, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        b_dh -= tl.dot(b_d, b_dv, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    if USE_DHO:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * DK * DV, (DK, DV), (DV, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n        p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dd = tl.dot(b_dv, b_h, allow_tf32=False)\n        p_dd = tl.make_block_ptr(dd + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_dd, -b_dd, boundary_check=(0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BK'])\n@triton.jit\ndef fwd_prepare_wy_repr_kernel_chunk32(k, v, beta, w, u, A, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, T, K, V, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BC: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (\n        BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_A += tl.dot(b_kb, tl.trans(b_k), allow_tf32=False)\n    b_A = -tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :],\n        b_A, 0)\n    for i in range(1, BT):\n        mask = tl.arange(0, BT) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BT) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n    b_A += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n    b_A = b_A\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BK'])\n@triton.jit\ndef fwd_prepare_wy_repr_kernel_chunk64(k, v, beta, w, u, A, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, T, K, V, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BC: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A2 = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A3 = tl.zeros([BC, BC], dtype=tl.float32)\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (\n        BC,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    p_beta2 = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT + BC\n        ,), (BC,), (0,))\n    b_beta2 = tl.load(p_beta2, boundary_check=(0,))\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BC, BK), (1, 0))\n        p_k2 = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d\n            ), (i_t * BT + BC, i_k * BK), (BC, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_k2 = tl.load(p_k2, boundary_check=(0, 1))\n        b_kb2 = b_k2 * b_beta2[:, None]\n        b_A += tl.dot(b_kb, tl.trans(b_k), allow_tf32=False)\n        b_A2 += tl.dot(b_kb2, tl.trans(b_k2), allow_tf32=False)\n        b_A3 += tl.dot(b_kb2, tl.trans(b_k), allow_tf32=False)\n    b_A = -tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :],\n        b_A, 0)\n    b_A2 = -tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :],\n        b_A2, 0)\n    for i in range(1, BC):\n        mask = tl.arange(0, BC) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)\n        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n        b_A2 = tl.where(mask[:, None], b_a2, b_A2)\n    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A3 = -tl.dot(tl.dot(b_A2, b_A3), b_A)\n    p_A1 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BC, BC), (1, 0))\n    p_A2 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        BC, BC), (BC, BC), (1, 0))\n    p_A3 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        BC, 0), (BC, BC), (1, 0))\n    p_A4 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        BC), (BC, BC), (1, 0))\n    tl.store(p_A1, b_A, boundary_check=(0, 1))\n    tl.store(p_A2, b_A2, boundary_check=(0, 1))\n    tl.store(p_A3, b_A3, boundary_check=(0, 1))\n    tl.store(p_A4, tl.zeros([BC, BC], dtype=tl.float32), boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef fwd_recompute_w_u_kernel(k, v, beta, w, u, A, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, T, K, V, BT: 'tl.constexpr', BK: 'tl.constexpr',\n    BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (\n        BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_vb = b_v * b_beta[:, None]\n        b_u = tl.dot(b_A, b_vb, allow_tf32=False)\n        p_u = tl.make_block_ptr(u + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        tl.store(p_u, b_u, boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_w = tl.dot(b_A, b_kb, allow_tf32=False)\n        p_w = tl.make_block_ptr(w + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_w, b_w, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK'])\n@triton.jit\ndef fwd_recompute_w_kernel(k, beta, w, A, s_qk_h, s_qk_t, s_qk_d, T, K, BT:\n    'tl.constexpr', BK: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (\n        BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_w = tl.dot(b_A, b_kb, allow_tf32=False)\n        p_w = tl.make_block_ptr(w + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_w, b_w, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef bwd_prepare_wy_repr_kernel(k, v, beta, A, dw, du, dk, dv, dbeta, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, T, K, V, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dbeta = tl.zeros([BT], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (\n        BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_du = tl.make_block_ptr(du + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v_beta = b_v * b_beta[:, None]\n        b_du = tl.load(p_du, boundary_check=(0, 1))\n        b_dA += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)\n        b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)\n        b_dv = b_dv_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dv_beta * b_v, 1)\n        p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dw = tl.make_block_ptr(dw + i_bh * s_qk_h, (T, K), (s_qk_t,\n            s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_beta = b_k * b_beta[:, None]\n        b_dw = tl.load(p_dw, boundary_check=(0, 1))\n        b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)\n        b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)\n        b_dk = b_dk_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t,\n            s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :],\n        b_dA, 0)\n    b_dA = tl.dot(b_dA, b_A)\n    b_dA = tl.dot(b_A, b_dA)\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], \n        -b_dA, 0)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t,\n            s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dk = tl.load(p_dk, boundary_check=(0, 1))\n        b_k_beta = b_k * b_beta[:, None]\n        b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)\n        b_dk += b_dk_beta * b_beta[:, None]\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    p_dbeta = tl.make_block_ptr(dbeta + i_bh * T, (T,), (1,), (i_t * BT,),\n        (BT,), (0,))\n    tl.store(p_dbeta, b_dbeta, boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_inter(q, k, g, A, s_k_h, s_k_t, scale,\n    T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BT'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra(q, k, g, A, s_k_h, s_k_t, scale,\n    T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr'):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_j * BC\n    m_k = o_k < K\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, 0), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, 0), (BC, BK), (1, 0))\n    p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + \n        i_j * BC) * K + o_k, BK), BK)\n    p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + \n        i_j * BC) * K + o_k, BK), BK)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_k = tl.load(p_k, mask=m_k, other=0)\n        b_gk = tl.load(p_gk, mask=m_k, other=0)\n        b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_split(q, k, g, A, s_k_h, s_k_t,\n    scale, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    n_bh = tl.num_programs(2)\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0,\n        BC)) * BC\n    m_k = o_k < K\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + \n        i_j * BC) * K + o_k, BK), BK)\n    p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + \n        i_j * BC) * K + o_k, BK), BK)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        b_k = tl.load(p_k, mask=m_k, other=0)\n        b_gk = tl.load(p_gk, mask=m_k, other=0)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K\n        p_gk += K\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_merge(A, A2, T: 'tl.constexpr',\n    BT: 'tl.constexpr', BC: 'tl.constexpr', NK: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_c * BC >= T:\n        return\n    n_bh = tl.num_programs(2)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(A + (i_bh + i_k * n_bh) * T * BC, (T, BC),\n            (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_fwd_kernel_o(q, v, g, h, o, A, s_k_h, s_k_t, s_v_h, s_v_t,\n    s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.0)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'NC', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_intra(q, k, g, dA, dq, dk, s_k_h, s_k_t, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(0, i_i):\n            p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (\n                i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1),\n                (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1),\n                (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_kj = tl.max_contiguous(tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    p_gkj = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT +\n        i_i * BC) * K + o_k, BK), BK)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, mask=m_k, other=0)\n        b_gkj = tl.load(p_gkj, mask=m_k, other=0)\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tl.exp(b_g -\n            b_gkj[None, :]), 0.0)\n        p_kj += K\n        p_gkj += K\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    NC = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < NC - 1:\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t *\n            BT + i_i * BC + BC - 1) * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(i_i + 1, NC):\n            p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (\n                i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (\n                i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (BT, T), (1, BT),\n                (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_qg = b_q * tl.exp(b_g - b_gn[None, :])\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dk += tl.dot(b_dA, b_qg)\n        b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    p_qj = tl.max_contiguous(tl.multiple_of(q + i_bh * s_k_h + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    p_gqj = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + (i_t * BT +\n        i_i * BC) * K + o_k, BK), BK)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j * BT)\n        b_qj = tl.load(p_qj, mask=m_k, other=0)\n        b_gqj = tl.load(p_gqj, mask=m_k, other=0)\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n        p_qj += K\n        p_gqj += K\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_dA(v, do, dA, s_v_h, s_v_t, scale, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (1, s_v_t), (i_v *\n            BV, i_t * BT), (BV, BT), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, b_v)\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_dA = tl.where(m_s, b_dA * scale, 0.0)\n    tl.store(p_dA, b_dA, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_dv(k, g, A, do, dh, dv, s_k_h, s_k_t, s_v_h, s_v_t,\n    s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0.0)\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = tl.dot(b_A, b_do, allow_tf32=False)\n    last_idx = min(i_t * BT + BT, T) - 1\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + last_idx *\n            K + o_k, BK), BK)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_gn = tl.exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)\n        b_k = b_k * b_gn\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh)\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_inter(q, k, v, h, g, do, dh, dq, dk, dq2, dk2, dg,\n    s_k_h, s_k_t, s_v_h, s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    last_idx = min(T, i_t * BT + BT) - 1\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * s_k_h + last_idx * K +\n        o_k, BK), BK)\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros([BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (1,\n            s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * V * K, (V, K), (\n            1, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, b_dh)\n    b_dgk *= tl.exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gn = tl.exp(b_gn[None, :] - b_gk)\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * b_gn\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    b_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :\n        ] + b_dgk[None, :]\n    p_dg = tl.make_block_ptr(dg + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq2 + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk2 + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef prepare_qg_kg(q, k, g, qg, kg, s_qk_h, scale, K: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_g = g + i_bh * s_qk_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_qg = qg + i_bh * s_qk_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_kg = kg + i_bh * s_qk_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    last_decay = tl.load(g + i_bh * s_qk_h + (i_c * BT + BT - 1) * K + i_k *\n        BK + tl.arange(0, BK))\n    for i in range(BT):\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        _g = tl.load(p_g, mask=mask, other=0)\n        b_q *= tl.exp(_g) * scale\n        b_k *= tl.exp(last_decay - _g)\n        tl.store(p_kg, b_k, mask=mask)\n        tl.store(p_qg, b_q, mask=mask)\n        p_q += K\n        p_g += K\n        p_k += K\n        p_kg += K\n        p_qg += K\n"
    },
    {
      "input": "@triton.jit\ndef bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g,\n    dg, s_qk_h, BT: 'tl.constexpr', BK: 'tl.constexpr', K: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_g = g + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_dg = dg + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT +\n        BT - 1) * K\n    p_dq_inner = dq_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dk_inner = dk_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dq_inter = dq_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dk_inter = dk_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    last_g = tl.zeros([BK], dtype=tl.float32)\n    for j in range(BT - 1, -1, -1):\n        _g = tl.load(p_g, mask=mask, other=0)\n        if j == BT - 1:\n            last_g = _g\n        b_dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n        b_dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n        b_dq2 *= tl.exp(_g)\n        b_dq = b_dq1 + b_dq2\n        tl.store(p_dq_inter, b_dq, mask=mask)\n        b_dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n        b_dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n        b_dk2 *= tl.exp(last_g - _g)\n        b_dk = b_dk1 + b_dk2\n        tl.store(p_dk_inter, b_dk, mask=mask)\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        b_dg = b_dq * b_q - b_dk * b_k\n        cum_grad_dg += b_dg\n        tl.store(p_dg, cum_grad_dg, mask=mask)\n        p_g -= K\n        p_k -= K\n        p_q -= K\n        p_dq_inner -= K\n        p_dk_inner -= K\n        p_dq_inter -= K\n        p_dk_inter -= K\n        p_dg -= K\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_gla_fwd_kernel(q, k, v, g, o, h0, ht, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (0,\n        i_k * BK), (BT, BK), (1, 0))\n    p_db = g + i_bh * s_qk_h + (BT - 1) * s_qk_t + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0,\n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0)\n        if CHECK and i == 0:\n            b_o = tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[:, None] + tl.dot(b_k, b_v, allow_tf32=\n                False)\n        else:\n            b_o = tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[:, None] + tl.dot(b_k, b_v, allow_tf32=\n                False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_db += BT * K\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, h0, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + ((i + 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t),\n            (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[None, :] + tl.dot(b_v, b_k, allow_tf32=\n                False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[None, :] + tl.dot(b_v, b_k, allow_tf32=\n                False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + (T - (i - 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_db = tl.load(p_db, mask=mask, other=0)\n        if CHECK and i == 1:\n            b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh = b_dh * tl.exp(b_db)[:, None] + tl.dot(b_q, b_do,\n                allow_tf32=False)\n        else:\n            b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh = b_dh * tl.exp(b_db)[:, None] + tl.dot(b_q, b_do,\n                allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fwd_inner_chunk(q, k, g, A, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', K: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_qk_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_gq = g + i_bh * s_qk_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_A = A + (i_bh + i_k * B * H) * (tl.cdiv(T, BT) * BT * BT\n        ) + i_t * BT * BT + tl.arange(0, BT)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0) * scale\n        gq = tl.load(p_gq, mask=mask, other=0)\n        s = _q[None, :] * b_k * tl.exp(gq[None, :] - b_g)\n        score = tl.sum(s, axis=1)\n        score = tl.where(o_i <= i, score, 0)\n        tl.store(p_A, score)\n        p_q += K\n        p_gq += K\n        p_A += BT\n"
    },
    {
      "input": "@triton.jit\ndef bwd_inner_chunk(q, k, g, dA, dq, dk, s_qk_h, s_qk_t, s_qk_d, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_qk_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_dq = dq + i_bh * s_qk_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_gq = g + i_bh * s_qk_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_dA = dA + i_bh * (tl.cdiv(T, BT) * BT * BT) + i_t * BT * BT + tl.arange(\n        0, BT)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0)\n        gq = tl.load(p_gq, mask=mask, other=0)\n        score = tl.exp(gq[None, :] - b_g)\n        score = tl.where(o_i[:, None] <= i, score, 0)\n        _dA = tl.load(p_dA)\n        _dA = tl.where(o_i <= i, _dA, 0)\n        b_dk += _dA[:, None] * score * _q[None, :]\n        b_dq = tl.sum(_dA[:, None] * score * b_k, axis=0)\n        tl.store(p_dq, b_dq, mask=mask)\n        p_q += K\n        p_dq += K\n        p_gq += K\n        p_dA += BT\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_intra_K(v, g, o, A, s_v_h, s_v_t, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr'\n    ):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_g = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t * BT +\n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * s_v_h + (i_t * BT + \n        i_i * BC) * V + o_v, BV), BV)\n    b_gn = tl.load(p_gn, mask=o_v < V, other=0)\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_gv = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = b_v * tl.exp(b_gn[None, :] - b_gv)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, b_vg)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_v = tl.max_contiguous(tl.multiple_of(v + i_bg * s_v_h + (i_t * BT +\n            i_i * BC + j) * V + o_v, BV), BV)\n        p_gv = tl.max_contiguous(tl.multiple_of(g + i_bg * s_v_h + (i_t *\n            BT + i_i * BC + j) * V + o_v, BV), BV)\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n        b_v = tl.load(p_v, mask=o_v < V, other=0)\n        b_gv = tl.load(p_gv, mask=o_v < V, other=0)\n        b_vg = b_v[None, :] * tl.exp(b_g - b_gv[None, :])\n        b_o += tl.where(o_i[:, None] >= j, b_A[:, None] * b_vg, 0.0)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT +\n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_K(q, k, h, g, o, A, s_k_h, s_k_t, s_v_h, s_v_t,\n    s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NG: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bg * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bg * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h)\n        b_A += tl.dot(b_q, b_k)\n    p_g = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o = b_o * tl.exp(b_g)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_intra_Vk(q, k, g, A, s_k_h, s_k_t, i_k, i_c, i_bh,\n    scale, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr'\n    ):\n    i_bg = i_bh // NG\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    o_k = i_k * BK + tl.arange(0, BK)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    b_A = tl.zeros([BC, BC], tl.float32)\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bg * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bg * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * s_k_h + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=o_k < K, other=0.0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A = tl.dot(b_qg, b_kg)\n        if i_k != 0:\n            b_A += tl.load(p_A, boundary_check=(0, 1))\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_bg * s_k_h + (i_t * BT +\n            i_j * BC) * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_bg * s_k_h + (i_t *\n            BT + i_j * BC) * K + o_k, BK), BK)\n        m_k = o_k < K\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        o_i = tl.arange(0, BC)\n        m_A = o_i[:, None] >= o_i[None, :]\n        for j in range(0, BC):\n            b_k = tl.load(p_k, mask=m_k & i_t * BT + i_j * BC + j < T,\n                other=0.0)\n            b_gk = tl.load(p_gk, mask=m_k & i_t * BT + i_j * BC + j < T,\n                other=0.0)\n            b_Aj = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]) *\n                scale, 1)\n            b_A = tl.where((o_i == j)[None, :], b_Aj[:, None], b_A)\n            p_k += K\n            p_gk += K\n        b_A = tl.where(m_A, b_A, 0.0)\n        if i_k != 0:\n            b_A += tl.load(p_A, boundary_check=(0, 1))\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_k == 0:\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_intra_V(q, k, g, A, s_k_h, s_k_t, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr', NK:\n    'tl.constexpr', NG: 'tl.constexpr'):\n    i_c, i_bh = tl.program_id(0), tl.program_id(1)\n    for i_k in range(0, NK):\n        chunk_gsa_fwd_kernel_intra_Vk(q, k, g, A, s_k_h, s_k_t, i_k, i_c,\n            i_bh, scale, T, K, BT, BC, BK, NC, NG)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_V(q, v, g, h, o, A, s_k_h, s_k_t, s_v_h, s_v_t,\n    s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NG: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bg * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h)\n    p_v = tl.make_block_ptr(v + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_v)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_V(k, v, h, g, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_v_h, s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NG: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    n_bh = tl.num_programs(2)\n    o_t = min(i_t * BT + BT, T)\n    o_k = i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * s_k_h + (o_t - 1) *\n        K + o_k, BK), BK)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    m_k = o_k < K\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gn = tl.exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)\n    b_k = b_k * b_gn\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bg * s_h_h + i_t * V * K, (V, K), (1,\n            s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dh = b_dh\n        b_dv = tl.dot(b_k, b_dh)\n        if i_k == 0:\n            b_dv += tl.dot(b_A, b_do)\n        b_do = b_do * scale\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, tl.trans(b_dh))\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * b_gn\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_dA = tl.where(m_s, b_dA, 0.0)\n    if i_k == 0:\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_intra_V(q, k, g, dA, dq, dk, dg, s_k_h, s_k_t, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr', NG:\n    'tl.constexpr', OVERWRITE: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_k = i_k * BK + tl.arange(0, BK)\n    p_g = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * s_k_h + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    m_k = o_k < K\n    b_gn = tl.load(p_gn, mask=m_k, other=0.0)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kg)\n    b_dq *= tl.exp(b_g - b_gn[None, :])\n    p_kj = tl.max_contiguous(tl.multiple_of(k + i_bg * s_k_h + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    p_gkj = tl.max_contiguous(tl.multiple_of(g + i_bg * s_k_h + (i_t * BT +\n        i_i * BC) * K + o_k, BK), BK)\n    m_k = o_k < K\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, mask=m_k, other=0)\n        b_gkj = tl.load(p_gkj, mask=m_k, other=0)\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tl.exp(b_g -\n            b_gkj[None, :]), 0.0)\n        p_kj += K\n        p_gkj += K\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_dq = b_dq + tl.load(p_dq, boundary_check=(0, 1))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * s_k_h + (i_t * BT + \n        i_i * BC + BC - 1) * K + o_k, BK), BK)\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(tl.trans(b_dA), b_qg)\n    b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    p_qj = tl.max_contiguous(tl.multiple_of(q + i_bh * s_k_h + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    p_gqj = tl.max_contiguous(tl.multiple_of(g + i_bg * s_k_h + (i_t * BT +\n        i_i * BC) * K + o_k, BK), BK)\n    m_k = o_k < K\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, BC):\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, mask=m_k, other=0.0)\n        b_gqj = tl.load(p_gqj, mask=m_k, other=0.0)\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n        p_qj += K\n        p_gqj += K\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_dg = tl.make_block_ptr(dg + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dk = b_dk + tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    if not OVERWRITE:\n        b_dg = b_dg + tl.load(p_dg, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_intra_K(v, g, do, dA, s_v_h, s_v_t, scale, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr'\n    ):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    i_bg = i_bh // NG\n    n_bh = tl.num_programs(2)\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n    p_dA = tl.make_block_ptr(dA + (i_bh + i_v * n_bh) * T * BT, (T, BT), (\n        BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    b_dA = tl.zeros([BC, BC], dtype=tl.float32)\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(v + i_bg * s_v_h, (V, T), (1, s_v_t), (i_v *\n            BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_gv = tl.make_block_ptr(g + i_bg * s_v_h, (V, T), (1, s_v_t), (i_v *\n            BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * s_v_h + (i_t *\n            BT + i_i * BC) * V + o_v, BV), BV)\n        p_g = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        b_gn = tl.load(p_gn, mask=m_v, other=0.0)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g - b_gn[None, :]) * scale\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = b_v * tl.exp(b_gn[:, None] - b_gv)\n        b_dA = tl.dot(b_do, b_vg)\n    elif i_i == i_j:\n        p_g = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_v = tl.max_contiguous(tl.multiple_of(v + i_bg * s_v_h + (i_t * BT +\n            i_j * BC) * V + o_v, BV), BV)\n        p_gv = tl.max_contiguous(tl.multiple_of(g + i_bg * s_v_h + (i_t *\n            BT + i_j * BC) * V + o_v, BV), BV)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        m_v = o_v < V\n        o_i = tl.arange(0, BC)\n        m_dA = o_i[:, None] >= o_i[None, :]\n        for j in range(0, BC):\n            b_v = tl.load(p_v, mask=m_v, other=0)\n            b_gv = tl.load(p_gv, mask=m_v, other=0)\n            b_dAj = tl.sum(b_do * b_v[None, :] * tl.exp(b_g - b_gv[None, :]), 1\n                )\n            b_dA = tl.where((o_i == j)[None, :], b_dAj[:, None], b_dA)\n            p_v += V\n            p_gv += V\n        b_dA = tl.where(m_dA, b_dA, 0.0)\n    tl.store(p_dA, b_dA, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_K(q, k, v, h, g, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_v_h, s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NG: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    o_t = min(i_t * BT + BT, T)\n    m_s = o_i[:, None] >= o_i[None, :]\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bg * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT,\n        1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A = tl.dot(b_q * scale, tl.trans(b_k))\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        o_v = i_v * BV + tl.arange(0, BV)\n        p_v = tl.make_block_ptr(v + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bg * s_h_h + i_t * K * V, (V, K), (1,\n            s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_g = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * s_v_h + (o_t - 1\n            ) * V + o_v, BV), BV)\n        m_v = o_v < V\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_gn = tl.load(p_gn, mask=m_v, other=0)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_v = b_v * tl.exp(b_gn[None, :] - b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_h = b_h\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g) * scale\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dh = b_dh\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, tl.trans(b_dh))\n        b_dv = tl.exp(b_gn[None, :] - b_g) * tl.dot(b_k, b_dh)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_k)\n    b_dk += tl.dot(tl.trans(b_dA), b_q)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_intra_KV(v, g, o, A, do, dv, dg, s_v_h, s_v_t, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr', NG:\n    'tl.constexpr', OVERWRITE: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_gv = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_gn = g + i_bg * s_v_h + (i_t * BT + i_i * BC + BC - 1) * V + o_v\n    p_gn = tl.max_contiguous(tl.multiple_of(p_gn, BV), BV)\n    m_v = o_v < V\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_g = tl.make_block_ptr(g + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (i_i *\n            BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g - b_gn[None, :])\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do)\n    b_dv *= tl.exp(b_gn[None, :] - b_gv)\n    o_i = tl.arange(0, BC)\n    o_c = i_i * BC + tl.arange(0, BC)\n    p_g = tl.max_contiguous(tl.multiple_of(g + i_bg * s_v_h + (i_t * BT + \n        i_i * BC) * V + o_v, BV), BV)\n    p_A = tl.max_contiguous(tl.multiple_of(A + i_bh * T * BT + (i_t * BT + \n        i_i * BC) * BT + o_c, BV), BV)\n    p_do = tl.max_contiguous(tl.multiple_of(do + i_bh * s_v_h + (i_t * BT +\n        i_i * BC) * V + o_v, BV), BV)\n    for j in range(0, BC):\n        m_j = i_t * BT + i_i * BC + j < T\n        b_A = tl.load(p_A, mask=m_j, other=0)\n        b_g = tl.load(p_g, mask=m_j & m_v, other=0)\n        b_do = tl.load(p_do, mask=m_j & m_v, other=0)\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(m_i, tl.exp(b_g[None, :] - b_gv) * b_A[:, None] *\n            b_do[None, :], 0.0)\n        p_g += V\n        p_A += BT\n        p_do += V\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT +\n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bg * s_v_h, (T, V), (s_v_t, 1), (i_t * BT +\n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_dg = tl.make_block_ptr(dg + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_o = tl.load(p_o, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = b_dv + tl.load(p_dv, boundary_check=(0, 1))\n    b_dg = b_o * b_do - b_v * b_dv\n    if not OVERWRITE:\n        b_dg = b_dg + tl.load(p_dg, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_gsa_inference_kernel(q, k, v, s, g, o, hk0, hv0, hkt,\n    hvt, scale, K: 'tl.constexpr', V: 'tl.constexpr', M: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NG: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    i_bg = i_bh // NG\n    b_s = tl.load(s + i_bg * M + tl.arange(0, M))\n    b_g = tl.load(g + i_bg * M + tl.arange(0, M))\n    b_g = tl.exp(b_g)\n    b_ok = tl.zeros([M], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        p_hk0 = hk0 + i_bg * K * M + o_k[None, :] * M + tl.arange(0, M)[:, None\n            ]\n        mask_k = o_k < K\n        mask_hk = (tl.arange(0, M) < M)[:, None] & mask_k[None, :]\n        b_hk = tl.load(p_hk0, mask=mask_hk, other=0.0)\n        b_q = tl.load(q + i_bh * K + o_k, mask=mask_k, other=0.0) * scale\n        b_k = tl.load(k + i_bg * K + o_k, mask=mask_k, other=0.0)\n        b_hk = b_hk * b_g[:, None] + b_k[None, :] * b_s[:, None]\n        b_ok += tl.sum(b_hk * b_q[None, :], axis=1)\n        if i_bh % NG == 0:\n            p_hkt = hkt + i_bg * K * M + o_k[None, :] * M + tl.arange(0, M)[\n                :, None]\n            tl.store(p_hkt, b_hk, mask=mask_hk)\n    b_qv = tl.softmax(b_ok)\n    for i_v in range(tl.cdiv(V, BV)):\n        o_v = i_v * BV + tl.arange(0, BV)\n        p_hv0 = hv0 + i_bg * M * V + tl.arange(0, M)[None, :] * V + o_v[:, None\n            ]\n        mask_v = o_v < V\n        mask_hv = mask_v[:, None] & (tl.arange(0, M) < M)[None, :]\n        b_hv = tl.load(p_hv0, mask=mask_hv, other=0)\n        b_v = tl.load(v + i_bg * V + o_v, mask=mask_v, other=0)\n        b_hv = b_hv * b_g[None, :] + b_s[None, :] * b_v[:, None]\n        b_ov = tl.sum(b_hv * b_qv[None, :], axis=1)\n        tl.store(o + i_bh * V + o_v, b_ov, mask=mask_v)\n        if i_bh % NG == 0:\n            p_hvt = hvt + i_bg * M * V + tl.arange(0, M)[None, :] * V + o_v[\n                :, None]\n            tl.store(p_hvt, b_hv, mask=mask_hv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_gsa_fwd_kernel(q, k, v, gk, gv, o, h0, ht, s_k_h, s_v_h,\n    scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            K if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            V if REVERSE else 0)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_h, other=0)\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0)\n            b_h = b_h * tl.exp(b_gk)[None, :]\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0)\n            b_h = b_h * tl.exp(b_gv)[:, None]\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o, mask=mask_v)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        if USE_GK:\n            p_gk += -K if REVERSE else K\n        if USE_GV:\n            p_gv += -V if REVERSE else V\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_h)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_gsa_bwd_kernel(q, k, v, gk, gv, do, dq, dk, dv, dh0, h0,\n    s_k_h, s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    REVERSE: 'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            K if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            V if REVERSE else 0)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_h, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_do = tl.load(p_do, mask=mask_v, other=0)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0)\n            b_h = b_h * tl.exp(b_gk)[:, None]\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0)\n            b_h = b_h * tl.exp(b_gv)[None, :]\n        b_h += b_k[:, None] * b_v[None, :]\n        b_dq = tl.sum(b_h * b_do[None, :], axis=1) * scale\n        tl.store(p_dq, b_dq, mask=mask_k)\n        p_k += -K if REVERSE else K\n        p_v += -V if REVERSE else V\n        p_q += -K if REVERSE else K\n        p_do += -V if REVERSE else V\n        p_dq += -K if REVERSE else K\n        if USE_GK:\n            p_gk += -K if REVERSE else K\n        if USE_GV:\n            p_gv += -V if REVERSE else V\n    tl.debug_barrier()\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if \n        not REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        not REVERSE else 0)\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if not REVERSE else 0)\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if not REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            K if not REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            V if not REVERSE else 0)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_do = tl.load(p_do, mask=mask_v, other=0)\n        b_dh += b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n        if USE_GK:\n            b_gk = tl.load(p_gk, mask=mask_k, other=0)\n            b_dh *= tl.exp(b_gk)[:, None]\n        if USE_GV:\n            b_gv = tl.load(p_gv, mask=mask_v, other=0)\n            b_dh *= tl.exp(b_gv)[None, :]\n        tl.store(p_dk, b_dk, mask=mask_k)\n        tl.store(p_dv, b_dv, mask=mask_v)\n        p_q += K if REVERSE else -K\n        p_k += K if REVERSE else -K\n        p_v += V if REVERSE else -V\n        p_do += V if REVERSE else -V\n        p_dk += K if REVERSE else -K\n        p_dv += V if REVERSE else -V\n        if USE_GK:\n            p_gk += K if REVERSE else -K\n        if USE_GV:\n            p_gv += V if REVERSE else -V\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh, mask=mask_h)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef chunk_hgrn_fwd_kernel_h(x, g, gc, o, h0, T: 'tl.constexpr', D:\n    'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr'):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_x = x + i_b * T * D + i_t * BT * D + o_d\n    p_g = g + i_b * T * D + i_t * BT * D + o_d\n    p_gc = gc + i_b * T * D + i_t * BT * D + o_d\n    p_o = o + i_b * T * D + i_t * BT * D + o_d\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    b_gc = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        if i_t == 0:\n            b_h += tl.load(h0 + i_b * D + o_d, mask=mask, other=0)\n    for i in range(0, BT):\n        mask_t = mask & (i_t * BT + i < T)\n        b_x = tl.load(p_x, mask=mask_t, other=0)\n        b_g = tl.load(p_g, mask=mask_t, other=0)\n        b_h = tl.exp(b_g) * b_h + b_x\n        b_gc = b_gc + b_g\n        tl.store(p_gc, b_gc, mask=mask_t)\n        tl.store(p_o, b_h, mask=mask_t)\n        p_x += D\n        p_g += D\n        p_gc += D\n        p_o += D\n"
    },
    {
      "input": "@triton.jit\ndef chunk_hgrn_fwd_kernel_o(gc, o, s_b, s_t, s_d, T: 'tl.constexpr', D:\n    'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    for i_t in range(1, tl.cdiv(T, BT)):\n        p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        b_h0 = tl.load(o + i_b * T * D + i_t * BT * D - D + o_d, mask=mask,\n            other=0)\n        b_gc = tl.load(p_gc, boundary_check=(0, 1))\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_o = b_o + tl.exp(b_gc) * b_h0[None, :]\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef chunk_hgrn_bwd_kernel_h(g, gc, dx, do, T: 'tl.constexpr', D:\n    'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr'):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    BC = min(BT, T - i_t * BT)\n    NT = tl.num_programs(1)\n    p_g = g + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_gc = gc + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_dx = dx + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_do = do + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    if i_t == NT - 1:\n        b_gc = tl.zeros([BD], dtype=tl.float32)\n    else:\n        b_gc = tl.load(g + (i_b * T + i_t * BT + BT) * D + o_d, mask=mask,\n            other=0)\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for _ in range(BC - 1, -1, -1):\n        tl.store(p_gc, b_gc, mask=mask)\n        b_g = tl.load(p_g, mask=mask, other=0)\n        b_do = tl.load(p_do, mask=mask, other=0)\n        b_gc = b_gc + b_g\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * tl.exp(b_g)\n        tl.store(p_dx, b_dx, mask=mask)\n        p_g -= D\n        p_gc -= D\n        p_dx -= D\n        p_do -= D\n"
    },
    {
      "input": "@triton.jit\ndef chunk_hgrn_bwd_kernel_o(g, gc, o, dx, dg, s_b, s_t, s_d, T:\n    'tl.constexpr', D: 'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_g = tl.make_block_ptr(g + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT - 1, i_d * BD), (BT, BD), (1, 0))\n        p_dx = tl.make_block_ptr(dx + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        mask_t = mask & ((i_t + 1) * BT < T)\n        b_ht = tl.load(dx + i_b * T * D + (i_t + 1) * BT * D + o_d, mask=\n            mask_t, other=0)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_gc = tl.load(p_gc, boundary_check=(0, 1))\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_dx = tl.load(p_dx, boundary_check=(0, 1))\n        b_dx = b_dx + tl.exp(b_gc) * b_ht[None, :]\n        b_dg = b_o * b_dx * tl.exp(b_g)\n        tl.store(p_dx, b_dx, boundary_check=(0, 1))\n        tl.store(p_dg, b_dg, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef fused_recurrent_hgrn_fwd_kernel(x, g, o, h0, ht, T: 'tl.constexpr', D:\n    'tl.constexpr', BD: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_x = x + i_b * T * D + o_d\n    p_g = g + i_b * T * D + o_d\n    p_o = o + i_b * T * D + o_d\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_b * D + o_d\n        b_h += tl.load(p_h0, mask=mask, other=0)\n    for _ in range(0, T):\n        b_x = tl.load(p_x, mask=mask, other=0)\n        b_g = tl.load(p_g, mask=mask, other=0)\n        b_h = tl.exp(b_g) * b_h + b_x\n        tl.store(p_o, b_h, mask=mask)\n        p_x += D\n        p_g += D\n        p_o += D\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_b * D + o_d\n        tl.store(p_ht, b_h, mask=mask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef fused_recurrent_hgrn_bwd_kernel(g, o, dx, dg, do, h0, T: 'tl.constexpr',\n    D: 'tl.constexpr', BD: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_g = g + (i_b * T + T - 1) * D + o_d\n    p_o = o + (i_b * T + T - 2) * D + o_d\n    p_dx = dx + (i_b * T + T - 1) * D + o_d\n    p_dg = dg + (i_b * T + T - 1) * D + o_d\n    p_do = do + (i_b * T + T - 1) * D + o_d\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for i in range(T - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0)\n        b_do = tl.load(p_do, mask=mask, other=0)\n        if i > 0:\n            b_o = tl.load(p_o, mask=mask, other=0)\n        elif USE_INITIAL_STATE:\n            b_o = tl.load(h0 + i_b * D + o_d, mask=mask, other=0)\n        else:\n            b_o = tl.zeros([BD], dtype=tl.float32)\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * tl.exp(b_g)\n        b_dg = b_dh * b_o\n        tl.store(p_dx, b_dx, mask=mask)\n        tl.store(p_dg, b_dg, mask=mask)\n        p_g -= D\n        p_o -= D\n        p_dx -= D\n        p_dg -= D\n        p_do -= D\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(k, v, h, h0, ht, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dh(q, do, dh, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n"
    },
    {
      "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s = tl.where(o_i[:, None] <= o_i[None, :], b_s, 0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) + tl.dot(b_s, b_do,\n            allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale, 0)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_linear_attn_fwd_kernel(q, k, v, o, h0, ht, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B, H, T, K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (0,\n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0,\n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B, H, T, K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t),\n            (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_linear_attn_fwd_kernel(q, k, v, o, h0, ht, s_qk_h,\n    s_vo_h, scale, B, H, T, K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0,\n    s_qk_h, s_vo_h, scale, B, H, T, K: 'tl.constexpr', V: 'tl.constexpr',\n    BK: 'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_h += b_k[:, None] * b_v[None, :]\n        _d_q = b_h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dq += K\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * V\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * b_v[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n"
    },
    {
      "input": "@triton.jit\ndef parallel_rebased_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, scale, B, H, T, K: 'tl.constexpr', V:\n    'tl.constexpr', BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0,\n        i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (0,\n        i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, K), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (V, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (V, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, K), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_rebased_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B=B, H=H, T=\n        T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B=B, H=H,\n        T=T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_fwd_kernel_h(k, v, h, h0, ht, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if i_t == NT - 1 and T % BT != 0:\n            d_b = tl.math.exp2(T % BT * b_b)\n            d_i = tl.math.exp2((T % BT - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, b_v * d_i[:, None], allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_fwd_kernel_o(q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q * d_i[:, None], b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s *= d_s\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(q, do, dh, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, b_do * d_i[:, None], allow_tf32=False)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b\n        )\n    d_q = d_q * scale\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s,\n            b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_ds = b_ds * d_s\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_retention_fwd_kernel(q, k, v, o, h0, ht, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b, d_o, d_h = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b\n        ), tl.math.exp2((BT - o_i - 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (0,\n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0,\n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            if i == NT - 1 and T % BT != 0:\n                d_b = tl.math.exp2(T % BT * b_b)\n                d_h = tl.math.exp2((T % BT - o_i - 1) * b_b)\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_chunk_retention_bwd_kernel(q, k, v, do, dq, dk, dv, h0, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT -\n        o_i - 1) * b_b)\n    d_b = tl.math.exp2(BT * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t),\n            (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    d_s = tl.trans(d_s)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, K),\n            (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_retention_fwd_kernel(q, k, v, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        h = b_b * h + _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_retention_bwd_kernel(q, k, v, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        h = b_b * h + _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dq += DK\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        d_h *= b_b\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n"
    },
    {
      "input": "@triton.jit\ndef parallel_retention_fwd_kernel(q, k, v, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_k = tl.arange(0, BTS)\n    d_h = tl.math.exp2((BTS - o_k) * b_b)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (0,\n        i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_h[None, :]\n        b_o = b_o * tl.math.exp2(b_b * BTS)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    d_q = tl.math.exp2(tl.arange(0, BTL) * b_b)\n    b_o *= d_q[:, None]\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (0,\n        i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    d_h = tl.math.exp2((BTS - tl.arange(0, BTS)) * b_b)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        b_dq *= d_b\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= tl.math.exp2(tl.arange(0, BTL) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (V, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, K), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    d_h = tl.math.exp2((BTL - tl.arange(0, BTL)) * b_b)\n    b_kd = b_k * d_h[:, None]\n    d_q = tl.math.exp2(tl.arange(0, BTS) * b_b)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (V, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * d_q[None, :]\n        b_dv *= d_b\n        b_s = tl.dot(b_kd, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        b_dk *= d_b\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (V, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((-o_k[:, None] + o_q[None, :]) *\n            b_b), 0) * scale\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, K), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, V), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n"
    },
    {
      "input": "@triton.jit\ndef parallel_retention_bwd_kernel(q, k, v, do, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B=B, H=H, T=\n        T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, scale, B, H, T,\n        K, V, BTL, BTS, BK, BV)\n"
    },
    {
      "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    nheads, rotary_dim, seqlen_ro, CACHE_KEY_SEQLEN, stride_out_batch,\n    stride_out_seqlen, stride_out_nheads, stride_out_headdim,\n    stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim,\n    BLOCK_K: 'tl.constexpr', IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr',\n    IS_VARLEN: 'tl.constexpr', INTERLEAVED: 'tl.constexpr', CONJUGATE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv4_forward_kernel(w_ptr, w_s_c, u_ptr, u_s_c, k_ptr,\n    k_s_b, k_s_t, k_s_c, v_ptr, v_s_b, v_s_t, v_s_c, state_ptr, state_s_b,\n    state_s_abe, state_s_c, wkv_ptr, wkv_s_b, wkv_s_t, wkv_s_c,\n    state_out_ptr, state_out_s_b, state_out_s_abe, state_out_s_t,\n    state_out_s_c, chans, tsz, BLOCK_SIZE_C: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    c_idx = tl.program_id(1)\n    cs = c_idx * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)\n    cmask = cs < chans\n    k_ptr = k_ptr + b_idx * k_s_b\n    v_ptr = v_ptr + b_idx * v_s_b\n    alpha_ptr = state_ptr + b_idx * state_s_b\n    beta_ptr = state_ptr + b_idx * state_s_b + state_s_abe\n    eps_ptr = state_ptr + b_idx * state_s_b + 2 * state_s_abe\n    wkv_ptr = wkv_ptr + b_idx * wkv_s_b\n    alpha_out_ptr = state_out_ptr + b_idx * state_out_s_b\n    beta_out_ptr = state_out_ptr + b_idx * state_out_s_b + state_out_s_abe\n    eps_out_ptr = state_out_ptr + b_idx * state_out_s_b + 2 * state_out_s_abe\n    alpha = tl.load(alpha_ptr + cs * state_s_c, mask=cmask)\n    beta = tl.load(beta_ptr + cs * state_s_c, mask=cmask)\n    eps = tl.load(eps_ptr + cs * state_s_c, mask=cmask)\n    w = tl.load(w_ptr + cs * w_s_c, mask=cmask)\n    u = tl.load(u_ptr + cs * u_s_c, mask=cmask)\n    for t in range(tsz):\n        kt = tl.load(k_ptr + t * k_s_t + cs * k_s_c, mask=cmask)\n        vt = tl.load(v_ptr + t * v_s_t + cs * v_s_c, mask=cmask)\n        ukt = u + kt\n        tau = tl.maximum(ukt, eps)\n        e1a = tl.exp(eps - tau)\n        e2a = tl.exp(ukt - tau)\n        wkv = (e1a * alpha + e2a * vt) / (e1a * beta + e2a)\n        tl.store(wkv_ptr + t * wkv_s_t + cs * wkv_s_c, wkv, mask=cmask)\n        w_eps = w + eps\n        eps = tl.maximum(w_eps, kt)\n        e1b = tl.exp(w_eps - eps)\n        e2b = tl.exp(kt - eps)\n        alpha = e1b * alpha + e2b * vt\n        beta = e1b * beta + e2b\n        tl.store(alpha_out_ptr + t * state_out_s_t + cs * state_out_s_c,\n            alpha, mask=cmask)\n        tl.store(beta_out_ptr + t * state_out_s_t + cs * state_out_s_c,\n            beta, mask=cmask)\n        tl.store(eps_out_ptr + t * state_out_s_t + cs * state_out_s_c, eps,\n            mask=cmask)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv4_backward_kernel(w_ptr, w_s_c, u_ptr, u_s_c, k_ptr,\n    k_s_b, k_s_t, k_s_c, v_ptr, v_s_b, v_s_t, v_s_c, state_ptr, state_s_b,\n    state_s_abe, state_s_t, state_s_c, gwkv_ptr, gwkv_s_b, gwkv_s_t,\n    gwkv_s_c, gstate_out_ptr, gstate_out_s_b, gstate_out_s_abe,\n    gstate_out_s_c, gw_ptr, gw_s_c, gu_ptr, gu_s_c, gk_ptr, gk_s_b, gk_s_t,\n    gk_s_c, gv_ptr, gv_s_b, gv_s_t, gv_s_c, gstate_ptr, gstate_s_b,\n    gstate_s_abe, gstate_s_c, tsz, chans, BLOCK_SIZE_C: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    c_idx = tl.program_id(1)\n    cs = c_idx * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)\n    cmask = cs < chans\n    k_ptr = k_ptr + b_idx * k_s_b\n    v_ptr = v_ptr + b_idx * v_s_b\n    alpha_ptr = state_ptr + b_idx * state_s_b\n    beta_ptr = state_ptr + b_idx * state_s_b + state_s_abe\n    eps_ptr = state_ptr + b_idx * state_s_b + 2 * state_s_abe\n    gk_ptr = gk_ptr + b_idx * gk_s_b\n    gv_ptr = gv_ptr + b_idx * gv_s_b\n    gwkv_ptr = gwkv_ptr + b_idx * gwkv_s_b\n    galpha_out_ptr = gstate_out_ptr + b_idx * gstate_out_s_b\n    gbeta_out_ptr = gstate_out_ptr + b_idx * gstate_out_s_b + gstate_out_s_abe\n    geps_out_ptr = (gstate_out_ptr + b_idx * gstate_out_s_b + 2 *\n        gstate_out_s_abe)\n    galpha = tl.load(galpha_out_ptr + gstate_out_s_c * cs, mask=cmask)\n    gbeta = tl.load(gbeta_out_ptr + gstate_out_s_c * cs, mask=cmask)\n    geps = tl.load(geps_out_ptr + gstate_out_s_c * cs, mask=cmask)\n    w = tl.load(w_ptr + w_s_c * cs, mask=cmask)\n    u = tl.load(u_ptr + u_s_c * cs, mask=cmask)\n    gw = tl.zeros_like(w)\n    gu = tl.zeros_like(u)\n    alpha_prev = tl.load(alpha_ptr + tsz * state_s_t + state_s_c * cs, mask\n        =cmask)\n    beta_prev = tl.load(beta_ptr + tsz * state_s_t + state_s_c * cs, mask=cmask\n        )\n    eps_prev = tl.load(eps_ptr + tsz * state_s_t + state_s_c * cs, mask=cmask)\n    for t in range(tsz):\n        tc = tsz - t - 1\n        kt = tl.load(k_ptr + tc * k_s_t + k_s_c * cs, mask=cmask)\n        vt = tl.load(v_ptr + tc * v_s_t + v_s_c * cs, mask=cmask)\n        alpha_curr = alpha_prev\n        beta_curr = beta_prev\n        eps_curr = eps_prev\n        alpha_prev = tl.load(alpha_ptr + tc * state_s_t + state_s_c * cs,\n            mask=cmask)\n        beta_prev = tl.load(beta_ptr + tc * state_s_t + state_s_c * cs,\n            mask=cmask)\n        eps_prev = tl.load(eps_ptr + tc * state_s_t + state_s_c * cs, mask=\n            cmask)\n        ukt = u + kt\n        tau = tl.maximum(ukt, eps_prev)\n        e1 = tl.exp(eps_prev - tau)\n        e2 = tl.exp(ukt - tau)\n        euke = tl.exp(ukt + eps_prev - 2 * tau)\n        denom = e1 * beta_prev + e2\n        denom_sq = denom * denom\n        gwkvt = tl.load(gwkv_ptr + tc * gwkv_s_t + gwkv_s_c * cs, mask=cmask)\n        guk = gwkvt * e2 * (e1 * beta_prev * vt - e1 * alpha_prev) / denom_sq\n        gu += guk\n        gk = guk\n        gv = gwkvt * e2 / denom\n        galpha_wkv = gwkvt * e1 / denom\n        gbeta_wkv = -gwkvt * e1 * (e2 * vt + e1 * alpha_prev) / denom_sq\n        geps_wkv_denom = e1 * beta_prev + e2\n        geps_wkv = gwkvt * euke * (alpha_prev - vt * beta_prev) / (\n            geps_wkv_denom * geps_wkv_denom)\n        e1 = tl.exp(w + eps_prev - eps_curr)\n        e2 = tl.exp(kt - eps_curr)\n        galpha_we = galpha * e1 * alpha_prev\n        gw += galpha_we\n        gk += galpha * e2 * vt\n        gv += galpha * e2\n        geps += galpha * -alpha_curr\n        gbeta_we = gbeta * e1 * beta_prev\n        gw += gbeta_we\n        gk += gbeta * e2\n        geps += gbeta * -beta_curr\n        geps_mask = w + eps_prev > kt\n        geps_we = tl.where(geps_mask, geps, tl.zeros_like(geps))\n        gw += geps_we\n        gk += tl.where(geps_mask, tl.zeros_like(geps), geps)\n        tl.store(gk_ptr + tc * gk_s_t + gk_s_c * cs, gk, mask=cmask)\n        tl.store(gv_ptr + tc * gv_s_t + gv_s_c * cs, gv, mask=cmask)\n        galpha = galpha * e1 + galpha_wkv\n        gbeta = gbeta * e1 + gbeta_wkv\n        geps = galpha_we + gbeta_we + geps_we + geps_wkv\n    galpha_ptr = gstate_ptr + b_idx * gstate_s_b\n    gbeta_ptr = gstate_ptr + b_idx * gstate_s_b + gstate_s_abe\n    geps_ptr = gstate_ptr + b_idx * gstate_s_b + 2 * gstate_s_abe\n    tl.store(galpha_ptr + gstate_s_c * cs, galpha, mask=cmask)\n    tl.store(gbeta_ptr + gstate_s_c * cs, gbeta, mask=cmask)\n    tl.store(geps_ptr + gstate_s_c * cs, geps, mask=cmask)\n    gw_temp = tl.load(gw_ptr + gw_s_c * cs, mask=cmask)\n    gw_temp += gw\n    tl.store(gw_ptr + gw_s_c * cs, gw_temp, mask=cmask)\n    gu_temp = tl.load(gu_ptr + gu_s_c * cs, mask=cmask)\n    gu_temp += gu\n    tl.store(gu_ptr + gu_s_c * cs, gu_temp, mask=cmask)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_rwkv6_fwd_cumsum_kernel(s, o, o_minus_s, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o_minus_s = tl.make_block_ptr(o_minus_s + i_bh * s_s_h, (T, S), (\n        s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_o_minus_s, b_o - b_s, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_inter(q, k, gi, ge, A, s_k_h, s_k_t,\n    s_k_d, scale, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr',\n    BC: 'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_i <= i_j:\n        return\n    if i_t * BT + i_i * BC >= T:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gq = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (K, T), (s_k_d, s_k_t),\n            (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_gq - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BT'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra(q, k, gi, ge, u, A, s_k_h,\n    s_k_t, s_k_d, scale, H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', NC: 'tl.constexpr'):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_i * BC >= T:\n        return\n    i_j = i_i\n    i_h = i_bh % H\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_j * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    i_k = 0\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    p_u = tl.make_block_ptr(u + i_h * s_k_t, (s_k_t,), (1,), i_k * BK, (BK,\n        ), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        p_qj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        p_qi = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qi = tl.load(p_qi, boundary_check=(0,))\n        A_jj = tl.sum(b_qi * b_k * b_u * scale)\n        b_A = tl.where(o_i != j, b_A, A_jj)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split(q, k, gi, ge, u, A,\n    s_k_h, s_k_t, s_k_d, scale, H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    i_t, i_i = i_tc // NC, i_tc % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    i_j = i_i\n    i_h = i_bh % H\n    o_i = tl.arange(0, BC)\n    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0,\n        BC)) * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    p_u = tl.make_block_ptr(u + i_h * s_k_t, (s_k_t,), (1,), i_k * BK, (BK,\n        ), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        p_qj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        p_qi = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qi = tl.load(p_qi, boundary_check=(0,))\n        A_jj = tl.sum(b_qi * b_k * b_u * scale)\n        b_A = tl.where(o_i != j, b_A, A_jj)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge(A, A2, T: 'tl.constexpr',\n    BT: 'tl.constexpr', BC: 'tl.constexpr', NK: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_c * BC >= T:\n        return\n    n_bh = tl.num_programs(2)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(A + (i_bh + i_k * n_bh) * T * BC, (T, BC),\n            (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_rwkv6_fwd_kernel_inter(q, v, g, h, o, A, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_ge = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_g = tl.load(p_ge, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_A = tl.where(m_s, b_A, 0.0)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'NC', 'BT'])\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_intra(q, k, gi, ge, dA, dq, dk, s_k_h, s_k_t,\n    s_k_d, scale, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr',\n    BC: 'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_q = i_t * BT + i_i * BC\n    m_k = o_k < K\n    p_ge = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_ge = tl.load(p_ge, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    o_i = tl.arange(0, BC)\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        b_gn = tl.load(gi + i_bh * T * K + (o_q - 1) * K + o_k, mask=m_k &\n            (i_i > 0) & (o_q <= T), other=0)\n        for i_j in range(0, i_i):\n            p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n                ), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t,\n                s_k_d), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1),\n                (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= tl.exp(b_ge - b_gn[None, :])\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gkj = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,))\n        b_gkj = tl.load(p_gkj, boundary_check=(0,))\n        m_i = o_i[:, None] > j\n        tmp = tl.exp(b_ge - b_gkj[None, :])\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tmp, 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    max_block_idx = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < max_block_idx - 1:\n        p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        for i_j in range(i_i + 1, NC):\n            p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n                ), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_ge = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t,\n                s_k_d), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1),\n                (i_t * BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_ge = tl.load(p_ge, boundary_check=(0, 1))\n            b_qg = b_q * tl.exp(b_ge - b_gn[None, :])\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dk += tl.dot(tl.trans(b_dA), b_qg, allow_tf32=False)\n        b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gqj = tl.make_block_ptr(ge + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        b_gqj = tl.load(p_gqj, boundary_check=(0,))\n        m_i = o_i[:, None] < j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_inter(q, k, v, h, gi, ge, u, do, dh, dA, dq, dk,\n    dq2, dk2, dg, du, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h,\n    s_h_t, s_h_d, scale, H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    last_idx = min(T, i_t * BT + BT) - 1\n    p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), (\n        last_idx * K + i_k * BK,), (BK,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros([BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, b_dh)\n    p_gk = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dgk *= tl.exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    p_gi = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_gi = tl.load(p_gi, boundary_check=(0, 1))\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * tl.exp(b_gn[None, :] - b_gi)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    b_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :\n        ] + b_dgk[None, :] - b_q * b_dq\n    o_i = tl.arange(0, BT)\n    p_dA_dig = dA + i_bh * T * BT + (i_t * BT + o_i) * BT + o_i\n    b_dA_dig = tl.load(p_dA_dig, mask=i_t * BT + o_i < T, other=0)\n    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK,), (BK,), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    b_dq += b_dA_dig[:, None] * b_u[None, :] * b_k\n    b_dk += b_dA_dig[:, None] * b_u[None, :] * b_q\n    b_du = tl.sum(b_dA_dig[:, None] * b_q * b_k, axis=0)\n    p_du = tl.make_block_ptr(du + (i_h + i_t * n_bh) * K, (K,), (1,), (i_k *\n        BK,), (BK,), (0,))\n    tl.store(p_du, b_du, boundary_check=(0,))\n    p_dg = tl.make_block_ptr(dg + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq2 + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk2 + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_dh(q, gi, ge, do, dh, dht, dh0, s_k_h, s_k_t,\n    s_v_h, s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NG: 'tl.constexpr',\n    STORE_INITIAL_STATE_GRADIENT: 'tl.constexpr', USE_FINAL_STATE_GRADIENT:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        p_gk = tl.make_block_ptr(ge + i_bg * s_k_h, (K, T), (1, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_q = b_q * tl.exp(b_gk) * scale\n        p_gk_last = gi + i_bg * s_k_h + last_idx * K + i_k * BK + tl.arange(\n            0, BK)\n        p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K,\n            other=0.0)\n        b_dh *= tl.exp(b_gk_last)[:, None]\n        b_dh += tl.dot(b_q, b_do)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel(q, k, v, w, u, o, h0, ht, s_k_h, s_v_h,\n    scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv6_bwd_kernel_dq(k, v, w, u, do, dq, dq_aux, h0,\n    s_k_h, s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if REVERSE else 0)\n    p_dq_aux = dq_aux + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK) + ((T - 1) * K if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w)\n        h_q = b_h * b_do[:, None]\n        b_dq = tl.sum(h_q + b_kv * b_u[None, :] * b_do[:, None], axis=0)\n        b_dq *= scale\n        b_dq_aux = tl.sum(h_q, axis=0)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_dq, b_dq, mask=mask_bk)\n        tl.store(p_dq_aux, b_dq_aux, mask=mask_bk)\n        p_k += -K if REVERSE else K\n        p_do += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n        p_dq += -K if REVERSE else K\n        p_dq_aux += -K if REVERSE else K\n"
    },
    {
      "input": "@triton.jit\ndef fused_recurrent_rwkv6_bwd_kernel_dkv(q, k, v, w, u, do, dk, dk_aux, dv,\n    dh0, s_k_h, s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        not REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if \n        not REVERSE else 0)\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if not REVERSE else 0)\n    p_dk_aux = dk_aux + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK) + ((T - 1) * K if not REVERSE else 0)\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if not REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[:, None] & mask_bv[None, :]\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(T - 1, -1, -1):\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_dkv = b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        tl.store(p_dk_aux, b_dk, mask=mask_bk)\n        b_dk += tl.sum(b_dkv * b_u[:, None] * b_v[None, :], axis=1)\n        b_dv = tl.sum((b_dh + b_dkv * b_u[:, None]) * b_k[:, None], axis=0)\n        tl.store(p_dk, b_dk, mask=mask_bk)\n        tl.store(p_dv, b_dv, mask=mask_bv)\n        b_dh *= tl.exp(b_w)[:, None]\n        b_dh += b_dkv\n        p_q += K if REVERSE else -K\n        p_k += K if REVERSE else -K\n        p_v += V if REVERSE else -V\n        p_w += K if REVERSE else -K\n        p_do += V if REVERSE else -V\n        p_dk += K if REVERSE else -K\n        p_dk_aux += K if REVERSE else -K\n        p_dv += V if REVERSE else -V\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh, mask=mask_kv)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4)], key=['BT', 'BK',\n    'BV'])\n@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(q, k, v, h, g, o, s_k_h, s_k_t, s_v_h,\n    s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_g)[:, None]\n    b_s = b_s * tl.exp(b_g[:, None] - b_g[None, :])\n    b_s = tl.where(m_s, b_s, 0)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkg(q, k, v, h, g, do, dh, dq, dk, dg,\n    s_k_h, s_k_t, s_v_h, s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    last_idx = min(i_t * BT + BT, T) - 1\n    b_g_last = tl.load(g + i_bh * T + last_idx)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg_last = tl.zeros([1], dtype=tl.float32)\n    b_dg = tl.zeros([BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dg_last += tl.sum(b_h * b_dh)\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, b_dh)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg_last *= tl.exp(b_g_last)\n    b_dq = b_dq * tl.exp(b_g)[:, None] * scale\n    b_dk = b_dk * tl.exp(-b_g + b_g_last)[:, None]\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale * tl.exp(b_g\n        [:, None] - b_g[None, :]), 0)\n    b_ds = b_ds\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q)\n    b_dg += tl.sum(b_q * b_dq - b_k * b_dk, axis=1)\n    b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (i_k * n_bh + i_bh) * T, (T,), (1,), (i_t *\n        BT,), (BT,), (0,))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT'])\n@triton.jit\ndef compute_final_dg(dg, o, T: 'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_o = tl.make_block_ptr(dg + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    b_o = tl.load(p_o, boundary_check=(0,))\n    b_o = b_o - tl.cumsum(b_o, axis=0) + tl.sum(b_o, axis=0)\n    p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_o, b_o, boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_bwd_dv_kernel(q, k, g, do, dv, dh, s_k_h, s_k_t, s_v_h, s_v_t,\n    s_h_h, s_h_t, T, K, V, scale, BT: 'tl.constexpr', BK: 'tl.constexpr',\n    BV: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    last_idx = min(i_t * BT + BT, T) - 1\n    b_g = tl.load(g + i_bh * T + i_t * BT + tl.arange(0, BT))\n    b_g_last = tl.load(g + i_bh * T + last_idx)\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh) * tl.exp(-b_g + b_g_last)[:, None]\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_A += tl.dot(b_k, b_q, allow_tf32=False)\n    b_A = b_A * tl.exp(b_g[None, :] - b_g[:, None]) * scale\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0)\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv += tl.dot(b_A, b_do)\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef logcumsumexp_fwd_kernel(s, z, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_mp = tl.full([S], float('-inf'), dtype=tl.float32)\n    b_zp = tl.zeros([S], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_mc = tl.max(b_s, 0)\n        if i_t > 0:\n            b_mc = tl.maximum(b_mp, b_mc)\n        b_zp = b_zp * tl.exp(b_mp - b_mc)\n        b_s = tl.exp(b_s - b_mc)\n        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp\n        b_zc = tl.max(b_z, 0)\n        b_mp = b_mc\n        b_zp = b_zc\n        b_z = tl.log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc\n        tl.store(p_z, b_z, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=2), triton.Config({},\n    num_warps=4), triton.Config({}, num_warps=8)], key=['S'])\n@triton.jit\ndef softmax_fwd_kernel(s, p, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    p_p = tl.make_block_ptr(p + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_m = tl.max(b_s, 1)\n    b_s = tl.exp(b_s - b_m[:, None])\n    b_z = tl.sum(b_s, 1)\n    b_p = tl.where(b_s != 0, b_s / b_z[:, None], 0.0)\n    tl.store(p_p, b_p, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=2), triton.Config({},\n    num_warps=4), triton.Config({}, num_warps=8)], key=['S'])\n@triton.jit\ndef softmax_bwd_kernel(p, dp, ds, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_p = tl.make_block_ptr(p + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    p_dp = tl.make_block_ptr(dp + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT, 0), (BT, S), (1, 0))\n    p_ds = tl.make_block_ptr(ds + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT, 0), (BT, S), (1, 0))\n    b_p = tl.load(p_p, boundary_check=(0, 1))\n    b_dp = tl.load(p_dp, boundary_check=(0, 1))\n    b_pp = tl.sum(b_p * b_dp, 1)\n    b_ds = b_p * b_dp - b_p * b_pp[:, None]\n    tl.store(p_ds, b_ds, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_global_reversed_cumsum_vector_kernel(s, z, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_global_cumsum_vector_kernel(s, z, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2),\n    triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64},\n    num_warps=4)], key=[])\n@triton.jit\ndef chunk_global_reversed_cumsum_scalar_kernel(s, o, T: 'tl.constexpr', BT:\n    'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,))\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        b_o = b_s - tl.cumsum(b_s, axis=0) + b_z[None]\n        tl.store(p_o, b_o, boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2),\n    triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64},\n    num_warps=4)], key=[])\n@triton.jit\ndef chunk_global_cumsum_scalar_kernel(s, o, T: 'tl.constexpr', BT:\n    'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,))\n        b_o = tl.cumsum(b_s, axis=0) + b_z[None]\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        tl.store(p_o, b_o, boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S', 'BT'])\n@triton.jit\ndef chunk_local_cumsum_vector_kernel(s, o, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT'])\n@triton.jit\ndef chunk_local_cumsum_scalar_kernel(s, o, T: 'tl.constexpr', BT:\n    'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_s = tl.load(p_s, boundary_check=(0,))\n    b_o = tl.cumsum(b_s, axis=0)\n    tl.store(p_o, b_o, boundary_check=(0,))\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.heuristics({'HAS_SCALE': lambda args: args['scale'] is not None})\n@triton.jit\ndef logsumexp_fwd_kernel(x, z, scale, D: 'tl.constexpr', B: 'tl.constexpr',\n    HAS_SCALE: 'tl.constexpr'):\n    i_n, i_d = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * B + tl.arange(0, B)\n    m_d = o_d < D\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\n    if HAS_SCALE:\n        b_x = b_x * scale\n    b_m = tl.max(b_x, 0)\n    b_z = tl.log(tl.sum(tl.exp(b_x - b_m), 0)) + b_m\n    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)\n"
    },
    {
      "input": "@triton.jit\ndef leaky_relu(x):\n    return tl.where(x >= 0, x, 0.01 * x)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BM': 128, 'BK': 64, 'BN': 256,\n    'G': 4}, num_stages=3, num_warps=8), triton.Config({'BM': 64, 'BK': 32,\n    'BN': 256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': \n    128, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.\n    Config({'BM': 128, 'BK': 32, 'BN': 64, 'G': 4}, num_stages=4, num_warps\n    =4), triton.Config({'BM': 64, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=\n    4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 32, 'G': 4},\n    num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 32, 'BN': 32,\n    'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 32, 'BK': 32,\n    'BN': 64, 'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 128,\n    'BK': 128, 'BN': 256, 'G': 4}, num_stages=3, num_warps=8), triton.\n    Config({'BM': 256, 'BK': 128, 'BN': 128, 'G': 4}, num_stages=3,\n    num_warps=8), triton.Config({'BM': 256, 'BK': 128, 'BN': 64, 'G': 4},\n    num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 128, 'BN': \n    256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128,\n    'BK': 128, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.\n    Config({'BM': 128, 'BK': 64, 'BN': 64, 'G': 4}, num_stages=4, num_warps\n    =4), triton.Config({'BM': 64, 'BK': 64, 'BN': 128, 'G': 4}, num_stages=\n    4, num_warps=4), triton.Config({'BM': 128, 'BK': 64, 'BN': 32, 'G': 4},\n    num_stages=4, num_warps=4)], key=['M', 'N', 'K'])\n@triton.heuristics({'HAS_INPUT': lambda args: args['input'] is not None,\n    'HAS_ALPHA': lambda args: args['alpha'] is not None, 'HAS_BETA': lambda\n    args: args['beta'] is not None})\n@triton.jit\ndef matmul_kernel(a, b, c, input, alpha, beta, M, N, K, s_am, s_ak, s_bk,\n    s_bn, s_cm, s_cn, BM: 'tl.constexpr', BK: 'tl.constexpr', BN:\n    'tl.constexpr', G: 'tl.constexpr', ACTIVATION: 'tl.constexpr',\n    HAS_INPUT: 'tl.constexpr', HAS_ALPHA: 'tl.constexpr', HAS_BETA:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    NM, NN = tl.num_programs(0), tl.num_programs(1)\n    i_m, i_n = tl.program_id(0), tl.program_id(1)\n    i_m, i_n = tl.swizzle2d(i_m, i_n, NM, NN, G)\n    o_am = (i_m * BM + tl.arange(0, BM)) % M\n    o_bn = (i_n * BN + tl.arange(0, BN)) % N\n    o_k = tl.arange(0, BK)\n    p_a = a + (o_am[:, None] * s_am + o_k[None, :] * s_ak)\n    p_b = b + (o_k[:, None] * s_bk + o_bn[None, :] * s_bn)\n    b_acc = tl.zeros((BM, BN), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BK)):\n        b_a = tl.load(p_a, mask=o_k[None, :] < K - k * BK, other=0.0)\n        b_b = tl.load(p_b, mask=o_k[:, None] < K - k * BK, other=0.0)\n        b_acc += tl.dot(b_a, b_b, allow_tf32=False)\n        p_a += BK * s_ak\n        p_b += BK * s_bk\n    o_cm = i_m * BM + tl.arange(0, BM)\n    o_cn = i_n * BN + tl.arange(0, BN)\n    mask = (o_cm[:, None] < M) & (o_cn[None, :] < N)\n    b_c = b_acc\n    if ACTIVATION == 'leaky_relu':\n        b_c = leaky_relu(b_c)\n    if HAS_ALPHA:\n        b_c *= tl.load(alpha)\n    if HAS_INPUT:\n        p_i = input + s_cm * o_cm[:, None] + s_cn * o_cn[None, :]\n        b_i = tl.load(p_i, mask=mask, other=0.0)\n        if HAS_BETA:\n            b_i *= tl.load(beta)\n        b_c += b_i\n    p_c = c + s_cm * o_cm[:, None] + s_cn * o_cn[None, :]\n    tl.store(p_c, b_c, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef attention_fwd_kernel(q, k, v, h, o, s_qh, s_qt, s_qd, s_hh, s_ht, T,\n    scale, BT: 'tl.constexpr', BD: 'tl.constexpr', NT: 'tl.constexpr',\n    STORE: 'tl.constexpr', IFCOND: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    b_h = tl.zeros([BD, BD], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qh, (BD, T), (s_qd, s_qt), (0,\n            i * BT), (BD, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (NT * BD, BD), (s_ht, s_qd\n            ), (i * BD, 0), (BD, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        if STORE:\n            tl.store(p_h, b_h)\n        b_q = tl.load(p_q)\n        b_q = b_q * scale\n        b_k = tl.load(p_k)\n        b_v = tl.load(p_v)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if IFCOND:\n            if i == 0:\n                b_h = tl.dot(b_k, b_v, allow_tf32=False)\n            else:\n                b_o += tl.dot(b_q, b_h, allow_tf32=False)\n                b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o)\n"
    },
    {
      "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs, K_scale_ptr, V_ptrs,\n    start_m, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n        K_scale_ptr += lo // BLOCK_N\n        K_ptrs += HEAD_DIM * lo\n        V_ptrs += HEAD_DIM * lo\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k_mask = offs_n[None, :] < N_CTX - start_n\n        k = tl.load(K_ptrs, mask=k_mask)\n        k_scale = tl.load(K_scale_ptr)\n        qk = tl.dot(q, k) * q_scale * k_scale\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_ptrs, mask=offs_n[:, None] < N_CTX - start_n)\n        p = p\n        acc += tl.dot(p, v, out_dtype=tl.float16)\n        m_i = m_ij\n        K_ptrs += BLOCK_N * HEAD_DIM\n        K_scale_ptr += 1\n        V_ptrs += BLOCK_N * HEAD_DIM\n    return acc, l_i, m_i\n"
    },
    {
      "input": "@triton.jit\ndef _attn_fwd(Q, K, V, Q_scale, K_scale, Out, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh,\n    stride_om, stride_on, Z, H, N_CTX, HEAD_DIM: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', STAGE: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    q_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_M)\n    k_scale_offset = off_hz * tl.cdiv(N_CTX, BLOCK_N)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, HEAD_DIM)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :\n        ] * stride_qk\n    Q_scale_ptr = Q_scale + q_scale_offset + start_m\n    K_ptrs = K + qvk_offset + offs_k[:, None] + offs_n[None, :] * stride_kn\n    K_scale_ptr = K_scale + k_scale_offset\n    V_ptrs = V + qvk_offset + offs_n[:, None] * stride_qm + offs_k[None, :\n        ] * stride_qk\n    O_block_ptr = Out + qvk_offset + offs_m[:, None] * stride_qm + offs_k[\n        None, :] * stride_qk\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n    q = tl.load(Q_ptrs, mask=offs_m[:, None] < N_CTX)\n    q_scale = tl.load(Q_scale_ptr)\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs,\n        K_scale_ptr, V_ptrs, start_m, BLOCK_M, HEAD_DIM, BLOCK_N, 4 - STAGE,\n        offs_m, offs_n, N_CTX)\n    acc, l_i, _ = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, K_ptrs,\n        K_scale_ptr, V_ptrs, start_m, BLOCK_M, HEAD_DIM, BLOCK_N, 2, offs_m,\n        offs_n, N_CTX)\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc, mask=offs_m[:, None] < N_CTX)\n"
    },
    {
      "input": "@triton.jit\ndef q_kernel_per_block_int8(X, X_int8, BLK: 'tl.constexpr', Scale, L, C:\n    'tl.constexpr', scale_stride):\n    off_b = tl.program_id(1)\n    off_blk = tl.program_id(0)\n    x_offset = off_b * L * C\n    offs_m = off_blk * BLK + tl.arange(0, BLK)\n    offs_k = tl.arange(0, 128)\n    x_ptrs = X + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    x_int8_ptrs = X_int8 + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    scale_ptrs = Scale + off_b * scale_stride + off_blk\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < L) & (tl.arange(0, 128) < \n        96)[None, :])\n    x *= C ** -0.5 * 1.44269504\n    scale = tl.max(tl.abs(x)) / 127.0\n    x_int8 = x / scale\n    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)\n    x_int8 = x_int8\n    tl.store(x_int8_ptrs, x_int8, mask=(offs_m[:, None] < L) & (tl.arange(0,\n        128) < 96)[None, :])\n    tl.store(scale_ptrs, scale)\n"
    },
    {
      "input": "@triton.jit\ndef k_kernel_per_block_int8(X, X_int8, BLK: 'tl.constexpr', Scale, L, C:\n    'tl.constexpr', scale_stride):\n    off_b = tl.program_id(1)\n    off_blk = tl.program_id(0)\n    x_offset = off_b * L * C\n    offs_m = off_blk * BLK + tl.arange(0, BLK)\n    offs_k = tl.arange(0, 128)\n    x_ptrs = X + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    x_int8_ptrs = X_int8 + x_offset + offs_m[:, None] * C + offs_k[None, :]\n    scale_ptrs = Scale + off_b * scale_stride + off_blk\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < L) & (tl.arange(0, 128) < \n        96)[None, :])\n    scale = tl.max(tl.abs(x)) / 127.0\n    x_int8 = x / scale\n    x_int8 += 0.5 * tl.where(x_int8 >= 0, 1, -1)\n    x_int8 = x_int8\n    tl.store(x_int8_ptrs, x_int8, mask=(offs_m[:, None] < L) & (tl.arange(0,\n        128) < 96)[None, :])\n    tl.store(scale_ptrs, scale)\n"
    },
    {
      "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, N_CTX, sliding_window_offset, sliding_window_size, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    if SLIDING_WINDOW and not COMPLEMENT_SLIDING_WINDOW:\n        if COMPLEMENT_SLIDING_WINDOW:\n            lo = 0\n            hi = ((start_m + 1) * BLOCK_M + sliding_window_offset -\n                sliding_window_size + BLOCK_N - 1) // BLOCK_N * BLOCK_N\n        else:\n            lo = (start_m * BLOCK_M + sliding_window_offset -\n                sliding_window_size + 1) // BLOCK_N * BLOCK_N\n            hi = ((start_m + 1) * BLOCK_M - 1 + sliding_window_offset + BLOCK_N\n                ) // BLOCK_N * BLOCK_N\n            if lo < 0:\n                lo = 0\n            if hi > N_CTX:\n                hi = N_CTX\n            lo = tl.multiple_of(lo, BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n            V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    else:\n        lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if IS_EVEN_N:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m * BLOCK_M - start_n + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n            qk = tl.where(mask, qk, float('-inf'))\n        if not IS_EVEN_N:\n            qk = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :\n                ], qk, float('-inf'))\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :],\n                p, 0)\n        l_ij = tl.sum(p, 1)\n        tmp = m_i - m_ij\n        alpha_mask = tmp != tmp\n        alpha = tl.math.exp2(tmp)\n        alpha = tl.where(alpha_mask, 1.0, alpha)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        if IS_EVEN_N:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        acc += tl.dot(p, v)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n"
    },
    {
      "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, L, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX, sliding_window_offset,\n    sliding_window_size, IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', END: 'tl.constexpr', INIT: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    v_offset = off_z * stride_vz + off_hkv * stride_vh\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + v_offset, shape=(NKV_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(ROUND_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n        m_i = tl.load(m_ptrs)\n        l_i = tl.load(l_ptrs)\n        acc = tl.load(O_block_ptr)\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n        V_block_ptr, start_m, qk_scale, NKV_CTX, sliding_window_offset,\n        sliding_window_size, BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW,\n        IS_EVEN_M, IS_EVEN_N, COMPLEMENT_SLIDING_WINDOW)\n    if END:\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n"
    },
    {
      "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _score_kernel(Q, K, M, sm_scale, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_oz,\n    stride_oh, stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX,\n    sliding_window_offset, sliding_window_size, SLIDING_WINDOW:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M:\n    'tl.constexpr', IS_EVEN_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, start_n *\n        BLOCK_N), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        m = tl.load(m_ptrs)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m - start_n * BLOCK_N + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_M) + start_m < N_CTX)[:, None],\n                p, 0)\n        o += tl.sum(p, axis=0)\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o, mask=o_range < NKV_CTX)\n"
    },
    {
      "input": "@triton.jit\ndef liger_cross_entropy_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr,\n    loss_stride, n_cols, n_non_ignore, ignore_index, label_smoothing:\n    'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    Y_ptr += program_id * Y_stride\n    y = tl.load(Y_ptr)\n    X_ptr += program_id * X_stride\n    if y == ignore_index:\n        for i in range(0, n_cols, BLOCK_SIZE):\n            X_offsets = i + tl.arange(0, BLOCK_SIZE)\n            tl.store(X_ptr + X_offsets, 0.0, mask=X_offsets < n_cols)\n        return\n    loss_ptr += program_id * loss_stride\n    m = float('-inf')\n    d = 0.0\n    ori_X_y = tl.load(X_ptr + y)\n    scaled_x_sum = 0.0\n    eps = label_smoothing / n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        block_max = tl.max(X_block)\n        if label_smoothing > 0:\n            scaled_x_sum += tl.sum(tl.where(X_offsets < n_cols, -eps *\n                X_block, 0.0))\n        m_new = tl.maximum(m, block_max)\n        d = d * tl.exp(m - m_new) + tl.sum(tl.exp(X_block - m_new))\n        m = m_new\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        if reduction == 'mean':\n            X_block = (tl.exp(X_block - m) / d - eps) / n_non_ignore\n        else:\n            X_block = tl.exp(X_block - m) / d - eps\n        tl.store(X_ptr + X_offsets, X_block, mask=X_offsets < n_cols)\n    tl.debug_barrier()\n    loss = -(ori_X_y - m - tl.log(d))\n    if label_smoothing > 0:\n        smooth_loss = scaled_x_sum + label_smoothing * (m + tl.log(d))\n        loss = loss * (1 - label_smoothing) + smooth_loss\n    if reduction == 'mean':\n        loss = loss / n_non_ignore\n    X_y = tl.load(X_ptr + y)\n    if reduction == 'mean':\n        X_y += -(1 - label_smoothing) / n_non_ignore\n    else:\n        X_y += -(1 - label_smoothing)\n    tl.store(loss_ptr, loss)\n    tl.store(X_ptr + y, X_y)\n"
    },
    {
      "input": "@triton.jit\ndef element_mul_kernel(X_ptr, X_stride, grad_output_ptr, n_cols, BLOCK_SIZE:\n    'tl.constexpr'):\n    program_id = tl.program_id(0)\n    X_ptr += program_id * X_stride\n    grad_output = tl.load(grad_output_ptr)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols)\n        tl.store(X_ptr + X_offsets, X_block * grad_output, mask=X_offsets <\n            n_cols)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_forward_kernel(a, b, c, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _geglu_tanh_backward_kernel(dc, a, b, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    db_row = dc_row * geglu_a\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = 0.5 * a_row * (1 - tanh_sq) * (sqrt_2_over_pi * (1 + 3 * \n        0.044715 * a_row * a_row))\n    da_row = dc_row * b_row * (term1 + term2)\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _triton_rope(q_ptr, q_row_stride, k_ptr, k_row_stride, cos,\n    cos_row_stride, sin, sin_row_stride, sl, bs: 'tl.constexpr', n_qh:\n    'tl.constexpr', n_kh: 'tl.constexpr', hd: 'tl.constexpr', pad_n_qh:\n    'tl.constexpr', pad_n_kh: 'tl.constexpr', pad_hd: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', BACKWARD_PASS: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n    cos_row_idx = pid % sl\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        )\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        )\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n"
    },
    {
      "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: args['DO_SOFTCAPPING'],\n    'DO_LOGIT_SCALING': lambda args: args['DO_LOGIT_SCALING']})\n@triton.jit\ndef _cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', DO_SOFTCAPPING: 'tl.constexpr', SOFTCAP: 'tl.constexpr',\n    DO_LOGIT_SCALING: 'tl.constexpr', LOGIT_SCALE: 'tl.constexpr'):\n    \"\"\"\n        Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]\n        Pi = exp(xi) / sum(exp(xi))\n        CE_i = -y log(p) = -y log[ exp(x) / sum(exp(x)) ]\n             = -y [ x - log[sum(exp(x))] ]\n             = y * (log[sum(exp(x))] - x)\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        logsumexp is also stable\n        Take    y =         log[sum(exp(x))]\n           exp(y) =             sum(exp(x))\n           exp(y) =             sum(exp(x - c)*exp(c)) Since e^(x-c)*e^c = e^x\n           exp(y) =      exp(c)*sum(exp(x - c))\n               y  = log(exp(c)*sum(exp(x - c)))\n               y  = c + log[sum(exp(x - c))]\n        This means we can set c = max(x) to make sure\n        exp(x - c) always is exp(x - max(x)).\n        This ensures exp(x - max(x))'s maximum is 1 as exp(0) = 1.\n    \"\"\"\n    row_idx = tl.program_id(0)\n    logits_ptr += row_idx * logits_row_stride\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx\n    labels_ptr += row_idx\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n    logits = logits\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if label_idx != -100:\n        x = tl.load(logits_ptr + label_idx)\n        if DO_LOGIT_SCALING:\n            x = LOGIT_SCALE * x\n        if DO_SOFTCAPPING:\n            x = SOFTCAP * triton_tanh(x / SOFTCAP)\n        loss = logsumexp - x\n    else:\n        loss = 0.0\n    tl.store(logsumexp_ptr, logsumexp)\n    tl.store(loss_ptr, loss)\n"
    },
    {
      "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: args['DO_SOFTCAPPING'],\n    'DO_LOGIT_SCALING': lambda args: args['DO_LOGIT_SCALING']})\n@triton.jit\ndef _chunked_cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE: 'tl.constexpr', N_CHUNKS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', DO_SOFTCAPPING:\n    'tl.constexpr', SOFTCAP: 'tl.constexpr', DO_LOGIT_SCALING:\n    'tl.constexpr', LOGIT_SCALE: 'tl.constexpr'):\n    \"\"\"\n        256K vocab divided in 4 chunks\n\n        |-65536-| |-65536-| |-65536-| |-65536-|\n        |-------| |-------| |-------| |-------|\n        |-------| |-------| |-------| |-------|\n\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        Notice we can do logsumexp for each chunk and then\n        logsumexp[chunk_sum(logsumexp)] == logsumexp\n\n        chunk_sum = log[chunk_sum(logsumexp)]\n                  = log[exp(logsumexp(a)) + ... + exp(logsumexp(z))]\n                  = log[exp(log[sum(exp(a))]) + ... + exp(log[sum(exp(z))])]\n                  = log[sum(exp(a)) + ... + sum(exp(z))]\n                  = logsumexp(x)\n\n        This means we can perform a logsumexp for each chunk, then do a\n        final logsumexp reduction!\n\n        Ie do: logsumexp(chunked_logsumexp) - x\n    \"\"\"\n    row_idx = tl.program_id(0)\n    chunk_idx = tl.program_id(1)\n    logits_ptr += row_idx * logits_row_stride\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\n    labels_ptr += row_idx\n    col_offsets = chunk_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n    logits = logits\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if chunk_idx == 0:\n        if label_idx != -100:\n            x = tl.load(logits_ptr + label_idx)\n            if DO_LOGIT_SCALING:\n                x = LOGIT_SCALE * x\n            if DO_SOFTCAPPING:\n                x = SOFTCAP * triton_tanh(x / SOFTCAP)\n            loss = -1.0 * x\n        else:\n            loss = 0.0\n        tl.store(loss_ptr, loss)\n    pass\n    tl.store(logsumexp_ptr, logsumexp)\n"
    },
    {
      "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: args['DO_SOFTCAPPING'],\n    'DO_LOGIT_SCALING': lambda args: args['DO_LOGIT_SCALING']})\n@triton.jit\ndef _cross_entropy_backward(logits_ptr, logits_row_stride, dloss_ptr,\n    dloss_row_stride, logsumexp_ptr, labels_ptr, VOCAB_SIZE: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', DO_SOFTCAPPING: 'tl.constexpr', SOFTCAP:\n    'tl.constexpr', DO_LOGIT_SCALING: 'tl.constexpr', LOGIT_SCALE:\n    'tl.constexpr'):\n    \"\"\"\n        CE_i = -y log(P) = y * (log[sum(exp(x))] - x)\n        dC/dx = d/dx (y * log[sum(exp(x))] - x * y)\n\n        From https://en.wikipedia.org/wiki/LogSumExp\n        d/dx logsumexp = exp(x) / sum(exp(x)) = softmax(x)\n\n        dC/dx = y * exp(x) / sum(exp(x)) - d/dx (x * y)\n        dC/dx = y * exp[ log[exp(x) / sum(exp(x))] ] using x = exp(log(x)) trick\n        dC/dx = y * exp[x - logsumexp] - d/dx (x * y)\n\n        If y == 0: dC/dx = 0\n        If y == 1 and x == label: dC/dlabel = exp[x - logsumexp] - 1\n        If y == 1 and x != label: dC/dx     = exp[x - logsumexp]\n    \"\"\"\n    row_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n    logits_ptr += row_idx * logits_row_stride\n    dloss_ptr += row_idx * dloss_row_stride\n    col_offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != -100:\n        dloss = tl.load(dloss_ptr)\n    else:\n        dloss = 0.0\n    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    if DO_LOGIT_SCALING:\n        x = x * LOGIT_SCALE\n    pass\n    if DO_SOFTCAPPING:\n        partial = triton_tanh(x / SOFTCAP)\n        x = SOFTCAP * partial\n    pass\n    logsumexp = tl.load(logsumexp_ptr + row_idx)\n    y = tl.exp(x - logsumexp)\n    y = tl.where(col_offsets == label_idx, y - 1.0, y)\n    if DO_LOGIT_SCALING:\n        y = y * LOGIT_SCALE\n    pass\n    if DO_SOFTCAPPING:\n        y = y * (1.0 - partial * partial)\n    pass\n    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\n    h = f * up\n\n    df/de (with help of Wolfram :)\n    df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)\n\n    Reuse via\n    f =        1/2 * (1 + erf(1/sqrt(2) * e)) * e\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_partial_row = 0.5 * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_partial_row * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    t = 0.3989422804014327\n    df_de = f_partial_row + t * e_row * tl.exp(-0.5 * e_row * e_row)\n    de_row = dg_row * df_de\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    s = 0.7978845608028654\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (triton_tanh(s * e_row * (1.0 + 0.044715 * e_row *\n        e_row)) + 1.0)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\n    h = f * up\n\n    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))\n    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +\n            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] *                            ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )\n\n    Notice sech^2(x) = 1 - tanh^2(x)\n    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )\n\n    See https://www.desmos.com/calculator/nqprfoni6x\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    s = 0.7978845608028654\n    a = s * e_row\n    b = a * 0.044715 * e_row * e_row\n    T = 1.0 + triton_tanh(a + b)\n    T2 = 0.5 * T\n    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b)\n    df_de = T2 + Q2\n    f_row = T2 * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row * df_de\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef layernorm_forward(Y, Y_row_stride, X, X_row_stride, W, b, r, mu, n_cols,\n    eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    mean_X = tl.sum(X_row, axis=0) / n_cols\n    XX = X_row - mean_X\n    row_var = tl.sum(XX * XX, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    tl.store(mu, mean_X)\n    output = XX * inv_var * W_row + b_row\n    tl.store(Y + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef layernorm_backward(dY, dY_row_stride, X, X_row_stride, W, b, r, mu,\n    n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    inv_var = tl.load(r)\n    mean = tl.load(mu)\n    normed = (X_row - mean) * inv_var\n    dY_W = dY_row * W_row\n    dX_row = dY_W - tl.sum(dY_W, axis=0) / n_cols - normed * tl.sum(dY_W *\n        normed, axis=0) / n_cols\n    dX_row = dX_row * inv_var\n    tl.store(dY + col_offsets, dX_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Fast RMS Layernorm kernel\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    normed = normed\n    output = normed * W_row\n    tl.store(Y + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'GEMMA': lambda args: args['GEMMA']})\n@triton.jit\ndef _rms_layernorm_backward(dY, dY_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, dW, dW_row_stride, n_cols, eps, GEMMA:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Fast RMS Layernorm kernel for the backward pass\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    inv_var = tl.load(r)\n    normed = X_row * inv_var\n    if GEMMA:\n        dY_W = dY_row * (W_row + 1.0)\n    else:\n        dY_W = dY_row * W_row\n    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)\n    output = inv_var / n_cols * (n_cols * dY_W - normed * rowsum_dY_normed)\n    tl.store(dY + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _gemma_rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    output = normed * (W_row + 1.0)\n    tl.store(Y + col_offsets, output, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'BACKWARD_PASS': lambda args: args['BACKWARD_PASS']})\n@triton.jit\ndef _rope_embedding(Q, Q_row_stride, cos, cos_row_stride, sin,\n    sin_row_stride, seqlen, head_dim: 'tl.constexpr', n_heads:\n    'tl.constexpr', BACKWARD_PASS: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    ROPE_GROUP_SIZE = 4\n    row_position = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n    sin1 = tl.load(sin + row_position % seqlen * sin_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    cos1 = tl.load(cos + row_position % seqlen * cos_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    if BACKWARD_PASS:\n        sin1 = -sin1\n    pass\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min(head_start + ROPE_GROUP_SIZE, n_heads)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = (row_position * Q_row_stride + k * head_dim + col_offsets +\n            half_head_dim)\n        Q1 = tl.load(Q + offs_q1, mask=mask, other=0)\n        Q2 = tl.load(Q + offs_q2, mask=mask, other=0)\n        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)\n    pass\n"
    },
    {
      "input": "@triton.jit\ndef _fg_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = e_row * tl.sigmoid(e_row)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    e = e.float()\n    se = 1.0 / (1.0 + torch.exp(-e))\n    f = (se * e).to(dtype)\n    h = f * g\n    df = DW * f\n    dg = DW * g\n    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    se_row = tl.sigmoid(e_row)\n    f_row = se_row * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row * se_row * (1.0 + e_row * (1.0 - se_row))\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n"
    },
    {
      "input": "@triton.jit\ndef rms_norm_fwd_fused(X, Y, W, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n"
    },
    {
      "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n"
    },
    {
      "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n"
    },
    {
      "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n"
    }
  ]
[
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, DO, Delta, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_hz = tl.program_id(1)\n    off_n = tl.arange(0, HEAD_DIM)\n    o = tl.load(O + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    do = tl.load(DO + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "b465630f-ccae-4f90-b94e-644e3f160115"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dv(q, k, g, do, dv, dh, scale, T:\n    'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        b_g = tl.load(g + i_bh * T + i_t * BT + tl.arange(0, BT))\n        b_g_last = tl.load(g + i_bh * T + min(i_t * BT + BT, T) - 1)\n    else:\n        b_g = tl.load(g + i_b * T * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n            )\n        b_g_last = tl.load(g + i_b * T * H + (min(i_t * BT + BT, T) - 1) *\n            H + i_h)\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (NT * K, V), (V, 1\n            ), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh) * tl.exp(-b_g + b_g_last)[:, None]\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_A += tl.dot(b_k, b_q, allow_tf32=False)\n    b_A = b_A * tl.exp(b_g[None, :] - b_g[:, None]) * scale\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0)\n    if HEAD_FIRST:\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv += tl.dot(b_A, b_do)\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "e86038f5-5be6-45e5-8468-4aa96a9f1c05"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(k, v, d, v_new, h, h0, ht, T:\n    'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BC: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_hc = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):\n            if HEAD_FIRST:\n                p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (\n                    i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n                p_d = tl.make_block_ptr(d + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n                p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (\n                    i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n                p_v_new = tl.make_block_ptr(v_new + i_bh * T * V, (T, V), (\n                    V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            else:\n                p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK,\n                    BC), (0, 1))\n                p_d = tl.make_block_ptr(d + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_c * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC,\n                    BV), (1, 0))\n                p_v_new = tl.make_block_ptr(v_new + i_b * T * H * V + i_h *\n                    V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV),\n                    (BC, BV), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_v -= tl.dot(b_d, b_h)\n            tl.store(p_v_new, b_v, boundary_check=(0, 1))\n            b_hc += tl.dot(b_k, b_v, allow_tf32=False)\n        b_h += b_hc\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "47d7aabd-c1b3-40c7-8bd0-6cedfa5ebb1a"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, sliding_window, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_att_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=start_n + offs_n <\n            cur_att_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n +\n            cur_batch_start_index < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] + cur_batch_start_index <\n            cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "70905d83-e9df-4abf-a901-c2a22064977f"
  },
  {
    "input": "@triton.jit\ndef moe_align_block_size_stage4(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    start_idx = tl.load(cumsum_ptr + pid)\n    end_idx = tl.load(cumsum_ptr + pid + 1)\n    for i in range(start_idx, end_idx, block_size):\n        tl.store(expert_ids_ptr + i // block_size, pid)\n    start_idx = pid * tokens_per_thread\n    off_t = pid * num_experts\n    for i in range(start_idx, tl.minimum(start_idx + tokens_per_thread, numel)\n        ):\n        expert_id = tl.load(topk_ids_ptr + i)\n        token_cnt = tl.load(tokens_cnts_ptr + off_t + expert_id)\n        rank_post_pad = token_cnt + tl.load(cumsum_ptr + expert_id)\n        tl.store(sorted_token_ids_ptr + rank_post_pad, i)\n        tl.store(tokens_cnts_ptr + off_t + expert_id, token_cnt + 1)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "852de209-6217-4227-9a62-1c539e5c524e"
  },
  {
    "input": "@triton.autotune(configs=get_cuda_autotune_config(), key=['N_ITER', 'M',\n    'N', 'K'], reset_to_zero=['output_ptr'])\n@triton.jit\ndef fused_unpack_and_reconstruct_kernel_v3(packed_sign_ptr, u_ptr, vt_ptr,\n    output_ptr, N_ITER, M, N, K, n_sign_elements, stride_packed_sign_iter,\n    stride_packed_sign_m, stride_packed_sign_n, stride_u_iter, stride_u_m,\n    stride_u_k, stride_vt_iter, stride_vt_k, stride_vt_n,\n    stride_output_iter, stride_output_m, stride_output_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', num_warps: 'tl.constexpr',\n    num_stages: 'tl.constexpr'):\n    pid_spatial = tl.program_id(axis=0)\n    pid_iter = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid_spatial // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    local_pid = pid_spatial % num_pid_in_group\n    pid_n = local_pid // group_size_m\n    pid_m = first_pid_m + local_pid % group_size_m\n    offsets_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offsets_k = tl.arange(0, BLOCK_SIZE_K)\n    PACKED_BLOCK_SIZE_N: 'tl.constexpr' = BLOCK_SIZE_N // 8\n    offsets_n_packed = pid_n * PACKED_BLOCK_SIZE_N + tl.arange(0,\n        PACKED_BLOCK_SIZE_N)\n    packed_sign_ptrs = packed_sign_ptr + (pid_iter *\n        stride_packed_sign_iter + offsets_m[:, None] * stride_packed_sign_m +\n        offsets_n_packed[None, :] * stride_packed_sign_n)\n    packed_bytes = tl.load(packed_sign_ptrs)\n    bit_offsets = tl.arange(0, 8)\n    packed_bytes = packed_bytes[:, :, None]\n    bits = packed_bytes >> 7 - bit_offsets & 1\n    signs = bits * 2 - 1\n    signs = tl.reshape(signs, (BLOCK_SIZE_M, BLOCK_SIZE_N))\n    u_ptrs = u_ptr + pid_iter * stride_u_iter + offsets_m[:, None] * stride_u_m\n    vt_ptrs = vt_ptr + pid_iter * stride_vt_iter + offsets_n[None, :\n        ] * stride_vt_n\n    iter_acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        u_block_ptrs = u_ptrs + offsets_k[None, :] * stride_u_k\n        vt_block_ptrs = vt_ptrs + offsets_k[:, None] * stride_vt_k\n        u = tl.load(u_block_ptrs, mask=offsets_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        vt = tl.load(vt_block_ptrs, mask=offsets_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        iter_acc += tl.dot(u, vt, out_dtype=tl.float32)\n    output = signs * iter_acc\n    offsets_output_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_output_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    output_ptrs = (output_ptr + pid_iter * stride_output_iter + \n        stride_output_m * offsets_output_m[:, None] + stride_output_n *\n        offsets_output_n[None, :])\n    output_mask = (offsets_output_m[:, None] < M) & (offsets_output_n[None,\n        :] < N)\n    tl.atomic_add(output_ptrs, output, mask=output_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4d961dd0-9dc3-4135-8c58-57b176fa0026"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_bidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y1 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y2 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y3 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y4 + _idx, _x, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "85b93263-5128-40af-a1f6-c0a41bb8402e"
  },
  {
    "input": "@triton.jit\ndef _bwd_intra_kernel(Q, K, V, DO, DQ, DK, DV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK + tl.arange(0, BLOCK)\n    Q_trans_block_ptr = Q + qk_offset + block_offset[None, :] * d + tl.arange(\n        0, d)[:, None]\n    K_block_ptr = K + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[\n        None, :]\n    V_trans_block_ptr = V + v_offset + block_offset[None, :] * e + tl.arange(\n        0, e)[:, None]\n    DQ_block_ptr = DQ + qk_offset + block_offset[:, None] * d + tl.arange(0, d\n        )[None, :]\n    DK_trans_block_ptr = DK + qk_offset + block_offset[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + block_offset[:, None] * e + tl.arange(0, e)[\n        None, :]\n    DO_block_ptr = DO + o_offset + block_offset[:, None] * e + tl.arange(0, e)[\n        None, :]\n    array = tl.arange(0, BLOCK)\n    index = array[:, None] - array[None, :]\n    k = tl.load(K_block_ptr, mask=block_offset[:, None] < n, other=0.0)\n    v_trans = tl.load(V_trans_block_ptr, mask=block_offset[None, :] < n,\n        other=0.0)\n    do = tl.load(DO_block_ptr, mask=block_offset[:, None] < n, other=0.0)\n    q_trans = tl.load(Q_trans_block_ptr, mask=block_offset[None, :] < n,\n        other=0.0)\n    dqk = tl.dot(do, v_trans)\n    dqk = tl.where(index >= 0, dqk, 0)\n    dq_intra = tl.dot(dqk, k)\n    dk_intra_trans = tl.dot(q_trans, dqk)\n    qk_trans = tl.dot(k, q_trans)\n    qk_trans = tl.where(index <= 0, qk_trans, 0)\n    dv_intra = tl.dot(qk_trans, do)\n    dq = dq_intra\n    dk_trans = dk_intra_trans\n    dv = dv_intra\n    tl.store(DQ_block_ptr, dq, mask=block_offset[:, None] < n)\n    tl.store(DK_trans_block_ptr, dk_trans, mask=block_offset[None, :] < n)\n    tl.store(DV_block_ptr, dv, mask=block_offset[:, None] < n)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "888c1d26-73ca-4b72-9f82-34977a9350e1"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_intra_V(q, k, g, dA, dq, dk, dg, T: 'tl.constexpr',\n    K: 'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr', OVERWRITE_DG:\n    'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_k = i_k * BK + tl.arange(0, BK)\n    if i_t * BT + i_i * BC > T:\n        return\n    p_g = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t * BT + \n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * K + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    m_k = o_k < K\n    b_gn = tl.load(p_gn, mask=m_k, other=0.0)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + i_bg * T * K, (T, K), (K, 1), (i_t * BT +\n            i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t *\n            BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kg)\n    b_dq *= tl.exp(b_g - b_gn[None, :])\n    p_kj = tl.max_contiguous(tl.multiple_of(k + i_bg * T * K + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    p_gkj = tl.max_contiguous(tl.multiple_of(g + i_bg * T * K + (i_t * BT +\n        i_i * BC) * K + o_k, BK), BK)\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, mask=m_k, other=0)\n        b_gkj = tl.load(p_gkj, mask=m_k, other=0)\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tl.exp(b_g -\n            b_gkj[None, :]), 0.0)\n        p_kj += K\n        p_gkj += K\n    p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_dq = b_dq + tl.load(p_dq, boundary_check=(0, 1))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bg * T * K, (T, K), (K, 1), (i_t * BT + \n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t * BT + \n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * K + min(i_t * BT +\n        i_i * BC + BC, T) * K - K + o_k, BK), BK)\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    for i_j in range(i_i + 1, min(NC, tl.cdiv(T - i_t * BT, BC))):\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t * BT +\n            i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (BT, T), (1, BT), (i_i *\n            BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(b_dA, b_qg)\n    b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    p_qj = tl.max_contiguous(tl.multiple_of(q + i_bh * T * K + (i_t * BT + \n        i_i * BC) * K + o_k, BK), BK)\n    p_gqj = tl.max_contiguous(tl.multiple_of(g + i_bg * T * K + (i_t * BT +\n        i_i * BC) * K + o_k, BK), BK)\n    m_k = o_k < K\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j * BT)\n        b_qj = tl.load(p_qj, mask=m_k, other=0.0)\n        b_gqj = tl.load(p_gqj, mask=m_k, other=0.0)\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n        p_qj += K\n        p_gqj += K\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT + \n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_dg = tl.make_block_ptr(dg + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n        i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dk = b_dk + tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    if not OVERWRITE_DG:\n        b_dg = b_dg + tl.load(p_dg, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "aea6d4dc-d5c5-49c3-96bd-c0c7aa96ff4b"
  },
  {
    "input": "@triton.jit\ndef silu(input):\n    return input * tl.sigmoid(input)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "c03a18c7-0f6f-4cf0-8a52-039807dc80f2"
  },
  {
    "input": "@triton.jit\ndef gelu_new(x):\n    pi = math.pi\n    a = tl.math.sqrt(2.0 / pi)\n    b = x + 0.044715 * x * x * x\n    return 0.5 * x * (1.0 + tanh(a * b))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "235dff67-7353-4c2b-8114-c18b97ccdb80"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "bda1af79-d59b-47d5-a163-2964fe845301"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x.to(tl.float32))\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "c105ce0d-7d7e-4163-8924-6ccbb454786c"
  },
  {
    "input": "@triton.jit\ndef prepare_qg_kg(q, k, g, qg, kg, s_k_h, scale, K: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_g = g + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_qg = qg + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    p_kg = kg + i_bh * s_k_h + i_c * BT * K + i_k * BK + tl.arange(0, BK)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    last_decay = tl.load(g + i_bh * s_k_h + (i_c * BT + BT - 1) * K + i_k *\n        BK + tl.arange(0, BK))\n    for i in range(BT):\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        _g = tl.load(p_g, mask=mask, other=0)\n        b_q *= tl.exp(_g) * scale\n        b_k *= tl.exp(last_decay - _g)\n        tl.store(p_kg, b_k, mask=mask)\n        tl.store(p_qg, b_q, mask=mask)\n        p_q += K\n        p_g += K\n        p_k += K\n        p_kg += K\n        p_qg += K\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "87ad8baf-f469-4aff-aead-5b352c74498a"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['hdim', 'dstate', 'chunk_size'])\n@triton.jit\ndef _chunk_state_varlen_kernel(x_ptr, b_ptr, dt_ptr, dA_cumsum_ptr,\n    chunk_states_ptr, cu_seqlens_ptr, states_ptr, hdim, dstate, chunk_size,\n    seqlen, nheads_ngroups_ratio, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_b_seqlen, stride_b_head, stride_b_dstate,\n    stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_chunk_states_chunk,\n    stride_chunk_states_head, stride_chunk_states_hdim,\n    stride_chunk_states_dstate, stride_states_batch, stride_states_head,\n    stride_states_hdim, stride_states_dstate, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)\n    pid_c = (end_idx - 1) // chunk_size\n    b_ptr += (pid_c * chunk_size * stride_b_seqlen + pid_h //\n        nheads_ngroups_ratio * stride_b_head)\n    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\n    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head\n    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\n    chunk_states_ptr += (pid_c * stride_chunk_states_chunk + pid_h *\n        stride_chunk_states_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim + offs_k[None, :] *\n        stride_x_seqlen)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate + offs_k[:, None] *\n        stride_b_seqlen)\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cs_last = tl.load(dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) *\n        stride_dA_cs_csize)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    chunk_size_limit = end_idx - pid_c * chunk_size\n    start_idx = tl.load(cu_seqlens_ptr + pid_b)\n    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :\n            ] < chunk_size_limit - k) & (offs_k[None, :] >= start_idx_cur -\n            k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) &\n            (offs_n[None, :] < dstate) & (offs_k[:, None] >= start_idx_cur -\n            k), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size_limit -\n            k, other=0.0)\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0)\n        scale = tl.where((offs_k >= start_idx_cur - k) & (offs_k < \n            chunk_size_limit - k), tl.exp(dA_cs_last - dA_cs_k) * dt_k, 0.0)\n        b *= scale[:, None]\n        b = b\n        acc += tl.dot(x, b)\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    if start_idx < pid_c * chunk_size:\n        chunk_states_ptrs = chunk_states_ptr + (offs_m[:, None] *\n            stride_chunk_states_hdim + offs_n[None, :] *\n            stride_chunk_states_dstate)\n        chunk_states = tl.load(chunk_states_ptrs, mask=(offs_m[:, None] <\n            hdim) & (offs_n[None, :] < dstate), other=0.0)\n        scale = tl.exp(dA_cs_last)\n        acc += chunk_states * scale\n    states = acc\n    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    states_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim + \n        offs_n[None, :] * stride_states_dstate)\n    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\n    tl.store(states_ptrs, states, mask=c_mask)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "4767b825-aa76-4aba-875f-f6d20a6a5191"
  },
  {
    "input": "@triton.jit\ndef softmax_bwd_kernel(p, dp, ds, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_p = tl.make_block_ptr(p + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    p_dp = tl.make_block_ptr(dp + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT, 0), (BT, S), (1, 0))\n    p_ds = tl.make_block_ptr(ds + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT, 0), (BT, S), (1, 0))\n    b_p = tl.load(p_p, boundary_check=(0, 1))\n    b_dp = tl.load(p_dp, boundary_check=(0, 1))\n    b_pp = tl.sum(b_p * b_dp, 1)\n    b_ds = b_p * b_dp - b_p * b_pp[:, None]\n    tl.store(p_ds, b_ds, boundary_check=(0, 1))\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "c118f7f2-a782-46f6-aa05-bf3ef4792cdb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size',\n    'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_bwd_kernel(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m *\n        stride_ddA_cs_csize_m)\n    start = chunk_size - BLOCK_SIZE_N\n    offs_m = chunk_size - (pid_m + 1) * BLOCK_SIZE_M + tl.arange(0,\n        BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + ((start + offs_n[None, :]) * stride_x_seqlen + offs_k[\n        :, None] * stride_x_hdim)\n    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + (start +\n        offs_n[None, :]) * stride_cb_csize_n)\n    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_m[:, None] >= 0) & (offs_k[None, :] < hdim), other=0.0)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m >= 0, other=0.0)\n    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M\n    for start_n in range(lo, hi, BLOCK_SIZE_N):\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit + start_n - start) & (offs_n >= start_n -\n            start), other=0.0)\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n >= start_n - start, other=0.0)\n        acc *= dt_n\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] >= 0) & (offs_n[None, :\n            ] >= start_n - start), other=0.0)\n        acc *= cb\n        dA_cs_n = tl.load(dA_cumsum_ptr + (start - start_n + offs_n) *\n            stride_dA_cs_csize, mask=offs_n >= start_n - start, other=0.0)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] <= start - start_n + offs_n[None, :] - 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=offs_n >=\n            1 + start_n - start)\n        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N\n            ,), dtype=tl.float32), mask=offs_n >= 1 + start_n - start)\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n",
    "category": "Memory Management",
    "subcategory": "memory padding",
    "uuid": "f75a0cf8-59ac-4a96-bca1-2a457400fd86"
  },
  {
    "input": "@triton_autotune(configs=_get_configs(), key=['Z', 'H', 'N'])\n@triton.heuristics({'EVEN_M': lambda args: args['N'] % args['BLOCK_M'] == 0,\n    'EVEN_N': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _dense_bias_to_jagged(Z, H, N, jg_offsets_ptr, jg2_offsets_ptr,\n    dense_bias_ptr, jagged_ptr, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr'):\n    start_m = tl.program_id(0) * BLOCK_M\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    seq_start = tl.load(jg_offsets_ptr + off_z)\n    seq_end = tl.load(jg_offsets_ptr + off_z + 1)\n    seq_len = seq_end - seq_start\n    if start_m >= seq_len:\n        return\n    bias_start = tl.load(jg2_offsets_ptr + off_z\n        ) * H + off_h * seq_len * seq_len\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    mask_m = (offs_m < seq_len)[:, None]\n    off_jg_bias = bias_start + offs_m[:, None] * seq_len + offs_n[None, :]\n    jg_bias_ptrs = jagged_ptr + off_jg_bias\n    off_d_bias = off_hz * N * N + offs_m[:, None] * N + offs_n[None, :]\n    d_bias_ptrs = dense_bias_ptr + off_d_bias\n    for start_n in range(0, seq_len, BLOCK_N):\n        maxk_n = (offs_n < seq_len - start_n)[None, :]\n        d_bias = tl.load(d_bias_ptrs + start_n, mask=mask_m & maxk_n, other=0.0\n            )\n        tl.store(jg_bias_ptrs + start_n, d_bias, mask=mask_m & maxk_n)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "0f697c03-65c3-4569-835d-68b0b5b16593"
  },
  {
    "input": "@triton.jit\ndef update_welford(input, prev_count, prev_mean, prev_var, curr_count, mask:\n    'tl.constexpr'):\n    \"\"\"\n    Updates count, mean, and variance (M2) statistics for Welford's algorithm.\n\n    Args:\n        input: Input used to update statistics.\n            The input must be of the same shape as the mask.\n        prev_count: Previous count statistic to update.\n        prev_mean: Previous mean statistic to update.\n        prev_var: Previous variance (M2) statistic to update.\n        curr_count: Count of elements in current input.\n        mask: Mask indicating which elements should be included in the calculations.\n            The mask must be of the same shape as the input.\n\n    Returns:\n        Updated count, mean, and variance (M2) statistics\n    \"\"\"\n    input = input\n    count = prev_count + curr_count\n    mean = (tl.sum(input) - curr_count * prev_mean) / count\n    deltas = tl.where(mask, (input - mean) * (input - prev_mean), 0.0)\n    var = prev_var + tl.sum(deltas)\n    return count, mean, var\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "d62f5d27-143e-4dc5-ba2c-fa25aa4192fb"
  },
  {
    "input": "@triton.jit\ndef _softmax_kernel_bwd(output_ptr, stride_output_row, grad_ptr,\n    stride_grad_row, input_ptr, stride_input_row, n_cols, block_size:\n    'tl.constexpr'):\n    row_index = tl.program_id(0)\n    input_row_ptr = input_ptr + row_index * stride_input_row\n    grad_row_ptr = grad_ptr + row_index * stride_grad_row\n    col_offsets = tl.arange(0, block_size)\n    rw_mask = col_offsets < n_cols\n    input_row_ptrs = input_row_ptr + col_offsets\n    grad_row_ptrs = grad_row_ptr + col_offsets\n    probs_row = tl.load(input_row_ptrs, mask=rw_mask, other=0)\n    grads_row = tl.load(grad_row_ptrs, mask=rw_mask, other=0)\n    dx = probs_row * grads_row\n    dsm_out = dx - probs_row * tl.sum(dx, axis=0)\n    output_row_ptr = output_ptr + row_index * stride_output_row\n    output_ptrs = output_row_ptr + col_offsets\n    tl.store(output_ptrs, dsm_out, mask=rw_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "05893f6f-984e-4b2d-b631-a96af056c542"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n    BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', start_m, start_n, num_steps, MASK: 'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    offs_n = start_n + tl.arange(0, BLOCK_N2)\n    offs_k = tl.arange(0, HEAD_DIM)\n    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    Di = tl.load(D + offs_m)\n    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n    curr_n = start_n\n    step_n = BLOCK_N2\n    for blk_idx in range(num_steps):\n        kT = tl.load(kT_ptrs)\n        vT = tl.load(vT_ptrs)\n        qk = tl.dot(q, kT)\n        p = tl.math.exp2(qk - m)\n        if MASK:\n            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(mask, p, 0.0)\n        dp = tl.dot(do, vT)\n        ds = p * (dp - Di[:, None])\n        ds = ds\n        dq += tl.dot(ds, tl.trans(kT))\n        curr_n += step_n\n        kT_ptrs += step_n * stride_tok\n        vT_ptrs += step_n * stride_tok\n    return dq\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "d707587c-64df-419b-bfea-6c441247aece"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_V(q, v, z, h, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_zp = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_p *\n            K + i_k * BK,), (BK,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_q = b_q * tl.exp(b_zp[None, :] - b_z)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "0d5f2d2a-4bda-44c7-8914-f24d0ac77511"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "6dd34fe8-3b77-4041-bee5-49b989f70135"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_K(q, k, h, g, o, A, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NG: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bg * T * K, (K, T), (1, K), (i_k * BK,\n            i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bg * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h)\n        b_A += tl.dot(b_q, b_k)\n    p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o = b_o * tl.exp(b_g)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "27a44e5f-d83e-4802-a6d6-6d58cb7abf0d"
  },
  {
    "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    rotary_dim, seqlen_ro, stride_out_batch, stride_out_seqlen,\n    stride_out_nheads, stride_out_headdim, stride_x_batch, stride_x_seqlen,\n    stride_x_nheads, stride_x_headdim, BLOCK_K: 'tl.constexpr',\n    IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr', IS_VARLEN: 'tl.constexpr',\n    INTERLEAVED: 'tl.constexpr', CONJUGATE: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "62be55ba-417d-4ed6-ae72-25e500eb0b9a"
  },
  {
    "input": "@triton.jit\ndef _get_plane_grid_sample_locs_weights(ix, iy, ix0, iy0):\n    return (ix0, iy0, ix0, iy0 + 1, ix0 + 1, iy0, ix0 + 1, iy0 + 1, ix -\n        ix0, iy - iy0)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "31c9300f-b130-43a3-a557-a23183e2f139"
  },
  {
    "input": "@triton.jit\ndef zeroth_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, 1.0, mask=output_row_offset <\n        output_numel)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "0e802785-c99a-41ab-b811-f2f55fcf159d"
  },
  {
    "input": "@triton.jit\ndef _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    s = 0.7978845608028654\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (triton_tanh(s * e_row * (1.0 + 0.044715 * e_row *\n        e_row)) + 1.0)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "d583a58b-7d8c-4380-9b39-22ba7f11e66c"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_inter(s, z, ss, doo, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n    b_sp = tl.zeros([BS], dtype=tl.float32)\n    b_zp = tl.full([BS], float('inf'), dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_zc = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (s_s_d,), (i_t *\n            BT * S + i_m * BS,), (BS,), (0,))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        p_doo = tl.make_block_ptr(doo + i_bh * s_s_h, (T, S), (s_s_t, s_s_d\n            ), (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n        b_doo = tl.exp(b_s - b_zp[None, :]) * b_sp[None, :]\n        tl.store(p_doo, b_doo, boundary_check=(0, 1))\n        b_sp = b_sp * tl.exp(b_zc - b_zp) + tl.sum(b_ss * tl.exp(b_zc[None,\n            :] - b_z), 0)\n        b_zp = b_zc\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "12d1cd6a-427d-456f-a658-847164bdbf23"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_backward_kernel(dY_ptr, dY_row_stride, dX_ptr, dX_row_stride,\n    X_ptr, X_row_stride, X_dtype: 'tl.constexpr', W_ptr, W_row_stride,\n    RSTD_ptr, RSTD_row_stride, dW_ptr, dW_row_stride, n_rows, n_cols,\n    offset, rows_per_program: 'tl.constexpr', casting_mode: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    dx = (1 / RMS) * [dy * (w + offset - (1 / N) * (1 / RMS^2) * ((dy * (w + offset)) dot x) * x]. * means element-wise multiplication, whileas dot means dot product\n    dw = sum(dy * (x / RMS)). summation over BxT dimension\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dW_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    dY_ptr += row_start * dY_row_stride\n    dX_ptr += row_start * dX_row_stride\n    X_ptr += row_start * X_row_stride\n    RSTD_ptr += row_start\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0.0)\n    W_row = W_row + offset\n    for _ in range(row_start, row_end):\n        dY_row = tl.load(dY_ptr + col_offsets, mask=mask, other=0.0)\n        X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0.0)\n        rstd_row = tl.load(RSTD_ptr)\n        X_row = X_row\n        if casting_mode == _CASTING_MODE_LLAMA:\n            m = dY_row * W_row\n        elif casting_mode == _CASTING_MODE_GEMMA:\n            dY_row = dY_row\n            m = dY_row * W_row\n        else:\n            m = dY_row * W_row\n        dX_row = rstd_row * m\n        dX_row += rstd_row * (-(1 / n_cols) * rstd_row * rstd_row * tl.sum(\n            m * X_row, axis=0) * X_row)\n        if casting_mode == _CASTING_MODE_LLAMA:\n            dW_row += dY_row * (X_row * rstd_row)\n        else:\n            dW_row += dY_row * (X_row * rstd_row)\n        tl.store(dX_ptr + col_offsets, dX_row, mask=mask)\n        dY_ptr += dY_row_stride\n        dX_ptr += dX_row_stride\n        X_ptr += X_row_stride\n        RSTD_ptr += RSTD_row_stride\n    tl.store(dW_ptr + row_block_id * dW_row_stride + col_offsets, dW_row,\n        mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "abbf6883-ae48-4653-a7fc-f389a9d2423e"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef act_func_backward_kernel(output_grad_pointer, input_pointer,\n    input_grad_pointer, size, drop_p, seed, param, act_func: 'tl.constexpr',\n    dropout: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of an activation function.\n\n    Args:\n        output_grad_pointer: Pointer to the activation's output gradients.\n            The output gradients must be of shape [size].\n        input_pointer: Pointer to the activation's input.\n            The input must be of shape [size].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function whose gradient is calculated.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input = tl.load(input_pointer + offset, mask=mask)\n    tl.store(input_grad_pointer + offset, apply_act_func_grad(output_grad,\n        input, drop_p, seed, offset, param, act_func, dropout), mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a247d862-c3d3-4408-870a-87806b779a80"
  },
  {
    "input": "@triton.jit\ndef _contiguous_block(input_tiles, next_id, pid_n, input, other, output, K,\n    N, stride_input_m, stride_input_k, stride_other_b, stride_other_k,\n    stride_other_n, stride_output_m, stride_output_n, out_dtype:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', TILE_M: 'tl.constexpr',\n    TILE_N: 'tl.constexpr', TILE_K: 'tl.constexpr', EVEN_K: 'tl.constexpr',\n    EVEN_N: 'tl.constexpr', EQUAL_K: 'tl.constexpr'):\n    start_off = tl.load(input_tiles + 5 * next_id + 2)\n    type_id = tl.load(input_tiles + 5 * next_id + 1)\n    if EQUAL_K:\n        _reg_matmul(pid_n, type_id, start_off, input, other, output, N,\n            stride_input_m, stride_input_k, stride_other_b, stride_other_k,\n            stride_other_n, stride_output_m, stride_output_n, out_dtype=\n            out_dtype, BLOCK_SIZE=BLOCK_SIZE, TILE_M=TILE_M, TILE_N=TILE_N,\n            TILE_K=TILE_K, EVEN_N=EVEN_N)\n    else:\n        for i in range(0, BLOCK_SIZE):\n            _general_matmul(pid_n, start_off + i * TILE_M, start_off + (i +\n                1) * TILE_M, input, other + type_id * stride_other_b,\n                output, K, N, stride_input_m, stride_input_k,\n                stride_other_k, stride_other_n, stride_output_m,\n                stride_output_n, out_dtype=out_dtype, MASK_M=True, EVEN_K=\n                EVEN_K, EVEN_N=EVEN_N, TILE_M=TILE_M, TILE_N=TILE_N, TILE_K\n                =TILE_K)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "24204837-76e2-4df5-85c4-206479a10b22"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS,\n    NUM_STAGES)], key=['M'])\n@triton.jit\ndef triton_jagged_mean_kernel_simple_fused_sum_then_buffer(input_ptr_values,\n    input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_b), tl.load(\n        input_ptr_offsets + (pid_b + 1))\n    ragged_len = ragged_end - ragged_start\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n        buffer += tl.sum(input, axis=0)\n    buffer_view = buffer.reshape((BLOCK_SIZE_M,))\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n    output_offsets = offsets_m + pid_b * M\n    output_mask = output_offsets < M * (pid_b + 1)\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "16a36cc1-354a-420e-81a1-2ce6a7b1a7d6"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n    BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', start_m, start_n, num_steps, MASK: 'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    offs_n = start_n + tl.arange(0, BLOCK_N2)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    Di = tl.load(D + offs_m)\n    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n    curr_n = start_n\n    step_n = BLOCK_N2\n    for blk_idx in range(num_steps):\n        kT = tl.load(kT_ptrs)\n        vT = tl.load(vT_ptrs)\n        qk = tl.dot(q, kT)\n        p = tl.math.exp2(qk - m)\n        if MASK:\n            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(mask, p, 0.0)\n        dp = tl.dot(do, vT)\n        ds = p * (dp - Di[:, None])\n        ds = ds\n        dq += tl.dot(ds, tl.trans(kT))\n        curr_n += step_n\n        kT_ptrs += step_n * stride_tok\n        vT_ptrs += step_n * stride_tok\n    return dq\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "8a46f984-aca6-4ce6-89f3-97a5c8c4169b"
  },
  {
    "input": "@triton.jit\ndef _hash(x):\n    x = (x >> 16 ^ x) * 73244475\n    x = (x >> 16 ^ x) * 73244475\n    x = x >> 16 ^ x\n    return x\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "438391fa-a55a-450c-8419-1e8db9088ea7"
  },
  {
    "input": "@triton_autotune(configs=_get_bmm_configs(), key=['M', 'N',\n    'AUTOTUNE_MAX_SEQ_LEN'])\n@triton.jit\ndef _jagged_jagged_bmm_reduce_sum(seq_offsets, JaggedA, JaggedB, Out,\n    ReduceOut, M, N, AUTOTUNE_MAX_SEQ_LEN, stride_ak, stride_bk, stride_ob,\n    stride_om, stride_on, stride_orb, stride_orn, REDUCE_JAGGEDB:\n    'tl.constexpr', ALLOW_TF32: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr'):\n    \"\"\"\n    Computing bmm Out = Jagged x Jagged\n    K is the jagged dimension\n    JaggedA has shape (sum_B(K_i), M), JaggedB has shape (sum_B(K_i), N), and Out has shape (B, M, N)\n    \"\"\"\n    off_b = tl.program_id(0)\n    off_m = tl.program_id(1)\n    off_n = tl.program_id(2)\n    seq_start = tl.load(seq_offsets + off_b)\n    seq_end = tl.load(seq_offsets + off_b + 1)\n    seq_len = seq_end - seq_start\n    start_m = off_m * BLOCK_M\n    start_n = off_n * BLOCK_N\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    Out += off_b * stride_ob\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    out_ptrs = Out + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on\n    if REDUCE_JAGGEDB:\n        out_reduce_ptrs = ReduceOut + off_b * stride_orb + offs_n * stride_orn\n        acc_reduce = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if seq_len == 0:\n        out = accumulator\n        tl.store(out_ptrs, out, mask=(offs_m[:, None] < M) & (offs_n[None,\n            :] < N))\n        if REDUCE_JAGGEDB:\n            if off_m == 0:\n                tl.store(out_reduce_ptrs, acc_reduce, mask=offs_n < N)\n        return\n    JaggedA += seq_start * stride_ak\n    JaggedB += seq_start * stride_bk\n    offs_k = tl.arange(0, BLOCK_K)\n    jg_a_ptrs = JaggedA + offs_k[None, :] * stride_ak + offs_m[:, None]\n    jg_b_ptrs = JaggedB + offs_k[:, None] * stride_bk + offs_n[None, :]\n    for k in range(0, seq_len, BLOCK_K):\n        jg_a = tl.load(jg_a_ptrs, mask=offs_m[:, None] < M and (k + offs_k)\n            [None, :] < seq_len, other=0.0)\n        jg_b = tl.load(jg_b_ptrs, mask=offs_n[None, :] < N and (k + offs_k)\n            [:, None] < seq_len, other=0.0)\n        accumulator += tl.dot(jg_a, jg_b, allow_tf32=ALLOW_TF32)\n        if REDUCE_JAGGEDB:\n            if off_m == 0:\n                acc_reduce += tl.sum(jg_b, axis=0)\n        jg_a_ptrs += BLOCK_K * stride_ak\n        jg_b_ptrs += BLOCK_K * stride_bk\n    out = accumulator\n    tl.store(out_ptrs, out, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n    if REDUCE_JAGGEDB:\n        if off_m == 0:\n            tl.store(out_reduce_ptrs, acc_reduce, mask=offs_n < N)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b1e46d60-2f97-422c-bdf6-05999e228d15"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['hdim', 'dstate', 'chunk_size'])\n@triton.jit\ndef _chunk_scan_bwd_dstates_kernel(dout_ptr, c_ptr, dprev_states_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, hdim, dstate, chunk_size, batch, seqlen,\n    nchunks, nheads_ngroups_ratio, stride_dout_batch, stride_dout_seqlen,\n    stride_dout_head, stride_dout_hdim, stride_c_batch, stride_c_seqlen,\n    stride_c_head, stride_c_dstate, stride_dprev_states_batch,\n    stride_dprev_states_chunk, stride_dprev_states_head,\n    stride_dprev_states_hdim, stride_dprev_states_dstate,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    c_ptr += (pid_b * stride_c_batch + pid_c * chunk_size * stride_c_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_c_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_hdim + offs_k[\n        None, :] * stride_dout_seqlen)\n    c_ptrs = c_ptr + (offs_n[None, :] * stride_c_dstate + offs_k[:, None] *\n        stride_c_seqlen)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[\n            None, :] < chunk_size_limit - k), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k,\n            other=0.0)\n        if not HAS_SEQ_IDX:\n            scale_k = tl.exp(dA_cs_k)\n        else:\n            seq_idx_k = tl.load(seq_idx_ptrs, mask=offs_k < \n                chunk_size_limit - k, other=-1)\n            scale_k = tl.where(seq_idx_k == seq_idx_prev, tl.exp(dA_cs_k), 0.0)\n        dout = dout * scale_k\n        c = tl.load(c_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) &\n            (offs_n[None, :] < dstate), other=0.0)\n        acc += tl.dot(dout, c)\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n        if HAS_SEQ_IDX:\n            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\n    out = acc\n    dprev_states_ptr += (pid_b * stride_dprev_states_batch + pid_c *\n        stride_dprev_states_chunk + pid_h * stride_dprev_states_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dprev_states_ptrs = dprev_states_ptr + (offs_m[:, None] *\n        stride_dprev_states_hdim + offs_n[None, :] * stride_dprev_states_dstate\n        )\n    tl.store(dprev_states_ptrs, out, mask=(offs_m[:, None] < hdim) & (\n        offs_n[None, :] < dstate))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "4f775bf3-b3c4-46f1-88f2-f8c1fa6cda35"
  },
  {
    "input": "@triton.jit\ndef bwd_sequential_scan(grad_output, v, f, h, B, L, C, BLOCK_M: 'tl.constexpr'\n    ):\n    offset_b = tl.program_id(0)\n    if offset_b >= B:\n        return\n    offset_n = tl.program_id(1)\n    ptr = tl.arange(0, BLOCK_M) + offset_b * L * C + (L - 1\n        ) * C + offset_n * BLOCK_M\n    grad_h = tl.zeros([BLOCK_M], dtype=tl.float32)\n    for time_step in range(L - 1, -1, -1):\n        grad = tl.load(grad_output + ptr)\n        grad_h += grad\n        decay = tl.load(f + ptr)\n        input = tl.load(v + ptr)\n        grad_v = (1 - decay) * grad_h\n        tl.store(v + ptr, grad_v)\n        hidden_state = tl.load(h + ptr - C, mask=ptr >= offset_b * L * C +\n            C, other=0.0)\n        grad_f = grad_h * (hidden_state - input)\n        tl.store(f + ptr, grad_f)\n        grad_h *= decay\n        ptr -= C\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "7689a9f8-075f-43f3-9047-5595554ed7c2"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=offs_h[:, None] <\n        head_num, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0)[:, None]\n    q_src_data = src_data / data_scale\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=offs_h[:, None] < head_num)\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "d3e75a09-f993-45b6-9764-4659e0d2d648"
  },
  {
    "input": "@triton.autotune(configs=_get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef _gemm_activation_kernel(a_ptr, b_ptr, c_ptr, M: 'tl.constexpr', N:\n    'tl.constexpr', K: 'tl.constexpr', stride_am: 'tl.constexpr', stride_ak:\n    'tl.constexpr', stride_bk: 'tl.constexpr', stride_bn: 'tl.constexpr',\n    stride_cm: 'tl.constexpr', stride_cn: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', activation: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a_data = tl.load(a_ptrs, mask=offs_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        b_data = tl.load(b_ptrs, mask=offs_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a_data, b_data)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    acc = activation(acc)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] *\n        stride_cn)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "0bc0bf1c-804a-4c46-8f03-afad86004b92"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, HEAD_DIM: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', STAGE: 'tl.constexpr'):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    v_order: 'tl.constexpr' = (1, 0)\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, HEAD_DIM), order=v_order)\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(HEAD_DIM,\n        N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape\n        =(HEAD_DIM, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    q = tl.load(Q_block_ptr)\n    if STAGE & 1:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 4 -\n            STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5)\n    if STAGE & 2:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 2,\n            offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5)\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "71222be1-1e23-4148-a7c9-fc46e8e63295"
  },
  {
    "input": "@triton.jit\ndef kernel_vector_addition(a_ptr, b_ptr, out_ptr, num_elems: 'tl.constexpr',\n    block_size: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * block_size\n    thread_offsets = block_start + tl.arange(0, block_size)\n    mask = thread_offsets < num_elems\n    a_pointers = tl.load(a_ptr + thread_offsets, mask=mask)\n    b_pointers = tl.load(b_ptr + thread_offsets, mask=mask)\n    res = a_pointers + b_pointers\n    tl.store(out_ptr + thread_offsets, res, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4e09654c-4f9f-425e-8bd1-3b3e2d4cb53f"
  },
  {
    "input": "@triton.jit\ndef _ragged_hstu_attn_fwd_one_block(start_n, seq_len, offs_m, offs_n,\n    mask_m, mask_n, q, K_block_ptr, V_block_ptr, n_targets, ts_1_ptrs, ts_0,\n    TW, PW, alpha, MAX_SEQ_LEN, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, bias_ptrs, MAX_ATTN_LEN: 'tl.constexpr',\n    INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL: 'tl.constexpr', BUCKET_FN:\n    'tl.constexpr', ATTN_BIAS_TYPE: 'tl.constexpr', USE_TIME_BIAS:\n    'tl.constexpr', USE_POS_BIAS: 'tl.constexpr', HAS_MAX_POS_IND:\n    'tl.constexpr', HAS_MULTIPLE_TARGETS: 'tl.constexpr',\n    CONTEXTUAL_SEQ_LEN: 'tl.constexpr', IS_DELTA_Q: 'tl.constexpr',\n    ALLOW_TF32: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    k = tl.load(K_block_ptr, boundary_check=(1,), padding_option='zero')\n    qk = tl.dot(q, k, allow_tf32=ALLOW_TF32) * alpha\n    invalid_mask = offs_m[:, None] == offs_n[None, :]\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            offs_m = tl.where(offs_m < seq_len - n_targets, offs_m, seq_len -\n                n_targets)\n            offs_n = tl.where(offs_n < seq_len - n_targets, offs_n, seq_len -\n                n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            offs_m = tl.where(offs_m > n_targets - 1, offs_m, n_targets - 1)\n            offs_n = tl.where(offs_n > n_targets - 1, offs_n, n_targets - 1)\n    offs_n_minus_m = offs_n[None, :] - offs_m[:, None]\n    if MAX_ATTN_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            invalid_mask = (invalid_mask or offs_n_minus_m < 0 and \n                offs_n_minus_m >= -MAX_ATTN_LEN)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            invalid_mask = (invalid_mask or offs_n_minus_m > 0 and \n                offs_n_minus_m <= MAX_ATTN_LEN)\n    elif INVALID_MASK_TYPE == 'lower_triangular':\n        invalid_mask = invalid_mask or offs_n_minus_m < 0\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        invalid_mask = invalid_mask or offs_n_minus_m > 0\n    if CONTEXTUAL_SEQ_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            row_filter = offs_m < CONTEXTUAL_SEQ_LEN\n            if HAS_MULTIPLE_TARGETS:\n                col_filter = offs_n < seq_len - n_targets\n            else:\n                col_filter = offs_n < seq_len\n            invalid_mask = invalid_mask or row_filter[:, None] and col_filter[\n                None, :]\n    if ATTN_BIAS_TYPE == 'fused':\n        attn_bias = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if USE_TIME_BIAS:\n            if CAUSAL:\n                ts_1 = tl.load(ts_1_ptrs + start_n, mask=mask_n)\n            else:\n                ts_1 = tl.load(ts_1_ptrs + start_n + 1, mask=mask_n)\n            ts = ts_0[:, None] - ts_1[None, :]\n            ts = ts + time_delta\n            ts = tl.where(ts > 1e-06, ts, 1e-06)\n            ts = ts * (1.0 / time_bucket_incr)\n            if BUCKET_FN == 'log':\n                ts = tl.log(ts)\n            elif BUCKET_FN == 'sqrt':\n                ts = tl.sqrt(ts)\n            ts = ts * (1.0 / time_bucket_div)\n            ts = ts\n            ts = tl.where(ts > 0, ts, 0)\n            ts = tl.where(ts < num_buckets, ts, num_buckets)\n            ts_w = tl.load(TW + ts, mask=mask_m[:, None] and mask_n[None, :])\n            attn_bias = attn_bias + ts_w\n        if USE_POS_BIAS:\n            if HAS_MAX_POS_IND:\n                offs_pos_w = offs_n_minus_m + max_pos_ind - 1\n                offs_pos_w = tl.where(offs_pos_w > 0, offs_pos_w, 0)\n                offs_pos_w = tl.where(offs_pos_w < 2 * max_pos_ind - 2,\n                    offs_pos_w, 2 * max_pos_ind - 2)\n            else:\n                offs_pos_w = offs_n_minus_m + MAX_SEQ_LEN - 1\n            pos_w = tl.load(PW + offs_pos_w, mask=mask_m[:, None] and\n                mask_n[None, :])\n            attn_bias = attn_bias + pos_w\n        qk = qk + attn_bias\n    elif ATTN_BIAS_TYPE == 'separate':\n        attn_bias = tl.load(bias_ptrs + start_n, mask=mask_m[:, None] &\n            mask_n[None, :], other=0.0)\n        qk = qk + attn_bias\n    silu = fast_dividef(qk, 1.0 + tl.exp(-qk)) * (1.0 / MAX_SEQ_LEN)\n    silu = tl.where(invalid_mask, silu, 0)\n    v = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero')\n    silu = silu\n    return tl.dot(silu, v, allow_tf32=ALLOW_TF32)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "8a933216-e65e-4eda-87d7-5aeb68fb37fc"
  },
  {
    "input": "@triton.jit\ndef _weighted_layer_norm_bwd_dx(DX, DY, DW, DB, X, W, B, Mean, Rstd,\n    stride_dx, stride_dy, stride_x, D, eps, IS_SWISH: 'tl.constexpr', N,\n    BLOCK_D: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    tile_num = tl.num_programs(0)\n    rows_per_tile = N // tile_num\n    if pid < N % tile_num:\n        rows_per_tile += 1\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    row = pid\n    for idx in range(rows_per_tile):\n        x_ptrs = X + row * stride_x\n        dy_ptrs = DY + row * stride_dy\n        dx_ptrs = DX + row * stride_dx\n        dw_ptrs = DW + pid * D\n        dw_ptrs += cols\n        db_ptrs = DB + pid * D\n        db_ptrs += cols\n        x = tl.load(x_ptrs + cols, mask=mask, other=0)\n        dy = tl.load(dy_ptrs + cols, mask=mask, other=0)\n        mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd\n        w = tl.load(W + cols, mask=mask)\n        wdy = w * dy\n        xhat = tl.where(mask, xhat, 0.0)\n        wdy = tl.where(mask, wdy, 0.0)\n        sigmoid_layer_norm = None\n        if IS_SWISH:\n            b = tl.load(B + cols, mask=mask)\n            sigmoid_layer_norm = tl.sigmoid(xhat * w + b)\n            sigmoid_layer_norm = tl.where(mask, sigmoid_layer_norm, 0.0)\n            x_ = wdy * x * sigmoid_layer_norm * (1 - sigmoid_layer_norm)\n            x_ = tl.where(mask, x_, 0.0)\n            c1 = tl.sum(xhat * x_, axis=0) / D\n            c2 = tl.sum(x_, axis=0) / D\n            dx = (x_ - (xhat * c1 + c2)) * rstd\n            dx = dy * sigmoid_layer_norm + dx\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / D\n            c2 = tl.sum(wdy, axis=0) / D\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        tl.store(dx_ptrs + cols, dx, mask=mask)\n        if IS_SWISH:\n            partial_dw = dy * x * xhat * sigmoid_layer_norm * (1 -\n                sigmoid_layer_norm)\n            partial_db = dy * x * sigmoid_layer_norm * (1 - sigmoid_layer_norm)\n        else:\n            partial_dw = dy * xhat\n            partial_db = dy\n        if idx > 0:\n            partial_dw += tl.load(dw_ptrs, mask=mask)\n            partial_db += tl.load(db_ptrs, mask=mask)\n        tl.store(dw_ptrs, partial_dw, mask=mask)\n        tl.store(db_ptrs, partial_db, mask=mask)\n        row += tile_num\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "dcb748a8-ba13-4004-8e5c-ccc59b09aba0"
  },
  {
    "input": "@triton.jit\ndef _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0).cast(tl.int64)\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "091243c9-18a3-46fa-8f03-d42b1bbc484b"
  },
  {
    "input": "@triton.jit\ndef _depth_lin(near, far, n, step):\n    frac_step = step / (n - 1)\n    return (far - near) * frac_step + near\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "c998fc24-5154-4c11-aaca-0fd5056e1346"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for\n    num_warps in [1, 2, 4, 8, 16, 32]], key=['num_batches', 'num_frames',\n    'fft_size'])\n@triton.jit\ndef complex_mul_conjugate_kernel(a_real_ptr, b_real_ptr, a_imag_ptr,\n    b_imag_ptr, output1_ptr, output2_ptr, num_batches, num_frames, fft_size,\n    BLOCK_SIZE: 'tl.constexpr'):\n    batch_idx = tl.program_id(0)\n    if batch_idx >= num_batches:\n        return\n    fft_idx = tl.arange(0, BLOCK_SIZE)\n    fft_mask = fft_idx < fft_size\n    batch_by_fft = batch_idx * fft_size\n    b_real_val = tl.load(b_real_ptr + batch_by_fft + fft_idx, mask=fft_mask)\n    b_imag_val = tl.load(b_imag_ptr + batch_by_fft + fft_idx, mask=fft_mask)\n    for frame_idx in range(num_frames):\n        global_idx = num_frames * batch_by_fft + frame_idx * fft_size + fft_idx\n        a_real_val = tl.load(a_real_ptr + global_idx, mask=fft_mask)\n        a_imag_val = tl.load(a_imag_ptr + global_idx, mask=fft_mask)\n        result1 = a_real_val * b_real_val + a_imag_val * b_imag_val\n        result2 = a_imag_val * b_real_val - a_real_val * b_imag_val\n        tl.store(output1_ptr + global_idx, result1, mask=fft_mask)\n        tl.store(output2_ptr + global_idx, result2, mask=fft_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "6138540a-872e-4521-ad14-2f7a5acad2d1"
  },
  {
    "input": "@triton.jit\ndef softmax(input, log: 'tl.constexpr'):\n    \"\"\"\n    Normalizes the input using softmax along the last dimension.\n\n    Args:\n        input: Input to normalize.\n            The input must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        log: Flag for indicating if the log of softmax should be taken.\n\n    Returns:\n        Input normalized by softmax.\n    \"\"\"\n    input = input\n    input = input - tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    if log:\n        output = input - tl.log(denominator)\n    else:\n        output = numerator / denominator\n    return output\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "4211000c-5c2a-4be6-a9f6-93e3c435c8e9"
  },
  {
    "input": "@triton.jit\ndef rope_kernel_fw(input_ptr, in_seq_len_stride, in_batch_stride,\n    output_ptr, cos_ptr, sin_ptr, cos_stride, sin_stride, seq_len, head_dim,\n    BLOCK_SIZE: 'tl.constexpr', BATCH_NUM: 'tl.constexpr'):\n    pid_seq = tl.program_id(axis=0)\n    pid_head = tl.program_id(axis=1)\n    head_dim_offset = tl.arange(0, BLOCK_SIZE)\n    head_dim_mid = head_dim // 2\n    mask = head_dim_offset < head_dim_mid\n    cos_offset = pid_seq % seq_len * cos_stride + head_dim_offset\n    sin_offset = pid_seq % seq_len * sin_stride + head_dim_offset\n    cos = tl.load(cos_ptr + cos_offset, mask=mask, other=0.0)\n    sin = tl.load(sin_ptr + sin_offset, mask=mask, other=0.0)\n    for batch_idx in tl.static_range(0, BATCH_NUM):\n        x1_offset = (pid_seq * in_seq_len_stride + batch_idx *\n            in_batch_stride + pid_head * head_dim + head_dim_offset)\n        x2_offset = (pid_seq * in_seq_len_stride + batch_idx *\n            in_batch_stride + pid_head * head_dim + head_dim_mid +\n            head_dim_offset)\n        x1 = tl.load(input_ptr + x1_offset, mask=mask, other=0.0)\n        x2 = tl.load(input_ptr + x2_offset, mask=mask, other=0.0)\n        y1 = x1 * cos - x2 * sin\n        y2 = x1 * sin + x2 * cos\n        tl.store(output_ptr + x1_offset, y1, mask=mask)\n        tl.store(output_ptr + x2_offset, y2, mask=mask)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "561327a9-7f26-4591-900f-113ef2f19766"
  },
  {
    "input": "@triton.autotune(DEFAULT_DEQUANT_CONFIGS, key=['numels'])\n@triton.jit\ndef dequant_kernel_248(g_idx_ptr, scales_ptr, qweight_ptr, qzeros_ptr,\n    out_ptr, numels, maxq: 'tl.constexpr', bits: 'tl.constexpr',\n    outfeatures: 'tl.constexpr', num_groups: 'tl.constexpr', X_BLOCK:\n    'tl.constexpr'=1024):\n    xoffset = tl.program_id(0) * X_BLOCK\n    x_index = xoffset + tl.arange(0, X_BLOCK)\n    xmask = x_index < numels\n    row_idx = x_index // outfeatures\n    col_idx = x_index % outfeatures\n    elements_per_feature: 'tl.constexpr' = 32 // bits\n    g_idx = tl.load(g_idx_ptr + row_idx, None, eviction_policy='evict_last')\n    qweights = tl.load(qweight_ptr + (col_idx + outfeatures * (row_idx //\n        elements_per_feature)), None)\n    wf_weights = row_idx % elements_per_feature * bits\n    wf_zeros = col_idx % elements_per_feature * bits\n    tmp1 = g_idx + num_groups\n    tmp2 = g_idx < 0\n    tl.device_assert(g_idx >= 0, 'index out of bounds: 0 <= tmp0 < 0')\n    groups = tl.where(tmp2, tmp1, g_idx)\n    scales = tl.load(scales_ptr + (col_idx + outfeatures * groups), None)\n    weights = qweights >> wf_weights\n    weights = weights & maxq\n    qzero_ncols: 'tl.constexpr' = outfeatures // elements_per_feature\n    qzeros = tl.load(qzeros_ptr + (qzero_ncols * groups + col_idx //\n        elements_per_feature), None, eviction_policy='evict_last')\n    zeros = qzeros >> wf_zeros\n    zeros = zeros & maxq\n    if getattr(qzeros_ptr, 'offset', None) is not None:\n        zeros = zeros + qzeros_ptr.offset\n    weights = weights - zeros\n    weights = weights\n    weights = scales * weights\n    tl.store(out_ptr + x_index, weights, mask=xmask)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "118968ed-fd1b-41af-a94d-fe3c5c0094ad"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage2(B_Seqlen, Mid_O, Mid_O_LogExpSum, Out,\n    stride_mid_ob, stride_mid_oh, stride_mid_os, stride_mid_od,\n    stride_mid_o_eb, stride_mid_o_eh, stride_mid_o_es, stride_obs,\n    stride_oh, stride_od, head_dim, BLOCK_SEQ: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    block_n_size = tl.where(cur_batch_seq_len <= 0, 0, cur_batch_seq_len +\n        BLOCK_SEQ - 1) // BLOCK_SEQ\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    offs_v = cur_batch * stride_mid_ob + cur_head * stride_mid_oh + offs_d\n    offs_logic = cur_batch * stride_mid_o_eb + cur_head * stride_mid_o_eh\n    for block_seq_n in range(0, block_n_size, 1):\n        tv = tl.load(Mid_O + offs_v + block_seq_n * stride_mid_os, mask=\n            offs_d < head_dim, other=0.0)\n        tlogic = tl.load(Mid_O_LogExpSum + offs_logic + block_seq_n)\n        new_max_logic = tl.maximum(tlogic, max_logic)\n        old_scale = tl.exp(max_logic - new_max_logic)\n        acc *= old_scale\n        exp_logic = tl.exp(tlogic - new_max_logic)\n        acc += exp_logic * tv\n        sum_exp = sum_exp * old_scale + exp_logic\n        max_logic = new_max_logic\n    tl.store(Out + cur_batch * stride_obs + cur_head * stride_oh + offs_d, \n        acc / sum_exp, mask=offs_d < head_dim)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "bd0d4775-c0b9-4ccb-ad6c-ed4d77ae934b"
  },
  {
    "input": "@triton.jit\ndef gelu_grad(input):\n    \"\"\"\n    Calculates the gradient of GELU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of GELU.\n    \"\"\"\n    cdf = 0.5 * (1 + tl.math.erf(0.707106781 * input))\n    cdf_grad = 0.39894228 * tl.exp(-0.5 * input * input)\n    return cdf_grad * input + cdf\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5ce9d8ed-bbaf-4b55-88e8-88363940ff89"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att1_int8(Q, K, K_scale, sm_scale, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_ksbs, stride_ksh, stride_ksd, att_stride_h, att_stride_bs,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        off_ks = k_loc[:, None] * stride_ksbs + cur_head * stride_ksh\n        k_scale = tl.load(K_scale + off_ks, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k * k_scale, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "780d38c1-6819-4c3b-992b-5ea55d110087"
  },
  {
    "input": "@triton.jit\ndef logaddexp(a, b):\n    tmp = a - b\n    return tl.where(tmp > 0, tl.log(tl.exp(b - a) + 1) + a, tl.log(tl.exp(a -\n        b) + 1) + b)\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "f12ebecf-dee6-4dab-ba85-73b80e28b1f6"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "3d3e0712-ee0d-4969-8f0f-bcae201283a6"
  },
  {
    "input": "@triton.jit\ndef gelu_approx(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit, with tanh approximation\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1.0 + tanh(_sqrt2pi * x * (1.0 + 0.044715 * x * x)))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "f5be040c-207c-465b-bd5e-bbf6034cf4ed"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, Out, S, b: 'tl.constexpr', h: 'tl.constexpr', n:\n    'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', BLOCK_MODEL: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_h = off_bh % h\n    off_e = tl.program_id(1)\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    e_offset = off_e * BLOCK_MODEL\n    Q_block_ptr = Q + qk_offset + tl.arange(0, d)[None, :]\n    K_trans_block_ptr = K + qk_offset + tl.arange(0, d)[:, None]\n    V_block_ptr = V + v_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n    O_block_ptr = Out + o_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :\n        ]\n    S_block_ptr = S + off_h\n    s = tl.load(S_block_ptr)\n    off_block = tl.arange(0, BLOCK)\n    q_decay = tl.exp(-s * off_block[:, None])\n    k_trans_decay = tl.exp(-s * (BLOCK - off_block[None, :]))\n    block_decay = tl.exp(-s * BLOCK)\n    index = off_block[:, None] - off_block[None, :]\n    s_index = s * index\n    s_index = tl.where(index >= 0, -s_index, float('-inf'))\n    diag_decay = tl.exp(s_index)\n    kv = tl.zeros([d, BLOCK_MODEL], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        q = tl.load(Q_block_ptr + off_block[:, None] * d, mask=off_block[:,\n            None] < n, other=0.0)\n        k_trans = tl.load(K_trans_block_ptr + off_block[None, :] * d, mask=\n            off_block[None, :] < n, other=0.0)\n        v = tl.load(V_block_ptr + off_block[:, None] * e, mask=off_block[:,\n            None] < n, other=0.0)\n        qk = tl.dot(q, k_trans) * diag_decay\n        o_intra = tl.dot(qk, v)\n        o_inter = tl.dot(q, kv) * q_decay\n        o = o_intra + o_inter\n        tl.store(O_block_ptr + off_block[:, None] * e, o, mask=off_block[:,\n            None] < n)\n        kv = block_decay * kv + tl.dot(k_trans * k_trans_decay, v)\n        off_block += BLOCK\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "f2cf13c9-f15b-484f-aa9c-89e90abb6d75"
  },
  {
    "input": "@triton.jit\ndef fwd_inner_chunk(q, k, g, A, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', DK: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < DK\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_qk_h + i_k * BK + i_t * BT * DK + tl.arange(0, BK)\n    p_gq = g + i_bh * s_qk_h + i_k * BK + i_t * BT * DK + tl.arange(0, BK)\n    p_A = A + (i_bh + i_k * B * H) * (tl.cdiv(T, BT) * BT * BT\n        ) + i_t * BT * BT + tl.arange(0, BT)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0) * scale\n        gq = tl.load(p_gq, mask=mask, other=0)\n        s = _q[None, :] * b_k * tl.math.exp2(gq[None, :] - b_g)\n        score = tl.sum(s, axis=1)\n        score = tl.where(o_i <= i, score, 0)\n        tl.store(p_A, score)\n        p_q += DK\n        p_gq += DK\n        p_A += BT\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "ef6ce70e-a9e0-4007-91fa-ebf162363062"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_aligned(Q, K, V, B0, sm_scale, Out, stride_qh, stride_qm,\n    stride_qk, stride_kh, stride_kn, stride_kk, stride_vh, stride_vk,\n    stride_vn, stride_oh, stride_om, stride_on, stride_b0h, stride_b0m, Z,\n    H, N_CTX, P_SEQ, OUT_DTYPE: 'tl.constexpr', BIAS_LAST_SIZE:\n    'tl.constexpr', B0_NUMEL: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + kv_offset, shape=(BLOCK_DMODEL,\n        N_CTX + P_SEQ), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + kv_offset, shape=(N_CTX +\n        P_SEQ, BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0\n        ), block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = N_CTX + P_SEQ\n    b_ptr_offsets_m = tl.arange(0, BLOCK_M)\n    b_offset = off_hz * stride_b0h\n    b_ptr_offsets_n_1 = tl.arange(0, BLOCK_N) % BIAS_LAST_SIZE + BIAS_LAST_SIZE\n    b1 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m) *\n        stride_b0m)[:, None] + b_ptr_offsets_n_1[None, :])\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=OUT_DTYPE)\n        qk += tl.dot(q, k)\n        b0 = tl.load(B0 + b_offset + ((start_m * BLOCK_M + b_ptr_offsets_m) *\n            stride_b0m)[:, None] + start_n // BLOCK_N)\n        qk += (b0 + b1) * 1.44269504\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    O_block_ptr = tl.make_block_ptr(base=Out + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "d5557603-b353-42d5-bc18-dfc4b8bc65fe"
  },
  {
    "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x_hwc(x, w, bias, y, stride_xn, stride_xc, stride_xh,\n    stride_xw, stride_wn, stride_wc, stride_wh, stride_ww, stride_yn,\n    stride_yc, stride_yh, stride_yw, delta_xh_ptr, delta_xw_ptr,\n    delta_xc_ptr, BATCH, IN_C, IN_H, IN_W, KERNEL_N, KERNEL_H, KERNEL_W,\n    OUT_H, OUT_W, stride_h, stride_w, padding_h, padding_w, dilation_h,\n    dilation_w, output_padding_h, output_padding_w, groups, ACC_TYPE:\n    'tl.constexpr', CONV1X1_NHWC: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_H:\n    'tl.constexpr', WITH_BIAS: 'tl.constexpr'):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_xh_ptrs = delta_xh_ptr + off_x_crs\n        delta_xw_ptrs = delta_xw_ptr + off_x_crs\n        delta_xc_ptrs = delta_xc_ptr + off_x_crs\n        delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n        off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw * stride_xw +\n            delta_xc * stride_xc)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n        delta_xh = 0\n        delta_xw = 0\n    mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n        off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n        delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None, :] >= 0\n        ) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w, out_dtype=ACC_TYPE)\n        w_ptrs += BLOCK_K\n        off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n        if not CONV1X1_NHWC:\n            delta_xh_ptrs += BLOCK_K\n            delta_xw_ptrs += BLOCK_K\n            delta_xc_ptrs += BLOCK_K\n            delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n            off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw *\n                stride_xw + delta_xc * stride_xc)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            x_ptrs += BLOCK_K\n        mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n            off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n            delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None,\n            :] >= 0) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    if WITH_BIAS:\n        acc += tl.load(bias + off_y_k)[None, :]\n    acc = acc\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "3cb8b275-ed09-442e-b96b-3158a48bbb9e"
  },
  {
    "input": "@triton.jit\ndef _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "23b48f62-358d-46c5-a2a4-3406b8a2cbb9"
  },
  {
    "input": "@triton.jit\ndef _cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n        Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]\n        Pi = exp(xi) / sum(exp(xi))\n        CE_i = -y log(p) = -y log[ exp(x) / sum(exp(x)) ]\n             = -y [ x - log[sum(exp(x))] ]\n             = y * (log[sum(exp(x))] - x)\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        logsumexp is also stable\n        Take    y =         log[sum(exp(x))]\n           exp(y) =             sum(exp(x))\n           exp(y) =             sum(exp(x - c)*exp(c)) Since e^(x-c)*e^c = e^x\n           exp(y) =      exp(c)*sum(exp(x - c))\n               y  = log(exp(c)*sum(exp(x - c)))\n               y  = c + log[sum(exp(x - c))]\n        This means we can set c = max(x) to make sure\n        exp(x - c) always is exp(x - max(x)).\n        This ensures exp(x - max(x))'s maximum is 1 as exp(0) = 1.\n    \"\"\"\n    row_idx = tl.program_id(0)\n    logits_ptr += row_idx * logits_row_stride\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx\n    labels_ptr += row_idx\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if label_idx != -100:\n        x = tl.load(logits_ptr + label_idx)\n        loss = logsumexp - x\n    else:\n        loss = 0.0\n    tl.store(logsumexp_ptr, logsumexp)\n    tl.store(loss_ptr, loss)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "5612f1d8-208a-44eb-b2df-0ca0e2b2fa43"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(_DA, _DOut, _A, Weight, Mean, Rstd, stride,\n    NumRows, NumCols, eps, BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    row = pid\n    A = _A + row * stride\n    DOut = _DOut + row * stride\n    DA = _DA + row * stride\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    _mean1 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n    _mean2 = tl.zeros([BLOCK_SIZE_N], dtype=tl.float32)\n    for off in range(0, NumCols, BLOCK_SIZE_N):\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\n        mask = cols < NumCols\n        a = tl.load(A + cols, mask=mask, other=0)\n        dout = tl.load(DOut + cols, mask=mask, other=0)\n        weight = tl.load(Weight + cols, mask=mask, other=0)\n        a_hat = (a - mean) * rstd\n        wdout = weight * dout\n        _mean1 += a_hat * wdout\n        _mean2 += wdout\n    mean1 = tl.sum(_mean1, axis=0) / NumCols\n    mean2 = 0.0\n    mean2 = tl.sum(_mean2, axis=0) / NumCols\n    for off in range(0, NumCols, BLOCK_SIZE_N):\n        cols = off + tl.arange(0, BLOCK_SIZE_N)\n        mask = cols < NumCols\n        a = tl.load(A + cols, mask=mask, other=0)\n        dout = tl.load(DOut + cols, mask=mask, other=0)\n        weight = tl.load(Weight + cols, mask=mask, other=0)\n        a_hat = (a - mean) * rstd\n        wdout = weight * dout\n        da = (wdout - (a_hat * mean1 + mean2)) * rstd\n        tl.store(DA + cols, da, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "3f2620ab-4684-483f-bd81-52c56504a179"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_merge(A, A2, B: 'tl.constexpr',\n    T: 'tl.constexpr', H: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', NK: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if i_t * BT + i_c * BC >= T:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        if HEAD_FIRST:\n            p_A = tl.make_block_ptr(A + (i_k * B * H + i_bh) * T * BC, (T,\n                BC), (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        else:\n            p_A = tl.make_block_ptr(A + (i_k * B + i_b) * T * H * BC + i_h *\n                BC, (T, BC), (H * BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC\n                ), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    if HEAD_FIRST:\n        p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    else:\n        p_A2 = tl.make_block_ptr(A2 + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A, boundary_check=(0, 1))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "03304062-19f7-47cd-b64c-895135a1b188"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, B, sm_scale, L, O, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vn, stride_vk, stride_oz, stride_oh, stride_om,\n    stride_ok, stride_bz, stride_bh, stride_bm, stride_bn, Z, H, M, N,\n    P_SEQ, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', LARGER_M: 'tl.constexpr',\n    DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_h * stride_kh\n    V += off_z * stride_vz + off_h * stride_vh\n    O += off_z * stride_oz + off_h * stride_oh\n    if HAS_BIAS:\n        B += off_z * stride_bz + off_h * stride_bh\n    L += (off_z * H + off_h) * M\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_m = start_m * BLOCK_M + offs_m_base\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    o_ptrs = O + (offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)\n    l_ptrs = L + offs_m\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    mask_m = offs_m < M\n    if DIVISIBLE_M:\n        q = tl.load(q_ptrs, cache_modifier='.cg')\n    else:\n        q = tl.load(q_ptrs, mask=mask_m[:, None], cache_modifier='.cg')\n    if BLOCK_DMODEL < 128:\n        I = tl.where(offs_k[:, None] == offs_k, tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 1.0, dtype=input_dtype), tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 0.0, dtype=input_dtype))\n        q = tl.dot(q, I)\n    if IS_CAUSAL:\n        hi = tl.minimum(N, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = N\n    offs_n_init = offs_n_base\n    k_ptrs = K + (offs_k[:, None] * stride_vk + offs_n_init[None, :] *\n        stride_vn)\n    v_ptrs = V + (offs_n_init[:, None] * stride_kn + offs_k[None, :] *\n        stride_kk)\n    if HAS_BIAS:\n        bias_ptrs = B + (offs_m[:, None] * stride_bm + offs_n_init[None, :] *\n            stride_bn)\n    for start_n in range(0, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        mask_n = offs_n < N\n        if DIVISIBLE_N:\n            k = tl.load(k_ptrs, cache_modifier='.cg')\n            v = tl.load(v_ptrs, cache_modifier='.cg')\n        else:\n            k = tl.load(k_ptrs, mask=mask_n[None, :], cache_modifier='.cg')\n            v = tl.load(v_ptrs, mask=mask_n[:, None], cache_modifier='.cg')\n        if HAS_BIAS:\n            if DIVISIBLE_M and DIVISIBLE_N:\n                b = tl.load(bias_ptrs)\n            else:\n                b = tl.load(bias_ptrs, mask_m[:, None] & mask_n[None, :])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, k) * sm_scale\n        if HAS_BIAS:\n            s += b\n        if not DIVISIBLE_N:\n            s = tl.where(mask_n[None, :], s, float('-inf'))\n        if IS_CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            s = tl.where(causal_mask, s, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(s, 1))\n        alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n        p = tl.math.exp2((s - m_i_new[:, None]) * log2e)\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vn\n        if HAS_BIAS:\n            bias_ptrs += BLOCK_N * stride_bn\n    if IS_CAUSAL and LARGER_M:\n        is_empty_line = offs_m + P_SEQ < 0\n        acc = tl.where(is_empty_line[:, None], 0.0, acc * (1.0 / l_i[:, None]))\n        l = tl.where(is_empty_line, float('-inf'), m_i + tl.log(l_i))\n    else:\n        acc = acc * (1.0 / l_i[:, None])\n        l = m_i + tl.log(l_i)\n    if DIVISIBLE_M:\n        tl.store(l_ptrs, l, cache_modifier='.cg')\n        tl.store(o_ptrs, acc, cache_modifier='.cg')\n    else:\n        tl.store(l_ptrs, l, mask=mask_m, cache_modifier='.cg')\n        tl.store(o_ptrs, acc, mask=mask_m[:, None], cache_modifier='.cg')\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "7139269a-baa0-4e09-ac97-c65bfa1e814b"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv4_forward_kernel(w_ptr, w_s_c, u_ptr, u_s_c, k_ptr,\n    k_s_b, k_s_t, k_s_c, v_ptr, v_s_b, v_s_t, v_s_c, state_ptr, state_s_b,\n    state_s_abe, state_s_c, wkv_ptr, wkv_s_b, wkv_s_t, wkv_s_c,\n    state_out_ptr, state_out_s_b, state_out_s_abe, state_out_s_t,\n    state_out_s_c, chans, tsz, BLOCK_SIZE_C: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    c_idx = tl.program_id(1)\n    cs = c_idx * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)\n    cmask = cs < chans\n    k_ptr = k_ptr + b_idx * k_s_b\n    v_ptr = v_ptr + b_idx * v_s_b\n    alpha_ptr = state_ptr + b_idx * state_s_b\n    beta_ptr = state_ptr + b_idx * state_s_b + state_s_abe\n    eps_ptr = state_ptr + b_idx * state_s_b + 2 * state_s_abe\n    wkv_ptr = wkv_ptr + b_idx * wkv_s_b\n    alpha_out_ptr = state_out_ptr + b_idx * state_out_s_b\n    beta_out_ptr = state_out_ptr + b_idx * state_out_s_b + state_out_s_abe\n    eps_out_ptr = state_out_ptr + b_idx * state_out_s_b + 2 * state_out_s_abe\n    alpha = tl.load(alpha_ptr + cs * state_s_c, mask=cmask)\n    beta = tl.load(beta_ptr + cs * state_s_c, mask=cmask)\n    eps = tl.load(eps_ptr + cs * state_s_c, mask=cmask)\n    w = tl.load(w_ptr + cs * w_s_c, mask=cmask)\n    u = tl.load(u_ptr + cs * u_s_c, mask=cmask)\n    for t in range(tsz):\n        kt = tl.load(k_ptr + t * k_s_t + cs * k_s_c, mask=cmask)\n        vt = tl.load(v_ptr + t * v_s_t + cs * v_s_c, mask=cmask)\n        ukt = u + kt\n        tau = tl.maximum(ukt, eps)\n        e1a = tl.exp(eps - tau)\n        e2a = tl.exp(ukt - tau)\n        wkv = (e1a * alpha + e2a * vt) / (e1a * beta + e2a)\n        tl.store(wkv_ptr + t * wkv_s_t + cs * wkv_s_c, wkv, mask=cmask)\n        w_eps = w + eps\n        eps = tl.maximum(w_eps, kt)\n        e1b = tl.exp(w_eps - eps)\n        e2b = tl.exp(kt - eps)\n        alpha = e1b * alpha + e2b * vt\n        beta = e1b * beta + e2b\n        tl.store(alpha_out_ptr + t * state_out_s_t + cs * state_out_s_c,\n            alpha, mask=cmask)\n        tl.store(beta_out_ptr + t * state_out_s_t + cs * state_out_s_c,\n            beta, mask=cmask)\n        tl.store(eps_out_ptr + t * state_out_s_t + cs * state_out_s_c, eps,\n            mask=cmask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "24030e56-2ee5-45e5-b8da-67cf8dfc93e9"
  },
  {
    "input": "@triton.jit\ndef _dk_prob_bwd_kernel(Q, K, dK, LSE, dLSE, nheads, seqlen_q, seqlen_k,\n    BLOCK_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    ASM: 'tl.constexpr' = 'cvt.rna.tf32.f32 $0, $1;'\n    start_n = tl.program_id(0)\n    ndims = nheads * BLOCK_HEADDIM\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + ndims * offs_m[:, None]\n    k_ptrs = K + ndims * offs_n[:, None]\n    dk_ptrs = dK + ndims * offs_n[:, None]\n    end_m = seqlen_q\n    for start_m in range(0, end_m, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        lse = tl.load(LSE + offs_m + start_m, mask=offs_m < seqlen_q, other=0.0\n            )\n        dlse = tl.load(dLSE + offs_m + start_m, mask=offs_m < seqlen_q,\n            other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        for off_h in range(nheads):\n            offs_hd = (offs_d + off_h * BLOCK_HEADDIM)[None, :]\n            q = tl.load(q_ptrs + offs_hd + start_m * ndims, mask=(offs_m +\n                start_m)[:, None] < seqlen_q, other=0.0)\n            k = tl.load(k_ptrs + offs_hd, mask=offs_n[:, None] < seqlen_k,\n                other=0.0)\n            qk += tl.dot(q, tl.trans(k))\n        qk_grad = tl.exp(qk - lse[:, None])\n        qk_grad = tl.where((start_m + offs_m)[:, None] < seqlen_q, qk_grad, 0.0\n            )\n        qk_grad = qk_grad * dlse[:, None]\n        qk_grad = tl.inline_asm_elementwise(ASM, '=r, r', [qk_grad], dtype=\n            tl.float32, is_pure=True, pack=1)\n        for off_h in range(nheads):\n            offs_hd = (offs_d + off_h * BLOCK_HEADDIM)[None, :]\n            q = tl.load(q_ptrs + offs_hd + start_m * ndims, mask=(start_m +\n                offs_m)[:, None] < seqlen_q, other=0.0)\n            k = tl.load(k_ptrs + offs_hd, mask=offs_n[:, None] < seqlen_k,\n                other=0.0)\n            q = tl.inline_asm_elementwise(ASM, '=r, r', [q], dtype=tl.\n                float32, is_pure=True, pack=1)\n            k_grad = tl.dot(tl.trans(qk_grad), q)\n            dk_h = tl.load(dk_ptrs + offs_hd, mask=offs_n[:, None] <\n                seqlen_k, other=0.0)\n            tl.store(dk_ptrs + offs_hd, dk_h + k_grad, mask=offs_n[:, None] <\n                seqlen_k)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ec32897b-a5e5-48fe-b274-cf4adb67a264"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_copy_kv_index_to_req(req_to_token_indexs, b_req_idx,\n    b_split_seq_len, cumsum_split_seq_len, b_seq_len, memindex,\n    stride_req_to_token_b, stride_req_to_token_s, BLOCK_M: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    cur_req_idx = tl.load(b_req_idx + cur_index)\n    q_split_len = tl.load(b_split_seq_len + cur_index)\n    q_mem_end = tl.load(cumsum_split_seq_len + cur_index)\n    q_mem_start = q_mem_end - q_split_len\n    store_end = tl.load(b_seq_len + cur_index)\n    store_start = store_end - q_split_len\n    off_m = tl.arange(0, BLOCK_M)\n    for block_start in range(0, q_split_len, BLOCK_M):\n        read_index = tl.load(memindex + q_mem_start + block_start + off_m,\n            mask=q_mem_start + block_start + off_m < q_mem_end, other=0)\n        tl.store(req_to_token_indexs + cur_req_idx * stride_req_to_token_b +\n            (block_start + store_start + off_m), read_index, mask=\n            block_start + store_start + off_m < store_end)\n    return\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "12f524b2-6d6a-41fc-b0b0-72ab350d126a"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "af6ce137-9cd7-482a-90cc-40a4ad9c3b4b"
  },
  {
    "input": "@triton.jit\ndef softmax_fwd_kernel(s, p, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    p_p = tl.make_block_ptr(p + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_m = tl.max(b_s, 1)\n    b_s = tl.exp(b_s - b_m[:, None])\n    b_z = tl.sum(b_s, 1)\n    b_p = tl.where(b_s != 0, b_s / b_z[:, None], 0.0)\n    tl.store(p_p, b_p, boundary_check=(0, 1))\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "0e4c3c79-dfff-4f68-968a-f5afda6b3e75"
  },
  {
    "input": "@triton.jit\ndef dequantize(x: 'tl.tensor', scale: 'tl.tensor') ->tl.tensor:\n    \"\"\"Dequantize quantized tensor to floating point.\n\n    Args:\n        x (tl.tensor): quantized tensor.\n        scale (tl.tensor): quantization scaling factor\n\n    Returns:\n        tl.tensor: Dequantized floating-point tensor.\n    \"\"\"\n    return x * scale\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "01019d4e-6597-4fbd-a907-d4841852f744"
  },
  {
    "input": "@triton.jit\ndef sin_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.sin(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "07213045-1dcb-437c-8930-ff609a9a7f7e"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_attn_kernel(Q, K, V, B, softmax_scale, stride_qb, stride_qh,\n    stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh,\n    stride_vn, stride_bb, stride_bh, stride_bm, stride_bn, stride_ob,\n    stride_oh, stride_om, stride_lb, stride_lh, headdim, seqlen_q, seqlen_k,\n    O, L, HAVE_BIAS: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m, off_b, off_h = tl.program_id(0), tl.program_id(1), tl.program_id(2\n        )\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    offs_n = tl.arange(0, BLOCK_N)\n    q_ptrs = Q + (off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :]))\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    k_ptrs = K + (off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :]))\n    v_ptrs = V + (off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :]))\n    softmax_scale = softmax_scale\n    if HAVE_BIAS:\n        b_ptrs = B + (off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :]))\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    max_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    for j in range(0, seqlen_k, BLOCK_N):\n        j = tl.multiple_of(j, BLOCK_N)\n        if EVEN_N:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + j * stride_kn)\n            else:\n                k = tl.load(k_ptrs + j * stride_kn, mask=offs_d[None, :] <\n                    headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + j * stride_kn, mask=(j + offs_n)[:, None] <\n                seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + j * stride_kn, mask=((j + offs_n)[:, None] <\n                seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k.T)\n        if not EVEN_N:\n            qk += tl.where((j + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n        if HAVE_BIAS:\n            if EVEN_N & EVEN_M:\n                b = tl.load(b_ptrs + j)\n            else:\n                b = tl.load(b_ptrs + j, mask=(offs_m[:, None] < seqlen_q) &\n                    (j + offs_n)[None, :] < seqlen_k, other=0.0)\n            qk = qk * softmax_scale + b\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        else:\n            max_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - max_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(max_i - max_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + j * stride_vn)\n            else:\n                v = tl.load(v_ptrs + j * stride_vn, mask=offs_d[None, :] <\n                    headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + j * stride_vn, mask=(j + offs_n)[:, None] <\n                seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + j * stride_vn, mask=((j + offs_n)[:, None] <\n                seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n        acc_o += tl.dot(p, v)\n        max_i = max_ij\n        lin = tl.exp(lse_i - max_ij) + l_ij\n        lse_i = max_ij + tl.log(lin)\n    o_scale = tl.exp(max_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    lse_ptrs = L + (off_b * stride_lb + off_h * stride_lh + offs_m)\n    tl.store(lse_ptrs, lse_i, mask=offs_m < seqlen_q)\n    out_ptrs = O + (off_b * stride_ob + off_h * stride_oh + (offs_m[:, None\n        ] * stride_om + offs_d[None, :]))\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "489afb93-143f-4a15-b251-e6aa50f6be2c"
  },
  {
    "input": "@triton.jit\ndef _triton_third_order_bwd(x_ptr: 'tl.tensor', y_ptr: 'tl.tensor', z_ptr:\n    'tl.tensor', g_x_ptr: 'tl.tensor', g_y_ptr: 'tl.tensor', g_z_ptr:\n    'tl.tensor', g_1_0_ptr: 'tl.tensor', g_1_1_ptr: 'tl.tensor', g_1_2_ptr:\n    'tl.tensor', g_2_0_ptr: 'tl.tensor', g_2_1_ptr: 'tl.tensor', g_2_2_ptr:\n    'tl.tensor', g_2_3_ptr: 'tl.tensor', g_2_4_ptr: 'tl.tensor', g_3_0_ptr:\n    'tl.tensor', g_3_1_ptr: 'tl.tensor', g_3_2_ptr: 'tl.tensor', g_3_3_ptr:\n    'tl.tensor', g_3_4_ptr: 'tl.tensor', g_3_5_ptr: 'tl.tensor', g_3_6_ptr:\n    'tl.tensor', BLOCK_SIZE: 'tl.constexpr', vector_length: 'tl.constexpr'):\n    sqrt_3 = 3 ** 0.5\n    sqrt_5 = 5 ** 0.5\n    sqrt_15 = 15 ** 0.5\n    block_id = tl.program_id(0)\n    offset = tl.arange(0, BLOCK_SIZE) + BLOCK_SIZE * block_id\n    x_row_start = x_ptr + offset\n    y_row_start = y_ptr + offset\n    z_row_start = z_ptr + offset\n    x = tl.load(x_row_start, mask=offset < vector_length)\n    y = tl.load(y_row_start, mask=offset < vector_length)\n    z = tl.load(z_row_start, mask=offset < vector_length)\n    g_1_0 = tl.load(g_1_0_ptr + offset, mask=offset < vector_length)\n    g_1_1 = tl.load(g_1_1_ptr + offset, mask=offset < vector_length)\n    g_1_2 = tl.load(g_1_2_ptr + offset, mask=offset < vector_length)\n    g_x = sqrt_3 * g_1_0\n    g_y = sqrt_3 * g_1_1\n    g_z = sqrt_3 * g_1_2\n    g_2_0 = tl.load(g_2_0_ptr + offset, mask=offset < vector_length)\n    g_2_1 = tl.load(g_2_1_ptr + offset, mask=offset < vector_length)\n    g_2_2 = tl.load(g_2_2_ptr + offset, mask=offset < vector_length)\n    g_2_3 = tl.load(g_2_3_ptr + offset, mask=offset < vector_length)\n    g_2_4 = tl.load(g_2_4_ptr + offset, mask=offset < vector_length)\n    g_x += sqrt_15 * z * g_2_0\n    g_z += sqrt_15 * x * g_2_0\n    g_x += sqrt_15 * y * g_2_1\n    g_y += sqrt_15 * x * g_2_1\n    g_y += sqrt_15 * z * g_2_2\n    g_z += sqrt_15 * y * g_2_2\n    g_x += -1.0 * sqrt_5 * x * g_2_3\n    g_y += 2.0 * sqrt_5 * y * g_2_3\n    g_z += -1.0 * sqrt_5 * z * g_2_3\n    g_x += -1.0 * sqrt_15 * x * g_2_4\n    g_z += sqrt_15 * z * g_2_4\n    g_3_0 = tl.load(g_3_0_ptr + offset, mask=offset < vector_length)\n    g_3_1 = tl.load(g_3_1_ptr + offset, mask=offset < vector_length)\n    g_3_2 = tl.load(g_3_2_ptr + offset, mask=offset < vector_length)\n    g_3_3 = tl.load(g_3_3_ptr + offset, mask=offset < vector_length)\n    g_3_4 = tl.load(g_3_4_ptr + offset, mask=offset < vector_length)\n    g_3_5 = tl.load(g_3_5_ptr + offset, mask=offset < vector_length)\n    g_3_6 = tl.load(g_3_6_ptr + offset, mask=offset < vector_length)\n    sq_x = x * x\n    sq_y = y * y\n    sq_z = z * z\n    g_x += sqrt_15 * g_3_0 * (-1.62018517460196 * sq_x + 1.08012344973464 *\n        sq_z + 0.540061724867322 * sq_z)\n    g_x += 2.64575131106459 * sqrt_15 * g_3_1 * y * z\n    g_x -= g_3_2 * (4.8605555238059 * sq_x - 6.48074069840786 * sq_y + \n        1.62018517460197 * sq_z)\n    g_x -= 7.93725393319377 * g_3_3 * x * y\n    g_x -= 3.24037034920393 * g_3_4 * x * z\n    g_x -= 2.64575131106459 * sqrt_15 * g_3_5 * x * y\n    g_x -= sqrt_15 * g_3_6 * z * (1.08012344973464 * x + 2.16024689946929 * x)\n    g_y += 2.64575131106459 * sqrt_15 * g_3_1 * x * z\n    g_y += 12.9614813968157 * g_3_2 * x * y\n    g_y -= g_3_3 * (3.96862696659689 * sq_x - 7.93725393319377 * sq_y + \n        3.96862696659689 * sq_z)\n    g_y += 12.9614813968157 * g_3_4 * y * z\n    g_y -= 1.3228756555323 * sqrt_15 * g_3_5 * (sq_x - sq_z)\n    g_z += sqrt_15 * g_3_0 * x * (1.08012344973464 * z + 2.16024689946929 * z)\n    g_z += 2.64575131106459 * sqrt_15 * g_3_1 * x * y\n    g_z -= 3.24037034920393 * g_3_2 * x * z\n    g_z -= 7.93725393319377 * g_3_3 * y * z\n    g_z -= g_3_4 * (1.62018517460197 * sq_x - 6.48074069840786 * sq_y + \n        4.8605555238059 * sq_z)\n    g_z += 2.64575131106459 * sqrt_15 * g_3_5 * y * z\n    g_z -= sqrt_15 * g_3_6 * (1.08012344973464 * sq_x + 0.540061724867322 *\n        sq_x - 1.62018517460196 * sq_z)\n    tl.store(g_x_ptr + offset, g_x, mask=offset < vector_length)\n    tl.store(g_y_ptr + offset, g_y, mask=offset < vector_length)\n    tl.store(g_z_ptr + offset, g_z, mask=offset < vector_length)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "0094af85-2baf-4e62-a9bb-fbead69bf1d4"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_based_fwd_kernel(q, k, v, o, z, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_0o = tl.zeros([BV], dtype=tl.float32)\n    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_0o = 0\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT])\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1)) * scale\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_z = tl.zeros([BT], dtype=tl.float32)\n        b_o += b_h_0o\n        b_z += k_0o\n        b_o += tl.dot(b_q, b_h_1o, allow_tf32=False)\n        b_z += tl.sum(b_q * k_1o, axis=1)\n        b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK])\n        b_o += tl.dot(b_q_2o, b_h_2o, allow_tf32=False) * 0.5\n        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n        k_1o += tl.sum(b_k, axis=1)[None, :]\n        k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n        k_0o += BT\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        tl.store(p_z, b_z, mask=i * BT + tl.arange(0, BT) < T)\n        b_h_2o = b_h_2o + tl.dot(b_k_2o, b_v, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_z += BT\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "98938555-49d5-4196-b9c2-e830e237b3a7"
  },
  {
    "input": "@triton.jit\ndef bwd_kernel_dq(Q, K, V, B, sm_scale, Out, DO, DQ, DB, L, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_bz,\n    stride_bh, stride_bm, stride_bn, stride_oz, stride_oh, stride_om,\n    stride_ok, stride_dqz, stride_dqh, stride_dqm, stride_dqk, stride_dbz,\n    stride_dbh, stride_dbm, stride_dbn, num_head_q: \"'i32'\", num_head_k:\n    \"'i32'\", cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q,\n    max_seqlen_k, head_dim, dropout_p, philox_seed_ptr, philox_offset1:\n    \"'*u32'\", philox_offset2: \"'u32'\", BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    philox_seed = 0\n    philox_offset_base = philox_offset2\n    if ENABLE_DROPOUT:\n        philox_seed = tl.load(philox_seed_ptr)\n        philox_offset_base += tl.load(philox_offset1)\n    start_q = tl.program_id(0) * BLOCK_M\n    off_h_q = tl.program_id(1)\n    off_h_k = off_h_q if num_head_q == num_head_k else off_h_q // (num_head_q\n         // num_head_k)\n    off_z = tl.program_id(2)\n    num_z = tl.num_programs(2)\n    off_zh = off_z * num_head_q + off_h_q * 1\n    offs_q = start_q + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    ld_offs_d = None if not PADDED_HEAD else tl.arange(0, BLOCK_DMODEL)\n    cu_seqlens_q_start = 0\n    cu_seqlens_k_start = 0\n    seqlen_q = max_seqlen_q\n    seqlen_k = max_seqlen_k\n    batch_index = off_z\n    if num_seqlens > 0:\n        cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n        cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n        seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n        if start_q >= seqlen_q:\n            return\n        cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)\n        cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)\n        seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start\n        batch_index = 0\n    if num_seqlens < 0:\n        cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n        cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n        seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n        if start_q >= seqlen_q:\n            return\n        cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)\n        cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)\n        seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start\n        cu_seqlens_q_start = 0\n        cu_seqlens_k_start = 0\n        batch_index = off_z\n    q_offset = (off_h_q * stride_qh + batch_index * stride_qz + \n        cu_seqlens_q_start * stride_qm)\n    Q += q_offset\n    q_ptrs = Q + offs_q[:, None] * stride_qm + offs_d[None, :] * stride_qk\n    if start_q + BLOCK_M <= seqlen_q:\n        q = load_fn(q_ptrs, None, ld_offs_d, seqlen_q, head_dim)\n    else:\n        q = load_fn(q_ptrs, offs_q, ld_offs_d, seqlen_q, head_dim)\n    qk_scale = sm_scale * 1.44269504089\n    bias_scale = 1.0 / sm_scale\n    k_offset = (off_h_k * stride_kh + batch_index * stride_kz + \n        cu_seqlens_k_start * stride_kn)\n    K += k_offset\n    kt_ptrs = K + offs_d[:, None] * stride_kk + offs_n[None, :] * stride_kn\n    v_offset = (off_h_k * stride_vh + batch_index * stride_vz + \n        cu_seqlens_k_start * stride_vk)\n    V += v_offset\n    vt_ptrs = V + offs_d[:, None] * stride_vn + offs_n[None, :] * stride_vk\n    do_offset = (off_h_q * stride_oh + batch_index * stride_oz + \n        cu_seqlens_q_start * stride_om)\n    DO += do_offset\n    do_ptrs = DO + offs_q[:, None] * stride_om + offs_d[None, :] * stride_ok\n    if start_q + BLOCK_M <= seqlen_q:\n        do = load_fn(do_ptrs, None, ld_offs_d, seqlen_q, head_dim)\n    else:\n        do = load_fn(do_ptrs, offs_q, ld_offs_d, seqlen_q, head_dim)\n    D_ptrs = D + off_zh * max_seqlen_q\n    l_ptrs = L + off_zh * max_seqlen_q\n    if ENABLE_DROPOUT:\n        batch_philox_offset = (philox_offset_base + off_zh * max_seqlen_q *\n            max_seqlen_k)\n    else:\n        batch_philox_offset = 0\n    dq_offset = (batch_index * stride_dqz + off_h_q * stride_dqh + \n        cu_seqlens_q_start * stride_dqm)\n    DQ += dq_offset\n    store_db = True\n    if BIAS_TYPE == 0:\n        B_block_ptr = 0\n        DB_block_ptr = 0\n    elif BIAS_TYPE == 1:\n        B_block_ptr = tl.make_block_ptr(base=B + off_h_q * stride_bh + \n            batch_index * stride_bz, shape=(seqlen_q, seqlen_k), strides=(\n            stride_bm, stride_bn), offsets=(start_q, 0), block_shape=(\n            BLOCK_M, BLOCK_N), order=(1, 0))\n        if (stride_dbz == 0 and stride_dbh == 0) and stride_dbm == 0:\n            store_db = False\n        DB_block_ptr = tl.make_block_ptr(base=DB + off_h_q * stride_dbh + \n            batch_index * stride_dbz, shape=(seqlen_q, seqlen_k), strides=(\n            stride_dbm, stride_dbn), offsets=(start_q, 0), block_shape=(\n            BLOCK_M, BLOCK_N), order=(1, 0))\n    else:\n        tl.static_assert(False, f'Unsupported BIAS_TYPE {BIAS_TYPE}')\n    k_lo = 0\n    k_hi = min(start_q + BLOCK_M, seqlen_k) if CAUSAL else seqlen_k\n    real_seqlen_k = k_hi - k_lo\n    n_blocks = tl.cdiv(k_hi - k_lo, BLOCK_N)\n    n_extra_tokens = 0\n    if real_seqlen_k < BLOCK_N:\n        n_extra_tokens = BLOCK_N - real_seqlen_k\n    elif real_seqlen_k % BLOCK_N:\n        n_extra_tokens = real_seqlen_k % BLOCK_N\n    is_irregular_k = n_extra_tokens != 0\n    n_full_blocks = (k_hi - k_lo) // BLOCK_N\n    leading_masked_blocks = 0\n    trailing_masked_blocks = 0\n    if CAUSAL:\n        mask_top_edge = min(start_q, seqlen_k)\n        n_full_blocks = (mask_top_edge - k_lo) // BLOCK_N\n        trailing_masked_blocks = n_blocks - n_full_blocks\n    else:\n        trailing_masked_blocks = 1 if is_irregular_k else 0\n    q_boundary = tl.full((BLOCK_M,), seqlen_q, dtype=tl.int32)\n    d_lse_ptrs_mask = offs_q < q_boundary\n    Di = tl.load(D_ptrs + offs_q, mask=d_lse_ptrs_mask, other=0.0)\n    l_i = tl.load(l_ptrs + offs_q, mask=d_lse_ptrs_mask, other=0.0)\n    dq = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if n_full_blocks > 0:\n        lo = 0\n        hi = n_full_blocks * BLOCK_N\n        dq = bwd_inner_dq(dq, qk_scale, bias_scale, DB_block_ptr, store_db,\n            q, kt_ptrs, stride_kn, vt_ptrs, stride_vk, B_block_ptr, do, Di,\n            l_i, seqlen_q, seqlen_k, head_dim, start_q, lo, hi, dropout_p,\n            philox_seed, batch_philox_offset, max_seqlen_k, BLOCK_M,\n            BLOCK_DMODEL, BLOCK_N, True, False, ENABLE_DROPOUT, PADDED_HEAD,\n            BIAS_TYPE)\n    if trailing_masked_blocks > 0:\n        lo = n_full_blocks * BLOCK_N\n        hi = k_hi\n        tl.debug_barrier()\n        dq = bwd_inner_dq(dq, qk_scale, bias_scale, DB_block_ptr, store_db,\n            q, kt_ptrs, stride_kn, vt_ptrs, stride_vk, B_block_ptr, do, Di,\n            l_i, seqlen_q, seqlen_k, head_dim, start_q, lo, hi, dropout_p,\n            philox_seed, batch_philox_offset, max_seqlen_k, BLOCK_M,\n            BLOCK_DMODEL, BLOCK_N, False, CAUSAL, ENABLE_DROPOUT,\n            PADDED_HEAD, BIAS_TYPE)\n    dq = dq * sm_scale\n    mstore2d(dq, BLOCK_M, BLOCK_DMODEL, o_base=DQ, o_start_row=start_q,\n        o_start_col=0, o_rows=seqlen_q, o_cols=head_dim, stride_row=\n        stride_dqm, stride_col=stride_dqk)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "f0fb385d-0da2-486b-abeb-6cc82e8c5880"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_rwkv6_fwd_cumsum_kernel(s, o, o_minus_s, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o_minus_s = tl.make_block_ptr(o_minus_s + i_bh * s_s_h, (T, S), (\n        s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_o_minus_s, b_o - b_s, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "e3380058-8fe7-42f9-b371-4e7f256ba90b"
  },
  {
    "input": "@triton.jit\ndef int_to_randn(x1, x2, seed):\n    x_hash_1 = hash(x1)\n    x_hash_2 = hash(x2)\n    x_hash_1 = pair_hash(pair_hash(INT32_PRIME, seed), x_hash_1)\n    x_hash_2 = pair_hash(pair_hash(INT32_PRIME, seed + 1), x_hash_2)\n    x_01_1 = int32_to_float01(x_hash_1)\n    x_01_2 = int32_to_float01(x_hash_2)\n    z = tl.sqrt(-2 * tl.log(x_01_1)) * tl.cos(6.28318530718 * x_01_2)\n    return z\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "55eefa49-9947-4bf9-a2dc-b2b01d18eb01"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_intra(s, z, ss, doo, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BS: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_s, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    o_i = tl.arange(0, BC)\n    m_o = tl.full([BC, BC], 1.0, dtype=tl.float32)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (s_s_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * S + i_s * BS,), (BS,), (0,))\n    p_doo = tl.make_block_ptr(doo + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_doo = tl.zeros([BC, BS], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n        b_doo += b_ss * tl.exp(b_zn[None, :] - b_z)\n    b_doo = tl.exp(b_s - b_zn[None, :]) * tl.dot(m_o, b_doo, allow_tf32=False)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_ss = tl.load(p_ss, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_doo += tl.where(m_i, tl.exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)\n    b_doo += tl.load(p_doo, boundary_check=(0, 1))\n    tl.store(p_doo, b_doo, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8e82339e-f1a1-4880-8349-6c2d83de8a20"
  },
  {
    "input": "@triton.jit\ndef tl_lock_add(ptrs, v, mask, lock_ptr):\n    while tl.atomic_cas(lock_ptr, 0, 1) == 1:\n        pass\n    cur_v = tl.load(ptrs, mask=mask, other=0.0, eviction_policy='evict_last')\n    new_v = v + cur_v\n    tl.store(ptrs, new_v, mask=mask, eviction_policy='evict_last')\n    tl.atomic_xchg(lock_ptr, 0)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "6417f79a-131d-415c-81d0-096afc6e68e8"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_kernel(X, W, Y, stride_x_N, stride_x_hn, stride_x_hd,\n    stride_y_N, stride_y_hn, stride_y_hd, stride_w_hn, stride_w_hd, N, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    Seq = tl.program_id(0)\n    H = tl.program_id(1)\n    X += Seq * stride_x_N + H * stride_x_hn\n    Y += Seq * stride_y_N + H * stride_y_hn\n    W += H * stride_w_hn\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "e2453f59-228c-4999-baf5-0f2ed93e76c9"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    else:\n        raise ValueError(\"BIAS_TYPE must be one of {'vector', 'matrix'}\")\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            else:\n                raise ValueError(\n                    \"BIAS_TYPE must be one of {'vector', 'matrix'}\")\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "85e30503-ffb2-4660-8244-a24c33c810eb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d529ef0c-fc38-41bd-8649-5d8e59dc5a56"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_A': 4}, num_warps=1),\n    triton.Config({'BLOCK_A': 16}, num_warps=2), triton.Config({'BLOCK_A': \n    32}, num_warps=4), triton.Config({'BLOCK_A': 64}, num_warps=8), triton.\n    Config({'BLOCK_A': 128}, num_warps=16), triton.Config({'BLOCK_A': 256},\n    num_warps=32), triton.Config({'BLOCK_A': 8}, num_warps=1), triton.\n    Config({'BLOCK_A': 16}, num_warps=2), triton.Config({'BLOCK_A': 32},\n    num_warps=4), triton.Config({'BLOCK_A': 64}, num_warps=8), triton.\n    Config({'BLOCK_A': 128}, num_warps=16), triton.Config({'BLOCK_A': 256},\n    num_warps=32), triton.Config({'BLOCK_A': 16}, num_warps=1), triton.\n    Config({'BLOCK_A': 32}, num_warps=2), triton.Config({'BLOCK_A': 64},\n    num_warps=4), triton.Config({'BLOCK_A': 128}, num_warps=8), triton.\n    Config({'BLOCK_A': 256}, num_warps=16), triton.Config({'BLOCK_A': 512},\n    num_warps=32)], key=['A', 'MAX_INTERP'])\n@triton.jit\ndef __scan_col_compute(X, stride_xn, stride_xa, stride_xb, N, A, B:\n    'tl.constexpr', SCALE, stride_scale, NCOLS, stride_ncolsn,\n    stride_ncolsa, COL_INDICES, stride_coln, stride_cola, stride_colz,\n    MAX_Z, MAX_INTERP: 'tl.constexpr', ORIGINAL_WIDTH: 'tl.constexpr',\n    TARGET_WIDTH_MAX: 'tl.constexpr', BLOCK_A: 'tl.constexpr'):\n    n = tl.program_id(0)\n    pid_a = tl.program_id(1)\n    index_as = pid_a * BLOCK_A + tl.arange(0, BLOCK_A)\n    mask_as = index_as < A\n    scales_as = tl.load(SCALE + index_as * stride_scale, mask=mask_as, other=0)\n    last_index = tl.zeros((BLOCK_A,), dtype=tl.int32)\n    for _b in range(B):\n        b = _b % ORIGINAL_WIDTH\n        x_mask = tl.load(X + n * stride_xn + index_as * stride_xa + _b *\n            stride_xb, mask=mask_as, other=0)\n        v_start = tl.math.round(b * scales_as)\n        v_end = tl.math.round((b + 1) * scales_as)\n        n_pixel = (v_end - v_start) * x_mask\n        tl.store(COL_INDICES + n * stride_coln + index_as[:, None] *\n            stride_cola + (tl.arange(0, MAX_INTERP)[None, :] + last_index[:,\n            None]) * stride_colz, tl.arange(0, MAX_INTERP)[None, :] +\n            v_start[:, None] + tl.math.floor(tl.math.floor(_b /\n            ORIGINAL_WIDTH) * TARGET_WIDTH_MAX), mask=(tl.arange(0,\n            MAX_INTERP)[None, :] < n_pixel[:, None]) & mask_as[:, None])\n        last_index += n_pixel\n    tl.store(NCOLS + n * stride_ncolsn + index_as * stride_ncolsa,\n        last_index, mask=mask_as)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "036b3a9d-cbf1-4ef9-9804-db684deb7161"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _l2_norm_fwd_1pass_kernel(X, Y, stride_x_row, N, eps, BLOCK_N:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0)\n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    y = x * rstd\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "7ca39092-ebd6-4b42-9557-2496fbfd817c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, G,\n    rows_per_program, programs_per_group, IS_RMS_NORM: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL:\n    'tl.constexpr', HAS_WEIGHT: 'tl.constexpr', HAS_BIAS: 'tl.constexpr',\n    RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    group_id, program_id_in_group = (row_block_id // programs_per_group, \n        row_block_id % programs_per_group)\n    row_start = group_id + program_id_in_group * G * rows_per_program\n    row_end = min(row_start + G * rows_per_program, M)\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + group_id * stride_x_row + cols, mask=mask)\n        dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + group_id * stride_x_row + cols, mask=mask, other=0.0)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    for row in range(row_start, row_end, G):\n        x = tl.load(X + row * stride_x_row + cols, mask=mask, other=0)\n        dy = tl.load(DY + row * stride_dy_row + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w if HAS_WEIGHT else xhat\n            if HAS_BIAS:\n                y = y + b\n            tl.store(Y + row * stride_y_row + cols, y, mask=mask)\n        wdy = dy\n        if HAS_WEIGHT:\n            wdy = dy * w\n            dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + row * stride_dres_row + cols, mask=\n                mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + row * stride_dres_in_row + cols, dx,\n                mask=mask)\n        tl.store(DX + row * stride_dx_row + cols, dx, mask=mask)\n    if HAS_WEIGHT:\n        tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "64fd05dc-e3ee-4845-a233-de9f2d0b6d3e"
  },
  {
    "input": "@triton.jit\ndef softmax_grad_kernel_two_rows(d_output_ptr, output_ptr, d_input_ptr,\n    d_output_row_stride, output_row_stride, d_input_row_stride, n_cols,\n    BLOCK_SIZE: 'tl.constexpr', is_bf16: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    output_row_ptr = output_ptr + 2 * row_idx * output_row_stride\n    d_output_row_ptr = d_output_ptr + 2 * row_idx * d_output_row_stride\n    d_input_row_ptr = d_input_ptr + 2 * row_idx * d_input_row_stride\n    output_ptrs = output_row_ptr + col_offsets\n    d_output_ptrs = d_output_row_ptr + col_offsets\n    d_input_ptrs = d_input_row_ptr + col_offsets\n    _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n        col_offsets, n_cols, is_bf16)\n    _softmax_grad_core(output_ptrs + n_cols, d_output_ptrs + n_cols, \n        d_input_ptrs + n_cols, col_offsets, n_cols, is_bf16)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "8ac1ad91-7c7d-463e-bbb8-94ad6ef6278c"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_bwd_kernel(q, k, v, alpha, beta, ha, dht, dh0, do, dq,\n    dk, dv, dalpha, dbeta, dha, h0, s_k_h, s_v_h, NK, scale, B, H, T, K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', USE_DH0:\n    'tl.constexpr', USE_DHT: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_ha = ha + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_alpha = alpha + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_beta = beta + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (T\n         - 1) * K\n    p_dbeta = dbeta + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK\n        ) + (T - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (T\n         - 1) * V\n    p_dha = dha + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV\n        ) + (T - 1) * V\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        d_h += tl.load(p_ht, mask=mask_bk[:, None] & mask_bv[None, :], other=0)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0)\n        b_alpha = tl.load(p_alpha, mask=mask_bk, other=0)\n        b_ha = tl.load(p_ha, mask=mask_bv, other=0)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * b_v[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        b_dha = tl.sum(d_h * b_beta[:, None], axis=0)\n        tl.store(p_dha, b_dha, mask=mask_bv)\n        b_dbeta = tl.sum(d_h * b_ha[None, :], axis=1)\n        tl.store(p_dbeta, b_dbeta, mask=mask_bk)\n        d_h += b_dha[None, :] * b_alpha[:, None]\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n        p_beta -= K\n        p_dbeta -= K\n        p_alpha -= K\n        p_dha -= V\n        p_ha -= V\n    if USE_DH0:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, d_h, mask=mask_bk[:, None] & mask_bv[None, :])\n    tl.debug_barrier()\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_beta = beta + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_ha = ha + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_dha = dha + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_alpha = alpha + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK\n        )\n    p_dalpha = dalpha + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_h0, mask=mask_kv, other=0)\n    for i in range(0, T):\n        d_ha = tl.load(p_dha, mask=mask_bv, other=0)\n        d_alpha = tl.sum(d_ha[None, :] * h, axis=1)\n        tl.store(p_dalpha, d_alpha, mask=mask_bk)\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0)\n        b_ha = tl.load(p_ha, mask=mask_bv, other=0)\n        h += b_k[:, None] * b_v[None, :] + b_beta[:, None] * b_ha[None, :]\n        _d_q = h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dk += K\n        p_dalpha += K\n        p_dha += V\n        p_ha += V\n        p_dq += K\n        p_beta += K\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ecbceb0b-5186-457c-8b82-3876f1ff4160"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config(\n    {'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config\n    ({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.\n    Config({'BLOCK_SIZE': 2048})], key=['dim'])\n@triton.jit\ndef _state_passing_fwd_kernel(states_ptr, out_ptr, final_states_ptr,\n    dA_cs_ptr, initstates_ptr, seq_idx_ptr, dim, nchunks, seqlen,\n    chunk_size, stride_states_batch, stride_states_chunk,\n    stride_states_head, stride_states_dim, stride_out_batch,\n    stride_out_chunk, stride_out_head, stride_out_dim,\n    stride_final_states_batch, stride_final_states_head,\n    stride_final_states_dim, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_initstates_batch, stride_initstates_head,\n    stride_initstates_dim, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    HAS_INITSTATES: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\n    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    final_states_ptr += (pid_b * stride_final_states_batch + pid_h *\n        stride_final_states_head)\n    if HAS_INITSTATES:\n        initstates_ptr += (pid_b * stride_initstates_batch + pid_h *\n            stride_initstates_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    states_ptrs = states_ptr + offs_m * stride_states_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim\n    if not HAS_INITSTATES:\n        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    else:\n        initstates_ptrs = initstates_ptr + offs_m * stride_initstates_dim\n        states = tl.load(initstates_ptrs, mask=offs_m < dim, other=0.0)\n    tl.store(out_ptrs, states, mask=offs_m < dim)\n    out_ptrs += stride_out_chunk\n    seq_idx = 0\n    for c in range(nchunks):\n        new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0)\n        dA_cs = tl.load(dA_cs_ptr)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            seq_idx_new = tl.load(seq_idx_ptr + (min((c + 1) * chunk_size,\n                seqlen) - 1) * stride_seq_idx_seqlen)\n            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n            seq_idx = seq_idx_new\n        states = scale * states + new_states\n        if c < nchunks - 1:\n            tl.store(out_ptrs, states, mask=offs_m < dim)\n        else:\n            tl.store(final_states_ptrs, states, mask=offs_m < dim)\n        states_ptrs += stride_states_chunk\n        dA_cs_ptr += stride_dA_cs_chunk\n        out_ptrs += stride_out_chunk\n",
    "category": "Memory Management",
    "subcategory": "memory padding",
    "uuid": "b21b790c-e7b6-48b2-80a8-79d09fd64ccd"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size},\n    num_warps=num_warps) for block_size, num_warps in itertools.product([32,\n    64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=[\n    'n_audios', 'audio_len'])\n@triton.jit\ndef apply_gain_kernel(samples_ptr, amplitude_ratios_ptr, output_ptr,\n    n_audios, audio_len, BLOCK_SIZE: 'tl.constexpr'):\n    audio_idx = tl.program_id(0)\n    if audio_idx >= n_audios:\n        return\n    gain = tl.load(amplitude_ratios_ptr + audio_idx)\n    for i in range(0, audio_len, BLOCK_SIZE):\n        sample_idx = i + tl.arange(0, BLOCK_SIZE)\n        mask = sample_idx < audio_len\n        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx,\n            mask=mask)\n        result = samples * gain\n        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result,\n            mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "a0570dac-6162-4077-9600-145cd76605f1"
  },
  {
    "input": "@triton.jit\ndef _voxel_grid_splat(to_splat, grad_feature_grid, feature_grid_size,\n    batch_index, ix_in, iy_in, iz_in, C: 'tl.constexpr', NUM_GRIDS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr'):\n    feature_grid_offs = tl.zeros((1,), dtype=tl.int32)\n    for gi in range(NUM_GRIDS):\n        offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n        ID = tl.load(feature_grid_size + offs + 1)\n        IH = tl.load(feature_grid_size + offs + 2)\n        IW = tl.load(feature_grid_size + offs + 3)\n        ID_ = tl.sum(ID, axis=0) // BLOCK_SIZE\n        IH_ = tl.sum(IH, axis=0) // BLOCK_SIZE\n        IW_ = tl.sum(IW, axis=0) // BLOCK_SIZE\n        voxel_grid = (ID_ - 1) * (IH_ - 1) * (IW_ - 1)\n        if mask_out_of_bounds_samples:\n            in_bounds_mask = is_in_bounds(ix_in, iy_in, iz_in, C, BLOCK_SIZE)\n            if C == 1:\n                in_bounds_mask = in_bounds_mask[:, None]\n            to_splat = to_splat * in_bounds_mask\n        else:\n            to_splat = to_splat\n        if voxel_grid > 0:\n            grid_numel = _voxel_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, ix_in, iy_in, iz_in, IH, IW, ID, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        elif ID_ == 1:\n            grid_numel = _plane_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, ix_in, iy_in, IH, IW, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        elif IH_ == 1:\n            grid_numel = _plane_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, ix_in, iz_in, ID, IW, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        else:\n            grid_numel = _plane_grid_splat_one(gi, to_splat, \n                grad_feature_grid + feature_grid_offs, feature_grid_size,\n                batch_index, iy_in, iz_in, ID, IH, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        feature_grid_offs += grid_numel\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "2fd9bad2-b2b8-4440-8a75-2754095f29cc"
  },
  {
    "input": "@triton.jit\ndef demo1(x_ptr):\n    range = tl.arange(0, 8)\n    None\n    x = tl.load(x_ptr + range, range < 5, 0)\n    None\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "c6407c88-12a9-4b19-895b-54d5092c458f"
  },
  {
    "input": "@triton.jit\ndef __scan_col_compute_old(X, stride_xn, stride_xa, stride_xb, N, A, B:\n    'tl.constexpr', BLOCK_A: 'tl.constexpr', SCALE, stride_scale, NCOLS,\n    stride_ncolsn, stride_ncolsa, COL_INDICES, stride_coln, stride_cola,\n    stride_colz, MAX_Z: 'tl.constexpr', MAX_INTERP: 'tl.constexpr',\n    ORIGINAL_WIDTH: 'tl.constexpr', TARGET_WIDTH_MAX: 'tl.constexpr',\n    GRID_N, GRID_A):\n    n = tl.program_id(0)\n    pid_a = tl.program_id(1)\n    for ia in range(BLOCK_A):\n        a = ia * GRID_A + pid_a\n        mask_a = a < A\n        scales_a = tl.load(SCALE + a * stride_scale, mask=mask_a, other=0)\n        last_index = int(0)\n        for _b in range(B):\n            b = _b % ORIGINAL_WIDTH\n            x_mask = tl.load(X + n * stride_xn + a * stride_xa + _b *\n                stride_xb, mask=mask_a, other=0)\n            v_start = tl.math.round(b * scales_a)\n            v_end = tl.math.round((b + 1) * scales_a)\n            n_pixel = (v_end - v_start) * x_mask\n            tl.store(COL_INDICES + n * stride_coln + a * stride_cola + (tl.\n                arange(0, MAX_INTERP) + last_index) * stride_colz, tl.\n                arange(0, MAX_INTERP) + v_start + tl.math.floor(tl.math.\n                floor(_b / ORIGINAL_WIDTH) * TARGET_WIDTH_MAX), mask=(tl.\n                arange(0, MAX_INTERP) < n_pixel) & mask_a)\n            last_index += n_pixel\n        tl.store(NCOLS + n * stride_ncolsn + a * stride_ncolsa, last_index,\n            mask=mask_a)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "e1ac6852-91f2-4888-afba-43b5ab1d8ec2"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_retention_bwd_kernel(q, k, v, do, dq, dk, dv, initial_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT -\n        o_i - 1) * b_b)\n    d_b = tl.math.exp2(BT * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    d_s = tl.trans(d_s)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "9e177e2f-38ea-47d1-ae4f-0331449d866c"
  },
  {
    "input": "@triton.jit\ndef _seeded_triton_dropout(x_ptr, output_ptr, n_elements, p, seed,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    random = tl.rand(seed, offsets)\n    x_keep = random > p\n    output = tl.where(x_keep, x / (1 - p), 0.0)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "6e86ff8e-bf04-48dc-b68e-914084d5c0f6"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0570e8e5-ca74-4706-8433-49fd1b3fefdc"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef softmax_fwd_kernel(x, p, D: 'tl.constexpr', B: 'tl.constexpr'):\n    i_n = tl.program_id(0)\n    o_d = tl.arange(0, B)\n    m_d = o_d < D\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\n    b_m = tl.max(b_x, 0)\n    b_x = tl.exp(b_x - b_m)\n    b_p = b_x / tl.sum(b_x, 0)\n    tl.store(p + i_n * D + o_d, b_p, mask=m_d)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "b3106eab-9c3f-4350-bd9a-b322d55b8aeb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_fwd_kernel_o(q, v, g, h, o, A, scale, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h)\n    if HEAD_FIRST:\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(m_s, b_A, 0.0)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "742534f2-ebe9-43e6-9057-f21207ac3572"
  },
  {
    "input": "@triton.jit\ndef _d_color_activation(dy, x):\n    return dy * tl.sigmoid(x) * (1 - tl.sigmoid(x))\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "034acebc-f020-4def-b335-129a48efe476"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2),\n    triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64},\n    num_warps=4)], key=[])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_reversed_cumsum_scalar_kernel(s, o, offsets, T:\n    'tl.constexpr', H: 'tl.constexpr', BT: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr', USE_OFFSETS: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b), tl.load(offsets + i_b + 1)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n            p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n        else:\n            p_s = tl.make_block_ptr(s + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n            p_o = tl.make_block_ptr(o + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,))\n        b_zz = tl.sum(b_s, axis=0)\n        b_z += b_zz\n        b_o = b_s - tl.cumsum(b_s, axis=0) + b_z[None]\n        tl.store(p_o, b_o, boundary_check=(0,))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "8f71b90e-7a67-4421-94ff-542e40380642"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n    stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr', start_n, start_m, num_steps, MASK: 'tl.constexpr'\n    ):\n    offs_m = start_m + tl.arange(0, BLOCK_M1)\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    offs_k = tl.arange(0, HEAD_DIM)\n    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n    curr_m = start_m\n    step_m = BLOCK_M1\n    for blk_idx in range(num_steps):\n        qT = tl.load(qT_ptrs)\n        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n        m = tl.load(M + offs_m)\n        qkT = tl.dot(k, qT, allow_tf32=False)\n        pT = tl.math.exp2(qkT - m[None, :])\n        if MASK:\n            mask = offs_m[None, :] >= offs_n[:, None]\n            pT = tl.where(mask, pT, 0.0)\n        do = tl.load(do_ptrs)\n        ppT = pT\n        ppT = ppT\n        dv += tl.dot(ppT, do, allow_tf32=False)\n        Di = tl.load(D + offs_m)\n        dpT = tl.dot(v, tl.trans(do), allow_tf32=False)\n        dsT = pT * (dpT - Di[None, :])\n        dsT = dsT\n        dk += tl.dot(dsT, tl.trans(qT), allow_tf32=False)\n        curr_m += step_m\n        qT_ptrs += step_m * stride_tok\n        do_ptrs += step_m * stride_tok\n    return dk, dv\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "00d45e29-aec4-478f-9d7a-072cf74b079a"
  },
  {
    "input": "@triton.jit\ndef _single_query_cached_kv_attention_v2_unroll4(exp_sums, max_logits, out,\n    q, k_cache, v_cache, head_mapping, scale, block_tables, seq_lens,\n    partiton_size, max_num_blocks_per_seq, alibi_slopes, stride_qm,\n    stride_qn, stride_om, stride_on, stride_ok, stride_km, stride_kn,\n    stride_kk, stride_exp_m, stride_exp_n, BLOCK_SIZE: 'tl.constexpr',\n    HEAD_SIZE: 'tl.constexpr'):\n    seq_idx = tl.program_id(axis=1)\n    par_idx = tl.program_id(axis=2)\n    seq_len = tl.load(seq_lens + seq_idx)\n    if par_idx * partiton_size >= seq_len:\n        return\n    num_context_blocks = tl.cdiv(seq_len, BLOCK_SIZE)\n    num_blocks_per_par = partiton_size // BLOCK_SIZE\n    start_block_idx = par_idx * num_blocks_per_par\n    end_block_idx = tl.minimum(start_block_idx + num_blocks_per_par,\n        num_context_blocks)\n    head_idx = tl.program_id(axis=0)\n    kv_head_idx = tl.load(head_mapping + head_idx)\n    if alibi_slopes is None:\n        alibi_slope = 0.0\n    else:\n        alibi_slope = tl.load(alibi_slopes + head_idx)\n    block_offs = tl.arange(0, BLOCK_SIZE)\n    head_size_offs = tl.arange(0, HEAD_SIZE)\n    q = tl.load(q + seq_idx * stride_qm + head_idx * stride_qn + head_size_offs\n        )\n    q = q * scale\n    qkv = tl.zeros([BLOCK_SIZE, HEAD_SIZE], dtype=tl.float32)\n    qk_max = float('-inf')\n    exp_sum = 0.0\n    fp16_0 = tl.zeros([1, 1], dtype=k_cache.dtype.element_ty)\n    base_offs_kv = kv_head_idx * stride_kn + block_offs[:, None\n        ] * stride_kk + head_size_offs[None, :]\n    block_base_ptrs = block_tables + seq_idx * max_num_blocks_per_seq\n    for block_idx in range(start_block_idx, end_block_idx, 4):\n        mask_0 = block_offs[:, None] < seq_len - (block_idx + 0) * BLOCK_SIZE\n        mask_1 = block_offs[:, None] < seq_len - (block_idx + 1) * BLOCK_SIZE\n        mask_2 = block_offs[:, None] < seq_len - (block_idx + 2) * BLOCK_SIZE\n        mask_3 = block_offs[:, None] < seq_len - (block_idx + 3) * BLOCK_SIZE\n        offs_kv_0 = tl.load(block_base_ptrs + block_idx + 0\n            ) * stride_km + base_offs_kv\n        offs_kv_1 = tl.load(block_base_ptrs + block_idx + 1\n            ) * stride_km + base_offs_kv\n        offs_kv_2 = tl.load(block_base_ptrs + block_idx + 2\n            ) * stride_km + base_offs_kv\n        offs_kv_3 = tl.load(block_base_ptrs + block_idx + 3\n            ) * stride_km + base_offs_kv\n        k_0 = tl.load(k_cache + offs_kv_0, mask=mask_0, other=fp16_0)\n        k_1 = tl.load(k_cache + offs_kv_1, mask=mask_1, other=fp16_0)\n        k_2 = tl.load(k_cache + offs_kv_2, mask=mask_2, other=fp16_0)\n        k_3 = tl.load(k_cache + offs_kv_3, mask=mask_3, other=fp16_0)\n        v_0 = tl.load(v_cache + offs_kv_0, mask=mask_0, other=fp16_0)\n        v_1 = tl.load(v_cache + offs_kv_1, mask=mask_1, other=fp16_0)\n        v_2 = tl.load(v_cache + offs_kv_2, mask=mask_2, other=fp16_0)\n        v_3 = tl.load(v_cache + offs_kv_3, mask=mask_3, other=fp16_0)\n        _qk_0 = tl.sum(q[None, :] * k_0, axis=1)\n        _qk_1 = tl.sum(q[None, :] * k_1, axis=1)\n        _qk_2 = tl.sum(q[None, :] * k_2, axis=1)\n        _qk_3 = tl.sum(q[None, :] * k_3, axis=1)\n        _qk_0 += alibi_slope * ((block_idx + 0) * BLOCK_SIZE + block_offs -\n            seq_len + 1)\n        _qk_1 += alibi_slope * ((block_idx + 1) * BLOCK_SIZE + block_offs -\n            seq_len + 1)\n        _qk_2 += alibi_slope * ((block_idx + 2) * BLOCK_SIZE + block_offs -\n            seq_len + 1)\n        _qk_3 += alibi_slope * ((block_idx + 3) * BLOCK_SIZE + block_offs -\n            seq_len + 1)\n        _qk_max = tl.maximum(tl.max(_qk_0, axis=0), qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_1, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_2, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_3, axis=0), _qk_max)\n        qk_0 = tl.where(mask_0, _qk_0[:, None], float('-inf'))\n        qk_1 = tl.where(mask_1, _qk_1[:, None], float('-inf'))\n        qk_2 = tl.where(mask_2, _qk_2[:, None], float('-inf'))\n        qk_3 = tl.where(mask_3, _qk_3[:, None], float('-inf'))\n        _exp_sum = exp_sum * tl.exp(qk_max - _qk_max) + tl.sum(tl.exp(_qk_0 -\n            _qk_max), axis=0) + tl.sum(tl.exp(_qk_1 - _qk_max), axis=0\n            ) + tl.sum(tl.exp(_qk_2 - _qk_max), axis=0) + tl.sum(tl.exp(\n            _qk_3 - _qk_max), axis=0)\n        qkv = qkv * (exp_sum * tl.exp(qk_max - _qk_max) / _exp_sum) + tl.exp(\n            qk_0 - _qk_max) / _exp_sum * v_0 + tl.exp(qk_1 - _qk_max\n            ) / _exp_sum * v_1 + tl.exp(qk_2 - _qk_max\n            ) / _exp_sum * v_2 + tl.exp(qk_3 - _qk_max) / _exp_sum * v_3\n        qk_max = _qk_max\n        exp_sum = _exp_sum\n    offs_exp = seq_idx * stride_exp_m + head_idx * stride_exp_n + par_idx\n    tl.store(exp_sums + offs_exp, exp_sum)\n    tl.store(max_logits + offs_exp, qk_max)\n    offs_out = (seq_idx * stride_om + head_idx * stride_on + par_idx *\n        stride_ok + head_size_offs)\n    tl.store(out + offs_out, tl.sum(qkv, axis=0))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5588f0d4-22a1-444d-8787-23b5363f49b0"
  },
  {
    "input": "@triton.jit\ndef _flash_decoding_fwd_kernel(Q, KCache, VCache, mid_o, mid_o_lse,\n    kv_seq_len, q_len: 'tl.constexpr', batch_size, sm_scale, stride_qt,\n    stride_qh, stride_q_qlen, stride_qd, stride_kb, stride_kh, stride_kt,\n    stride_kd, stride_vb, stride_vh, stride_vt, stride_vd, stride_mid_ot,\n    stride_mid_oh, stride_mid_ob, stride_mid_oqlen, stride_mid_od,\n    stride_mid_o_lset, stride_mid_o_lseh, stride_mid_o_lseb, KV_GROUPS:\n    'tl.constexpr', BLOCK_KV: 'tl.constexpr', HEAD_DIM: 'tl.constexpr'):\n    cur_token_idx = tl.program_id(0)\n    cur_head_idx = tl.program_id(1)\n    block_start_kv = tl.program_id(2)\n    cur_kv_seq_len = tl.load(kv_seq_len + cur_token_idx)\n    if block_start_kv * BLOCK_KV >= cur_kv_seq_len:\n        return\n    offsets_dmodel = tl.arange(0, HEAD_DIM)\n    offsets_q = cur_token_idx * stride_qt + cur_head_idx * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + offsets_q, shape=(q_len,\n        HEAD_DIM), strides=(stride_q_qlen, stride_qd), offsets=(0, 0),\n        block_shape=(q_len, HEAD_DIM), order=(0, 1))\n    q = tl.load(Q_block_ptr)\n    cur_kv_head_idx = cur_head_idx // KV_GROUPS\n    cur_k_offset = (cur_token_idx * stride_kb + cur_kv_head_idx * stride_kh +\n        block_start_kv * BLOCK_KV * stride_kt)\n    cur_v_offset = (cur_token_idx * stride_vb + cur_kv_head_idx * stride_vh +\n        block_start_kv * BLOCK_KV * stride_vt)\n    K_block_ptr = tl.make_block_ptr(base=KCache + cur_k_offset, shape=(\n        cur_kv_seq_len, HEAD_DIM), strides=(stride_kd, stride_kt), offsets=\n        (0, 0), block_shape=(HEAD_DIM, BLOCK_KV), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=VCache + cur_v_offset, shape=(\n        cur_kv_seq_len, HEAD_DIM), strides=(stride_vt, stride_vd), offsets=\n        (0, 0), block_shape=(BLOCK_KV, HEAD_DIM), order=(0, 1))\n    block_mask = block_start_kv * BLOCK_KV + tl.arange(0, BLOCK_KV\n        ) < cur_kv_seq_len\n    k_cur_block = tl.load(K_block_ptr)\n    v_cur_block = tl.load(V_block_ptr)\n    acc = tl.zeros([q_len, HEAD_DIM], dtype=tl.float32)\n    S_ij = tl.zeros([q_len, BLOCK_KV], dtype=tl.float32)\n    S_ij += tl.dot(q, k_cur_block)\n    S_ij = tl.where(block_mask[None, :], S_ij, float('-inf'))\n    S_ij *= sm_scale\n    m = tl.max(S_ij, 1)\n    S_ij -= m[:, None]\n    p_ij_hat = tl.exp(S_ij)\n    l_i = tl.sum(p_ij_hat, 1)\n    p_ij_hat = p_ij_hat\n    acc += tl.dot(p_ij_hat, v_cur_block)\n    acc = acc / l_i[:, None]\n    cur_offest_mid = (cur_token_idx * stride_mid_ot + cur_head_idx *\n        stride_mid_oh + block_start_kv * stride_mid_ob)\n    offsets_mid_o = tl.make_block_ptr(base=mid_o + cur_offest_mid, shape=(\n        q_len, HEAD_DIM), strides=(stride_mid_oqlen, stride_mid_od),\n        offsets=(0, 0), block_shape=(q_len, HEAD_DIM), order=(0, 1))\n    tl.store(offsets_mid_o, acc)\n    offsets_qlen = tl.arange(0, q_len)\n    offsets_mid_o_lse = (cur_token_idx * stride_mid_o_lset + cur_head_idx *\n        stride_mid_o_lseh + block_start_kv * stride_mid_o_lseb + offsets_qlen)\n    tl.store(mid_o_lse + offsets_mid_o_lse, m + tl.log(l_i))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "928fc5b4-1f39-4891-b4c5-e5043fee419f"
  },
  {
    "input": "@triton.jit\ndef softmax_mask_bias_kernel(output_ptr, input_ptr, mask_ptr, bias_ptr,\n    input_row_stride, output_row_stride, n_cols, n_heads, BLOCK_SIZE:\n    'tl.constexpr', use_mask: 'tl.constexpr', use_bias: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_row_ptr = input_ptr + row_idx * input_row_stride\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n    input_ptrs = input_row_ptr + col_offsets\n    output_ptrs = output_row_ptr + col_offsets\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + row_idx // (n_heads * n_cols) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + row_idx % (n_heads * n_cols) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n        col_offsets, n_cols, use_mask, use_bias)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "b5ee5ae7-a90c-4e49-832d-16f0c02d58b1"
  },
  {
    "input": "@triton.jit\ndef mstore2d(registers, REG_ROWS: 'tl.constexpr', REG_COLS: 'tl.constexpr',\n    o_base, o_start_row, o_start_col, o_rows, o_cols, stride_row, stride_col):\n    off_rows = tl.arange(0, REG_ROWS) + o_start_row\n    off_cols = tl.arange(0, REG_COLS) + o_start_col\n    o_ptrs = o_base + off_rows[:, None] * stride_row + off_cols[None, :\n        ] * stride_col\n    o_ptrs_mask = tl.full([REG_ROWS, REG_COLS], 1, dtype=tl.int1)\n    row_overflow = o_start_row + REG_ROWS - o_rows\n    if row_overflow > 0:\n        o_ptrs_mask = o_ptrs_mask & (off_rows[:, None] < o_rows)\n    col_overflow = o_start_col + REG_COLS - o_cols\n    if col_overflow > 0:\n        o_ptrs_mask = o_ptrs_mask & (off_cols[None, :] < o_cols)\n    tl.store(o_ptrs, registers, mask=o_ptrs_mask)\n    return o_ptrs, o_ptrs_mask\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "e88fbe70-4956-4c3b-8549-8af2bf887466"
  },
  {
    "input": "@triton.jit\ndef _layernorm_kernel_fwd(x_ptr, weight_ptr, bias_ptr, z_ptr, H, eps=1e-05,\n    BLOCK_SIZE: 'tl.constexpr'=16):\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * H\n    z_row_ptr = z_ptr + row_idx * H\n    _sum = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for i in range(0, H, BLOCK_SIZE):\n        col_offsets = i + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(x_row_ptr + col_offsets, mask=col_offsets < H)\n        _sum += x\n    mean = tl.sum(_sum, axis=0) / H\n    x_var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for i in range(0, H, BLOCK_SIZE):\n        col_offsets = i + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(x_row_ptr + col_offsets, mask=col_offsets < H)\n        x = tl.where(col_offsets < H, x - mean, 0.0)\n        x_var += x * x\n    x_var = tl.sum(x_var, axis=0) / H\n    rtsd = tl.sqrt(x_var + eps)\n    for i in range(0, H, BLOCK_SIZE):\n        col_offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < H\n        x = tl.load(x_row_ptr + col_offsets, mask=mask)\n        w = tl.load(weight_ptr + col_offsets, mask=mask)\n        b = tl.load(bias_ptr + col_offsets)\n        x_hat = (x - mean) / rtsd\n        z = x_hat * w + b\n        tl.store(z_row_ptr + col_offsets, z, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "9a48547a-d576-4ed0-9320-fd319a52a73a"
  },
  {
    "input": "@triton.jit\ndef paged_attention_v2(scratchpad_key_ptr, scratchpad_value_ptr,\n    partition_buf_ptr, output_ptr, query_ptr, key_cache_ptr,\n    value_cache_ptr, block_tables_ptr, context_lens_ptr, scale, num_seqs,\n    num_heads, cache_block_stride, num_partitions, PARTITION_SIZE:\n    'tl.constexpr', MAX_SEQ_LEN: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    HEAD_SIZE: 'tl.constexpr', MAX_NUM_BLOCKS_PER_SEQ: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    partition_idx = tl.program_id(2)\n    query_offset = seq_idx * num_seqs + head_idx * HEAD_SIZE\n    query_head = tl.load(query_ptr + query_offset + tl.arange(0, HEAD_SIZE))\n    print_tensor_dim(query_head, 'query_head')\n    block_table_offset = seq_idx * MAX_NUM_BLOCKS_PER_SEQ\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    assert context_len <= MAX_SEQ_LEN\n    token_start_idx = partition_idx * PARTITION_SIZE\n    token_end_idx = min((partition_idx + 1) * PARTITION_SIZE, context_len)\n    for tok_idx in range(token_start_idx, token_end_idx):\n        logical_block_offset = tok_idx // BLOCK_SIZE\n        physical_block_idx = tl.load(block_tables_ptr + block_table_offset +\n            logical_block_offset)\n        start_of_block_offset = (physical_block_idx * cache_block_stride + \n            head_idx * HEAD_SIZE * BLOCK_SIZE)\n        tok_idx_within_block = tok_idx % BLOCK_SIZE\n        tok_offsets = start_of_block_offset + BLOCK_SIZE * tl.arange(0,\n            HEAD_SIZE) + tok_idx_within_block\n        tok_key = tl.load(key_cache_ptr + tok_offsets)\n        tok_value = tl.load(value_cache_ptr + tok_offsets)\n        scratchpad_offset = seq_idx * (MAX_SEQ_LEN * num_heads * HEAD_SIZE\n            ) + tok_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n        print_tensor_dim(scratchpad_key_ptr, 'scratchpad_key_ptr')\n        mask = tl.full([HEAD_SIZE], 1, dtype=tl.float32) > 0\n        tl.store(scratchpad_key_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_key, mask)\n        tl.store(scratchpad_value_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_value, mask)\n    tl.debug_barrier()\n    start_seq_offset = MAX_SEQ_LEN * num_heads * HEAD_SIZE * seq_idx\n    start_tok_offsets = start_seq_offset + tl.arange(0, PARTITION_SIZE) * (\n        num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    mask = tl.arange(0, PARTITION_SIZE)[:, None] < context_len\n    kv_offs = start_tok_offsets[:, None] + tl.arange(0, HEAD_SIZE)[None, :]\n    print_tensor_dim(kv_offs, 'kv_offs_v2')\n    keys = tl.load(scratchpad_key_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(keys, 'keys_v2')\n    scores = tl.sum(scale * keys * query_head[None, :], axis=1)\n    print_tensor_dim(keys, 'scores_v2')\n    partition_buf_offset = (start_seq_offset + head_idx * HEAD_SIZE + \n        partition_idx * PARTITION_SIZE)\n    print_tensor_dim(partition_buf_offset, 'partition_buf_offset_v2')\n    tl.store(partition_buf_ptr + partition_buf_offset + tl.arange(0,\n        PARTITION_SIZE), scores)\n    mask = tl.full([PARTITION_SIZE], -float('inf'), dtype=tl.float32)\n    cond = tl.arange(0, PARTITION_SIZE) < context_len\n    scores_masked = tl.where(cond, scores, mask)\n    scores_minus_max = scores_masked - tl.max(scores_masked, axis=0)\n    numerator = tl.exp(scores_minus_max)\n    denominator = tl.sum(numerator, axis=0) + float(1e-06)\n    logits = numerator / denominator\n    print_tensor_dim(logits, 'logits_v2')\n    values = tl.load(scratchpad_value_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(values, 'values_v2')\n    weighted_values += tl.sum(values * logits[:, None], axis=0)\n    print_tensor_dim(weighted_values, 'weighed_values_v2')\n    output_offset = seq_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    tl.store(output_ptr + output_offset + tl.arange(0, HEAD_SIZE),\n        weighted_values)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "f03acea1-b324-448b-9931-08eca8885618"
  },
  {
    "input": "@triton.jit\ndef gated_matmul_bwd_weights(input, y1_grad, y2_grad, dw1, dw2, M, N, K,\n    stride_dom, stride_im, stride_wn, dtype: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', GROUP_N: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', IS_EVEN_MNK: 'tl.constexpr'):\n    \"\"\"\n    Kernel for backward gated MLP\n    We group along the M axis\n\n    Ref :\n    w1_grad = torch.matmul(y1_grad.t(), x)\n    w2_grad = torch.matmul(y2_grad.t(), x)\n    \"\"\"\n    pid = tl.program_id(0)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_k = tl.cdiv(K, BLOCK_K)\n    num_pid_in_group = GROUP_N * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_n = group_id * GROUP_N\n    GROUP_N = min(num_pid_n - first_pid_n, GROUP_N)\n    pid_n = first_pid_n + pid % GROUP_N\n    pid_k = pid % num_pid_in_group // GROUP_N\n    y1_grad_block_ptr = tl.make_block_ptr(base=y1_grad, shape=(N, M),\n        strides=(1, stride_dom), offsets=(pid_n * BLOCK_N, 0), block_shape=\n        (BLOCK_N, BLOCK_M), order=(0, 1))\n    y2_grad_block_ptr = tl.make_block_ptr(base=y2_grad, shape=(N, M),\n        strides=(1, stride_dom), offsets=(pid_n * BLOCK_N, 0), block_shape=\n        (BLOCK_N, BLOCK_M), order=(0, 1))\n    input_block_ptr = tl.make_block_ptr(base=input, shape=(M, K), strides=(\n        stride_im, 1), offsets=(0, pid_k * BLOCK_K), block_shape=(BLOCK_M,\n        BLOCK_K), order=(1, 0))\n    ref = tl.load(input + tl.arange(0, 1))\n    acc_dw1 = tl.zeros((BLOCK_N, BLOCK_K), dtype=tl.float32)\n    acc_dw2 = tl.zeros((BLOCK_N, BLOCK_K), dtype=tl.float32)\n    for i in range(0, M, BLOCK_M):\n        if IS_EVEN_MNK:\n            y1grad_blk = tl.load(y1_grad_block_ptr)\n            y2grad_blk = tl.load(y2_grad_block_ptr)\n            x = tl.load(input_block_ptr)\n        else:\n            y1grad_blk = tl.load(y1_grad_block_ptr, boundary_check=(0, 1))\n            y2grad_blk = tl.load(y2_grad_block_ptr, boundary_check=(0, 1))\n            x = tl.load(input_block_ptr, boundary_check=(0, 1))\n        acc_dw1 += tl.dot(y1grad_blk, x)\n        acc_dw2 += tl.dot(y2grad_blk, x)\n        y1_grad_block_ptr = tl.advance(y1_grad_block_ptr, (0, BLOCK_M))\n        y2_grad_block_ptr = tl.advance(y2_grad_block_ptr, (0, BLOCK_M))\n        input_block_ptr = tl.advance(input_block_ptr, (BLOCK_M, 0))\n    dw1_ptrs = tl.make_block_ptr(base=dw1, shape=(N, K), strides=(stride_wn,\n        1), offsets=(pid_n * BLOCK_N, pid_k * BLOCK_K), block_shape=(\n        BLOCK_N, BLOCK_K), order=(1, 0))\n    dw2_ptrs = tl.make_block_ptr(base=dw2, shape=(N, K), strides=(stride_wn,\n        1), offsets=(pid_n * BLOCK_N, pid_k * BLOCK_K), block_shape=(\n        BLOCK_N, BLOCK_K), order=(1, 0))\n    if IS_EVEN_MNK:\n        tl.store(dw1_ptrs, acc_dw1)\n        tl.store(dw2_ptrs, acc_dw2)\n    else:\n        tl.store(dw1_ptrs, acc_dw1, boundary_check=(0, 1))\n        tl.store(dw2_ptrs, acc_dw2, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "78d56f05-0230-47c8-a4f8-aeeb125a530e"
  },
  {
    "input": "@triton.jit\ndef _bwd_kv_kernel(Q, K, V, B, sm_scale, DO, DK, DV, DS, L, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_bz,\n    stride_bh, stride_bm, stride_bn, stride_doz, stride_doh, stride_dom,\n    stride_dok, stride_dkz, stride_dkh, stride_dkn, stride_dkk, stride_dvz,\n    stride_dvh, stride_dvn, stride_dvk, Z, H, M, N, P_SEQ, lock, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    CAUSAL: 'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', RETURN_DS: 'tl.constexpr',\n    IS_BATCH_REDUCED: 'tl.constexpr', GROUP_SIZE_BIAS: 'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_n = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_h * stride_kh\n    V += off_z * stride_vz + off_h * stride_vh\n    if HAS_BIAS:\n        if IS_BATCH_REDUCED:\n            B += off_h * stride_bh\n        else:\n            B += off_z * stride_bz + off_h * stride_bh\n    DO += off_z * stride_doz + off_h * stride_doh\n    DK += off_z * stride_dkz + off_h * stride_dkh\n    DV += off_z * stride_dvz + off_h * stride_dvh\n    if RETURN_DS:\n        DS += off_z * stride_bz + off_h * stride_bh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    if CAUSAL:\n        lo = tl.maximum(start_n * BLOCK_N - P_SEQ, 0)\n        lo = lo // BLOCK_M * BLOCK_M\n    else:\n        lo = 0\n    offs_m_init = lo + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m_init[:, None] * stride_qm + offs_k[None, :] *\n        stride_qk)\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n    do_ptrs = DO + (offs_m_init[:, None] * stride_dom + offs_k[None, :] *\n        stride_dok)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_k[None, :] * stride_dvk\n        )\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_k[None, :] * stride_dkk\n        )\n    if HAS_BIAS:\n        bias_ptrs = B + (offs_m_init[:, None] * stride_bm + offs_n[None, :] *\n            stride_bn)\n    if RETURN_DS:\n        ds_ptrs = DS + (offs_m_init[:, None] * stride_bm + offs_n[None, :] *\n            stride_bn)\n    mask_n = offs_n < N\n    if DIVISIBLE_N:\n        v = tl.load(v_ptrs)\n        k = tl.load(k_ptrs)\n    else:\n        v = tl.load(v_ptrs, mask=mask_n[:, None])\n        k = tl.load(k_ptrs, mask=mask_n[:, None])\n    dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    for start_m in range(lo, M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m = start_m + offs_m_base\n        causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n        mask_m = offs_m < M\n        if DIVISIBLE_M:\n            q = tl.load(q_ptrs)\n        else:\n            valid_mask = mask_m[:, None]\n            q = tl.load(q_ptrs, mask=mask_m[:, None])\n        if HAS_BIAS:\n            if DIVISIBLE_M and DIVISIBLE_N:\n                b = tl.load(bias_ptrs)\n            else:\n                b = tl.load(bias_ptrs, mask=mask_m[:, None] & mask_n[None, :])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k)) * sm_scale\n        if HAS_BIAS:\n            s += b\n        if DIVISIBLE_M:\n            l = tl.load(L + offs_m)\n        else:\n            l = tl.load(L + offs_m, mask=mask_m)\n        p = tl.math.exp2((s - l[:, None]) * log2e)\n        if not DIVISIBLE_M:\n            p = tl.where(valid_mask, p, 0.0)\n        if CAUSAL:\n            p = tl.where(causal_mask, p, 0.0)\n        if DIVISIBLE_M:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=mask_m[:, None])\n        dv += tl.dot(tl.trans(p), do)\n        if DIVISIBLE_M:\n            delta = tl.load(D + offs_m)\n        else:\n            delta = tl.load(D + offs_m, mask=mask_m)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_M:\n            ds = tl.where(valid_mask, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        ds = ds\n        dk += tl.dot(tl.trans(ds), q)\n        if RETURN_DS:\n            if DIVISIBLE_M and DIVISIBLE_N:\n                tl.store(ds_ptrs, ds)\n            else:\n                tl.store(ds_ptrs, ds, mask=mask_m[:, None] & mask_n[None, :])\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if HAS_BIAS:\n            bias_ptrs += BLOCK_M * stride_bm\n        if RETURN_DS:\n            ds_ptrs += BLOCK_M * stride_bm\n    dk *= sm_scale\n    if DIVISIBLE_N:\n        tl.store(dk_ptrs, dk)\n        tl.store(dv_ptrs, dv)\n    else:\n        tl.store(dk_ptrs, dk, mask=mask_n[:, None])\n        tl.store(dv_ptrs, dv, mask=mask_n[:, None])\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "2ebb9433-3ac7-48b1-8535-0de2f7e74127"
  },
  {
    "input": "@triton.jit\ndef _rope_padded_kernel(xq, xk, xv, out_q, cache_k, cache_v, seqstartq,\n    seqstartk, seqlenk, theta, linear_scale, use_dynamic_scaling:\n    'tl.constexpr', dynamic_old_context_len: 'tl.constexpr',\n    dynamic_scale_factor: 'tl.constexpr', dynamic_low_freq_factor:\n    'tl.constexpr', dynamic_high_freq_factor: 'tl.constexpr', first_seqpos,\n    seqpos, k_start: 'tl.constexpr', v_start: 'tl.constexpr', n_groups, dim:\n    'tl.constexpr', stride_xqM, stride_xqG, stride_xqH, stride_xkM,\n    stride_xkG, stride_xkH, stride_xvM, stride_xvG, stride_xvH,\n    stride_cachekM, stride_cachekG, stride_cachekH, stride_cachevM,\n    stride_cachevG, stride_cachevH, stride_seqstartq, stride_seqstartk,\n    stride_seqlenk, stride_outqM, stride_outqG, stride_outqH, stride_seqpos,\n    internal_dtype: 'tl.constexpr', const_batch_strides: 'tl.constexpr',\n    cache_padding_length, seqlenk_shift: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', adjacents: 'tl.constexpr'):\n    \"\"\"\n    Each letter in this diagram is a whole row of length dim.\n\n     INPUT      xq        xk       xv\n\n        head_dim \u2500\u25ba\n\n      batch   qqqqqq      kk       vv\n        \u2502     qqqqqq      kk       vv\n        \u25bc     qqqqqq      kk       vv\n\n    head_idx:  (goes across all heads of all 3 inputs)\n              \u25b2     \u25b2     \u25b2 \u25b2      \u25b2 \u25b2\n              \u2502     \u2502     \u2502 \u2502      \u2502 \u2502\n                          \u2502        \u2502\n              0  k_start  \u2502v_start \u2502n_total_heads\n                          \u2502        \u2502\n                          \u2502        \u2502\n                      k_start    v_start\n\n    Output is to out_q (same shape as xq), an xk-shaped part\n    of cache_k and an xv-shaped part of cache_v\n    \"\"\"\n    query_pos_in_batch_elt = tl.program_id(0)\n    batch_elt = tl.program_id(1)\n    group_head_idx = tl.program_id(2)\n    group_idx = group_head_idx % n_groups\n    head_idx = group_head_idx // n_groups\n    if internal_dtype == 'f32':\n        theta = theta\n    elif internal_dtype == 'f64':\n        theta = theta\n    if const_batch_strides:\n        query_pos = query_pos_in_batch_elt + tl.num_programs(1) * batch_elt\n        end_query_pos = tl.num_programs(1) * (batch_elt + 1)\n    else:\n        query_pos = query_pos_in_batch_elt + tl.load(seqstartq + batch_elt *\n            stride_seqstartq)\n        end_query_pos = tl.load(seqstartq + (batch_elt + 1) * stride_seqstartq)\n        if query_pos >= end_query_pos:\n            return\n    is_q = head_idx < k_start\n    is_v = head_idx >= v_start\n    xq += (query_pos * stride_xqM + head_idx * stride_xqH + group_idx *\n        stride_xqG)\n    out_q += (query_pos * stride_outqM + head_idx * stride_outqH + \n        group_idx * stride_outqG)\n    if const_batch_strides:\n        cache_start = cache_padding_length * batch_elt\n    else:\n        cache_start = tl.load(seqstartk + batch_elt * stride_seqstartk)\n    end_of_batch_elt_cache = cache_start + tl.load(seqlenk + batch_elt *\n        stride_seqlenk) + seqlenk_shift\n    cache_pos = end_of_batch_elt_cache - (end_query_pos - query_pos)\n    if seqpos is not None:\n        seq_pos = tl.load(seqpos + query_pos * stride_seqpos)\n    else:\n        seq_pos = cache_pos - cache_start\n        if first_seqpos is not None:\n            seq_pos += tl.load(first_seqpos + batch_elt * stride_seqpos)\n    cache_k += ((head_idx - k_start) * stride_cachekH + cache_pos *\n        stride_cachekM + group_idx * stride_cachekG)\n    xk += query_pos * stride_xkM + (head_idx - k_start\n        ) * stride_xkH + group_idx * stride_xkG\n    in_qk = tl.where(is_q, xq, xk)\n    out_qk = tl.where(is_q, out_q, cache_k)\n    cache_v += ((head_idx - v_start) * stride_cachevH + cache_pos *\n        stride_cachevM + group_idx * stride_cachevG)\n    xv += query_pos * stride_xvM + (head_idx - v_start\n        ) * stride_xvH + group_idx * stride_xvG\n    out = tl.where(is_v, cache_v, out_qk)\n    x_in = tl.where(is_v, xv, in_qk)\n    for offset in range(0, dim // 2, BLOCK_SIZE // 2):\n        c = tl.arange(0, BLOCK_SIZE // 2)\n        powers = (offset + c) * 2.0\n        if adjacents:\n            cols_re = (offset + c) * 2\n            cols_im = cols_re + 1\n        else:\n            cols_re = offset + c\n            cols_im = cols_re + dim // 2\n        mask = cols_im < dim\n        re_x = tl.load(x_in + cols_re, mask=mask)\n        im_x = tl.load(x_in + cols_im, mask=mask)\n        freqs = pow(theta, powers / -dim)\n        if use_dynamic_scaling:\n            lo_freq_wavelen = dynamic_old_context_len / dynamic_low_freq_factor\n            hi_freq_wavelen = (dynamic_old_context_len /\n                dynamic_high_freq_factor)\n            wavelens = 6.28318530718 / freqs\n            is_low_freq = wavelens > lo_freq_wavelen\n            freqs = tl.where(is_low_freq, freqs / dynamic_scale_factor, freqs)\n            is_mid_freq = (hi_freq_wavelen < wavelens and wavelens <=\n                lo_freq_wavelen)\n            smooth = (dynamic_old_context_len / wavelens -\n                dynamic_low_freq_factor) / (dynamic_high_freq_factor -\n                dynamic_low_freq_factor)\n            freqs = tl.where(is_mid_freq, (1 - smooth) * freqs /\n                dynamic_scale_factor + smooth * freqs, freqs)\n        freqs = seq_pos * freqs / linear_scale\n        sines = tl.sin(freqs)\n        cosines = tl.cos(freqs)\n        re_out = re_x * cosines - im_x * sines\n        im_out = im_x * cosines + re_x * sines\n        re_out_ = tl.where(is_v, re_x, re_out)\n        im_out_ = tl.where(is_v, im_x, im_out)\n        if internal_dtype == 'f64':\n            if re_x.dtype == tl.bfloat16:\n                re_out_ = re_out_\n                im_out_ = im_out_\n        tl.store(out + cols_re, re_out_, mask=mask)\n        tl.store(out + cols_im, im_out_, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "161579ea-7016-462f-883e-f01964983b99"
  },
  {
    "input": "@triton.jit\ndef swiglu_forward_optimized(e_ptr, g_ptr, output_ptr, sigmoid_ptr, f_ptr,\n    e_stride, g_stride, output_stride, sigmoid_stride, f_stride, BLOCK_SIZE:\n    'tl.constexpr', n_cols):\n    row_idx = tl.program_id(axis=0)\n    col_offset = tl.arange(0, BLOCK_SIZE)\n    mask = col_offset < n_cols\n    e_ptr += row_idx * e_stride\n    g_ptr += row_idx * g_stride\n    output_ptr += row_idx * output_stride\n    sigmoid_ptr += row_idx * sigmoid_stride\n    f_ptr += row_idx * f_stride\n    e_row = tl.load(e_ptr + col_offset, mask=mask)\n    g_row = tl.load(g_ptr + col_offset, mask=mask)\n    sigmoid_e_row = tl.sigmoid(e_row)\n    f_row = e_row * sigmoid_e_row\n    tl.store(sigmoid_ptr + col_offset, sigmoid_e_row, mask=mask)\n    tl.store(f_ptr + col_offset, f_row, mask=mask)\n    output_row = f_row * g_row\n    tl.store(output_ptr + col_offset, output_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "fcb281cb-3120-47e4-9873-a5d65042e55d"
  },
  {
    "input": "@triton.jit\ndef offset_1d(sz: 'const', n_prev_chunks=0):\n    return n_prev_chunks * sz + tl.arange(0, sz)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "f2f6ca91-64cd-4f88-a3f6-9e68dd02ecda"
  },
  {
    "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, L, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX, sliding_window_offset,\n    sliding_window_size, IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', END: 'tl.constexpr', INIT: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    v_offset = off_z * stride_vz + off_hkv * stride_vh\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + v_offset, shape=(NKV_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(ROUND_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n        m_i = tl.load(m_ptrs)\n        l_i = tl.load(l_ptrs)\n        acc = tl.load(O_block_ptr)\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n        V_block_ptr, start_m, qk_scale, NKV_CTX, sliding_window_offset,\n        sliding_window_size, BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW,\n        IS_EVEN_M, IS_EVEN_N, COMPLEMENT_SLIDING_WINDOW)\n    if END:\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "809bc9b7-4e07-4284-914e-e9296d5a710f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_fwd_kernel_o(q, k, v, h, o, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_o = b_o * d_i[:, None]\n    b_s = b_s * d_s\n    if HEAD_FIRST:\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Memory Management",
    "subcategory": "memory padding",
    "uuid": "5a3de396-2bca-422b-909e-657f3c0bf238"
  },
  {
    "input": "@triton.jit\ndef _voxel_grid_splat(to_splat, grad_image, batch_index, ix, iy, iz, N:\n    'tl.constexpr', C: 'tl.constexpr', ID: 'tl.constexpr', IH:\n    'tl.constexpr', IW: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    iz = (iz + 1) / 2 * ID - 0.5\n    ix0 = ix - ix % 1\n    iy0 = iy - iy % 1\n    iz0 = iz - iz % 1\n    V000x = ix0\n    V000y = iy0\n    V000z = iz0\n    V100x = ix0\n    V100y = iy0\n    V100z = iz0 + 1\n    V010x = ix0\n    V010y = iy0 + 1\n    V010z = iz0\n    V001x = ix0 + 1\n    V001y = iy0\n    V001z = iz0\n    V101x = ix0 + 1\n    V101y = iy0\n    V101z = iz0 + 1\n    V011x = ix0 + 1\n    V011y = iy0 + 1\n    V011z = iz0\n    V110x = ix0\n    V110y = iy0 + 1\n    V110z = iz0 + 1\n    V111x = ix0 + 1\n    V111y = iy0 + 1\n    V111z = iz0 + 1\n    x = ix - ix0\n    y = iy - iy0\n    z = iz - iz0\n    _splat_3d(to_splat, grad_image, (1 - x) * (1 - y) * (1 - z),\n        batch_index, V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, (1 - x) * (1 - y) * z, batch_index,\n        V100x, V100y, V100z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, (1 - x) * y * (1 - z), batch_index,\n        V010x, V010y, V010z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * (1 - y) * (1 - z), batch_index,\n        V001x, V001y, V001z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * (1 - y) * z, batch_index, V101x,\n        V101y, V101z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * y * (1 - z), batch_index, V011x,\n        V011y, V011z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, (1 - x) * y * z, batch_index, V110x,\n        V110y, V110z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_image, x * y * z, batch_index, V111x, V111y,\n        V111z, ID, IH, IW, C, BLOCK_SIZE)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "334bea45-d762-42eb-a10c-17b8d44d3c79"
  },
  {
    "input": "@triton.jit\ndef _softplus(x):\n    return tl.where(x >= 0, x + tl.log(1 + tl.exp(-x)), tl.log(1 + tl.exp(x)))\n",
    "category": "Activation Functions",
    "subcategory": "softplus",
    "uuid": "322cf62b-f33c-459c-bf0e-cf09eea28ee2"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef rms_norm_backward_kernel(output_grad_pointer, input_pointer,\n    inv_rms_pointer, weight_pointer, input_grad_pointer,\n    weight_grad_pointer, batch_dim, feat_dim, output_grad_batch_stride,\n    output_grad_feat_stride, input_batch_stride, input_feat_stride,\n    input_grad_batch_stride, input_grad_feat_stride,\n    weight_grad_batch_stride, weight_grad_feat_stride, scale_by_weight:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT:\n    'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of root mean square normalization.\n\n    Args:\n        output_grad_pointer: Pointer to root mean square normalization's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        inv_rms_pointer: Pointer to the input's inverse root mean square.\n            The inverse root mean square should be of shape [batch_dim].\n        weight_pointer: Pointer to optional weights if affine transform occurred.\n            The weights, if provided, must be of shape [feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        weight_grad_pointer: Pointer to an optional container the weights' row-wise gradients\n            are written to if scale_by_weight is True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        bias_grad_pointer: Pointer to an optional container the bias vector's row-wise gradients\n            are written to if scale_by_weight and add_bias are True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        weight_grad_batch_stride: Stride necessary to jump one element along the\n            weight gradient container's batch dimension.\n        weight_grad_feat_stride: Stride necessary to jump one element along the\n            weight gradient container's feature dimension.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n        ] + output_grad_feat_stride * feat_offset[None, :]\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    inv_rms = tl.load(inv_rms_pointer + batch_offset, mask=batch_mask)\n    pre_lin = input * inv_rms[:, None]\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        weight_output_grad_prod = weight * output_grad\n    else:\n        weight_output_grad_prod = output_grad\n    term1 = input * tl.sum(input * weight_output_grad_prod, axis=1)\n    term2 = inv_rms[:, None] * inv_rms[:, None]\n    input_grad = inv_rms[:, None] * (weight_output_grad_prod - term1 *\n        term2 / feat_dim)\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    if scale_by_weight:\n        weight_grad_pointer += (weight_grad_batch_stride * batch_pid + \n            weight_grad_feat_stride * feat_offset)\n        tl.store(weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0),\n            mask=feat_mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "f693e774-8c4b-42ae-a2d8-fd65c2f6075b"
  },
  {
    "input": "@triton.jit\ndef _triton_gemm_a16w4_sub_channel_kernel(A, B, C, scale_b, bias,\n    zero_points, M, N, K, rescale_m, rescale_n, rescale_k, stride_am,\n    stride_ak, stride_bn, stride_bk, stride_cm, stride_cn, stride_zpk,\n    stride_zpn, stride_scalek, stride_scalen, add_bias: 'tl.constexpr',\n    add_zero_points: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr',\n    SPLIT_K: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rbn[:, None] * stride_bn + rk[None, :] * stride_bk)\n    acc_l = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)\n    acc_h = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)\n    _A0 = tl.zeros((1, 1), dtype=A.dtype.element_ty)\n    _B0 = tl.zeros((1, 1), dtype=B.dtype.element_ty)\n    if add_zero_points:\n        zero_points_offs = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n        _ZERO_POINT0 = tl.zeros([1], dtype=zero_points.dtype.element_ty)\n    scale_offs = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n    _SCALE0 = tl.zeros([1], dtype=scale_b.dtype.element_ty)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        k_remaining = K - k * (BLOCK_K * SPLIT_K)\n        b_int4_two = tl.load(B, mask=rk[None, :] < k_remaining, other=_B0)\n        b_int4_l = b_int4_two.__lshift__(4).__rshift__(4)\n        b_int4_h = b_int4_two.__rshift__(4)\n        if add_zero_points:\n            zero_points_ptrs = (zero_points + k * SPLIT_K * stride_zpk + \n                pid_z * stride_zpk + zero_points_offs)\n            zero_points_vals = tl.load(zero_points_ptrs, mask=\n                zero_points_offs < 2 * N, other=_ZERO_POINT0)\n            zero_points_vals = tl.reshape(zero_points_vals, (BLOCK_N, 2))\n            zp_l, zp_h = tl.split(zero_points_vals)\n            b_int4_l -= zp_l[:, None]\n            b_int4_h -= zp_h[:, None]\n        scales_val = tl.load(scale_b + k * SPLIT_K * stride_scalek + pid_z *\n            stride_scalek + scale_offs, mask=scale_offs < 2 * N, other=_SCALE0)\n        scales_val = tl.reshape(scales_val, (BLOCK_N, 2))\n        scale_l, scale_h = tl.split(scales_val)\n        b_int4_l = b_int4_l * scale_l[:, None]\n        b_int4_h = b_int4_h * scale_h[:, None]\n        a = tl.load(A, mask=rk[None, :] < k_remaining, other=_A0)\n        a = tl.trans(a)\n        acc_l += tl.dot(b_int4_l, a, out_dtype=tl.float32, allow_tf32=True)\n        acc_h += tl.dot(b_int4_h, a, out_dtype=tl.float32, allow_tf32=True)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    acc_l = tl.trans(acc_l)\n    acc_h = tl.trans(acc_h)\n    acc = tl.interleave(acc_l, acc_h)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n    mask = (rm < M)[:, None] & (rn < 2 * N)[None, :]\n    if add_bias:\n        offs_bias = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n        bias_ptrs = bias + offs_bias\n        _BIAS0 = tl.zeros([1], dtype=bias.dtype.element_ty)\n        bias_vals = tl.load(bias_ptrs, mask=offs_bias < 2 * N, other=_BIAS0)\n        if pid_z == 0:\n            acc += bias_vals[None, :]\n    if SPLIT_K == 1:\n        tl.store(C + rm[:, None] * stride_cm + rn[None, :], acc, mask=mask)\n    else:\n        tl.atomic_add(C + rm[:, None] * stride_cm + rn[None, :], acc, mask=mask\n            )\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b73e7600-7648-4961-b114-8e5cf6a2ff5b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK': 1024}, num_stages=\n    FORWARD_NUM_STAGES, num_warps=1), triton.Config({'BLOCK': 2048},\n    num_stages=FORWARD_NUM_STAGES, num_warps=8), triton.Config({'BLOCK': \n    4096}, num_stages=FORWARD_NUM_STAGES, num_warps=8), triton.Config({\n    'BLOCK': 8192}, num_stages=FORWARD_NUM_STAGES, num_warps=16), triton.\n    Config({'BLOCK': 16384}, num_stages=FORWARD_NUM_STAGES, num_warps=16)],\n    key=['N', 'CLASS_INDICES', 'log_size_logits', 'BUFFER_DTYPE'])\n@triton.jit\ndef _forward(LOGITS, PROBS, IDX, LOSS, weight, N, WEIGHT_BUFFER,\n    smoothing_factor, log_size_logits, WEIGHTS: 'tl.constexpr',\n    CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING: 'tl.constexpr',\n    IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE: 'tl.constexpr', BLOCK:\n    'tl.constexpr'):\n    if CLASS_INDICES:\n        _class_indices_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n            WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS,\n            CLASS_INDICES, LABEL_SMOOTHING, IGNORE_INDEX, BUFFER_DTYPE, BLOCK)\n    else:\n        _class_probs_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n            WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS,\n            CLASS_INDICES, LABEL_SMOOTHING, IGNORE_INDEX, BUFFER_DTYPE, BLOCK)\n",
    "category": "Kernel Operations",
    "subcategory": "kernel configuration",
    "uuid": "69f5bfe4-1398-43df-9ddc-ed3a2ac00fe0"
  },
  {
    "input": "@triton.jit\ndef _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "6fa6df79-2844-4ad7-9fa3-59c86acedfba"
  },
  {
    "input": "@triton.jit\ndef _softmax_kernel_bwd(output_ptr, stride_output_row, grad_ptr,\n    stride_grad_row, input_ptr, stride_input_row, n_cols, block_size:\n    'tl.constexpr'):\n    row_index = tl.program_id(0)\n    input_row_ptr = input_ptr + row_index * stride_input_row\n    grad_row_ptr = grad_ptr + row_index * stride_grad_row\n    col_offsets = tl.arange(0, block_size)\n    rw_mask = col_offsets < n_cols\n    input_row_ptrs = input_row_ptr + col_offsets\n    grad_row_ptrs = grad_row_ptr + col_offsets\n    probs_row = tl.load(input_row_ptrs, mask=rw_mask, other=0)\n    grads_row = tl.load(grad_row_ptrs, mask=rw_mask, other=0)\n    dx = probs_row * grads_row\n    dsm_out = dx - probs_row * tl.sum(dx, axis=0)\n    output_row_ptr = output_ptr + row_index * stride_output_row\n    output_ptrs = output_row_ptr + col_offsets\n    tl.store(output_ptrs, dsm_out, mask=rw_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "985579f5-7b8b-46f9-9259-b13d815eb09f"
  },
  {
    "input": "@triton.jit\ndef bw_kernel(grad_feature_grid, grad_feature_grid_sizes, directions,\n    origins, grid_idx, near, far, splatting_feature, mask, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', num_rays:\n    'tl.constexpr', grid_channel: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr',\n    feature_channel: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    mask_out_of_bounds_samples: 'tl.constexpr', contract_coords:\n    'tl.constexpr', disparity_at_inf: 'tl.constexpr', grad_splatting_feature):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    depth = near_buffer\n    grad_splatting_feature_buffer = tl.zeros((BLOCK_SIZE, feature_channel),\n        dtype=tl.float32)\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        grad_vec = sample_grid_rep(grad_feature_grid,\n            grad_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, grid_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_vec = grad_vec * mask_buffer\n        grad_splatting_feature_buffer += grad_vec\n    tl.store(grad_splatting_feature + offs_features,\n        grad_splatting_feature_buffer, mask=offs_features_mask)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "40b56c31-b8bb-4e92-8605-6ef0ca862048"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_retention_fwd_kernel(q, k, v, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.pow(2, -5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        h = b_b * h + _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "f11ab61a-a654-4d46-9f8a-b47c9a62fe71"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size',\n    'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_fwd_kernel(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m *\n        stride_ddA_cs_csize_m)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None,\n        :] * stride_cb_csize_n)\n    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr, 0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M\n    for start_n in range(lo, hi, BLOCK_SIZE_N):\n        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit - start_n), other=0.0)\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= dt_n\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n\n            [None, :] < chunk_size - start_n), other=0.0)\n        acc *= cb\n        dA_cs_n = tl.load(dA_cumsum_ptr + start_n + offs_n *\n            stride_dA_cs_csize, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=offs_n < \n            chunk_size - start_n - 1)\n        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n        tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N\n            ,), dtype=tl.float32), mask=offs_n < chunk_size - start_n - 1)\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "0a1951a8-0504-40c2-bb3e-8208013e8a16"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'bs_row': 256, 'bs_col': 256,\n    'group_sz': 8}, num_warps=8), triton.Config({'bs_row': 128, 'bs_col': \n    128, 'group_sz': 8}, num_warps=8), triton.Config({'bs_row': 64,\n    'bs_col': 64, 'group_sz': 8}, num_warps=8), triton.Config({'bs_row': 32,\n    'bs_col': 32, 'group_sz': 8}, num_warps=8), triton.Config({'bs_row': 16,\n    'bs_col': 16, 'group_sz': 8}, num_warps=8), triton.Config({'bs_row': \n    128, 'bs_col': 128, 'group_sz': 8}, num_warps=4), triton.Config({\n    'bs_row': 64, 'bs_col': 64, 'group_sz': 8}, num_warps=4), triton.Config\n    ({'bs_row': 32, 'bs_col': 32, 'group_sz': 8}, num_warps=4), triton.\n    Config({'bs_row': 16, 'bs_col': 16, 'group_sz': 8}, num_warps=4),\n    triton.Config({'bs_row': 256, 'bs_col': 256, 'group_sz': 4}, num_warps=\n    8), triton.Config({'bs_row': 128, 'bs_col': 128, 'group_sz': 4},\n    num_warps=8), triton.Config({'bs_row': 64, 'bs_col': 64, 'group_sz': 4},\n    num_warps=8), triton.Config({'bs_row': 32, 'bs_col': 32, 'group_sz': 4},\n    num_warps=8), triton.Config({'bs_row': 16, 'bs_col': 16, 'group_sz': 4},\n    num_warps=8), triton.Config({'bs_row': 128, 'bs_col': 128, 'group_sz': \n    4}, num_warps=4), triton.Config({'bs_row': 64, 'bs_col': 64, 'group_sz':\n    4}, num_warps=4), triton.Config({'bs_row': 32, 'bs_col': 32, 'group_sz':\n    4}, num_warps=4), triton.Config({'bs_row': 16, 'bs_col': 16, 'group_sz':\n    4}, num_warps=4)], key=['num_batches', 'num_rows', 'num_cols'])\n@triton.jit\ndef add_kernel(input1_ptr, input2_ptr, input_batch_stride, input_row_stride,\n    input_col_stride, num_batches, num_rows, num_cols, out_ptr, bs_row:\n    'tl.constexpr', bs_col: 'tl.constexpr', group_sz: 'tl.constexpr'):\n    batch_idx = tl.program_id(axis=0)\n    row_idx = tl.program_id(axis=1)\n    col_idx = tl.program_id(axis=2)\n    batch_offset = batch_idx * input_batch_stride\n    row_offset = row_idx * bs_row + tl.arange(0, bs_row)\n    col_offset = col_idx * bs_col + tl.arange(0, bs_col)\n    data_offset = row_offset[:, None] * input_row_stride + col_offset[None, :\n        ] * input_col_stride\n    row_mask = row_offset < num_rows\n    col_mask = col_offset < num_cols\n    data_mask = row_mask[:, None] & col_mask[None, :]\n    input1 = tl.load(input1_ptr + batch_offset + data_offset, data_mask)\n    input2 = tl.load(input2_ptr + batch_offset + data_offset, data_mask)\n    add = input1 + input2\n    tl.store(out_ptr + batch_offset + data_offset, add, mask=data_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "8893f53a-4467-420c-a406-48590c1f492c"
  },
  {
    "input": "@triton.jit\ndef unpack64(merged):\n    tl.static_assert(merged.dtype == tl.uint64)\n    b = (merged & 4294967295).to(tl.uint32)\n    a = (merged >> 32).to(tl.uint32)\n    return a, b\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "8a9b95a0-8045-4e89-b238-f29f3f81a798"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_intra_Vk(q, k, g, A, i_k, i_c, i_bh, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr'\n    ):\n    i_bg = i_bh // NG\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    o_k = i_k * BK + tl.arange(0, BK)\n    if i_t * BT + i_i * BC > T:\n        return\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    b_A = tl.zeros([BC, BC], tl.float32)\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bg * T * K, (K, T), (1, K), (i_k * BK,\n            i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bg * T * K, (K, T), (1, K), (i_k *\n            BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * K + min(i_t *\n            BT + i_i * BC, T) * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=o_k < K, other=0.0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A = tl.dot(b_qg, b_kg)\n        if i_k != 0:\n            b_A += tl.load(p_A, boundary_check=(0, 1))\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_bg * T * K + (i_t * BT +\n            i_j * BC) * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_bg * T * K + (i_t *\n            BT + i_j * BC) * K + o_k, BK), BK)\n        m_k = o_k < K\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        o_i = tl.arange(0, BC)\n        m_A = o_i[:, None] >= o_i[None, :]\n        for j in range(0, min(BC, T - i_t * BT - i_j * BC)):\n            b_k = tl.load(p_k, mask=m_k, other=0.0)\n            b_gk = tl.load(p_gk, mask=m_k, other=0.0)\n            b_Aj = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]) *\n                scale, 1)\n            b_A = tl.where((o_i == j)[None, :], b_Aj[:, None], b_A)\n            p_k += K\n            p_gk += K\n        b_A = tl.where(m_A, b_A, 0.0)\n        if i_k != 0:\n            b_A += tl.load(p_A, boundary_check=(0, 1))\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_k == 0:\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "57bb4dfb-0931-423b-8c05-53401a0cb13a"
  },
  {
    "input": "@triton.jit\ndef chunk_rwkv6_fwd_kernel_intra(q, k, g, gs, u, A, s_k_h, s_k_t, s_k_d,\n    scale, H, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr', DK: 'tl.constexpr'\n    ):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_q = i_t * BT + i_i * BC\n    m_k = o_k < K\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gs = tl.make_block_ptr(gs + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (s_k_d, s_k_t),\n            (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_gn = tl.load(g + i_bh * T * K + (o_q - 1) * K + o_k, mask=m_k & (\n            i_i > 0) & (o_q <= T), other=0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gs = tl.load(p_gs, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_gs - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A = tl.dot(b_qg, b_kg, allow_tf32=False)\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gs = tl.make_block_ptr(gs + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n        p_q_u = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gs = tl.load(p_gs, boundary_check=(0, 1))\n        o_i = tl.arange(0, BC)\n        o_g = i_bh * T * K + (i_t * BT + i_j * BC) * K + o_k\n        o_A = (i_bh + i_k * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        p_u = tl.make_block_ptr(u + i_h * DK, (DK,), (1,), i_k * BK, (BK,),\n            (0,))\n        b_u = tl.load(p_u, boundary_check=(0,))\n        for j in range(0, BC):\n            b_k = tl.load(p_k, boundary_check=(0,))\n            b_gk = tl.load(g + o_g + j * K, mask=m_k & (i_t * BT + i_j * BC +\n                j < T), other=0)\n            b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_gs - b_gk[None, :]) *\n                scale, 1)\n            b_A = tl.where(o_i > j, b_A, 0.0)\n            b_q_u = tl.load(p_q_u, boundary_check=(0,))\n            b_A_u = tl.sum(b_q_u * b_k * b_u * scale, axis=0)\n            m_u = tl.arange(0, BC) == j\n            b_A = tl.where(m_u, b_A_u, b_A)\n            tl.store(A + o_A + j, b_A, mask=m_A)\n            p_k = tl.advance(p_k, (K,))\n            p_q_u = tl.advance(p_q_u, (K,))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "fb29db39-5d5e-4232-a897-08c04f15e431"
  },
  {
    "input": "@triton.jit\ndef d_sigmoid(dy, x):\n    s = tl.sigmoid(x)\n    return dy * s * (1 - s)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "815927cd-1bfb-44ee-8933-098d0501636c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 16}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 16}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=4,\n    num_warps=8)], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_kernel_old(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddAcs_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddAcs_batch, stride_ddAcs_chunk, stride_ddAcs_head,\n    stride_ddAcs_csize_m, stride_ddAcs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None,\n        :] * stride_cb_csize_n)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] <\n        chunk_size_limit_n), other=0.0)\n    acc = tl.dot(dout, x)\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[\n        None, :] < chunk_size), other=0.0)\n    acc *= cb\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0)\n    acc *= dt_n\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    dA_cs_n = tl.load(dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=\n        offs_n < chunk_size, other=0.0)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] >= offs_n[None, :] + 1\n    acc = tl.where(mask, acc, 0.0)\n    acc = tl.cumsum(acc, axis=1)\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    ddAcs_ptr += (pid_b * stride_ddAcs_batch + pid_c * stride_ddAcs_chunk +\n        pid_h * stride_ddAcs_head + pid_m * stride_ddAcs_csize_m)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n\n    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < \n        chunk_size - 1)\n    tl.store(ddAcs_ptr, 0.0)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0c292fa8-dd2f-454a-87f5-5ab5f93299a1"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_inter(q, k, g, A, scale, T:\n    'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', BT:\n    'tl.constexpr', BC: 'tl.constexpr', BK: 'tl.constexpr', NC:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    if i_i <= i_j:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gk = tl.make_block_ptr(g + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n                BT + i_i * BC) * K + o_k, BK), BK)\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                i_t * BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n    if HEAD_FIRST:\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    else:\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "572b9b16-0eac-4ecf-9a93-af56099fa2f6"
  },
  {
    "input": "@triton.jit\ndef _triton_gemm_a16w8_sub_channel_kernel(A, B, C, scale_b, bias,\n    zero_points, M, N, K, stride_am, stride_ak, stride_bn, stride_bk,\n    stride_cm, stride_cn, stride_zpk, stride_zpn, stride_scalek,\n    stride_scalen, add_bias: 'tl.constexpr', add_zero_points:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr', SPLIT_K: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rbn[:, None] * stride_bn + rk[None, :] * stride_bk)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    scale_w_offs = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    _SCALE0 = tl.zeros([1], dtype=scale_b.dtype.element_ty)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        k_remaining = K - k * (BLOCK_K * SPLIT_K)\n        _A0 = tl.zeros((1, 1), dtype=A.dtype.element_ty)\n        a = tl.load(A, mask=rk[None, :] < k_remaining, other=_A0)\n        _B0 = tl.zeros((1, 1), dtype=B.dtype.element_ty)\n        b = tl.load(B, mask=rk[None, :] < k_remaining, other=_B0)\n        if add_zero_points:\n            _ZERO_POINT0 = tl.zeros([1], dtype=zero_points.dtype.element_ty)\n            zero_points_offs = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n            zero_points_ptrs = zero_points + (k * SPLIT_K + pid_z\n                ) * stride_zpk + zero_points_offs\n            zero_points_vals = tl.load(zero_points_ptrs, mask=\n                zero_points_offs < N, other=_ZERO_POINT0)\n            b = b - zero_points_vals[:, None]\n        scale_ptrs = (scale_b + k * SPLIT_K * stride_scalek + pid_z *\n            stride_scalek + scale_w_offs)\n        scales = tl.load(scale_ptrs, mask=scale_w_offs < N, other=_SCALE0)\n        b_fp = b * scales[:, None]\n        b_fp = tl.trans(b_fp)\n        acc += tl.dot(a, b_fp, out_dtype=tl.float32, allow_tf32=True)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    acc = acc\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if add_bias:\n        offs_bias = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n        bias_ptrs = bias + offs_bias\n        _BIAS0 = tl.zeros([1], dtype=bias.dtype.element_ty)\n        bias_vals = tl.load(bias_ptrs, mask=offs_bias < N, other=_BIAS0)\n        if pid_z == 0:\n            acc += bias_vals[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "7155c632-5580-492a-9273-9126ee6bb191"
  },
  {
    "input": "@triton.jit\ndef bwd_inner_dk_dv(dk, dv, qk_scale, bias_scale, q_ptrs, q_stride, kt, vt,\n    B_block_ptr, do_ptrs, do_stride, l_ptrs, D_ptrs, seqlen_q, seqlen_k,\n    head_dim, start_k, lo, hi, overflow_size, dropout_p, philox_seed,\n    batch_philox_offset, max_seqlen_k, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', FULL_BLOCKS:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr',\n    PADDED_HEAD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    offs_k = start_k + tl.arange(0, BLOCK_N)\n    offs_q = tl.arange(0, BLOCK_M)\n    ld_offs_d = None if not PADDED_HEAD else tl.arange(0, BLOCK_DMODEL)\n    q_ptrs += lo * q_stride\n    do_ptrs += lo * do_stride\n    if BIAS_TYPE == 1:\n        B_block_ptr = tl.advance(B_block_ptr, (lo, 0))\n    \"\"\"\n           K1   K2      (d)V      dO\n    Q1    qk11 qk12     (d)v1     dO1\n    Q2    qk21 qk22     (d)v2     dO2\n\n    QK: (seqlen_q, seqlen_k)\n    dO: (seqlen_q, hdim)\n    dV: (seqlen_k, hdim)\n\n    dV = (QK)^T dO\n\n    dV1 = qk11 dO1 + qk21 dO2 = q1 k1 dO1 + q2 k1 dO2\n    dV2 = qk12 dO1 + qk22 dO2 = q1 k2 dO1 + q2 k2 dO2\n                                ~~~~~ = 0\n    start_k: select k and dV\n    start_q: select q and dO\n    \"\"\"\n    for start_q in range(lo, hi, BLOCK_M):\n        offs_q_curr = offs_q[:, None] + start_q\n        if not FULL_BLOCKS:\n            q = load_fn(q_ptrs, offs_q + start_q, ld_offs_d, seqlen_q, head_dim\n                )\n        else:\n            q = load_fn(q_ptrs, None, ld_offs_d, seqlen_q, head_dim)\n        if not FULL_BLOCKS:\n            do = load_fn(do_ptrs, offs_q + start_q, ld_offs_d, seqlen_q,\n                head_dim)\n        else:\n            do = load_fn(do_ptrs, None, ld_offs_d, seqlen_q, head_dim)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if not FULL_BLOCKS:\n            if overflow_size > 0:\n                boundary_n = tl.full((BLOCK_N,), seqlen_q, dtype=tl.int32)\n                mask = offs_q_curr < boundary_n[None, :]\n                qk = tl.where(mask, qk, float('-inf'))\n        if CAUSAL:\n            qk = tl.where(offs_q_curr >= offs_k[None, :], qk, float('-inf'))\n        if BIAS_TYPE == 0:\n            pass\n        elif BIAS_TYPE == 1:\n            bias = tl.load(B_block_ptr, boundary_check=(0, 1),\n                padding_option='zero')\n            qk += bias * bias_scale\n        else:\n            tl.static_assert(False, f'Unsupported BIAS_TYPE {BIAS_TYPE}')\n        qk += dot(BLOCK_M, BLOCK_DMODEL, BLOCK_DMODEL, q, kt)\n        if FULL_BLOCKS:\n            Di = tl.load(D_ptrs + offs_q_curr)\n            l_i = tl.load(l_ptrs + offs_q_curr)\n        else:\n            boundary = tl.full((BLOCK_M,), BLOCK_M - overflow_size, dtype=\n                tl.int32)\n            d_lse_ptrs_mask = boundary > tl.arange(0, BLOCK_M)\n            d_lse_padding = tl.full((BLOCK_M,), 0, dtype=tl.float32)\n            Di = tl.load(D_ptrs + offs_q_curr, mask=d_lse_ptrs_mask[:, None\n                ], other=d_lse_padding[:, None])\n            l_i = tl.load(l_ptrs + offs_q_curr, mask=d_lse_ptrs_mask[:,\n                None], other=d_lse_padding[:, None])\n        p = tl.math.exp2(qk_scale * qk - l_i)\n        if not FULL_BLOCKS or CAUSAL:\n            if qk_scale == 0.0:\n                p = tl.where(libdevice.isnan(p), 0.0, p)\n        if ENABLE_DROPOUT:\n            philox_offset = (batch_philox_offset + start_q * max_seqlen_k +\n                start_k)\n            keep = dropout_mask(philox_seed, philox_offset, dropout_p,\n                BLOCK_M, BLOCK_N, max_seqlen_k)\n            if BLOCK_M == 1:\n                dv += tl.where(keep, p / (1 - dropout_p), 0.0) * do\n            else:\n                dv += tl.dot(tl.trans(tl.where(keep, p / (1 - dropout_p), \n                    0.0)), do)\n        elif BLOCK_M == 1:\n            dv += p * do\n        else:\n            dv += tl.dot(tl.trans(p), do)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, vt)\n        if ENABLE_DROPOUT:\n            dp = tl.where(keep, dp / (1 - dropout_p), 0)\n        ds = p * (dp - Di)\n        if BLOCK_M == 1:\n            dk += ds * q\n        else:\n            dk += tl.dot(tl.trans(ds), q)\n        q_ptrs += q_stride * BLOCK_M\n        do_ptrs += do_stride * BLOCK_M\n        if BIAS_TYPE == 1:\n            B_block_ptr = tl.advance(B_block_ptr, (BLOCK_M, 0))\n    return dk, dv\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b9483005-ed55-4b3f-be37-cf3f59669e3c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)],\n    key=['M'])\n@triton.jit\ndef triton_jagged_sum_kernel_simple_fused_buffer_then_sum(input_ptr_values,\n    input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_ragged\n        ), tl.load(input_ptr_offsets + (pid_ragged + 1))\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer_sum = tl.sum(buffer, axis=0)\n    buffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\n    output_offsets = offsets_m + pid_ragged * M\n    output_mask = output_offsets < M * (pid_ragged + 1)\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "22d12f5c-da3e-4281-b941-d15bcfdd078f"
  },
  {
    "input": "@triton.jit\ndef first_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST_00 = tl.sqrt(3.0)\n    Y10 = CONST_00 * x\n    Y11 = CONST_00 * y\n    Y12 = CONST_00 * z\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y10, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y11, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y12, mask=\n        output_row_offset + 2 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "e95560e2-3d9c-4ee4-93ec-0bafcb901163"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _l2_norm_bwd_kernel(X, DY, DX, stride_x_row, N, eps, BLOCK_N:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    DX += row * stride_x_row\n    DY += row * stride_x_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    x = tl.where(cols < N, x, 0.0)\n    var = tl.sum(x * x)\n    rstd = 1 / tl.sqrt(var + eps)\n    mask = cols < N\n    dy = tl.load(DY + cols, mask=cols < N, other=0.0)\n    dy = tl.where(cols < N, dy, 0.0)\n    dx = dy * rstd - tl.sum(dy * x) * (1 / (var + eps)) * rstd * x\n    tl.store(DX + cols, dx, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "7f440d2c-b6b1-4f79-845d-88e27197f132"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n    stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr', start_n, start_m, num_steps, MASK: 'tl.constexpr'\n    ):\n    offs_m = start_m + tl.arange(0, BLOCK_M1)\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    offs_k = tl.arange(0, HEAD_DIM)\n    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n    curr_m = start_m\n    step_m = BLOCK_M1\n    for blk_idx in range(num_steps):\n        qT = tl.load(qT_ptrs)\n        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n        m = tl.load(M + offs_m)\n        qkT = tl.dot(k, qT)\n        pT = tl.math.exp2(qkT - m[None, :])\n        if MASK:\n            mask = offs_m[None, :] >= offs_n[:, None]\n            pT = tl.where(mask, pT, 0.0)\n        do = tl.load(do_ptrs)\n        ppT = pT\n        ppT = ppT\n        dv += tl.dot(ppT, do)\n        Di = tl.load(D + offs_m)\n        dpT = tl.dot(v, tl.trans(do))\n        dsT = pT * (dpT - Di[None, :])\n        dsT = dsT\n        dk += tl.dot(dsT, tl.trans(qT))\n        curr_m += step_m\n        qT_ptrs += step_m * stride_tok\n        do_ptrs += step_m * stride_tok\n    return dk, dv\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "a5a9884a-8c0d-4c63-8c6f-814f5c0c11a6"
  },
  {
    "input": "@triton.jit\ndef cosh(x):\n    exp_x = tl.exp(x)\n    return (exp_x + 1.0 / exp_x) * 0.5\n",
    "category": "Activation Functions",
    "subcategory": "cosh",
    "uuid": "1f4c8b21-500d-454a-a60a-dd1781a14bd1"
  },
  {
    "input": "@triton.jit\ndef seventh_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_7 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_8 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    g_9 = tl.load(sph_grad_ptr + output_row_offset + 9, mask=\n        output_row_offset + 9 < output_numel)\n    g_10 = tl.load(sph_grad_ptr + output_row_offset + 10, mask=\n        output_row_offset + 10 < output_numel)\n    g_11 = tl.load(sph_grad_ptr + output_row_offset + 11, mask=\n        output_row_offset + 11 < output_numel)\n    g_12 = tl.load(sph_grad_ptr + output_row_offset + 12, mask=\n        output_row_offset + 12 < output_numel)\n    g_13 = tl.load(sph_grad_ptr + output_row_offset + 13, mask=\n        output_row_offset + 13 < output_numel)\n    g_14 = tl.load(sph_grad_ptr + output_row_offset + 14, mask=\n        output_row_offset + 14 < output_numel)\n    CONST000 = 1.66389743899677\n    CONST001 = 3.0\n    CONST003 = 5.0\n    CONST004 = 3.32779487799353\n    CONST009 = 11.7655316231354\n    CONST012 = 16.5555704843566\n    CONST014 = 20.4939015319192\n    CONST016 = 22.0740939791422\n    CONST018 = 23.5310632462709\n    CONST019 = 20.4939015319192\n    CONST020 = 27.1108834234519\n    CONST022 = 33.1111409687132\n    CONST024 = 36.7901566319036\n    CONST025 = 36.7901566319036\n    CONST026 = 38.4260653723485\n    CONST027 = 38.4260653723485\n    CONST029 = 38.4260653723485\n    CONST030 = 44.1481879582843\n    CONST032 = -4.9916923169903\n    CONST037 = 47.0621264925417\n    CONST039 = 56.2781179722634\n    CONST044 = -441.481879582843\n    CONST045 = -441.481879582843\n    CONST048 = 76.852130744697\n    CONST049 = 76.852130744697\n    CONST050 = -8.47215106982872\n    CONST054 = 110.370469895711\n    CONST055 = 110.370469895711\n    CONST056 = -399.335385359224\n    CONST057 = 117.655316231354\n    CONST058 = 122.963409191515\n    CONST059 = 122.963409191515\n    CONST061 = -376.497011940334\n    CONST062 = -376.497011940334\n    CONST064 = 141.186379477625\n    CONST066 = 147.160626527614\n    CONST067 = 153.704261489394\n    CONST069 = -350.955726374425\n    CONST072 = 203.331625675889\n    CONST073 = 203.331625675889\n    CONST074 = -307.408522978788\n    CONST075 = -9.60651634308713\n    CONST076 = -9.37968632871057\n    CONST079 = -281.390589861317\n    CONST080 = -1.66389743899677\n    CONST081 = -266.223590239483\n    CONST082 = -263.216794780819\n    CONST084 = -263.216794780818\n    CONST085 = -250.998007960223\n    CONST089 = 281.390589861317\n    CONST091 = -220.740939791422\n    CONST092 = -220.740939791422\n    CONST093 = -199.667692679612\n    CONST094 = -1.60108605718119\n    CONST095 = -187.593726574211\n    CONST096 = -177.482393492989\n    CONST097 = -9.60651634308712\n    CONST098 = -9.1975391579759\n    CONST100 = -153.704261489394\n    CONST101 = -147.160626527614\n    CONST102 = -140.695294930659\n    CONST104 = -133.111795119741\n    CONST105 = -133.111795119741\n    CONST106 = -125.499003980111\n    CONST107 = -125.499003980111\n    CONST109 = -105.286717912327\n    CONST110 = -101.665812837945\n    CONST111 = -99.833846339806\n    CONST112 = -101.665812837945\n    CONST113 = -4.80325817154356\n    CONST114 = -81.3326502703558\n    CONST115 = -81.3326502703557\n    CONST116 = -76.852130744697\n    CONST117 = -75.2994023880668\n    CONST119 = -70.5931897388126\n    CONST121 = -66.2222819374265\n    CONST122 = -66.5558975598707\n    CONST123 = -66.5558975598707\n    CONST124 = -62.7495019900557\n    CONST125 = -56.2781179722634\n    CONST126 = -55.1852349478554\n    CONST127 = -55.1852349478554\n    CONST128 = -50.8329064189723\n    CONST129 = -50.8329064189723\n    CONST130 = -562.781179722634\n    CONST131 = -47.0621264925418\n    CONST132 = -50.8329064189724\n    CONST133 = -44.1481879582843\n    CONST134 = -44.3705983732471\n    CONST135 = -40.6663251351779\n    CONST136 = -40.6663251351779\n    CONST137 = -8.31948719498384\n    CONST138 = -37.6497011940334\n    CONST139 = -33.2779487799353\n    CONST140 = -29.9501539019418\n    CONST141 = -25.4164532094862\n    CONST142 = -25.4164532094862\n    CONST143 = -23.5310632462709\n    CONST144 = -532.447180478965\n    CONST145 = -19.2130326861743\n    CONST146 = -17.5477863187212\n    CONST147 = -12.8765548211663\n    CONST148 = -11.6472820729774\n    CONST149 = -11.2076024002683\n    CONST150 = -9.1975391579759\n    CONST151 = -11.0370469895711\n    CONST152 = -11.7655316231354\n    CONST153 = -12.8765548211663\n    CONST154 = -4.80325817154356\n    CONST155 = -3.32779487799353\n    CONST156 = -1.60108605718119\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR04 = VAR07 * VAR07\n    VAR05 = VAR07 * VAR08\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR13 = VAR16 * VAR16\n    VAR14 = VAR16 * VAR17\n    VAR15 = VAR17 * VAR17\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR22 = VAR25 * VAR25\n    VAR23 = VAR25 * VAR26\n    VAR24 = VAR26 * VAR26\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += g_0 * (CONST082 * VAR08 * VAR24 - CONST084 * VAR06 * VAR26 + \n        CONST146 * VAR04 - CONST146 * VAR22) + g_1 * y * (CONST039 * VAR23 +\n        CONST089 * VAR06 * z + CONST130 * VAR08 * VAR25) + g_10 * (CONST155 *\n        VAR23 * x + VAR25 * (-CONST105 * VAR17 * x + CONST139 * VAR07) + z *\n        (-CONST056 * VAR07 * VAR17 + CONST081 * VAR15 * x + CONST140 * VAR05)\n        ) + g_11 * (VAR16 * (CONST044 * VAR26 * x - CONST101 * VAR07) + y *\n        (CONST054 * VAR24 * x - CONST091 * VAR07 * VAR26 + CONST121 * VAR05)\n        ) + g_12 * (CONST022 * VAR23 * x + VAR25 * (CONST024 * VAR07 + \n        CONST045 * VAR17 * x) + z * (-CONST044 * VAR07 * VAR17 + CONST126 *\n        VAR05)) + g_13 * y * (CONST079 * VAR24 * x + CONST125 * VAR05 - \n        CONST130 * VAR07 * VAR26) + g_14 * (-CONST069 * VAR07 * VAR25 + \n        CONST109 * VAR05 * z + CONST109 * VAR23 * x) + g_2 * (CONST001 *\n        VAR08 * (CONST091 * VAR17 * VAR26 - CONST150 * VAR24) + CONST003 *\n        VAR06 * (CONST012 * VAR26 + CONST016 * VAR17) + CONST055 * VAR17 *\n        VAR24 + CONST147 * VAR04 + CONST150 * VAR22) + g_3 * (VAR16 * (\n        CONST044 * VAR08 * z + CONST066 * VAR25) + y * (-CONST091 * VAR06 *\n        z + CONST133 * VAR23)) + g_4 * (CONST001 * VAR08 * (CONST122 *\n        VAR17 * VAR26 + CONST134 * VAR15 - CONST137 * VAR24) + CONST003 *\n        VAR06 * (CONST000 * VAR26 - CONST139 * VAR17) - CONST032 * VAR22 - \n        CONST105 * VAR15 * VAR26 + CONST111 * VAR17 * VAR24 + CONST148 * VAR04\n        ) + g_5 * (CONST001 * VAR08 * (CONST106 * VAR16 * z - CONST131 *\n        VAR25 * y) + CONST057 * VAR06 * y * z + CONST107 * VAR16 * VAR25 - \n        CONST117 * VAR14 * z - CONST143 * VAR23 * y) + g_6 * (CONST001 *\n        VAR08 * (CONST116 * VAR15 - CONST116 * VAR17 * VAR26 + CONST154 *\n        VAR24) + CONST003 * VAR06 * (CONST026 * VAR17 + CONST113 * VAR26) +\n        CONST014 * VAR13 + CONST027 * VAR17 * VAR24 + CONST116 * VAR15 *\n        VAR26 + CONST149 * VAR04 + CONST156 * VAR22) + g_7 * (CONST114 *\n        VAR14 * x + VAR16 * (CONST072 * VAR07 + CONST073 * VAR26 * x) + y *\n        (CONST110 * VAR07 * VAR26 + CONST128 * VAR05 + CONST129 * VAR24 * x)\n        ) + g_8 * (CONST075 * VAR23 * x + VAR25 * (-CONST100 * VAR17 * x + \n        CONST145 * VAR07) + z * (CONST067 * VAR07 * VAR17 + CONST097 *\n        VAR05 + CONST100 * VAR15 * x)) + g_9 * (-CONST085 * VAR07 * VAR16 +\n        CONST117 * VAR14 * x + y * (CONST018 * VAR24 * x + CONST119 * VAR05 +\n        CONST131 * VAR07 * VAR26))\n    g_y += g_1 * (CONST039 * VAR23 * x + CONST095 * VAR07 * VAR25 - \n        CONST125 * VAR05 * z) + g_10 * (CONST123 * VAR23 * y + VAR25 * (-\n        CONST096 * VAR16 - CONST105 * VAR08 * y) + z * (-CONST093 * VAR06 *\n        y + CONST144 * VAR08 * VAR16)) + g_11 * (CONST001 * VAR17 * (\n        CONST025 * VAR06 + CONST025 * VAR24 + CONST092 * VAR08 * VAR26) - \n        CONST126 * VAR06 * VAR26 - CONST126 * VAR08 * VAR24 + CONST151 *\n        VAR04 + CONST151 * VAR22) + g_12 * (CONST030 * VAR23 * y + CONST045 *\n        VAR08 * VAR25 * y - CONST092 * VAR06 * y * z) + g_13 * (CONST076 *\n        VAR04 - CONST076 * VAR22 - CONST102 * VAR06 * VAR26 + CONST102 *\n        VAR08 * VAR24) + g_2 * (CONST030 * VAR05 * y + CONST045 * VAR07 *\n        VAR26 * y - CONST092 * VAR24 * x * y) + g_3 * (CONST001 * VAR17 * (\n        CONST066 * VAR25 * x + CONST101 * VAR07 * z) - CONST133 * VAR05 * z +\n        CONST133 * VAR23 * x) + g_4 * (-CONST123 * VAR05 * y + VAR07 * (\n        CONST096 * VAR16 + CONST104 * VAR26 * y) + x * (CONST093 * VAR24 *\n        y - CONST144 * VAR16 * VAR26)) + g_5 * (-CONST143 * VAR05 * z + \n        VAR07 * (CONST062 * VAR17 * z - CONST131 * VAR25) + x * (CONST061 *\n        VAR17 * VAR25 - CONST062 * VAR15 * z - CONST143 * VAR23)) + g_6 * (\n        CONST048 * VAR05 * y + VAR07 * (CONST074 * VAR16 - CONST100 * VAR26 *\n        y) + x * (CONST058 * VAR14 + CONST074 * VAR16 * VAR26 - CONST116 *\n        VAR24 * y)) + g_7 * (CONST001 * VAR17 * (-CONST112 * VAR08 * VAR26 -\n        CONST128 * VAR06 - CONST128 * VAR24) + CONST003 * VAR15 * (CONST135 *\n        VAR08 + CONST136 * VAR26) + CONST020 * VAR13 + CONST050 * VAR04 + \n        CONST050 * VAR22 + CONST141 * VAR06 * VAR26 + CONST142 * VAR08 * VAR24\n        ) + g_8 * (CONST048 * VAR23 * y + VAR25 * (CONST074 * VAR16 - \n        CONST100 * VAR08 * y) + z * (CONST049 * VAR06 * y + CONST059 *\n        VAR14 + CONST074 * VAR08 * VAR16)) + g_9 * (CONST001 * VAR17 * (-\n        CONST124 * VAR06 + CONST124 * VAR24) + CONST003 * VAR15 * (CONST138 *\n        VAR08 - CONST138 * VAR26) + CONST009 * VAR08 * VAR24 + CONST152 *\n        VAR04 + CONST152 * VAR06 * VAR26 - CONST152 * VAR22)\n    g_z += g_0 * (CONST069 * VAR07 * VAR25 - CONST109 * VAR05 * z - \n        CONST109 * VAR23 * x) + g_1 * y * (-CONST079 * VAR24 * x - CONST125 *\n        VAR05 + CONST130 * VAR07 * VAR26) + g_10 * (CONST001 * VAR26 * (-\n        CONST123 * VAR08 * VAR17 - CONST134 * VAR15 + CONST137 * VAR06) + \n        CONST003 * VAR24 * (CONST080 * VAR08 + CONST139 * VAR17) + CONST032 *\n        VAR04 + CONST105 * VAR08 * VAR15 - CONST111 * VAR06 * VAR17 - \n        CONST148 * VAR22) + g_11 * (VAR16 * (CONST044 * VAR08 * z - \n        CONST101 * VAR25) + y * (CONST054 * VAR06 * z - CONST091 * VAR08 *\n        VAR25 + CONST121 * VAR23)) + g_12 * (CONST001 * VAR26 * (CONST091 *\n        VAR08 * VAR17 - CONST098 * VAR06) + CONST003 * VAR24 * (CONST012 *\n        VAR08 + CONST016 * VAR17) + CONST055 * VAR06 * VAR17 + CONST098 *\n        VAR04 + CONST153 * VAR22) + g_13 * y * (-CONST079 * VAR06 * z - \n        CONST125 * VAR23 + CONST130 * VAR08 * VAR25) + g_14 * (-CONST082 *\n        VAR06 * VAR26 + CONST084 * VAR08 * VAR24 + CONST146 * VAR04 - \n        CONST146 * VAR22) + g_2 * (CONST022 * VAR05 * z + VAR07 * (CONST025 *\n        VAR25 + CONST045 * VAR17 * z) + x * (-CONST044 * VAR17 * VAR25 + \n        CONST127 * VAR23)) + g_3 * (VAR16 * (-CONST045 * VAR26 * x + \n        CONST101 * VAR07) + y * (CONST091 * VAR24 * x - CONST133 * VAR05)\n        ) + g_4 * (CONST004 * VAR05 * z + VAR07 * (CONST104 * VAR17 * z - \n        CONST139 * VAR25) + x * (CONST056 * VAR17 * VAR25 - CONST081 *\n        VAR15 * z - CONST140 * VAR23)) + g_5 * (-CONST143 * VAR05 * y + \n        VAR07 * (CONST064 * VAR26 * y + CONST106 * VAR16) + x * (CONST057 *\n        VAR24 * y + CONST061 * VAR16 * VAR26 - CONST117 * VAR14)) + g_6 * (\n        CONST097 * VAR05 * z + VAR07 * (-CONST100 * VAR17 * z + CONST145 *\n        VAR25) + x * (CONST075 * VAR23 + CONST100 * VAR15 * z - CONST100 *\n        VAR17 * VAR25)) + g_7 * (CONST115 * VAR14 * z + VAR16 * (CONST072 *\n        VAR25 + CONST073 * VAR08 * z) + y * (CONST112 * VAR08 * VAR25 + \n        CONST128 * VAR23 + CONST132 * VAR06 * z)) + g_8 * (CONST001 * VAR26 *\n        (-CONST116 * VAR08 * VAR17 + CONST116 * VAR15 + CONST154 * VAR06) +\n        CONST003 * VAR24 * (CONST026 * VAR17 + CONST154 * VAR08) + CONST019 *\n        VAR13 + CONST029 * VAR06 * VAR17 + CONST094 * VAR04 + CONST116 *\n        VAR08 * VAR15 + CONST149 * VAR22) + g_9 * (CONST085 * VAR16 * VAR25 -\n        CONST117 * VAR14 * z + y * (CONST037 * VAR08 * VAR25 - CONST119 *\n        VAR23 + CONST143 * VAR06 * z))\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c87d058c-f06a-4a1f-9542-04c3c4b2197e"
  },
  {
    "input": "@triton.jit\ndef _rmsnorm_bwd_kernel(X, W, DY, DX, DW, Rstd, stride_x_row, stride_dy_row,\n    stride_dx_row, M, N, eps, rows_per_program, BLOCK_N: 'tl.constexpr',\n    IS_EVEN_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    w = tl.load(W + cols, mask=mask)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        if IS_EVEN_N:\n            x = tl.load(X + cols)\n            dy = tl.load(DY + cols)\n        else:\n            x = tl.load(X + cols, mask=mask, other=0)\n            dy = tl.load(DY + cols, mask=mask, other=0)\n        rstd = tl.load(Rstd + row)\n        xhat = x * rstd\n        if not IS_EVEN_N:\n            xhat = tl.where(mask, xhat, 0.0)\n        wdy = w * dy\n        dw += dy * xhat\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        dx = (wdy - xhat * c1) * rstd\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "9e4a29eb-5b19-4d6f-9405-635fbd24fb0d"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp0 + i_w * BW * DH + tl.arange(\n        0, BW)[None, :] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NW - i_w - 1\n        ) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]) * DH + (NH - i_h - 1\n        ) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (DH - NH * BH) + (DW -\n        NW * BW) * DH\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y1 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y2 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y3 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y4 + _idx, _x, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "07665bb0-30d9-4c3c-ae2f-5bc324cea449"
  },
  {
    "input": "@triton.jit\ndef grouped_launch(pid, m, n, block_m: 'tl.constexpr', block_n:\n    'tl.constexpr', group_m: 'tl.constexpr'):\n    grid_m = tl.cdiv(m, block_m)\n    grid_n = tl.cdiv(n, block_n)\n    width = group_m * grid_n\n    group_id = pid // width\n    group_size = tl.minimum(grid_m - group_id * group_m, group_m)\n    pid_m = group_id * group_m + pid % group_size\n    pid_n = pid % width // group_size\n    return pid_m, pid_n\n",
    "category": "Kernel Operations",
    "subcategory": "kernel launching",
    "uuid": "29d53ca7-e28e-4a12-ad07-7992b3cc2092"
  },
  {
    "input": "@triton.autotune(configs=TRITON_CONFIG_LIST_FWD, key=['max_seqlen_q',\n    'max_seqlen_k', 'CAUSAL'])\n@triton.jit\ndef tuned_attn_fwd(Q, K, V, B, sm_scale, M, Out, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn, stride_bz, stride_bh,\n    stride_bm, stride_bn, stride_oz, stride_oh, stride_om, stride_on,\n    num_head_q, num_head_k, cu_seqlens_q, cu_seqlens_k, num_seqlens,\n    max_seqlen_q, max_seqlen_k, head_dim, dropout_p, philox_seed_ptr,\n    philox_offset1, philox_offset2, philox_seed_output,\n    philox_offset_output, encoded_softmax, CAUSAL: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    pre_load_v: 'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr',\n    RETURN_ENCODED_SOFTMAX: 'tl.constexpr', PADDED_HEAD: 'tl.constexpr',\n    BIAS_TYPE: 'tl.constexpr'):\n    bare_attn_fwd(Q, K, V, B, sm_scale, M, Out, stride_qz, stride_qh,\n        stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n        stride_vz, stride_vh, stride_vk, stride_vn, stride_bz, stride_bh,\n        stride_bm, stride_bn, stride_oz, stride_oh, stride_om, stride_on,\n        num_head_q, num_head_k, cu_seqlens_q, cu_seqlens_k, num_seqlens,\n        max_seqlen_q, max_seqlen_k, head_dim, dropout_p, philox_seed_ptr,\n        philox_offset1, philox_offset2, philox_seed_output,\n        philox_offset_output, encoded_softmax, CAUSAL, BLOCK_M,\n        BLOCK_DMODEL, BLOCK_N, pre_load_v, ENABLE_DROPOUT,\n        RETURN_ENCODED_SOFTMAX, PADDED_HEAD, BIAS_TYPE=BIAS_TYPE)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "bdb61e8f-d7e0-4df6-90ac-820049a08049"
  },
  {
    "input": "@triton.jit\ndef MSAFwdFused(v_si_ptr, b_ij_ptr, g_si_ptr, output_ptr, vw_ptr,\n    logsumexp_ptr, C_hidden, N_head, C_LEN_POW2: 'tl.constexpr',\n    RES_LEN_POW2: 'tl.constexpr', SEQ_LEN: 'tl.constexpr', RES_LEN:\n    'tl.constexpr', BLOCK_SIZE_ROW: 'tl.constexpr', BLOCK_SIZE_COL:\n    'tl.constexpr', BLOCK_SIZE_SEQ: 'tl.constexpr'):\n    pid_z = tl.program_id(0)\n    pid_h = tl.program_id(1)\n    pid_i = tl.program_id(2)\n    z_off = pid_z\n    h_off = pid_h\n    i_off = pid_i * BLOCK_SIZE_ROW\n    offs_i = i_off + tl.arange(0, BLOCK_SIZE_ROW)\n    offs_c = tl.arange(0, C_LEN_POW2)\n    log2_e = 1.44269504089\n    prev_row_max = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    new_row_max = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    l = tl.full((BLOCK_SIZE_ROW, 1), 0.0, dtype=tl.float32)\n    for j in range(0, RES_LEN, BLOCK_SIZE_COL):\n        offs_j = j + tl.arange(0, BLOCK_SIZE_COL)\n        b_offs = z_off * RES_LEN * RES_LEN * N_head + offs_i[:, None\n            ] * RES_LEN * N_head + offs_j[None, :] * N_head + h_off\n        ij_mask = (offs_i < RES_LEN)[:, None] & (offs_j < RES_LEN)[None, :]\n        b = tl.load(b_ij_ptr + b_offs, ij_mask, -INF)\n        new_row_max = tl.maximum(tl.max(b, axis=1, keep_dims=True),\n            prev_row_max)\n        w = tl.exp2(log2_e * (b - new_row_max))\n        l *= tl.exp2(log2_e * (prev_row_max - new_row_max))\n        l += tl.sum(w, axis=1, keep_dims=True)\n        for s in range(0, SEQ_LEN, BLOCK_SIZE_SEQ):\n            for ch in range(0, C_hidden, 1):\n                offs_s = s + tl.arange(0, BLOCK_SIZE_SEQ)\n                si_off = (z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + \n                    offs_s[None, :] * RES_LEN * N_head * C_hidden + offs_i[\n                    :, None] * N_head * C_hidden + h_off * C_hidden + ch)\n                sj_off = (z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + \n                    offs_s[None, :] * RES_LEN * N_head * C_hidden + offs_j[\n                    :, None] * N_head * C_hidden + h_off * C_hidden + ch)\n                si_mask = (offs_s < SEQ_LEN)[None, :] & (offs_i < RES_LEN)[\n                    :, None]\n                sj_mask = (offs_s < SEQ_LEN)[None, :] & (offs_j < RES_LEN)[\n                    :, None]\n                v = tl.load(v_si_ptr + sj_off, sj_mask, 0)\n                vw = tl.load(output_ptr + si_off, si_mask, 0)\n                vw = vw * tl.exp2(log2_e * (prev_row_max - new_row_max))\n                vw = tl.dot(w, v, acc=vw)\n                tl.store(output_ptr + si_off, vw, si_mask)\n        prev_row_max = new_row_max\n    for s in range(0, SEQ_LEN, BLOCK_SIZE_SEQ):\n        for ch in range(0, C_hidden, 1):\n            offs_s = s + tl.arange(0, BLOCK_SIZE_SEQ)\n            si_off = z_off * SEQ_LEN * RES_LEN * N_head * C_hidden + offs_s[\n                None, :] * RES_LEN * N_head * C_hidden + offs_i[:, None\n                ] * N_head * C_hidden + h_off * C_hidden + ch\n            si_mask = (offs_s < SEQ_LEN)[None, :] & (offs_i < RES_LEN)[:, None]\n            g = tl.load(g_si_ptr + si_off, si_mask, 0)\n            g = tl.sigmoid(g)\n            vw = tl.load(output_ptr + si_off, si_mask, 0)\n            vw = vw / l\n            out = g * vw\n            tl.store(output_ptr + si_off, out, si_mask)\n            tl.store(vw_ptr + si_off, vw, si_mask)\n    lse_off = z_off * RES_LEN * N_head + offs_i[:, None] * N_head + h_off\n    lse_mask = (offs_i < RES_LEN)[:, None]\n    tl.store(logsumexp_ptr + lse_off, new_row_max + tl.log(l), lse_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "multi-head attention",
    "uuid": "56f30109-33e1-4b91-b0a4-1bbabc77a1e4"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel_backward(grad_out_ptr, probs_ptr, grad_in_ptr,\n    grad_stride, probs_stride, out_stride, seq_len, BLOCK_SIZE:\n    'tl.constexpr', num_warps: 'tl.constexpr'):\n    batch_idx = tl.program_id(0)\n    probs_start_ptr = probs_ptr + batch_idx * probs_stride\n    grad_start_ptr = grad_in_ptr + batch_idx * grad_stride\n    pos_offsets = tl.arange(0, BLOCK_SIZE)\n    probs_ptrs = probs_start_ptr + pos_offsets\n    grad_ptrs = grad_start_ptr + pos_offsets\n    valid_mask = pos_offsets < seq_len\n    probs_vals = tl.load(probs_ptrs, mask=valid_mask, other=0.0)\n    grad_vals = tl.load(grad_ptrs, mask=valid_mask, other=0.0)\n    grad_times_probs = probs_vals * grad_vals\n    final_grad = grad_times_probs - probs_vals * tl.sum(grad_times_probs,\n        axis=0)\n    out_start_ptr = grad_out_ptr + batch_idx * out_stride\n    out_ptrs = out_start_ptr + pos_offsets\n    tl.store(out_ptrs, final_grad, mask=valid_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "4e76d5f8-7c4e-4ab0-99f7-d4f379a94c8e"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dp(v, rv, cv, pv, do, dp, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BV: 'tl.constexpr', BM:\n    'tl.constexpr', DV: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_m, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_v = tl.make_block_ptr(v + i_bh * s_qk_h, (DV, T), (s_qk_d, s_qk_t), (\n        i_v * BV, 0), (BV, BT), (0, 1))\n    p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_cv = tl.make_block_ptr(cv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    p_pv = tl.make_block_ptr(pv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * s_qk_h, (T, DV), (s_qk_t, s_qk_d),\n        (0, i_v * BV), (BT, BV), (1, 0))\n    p_dp = tl.make_block_ptr(dp + (i_v * n_bh + i_bh) * s_sk_h, (T, DM), (\n        s_sk_t, s_sk_m), (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hv = tl.zeros([BV, BM], dtype=tl.float32)\n    for _ in range(NT):\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_rv = tl.load(p_rv, boundary_check=(0,))\n        b_cv = tl.load(p_cv, boundary_check=(0, 1))\n        b_pv = tl.load(p_pv, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_inter = tl.dot(b_do, b_hv, allow_tf32=False) * b_rv[None, :]\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_do, b_v, allow_tf32=False),\n            0), b_cv, allow_tf32=False)\n        b_dp = (b_inter + b_intra) * b_pv\n        b_hv = b_hv * b_rv[None, :] + tl.dot(b_v, b_cv, allow_tf32=False)\n        tl.store(p_dp, b_dp, boundary_check=(0, 1))\n        p_v = tl.advance(p_v, (0, BT))\n        p_rv = tl.advance(p_rv, (DM,))\n        p_cv = tl.advance(p_cv, (BT, 0))\n        p_pv = tl.advance(p_pv, (BT, 0))\n        p_do = tl.advance(p_do, (BT, 0))\n        p_dp = tl.advance(p_dp, (BT, 0))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c59ea4ee-ec84-44c7-a4a0-01e4e8642936"
  },
  {
    "input": "@triton.jit\ndef sqrt_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.sqrt(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "66f2d76a-acd2-45d8-a6a3-b437966c1c10"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n    stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', start_n, start_m, num_steps, MASK:\n    'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M1)\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n    curr_m = start_m\n    step_m = BLOCK_M1\n    for blk_idx in range(num_steps):\n        qT = tl.load(qT_ptrs)\n        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n        m = tl.load(M + offs_m)\n        qkT = tl.dot(k, qT)\n        pT = tl.math.exp2(qkT - m[None, :])\n        if MASK:\n            mask = offs_m[None, :] >= offs_n[:, None]\n            pT = tl.where(mask, pT, 0.0)\n        do = tl.load(do_ptrs)\n        ppT = pT\n        ppT = ppT\n        dv += tl.dot(ppT, do)\n        Di = tl.load(D + offs_m)\n        dpT = tl.dot(v, tl.trans(do))\n        dsT = pT * (dpT - Di[None, :])\n        dsT = dsT\n        dk += tl.dot(dsT, tl.trans(qT))\n        curr_m += step_m\n        qT_ptrs += step_m * stride_tok\n        do_ptrs += step_m * stride_tok\n    return dk, dv\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "dd905406-5ca3-4148-9d29-63124060987d"
  },
  {
    "input": "@triton.jit\ndef relu_grad(x):\n    return tl.where(x >= 0, 1.0, 0.0)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "13ab8df1-dff8-4dfd-b09f-feddb87e42bd"
  },
  {
    "input": "@triton.jit\ndef _d_linear_relu(d_y, w, b, xwb, x):\n    d_y_relu = d_y * (xwb > 0.0)\n    return _d_linear(d_y_relu, w, b, x)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c33300be-d8b2-4d6b-9e59-4ad365e77321"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_int8(Q, K, K_scale, V, V_scale, sm_scale, Req_to_tokens,\n    B_req_idx, B_split_start_loc, B_split_ready_cache_len, B_seqlen, Out,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_ksbs, stride_ksh, stride_ksd, stride_vbs, stride_vh, stride_vd,\n    stride_vsbs, stride_vsh, stride_vsd, stride_obs, stride_oh, stride_od,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, kv_group_num, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_q_split_start_loc = tl.load(B_split_start_loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_seqlen + cur_batch)\n    cur_batch_seq_start = tl.load(B_split_ready_cache_len + cur_batch)\n    cur_batch_q_split_seq_len = cur_batch_seq_len - cur_batch_seq_start\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :]\n    off_k = cur_kv_head * stride_kh + offs_d[:, None]\n    off_v = cur_kv_head * stride_vh + offs_d[None, :]\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_q_split_seq_len,\n        other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    ks_ptrs = K_scale + cur_kv_head * stride_ksh\n    vs_ptrs = V_scale + cur_kv_head * stride_vsh\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(start_m * BLOCK_M < cur_batch_q_split_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (cur_batch_seq_start + (start_m + \n        1) * BLOCK_M), BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + cur_batch_req_idx *\n            stride_req_to_tokens_b + start_n + offs_n, mask=start_n +\n            offs_n < cur_batch_seq_len, other=0)\n        k = tl.load(k_ptrs + kv_loc[None, :] * stride_kbs, mask=start_n +\n            offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        k_scale = tl.load(ks_ptrs + kv_loc[None, :] * stride_ksbs, mask=\n            start_n + offs_n[None, :] < cur_batch_seq_len, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k_scale * k)\n        qk *= sm_scale\n        qk = tl.where(cur_batch_seq_start + offs_m[:, None] >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + kv_loc[:, None] * stride_vbs, mask=start_n +\n            offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        v_scale = tl.load(vs_ptrs + kv_loc[:, None] * stride_vsbs, mask=(\n            start_n + offs_n)[:, None] < cur_batch_seq_len, other=0.0)\n        p = p\n        acc += tl.dot(p, v * v_scale)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_q_split_start_loc + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :]\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_q_split_seq_len)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "dff4529d-58c2-4a39-8c28-603798dbeff1"
  },
  {
    "input": "@triton.heuristics({'num_warps': lambda args: get_num_warps(args[\n    'QUERY_GROUP_SIZE'], args['HEAD_SIZE'], args['KV_BLOCK_SIZE']),\n    'num_stages': lambda args: get_num_stages(args['QUERY_GROUP_SIZE'],\n    args['KV_BLOCK_SIZE'])})\n@triton.jit\ndef _paged_attn_kernel(m_i_ptr, l_i_ptr, out_ptr, q_ptr, k_cache_ptr,\n    v_cache_ptr, context_lens_ptr, block_tables_ptr, attn_scale, stride_bt0,\n    stride_bt1, stride_q0, stride_q1, stride_q2, stride_kv0, stride_kv1,\n    stride_kv2, stride_kv3, stride_o0, stride_o1, stride_o2, stride_o3,\n    stride_o4, HEAD_SIZE: 'tl.constexpr', QUERY_GROUP_SIZE: 'tl.constexpr',\n    PADDED_QUERY_GROUP_SIZE: 'tl.constexpr', NUM_KV_HEADS: 'tl.constexpr',\n    KV_BLOCK_SIZE: 'tl.constexpr', PARTITION_SIZE: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    part_idx = tl.program_id(2)\n    max_num_partitions = tl.num_programs(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    USE_PARTITIONING = PARTITION_SIZE > 0\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    if USE_PARTITIONING:\n        context_start_idx = part_idx * PARTITION_SIZE\n        if context_start_idx >= context_len:\n            return\n        context_end_idx = tl.minimum(context_start_idx + PARTITION_SIZE,\n            context_len)\n        num_blocks = tl.cdiv(context_end_idx - context_start_idx, KV_BLOCK_SIZE\n            )\n    else:\n        num_blocks = tl.cdiv(context_len, KV_BLOCK_SIZE)\n    block_offset = tl.arange(0, KV_BLOCK_SIZE)\n    head_offset = tl.arange(0, HEAD_SIZE)\n    padding_group_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)\n    kv_offset = kv_head_idx * stride_kv1 + block_offset[:, None\n        ] * stride_kv2 + head_offset[None, :] * stride_kv3\n    q_offset = seq_idx * stride_q0 + (kv_head_idx * QUERY_GROUP_SIZE +\n        padding_group_offset[:, None]) * stride_q1 + head_offset[None, :\n        ] * stride_q2\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n    q = tl.load(q_ptr + q_offset, mask=group_mask, other=0.0)\n    m_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32)\n    acc = tl.zeros([PADDED_QUERY_GROUP_SIZE, HEAD_SIZE], dtype=tl.float32)\n    num_prev_blocks = part_idx * (PARTITION_SIZE // KV_BLOCK_SIZE)\n    for i in range(num_blocks):\n        block_idx = num_prev_blocks + i\n        block_number = tl.load(block_tables_ptr + seq_idx * stride_bt0 + \n            block_idx * stride_bt1)\n        kv_block_offset = block_number * stride_kv0 + kv_offset\n        mask_offset = block_idx * KV_BLOCK_SIZE + block_offset\n        kv_mask = mask_offset[:, None] < context_len\n        k = tl.load(k_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            qk = tl.sum(q[:, None, :] * k[None, :, :], axis=2)\n        else:\n            qk = tl.dot(q, k.T, out_dtype=tl.float32)\n        qk *= attn_scale\n        qk = tl.where(mask_offset < context_len, qk, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n        p = tl.math.exp2((qk - m_i_new[:, None]) * log2e)\n        alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n        acc *= alpha[:, None]\n        v = tl.load(v_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            acc += tl.sum(p.T[:, :, None] * v[:, None, :], axis=0)\n        else:\n            p = p\n            acc += tl.dot(p, v, out_dtype=tl.float32)\n        l_i = l_i * alpha + tl.sum(p, axis=1)\n        m_i = m_i_new\n    acc = acc / l_i[:, None]\n    if USE_PARTITIONING:\n        part_offset = ((seq_idx * NUM_KV_HEADS + kv_head_idx) *\n            max_num_partitions * QUERY_GROUP_SIZE + part_idx *\n            QUERY_GROUP_SIZE + padding_group_offset)\n        mask = padding_group_offset < QUERY_GROUP_SIZE\n        tl.store(m_i_ptr + part_offset, m_i, mask=mask)\n        tl.store(l_i_ptr + part_offset, l_i, mask=mask)\n    out_offset = seq_idx * stride_o0\n    if USE_PARTITIONING:\n        out_offset += kv_head_idx * stride_o1\n    else:\n        out_offset += kv_head_idx * QUERY_GROUP_SIZE * stride_o1\n    out_offset += part_idx * stride_o2 + padding_group_offset[:, None\n        ] * stride_o3 + head_offset[None, :] * stride_o4\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n    tl.store(out_ptr + out_offset, acc, mask=group_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "2bb2e7fe-532a-48b8-b4c4-abbf22dedcd2"
  },
  {
    "input": "@triton.jit\ndef load_rotary_embedded_vector(QK, stride_qk_n, stride_qk_t, stride_qk_hid,\n    COS, stride_cos_t, stride_cos_hid, SIN, stride_sin_t, stride_sin_hid,\n    idx_n, idx_t_qk, idx_t_rope, HID, BLOCK_HID):\n    idx_hid = tl.arange(0, BLOCK_HID)\n    mask_hid = idx_hid < HID\n    idx_hid_rot = (idx_hid + HID // 2) % HID\n    mask_hid_rot = mask_hid\n    vec = tl.load(QK + idx_n * stride_qk_n + idx_t_qk * stride_qk_t + \n        idx_hid * stride_qk_hid, mask=mask_hid, other=0)\n    vec_rot = tl.load(QK + idx_n * stride_qk_n + idx_t_qk * stride_qk_t + \n        idx_hid_rot * stride_qk_hid, mask=mask_hid_rot, other=0)\n    vec_rot = tl.where(idx_hid < HID // 2, -vec_rot, vec_rot)\n    cos = tl.load(COS + idx_t_rope * stride_cos_t + idx_hid *\n        stride_cos_hid, mask=mask_hid, other=0)\n    sin = tl.load(SIN + idx_t_rope * stride_sin_t + idx_hid *\n        stride_sin_hid, mask=mask_hid, other=0)\n    vec_rope = vec.to(tl.float32) * cos + vec_rot.to(tl.float32) * sin\n    return vec_rope, vec, vec_rot, cos, sin\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "049220de-b6ff-4ee5-85d0-5c9f765ae5d5"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, initial_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < DK\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + ((i + 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h * tl.math.exp2(d_b)[None, :] + tl.dot(b_v, b_k,\n                allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h * tl.math.exp2(d_b)[None, :] + tl.dot(b_v, b_k,\n                allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + (T - (i - 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_db = tl.load(p_db, mask=mask, other=0)\n        if CHECK and i == 1:\n            b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh = b_dh * tl.math.exp2(b_db)[:, None] + tl.dot(b_q, b_do,\n                allow_tf32=False)\n        else:\n            b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh = b_dh * tl.math.exp2(b_db)[:, None] + tl.dot(b_q, b_do,\n                allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "139f49d6-95f8-4431-8fe0-028f8f20341f"
  },
  {
    "input": "@triton.jit\ndef add_func(a_ptr, b_ptr, c_ptr, element_size, block_size: 'tl.constexpr'):\n    \"\"\"Triton Func.\"\"\"\n    block_id = tl.program_id(axis=0)\n    thread_id = block_id * block_size + tl.arange(0, block_size)\n    mask = thread_id < element_size\n    a = tl.load(a_ptr + thread_id, mask=mask)\n    b = tl.load(b_ptr + thread_id, mask=mask)\n    tl.store(c_ptr + thread_id, a + b, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "47c966f7-e842-48af-ba6b-a0b0f79c44eb"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=offs_h[:, None] <\n        head_num, other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0)[:, None]\n    q_src_data = src_data / data_scale\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=offs_h[:, None] < head_num)\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "e5f8e4c9-545d-4f27-99e4-625941bbdb9d"
  },
  {
    "input": "@triton.jit\ndef attention_fwd_kernel(q, k, v, h, o, s_qh, s_qt, s_qd, s_hh, s_ht, T,\n    scale, BT: 'tl.constexpr', BD: 'tl.constexpr', NT: 'tl.constexpr',\n    STORE: 'tl.constexpr', IFCOND: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    b_h = tl.zeros([BD, BD], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qh, (BD, T), (s_qd, s_qt), (0,\n            i * BT), (BD, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (NT * BD, BD), (s_ht, s_qd\n            ), (i * BD, 0), (BD, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        if STORE:\n            tl.store(p_h, b_h)\n        b_q = tl.load(p_q)\n        b_q = b_q * scale\n        b_k = tl.load(p_k)\n        b_v = tl.load(p_v)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if IFCOND:\n            if i == 0:\n                b_h = tl.dot(b_k, b_v, allow_tf32=False)\n            else:\n                b_o += tl.dot(b_q, b_h, allow_tf32=False)\n                b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "66081b6b-d3da-44d1-b9f8-5f047df87930"
  },
  {
    "input": "@triton.jit\ndef _fw_kernel(xy, yz, zx, xy_color, yz_color, zx_color, rays, centers,\n    weights, biases, weight_opacity, bias_opacity, weight_color, bias_color,\n    rays_encoding, negative_log_transmittance, expected_depth,\n    expected_features, near, far, effective_num_samples, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', gain: 'tl.constexpr',\n    batch_size: 'tl.constexpr', num_rays_per_batch: 'tl.constexpr', C:\n    'tl.constexpr', OUT_C: 'tl.constexpr', H: 'tl.constexpr', W:\n    'tl.constexpr', D: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    transmittance_thr: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr', inject_noise: 'tl.constexpr', inject_noise_sigma:\n    'tl.constexpr', inject_noise_seed, contract_coords: 'tl.constexpr',\n    contract_perc_foreground: 'tl.constexpr', disparity_at_inf:\n    'tl.constexpr', shape_representation: 'tl.constexpr', activation_fun:\n    'tl.constexpr', use_separate_color_rep: 'tl.constexpr'):\n    tot_num_samples = num_samples + num_samples_inf\n    num_rays = num_rays_per_batch * batch_size\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = pid * BLOCK_SIZE * OUT_C + OUT_C * tl.arange(0, BLOCK_SIZE\n        )[:, None] + tl.arange(0, OUT_C)[None, :]\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    center_x = tl.load(centers + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(centers + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(centers + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(rays + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(rays + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(rays + offs_z, mask=offs_z < num_rays * 3)\n    batch_index = (pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        ) // num_rays_per_batch\n    near_buffer = tl.load(near + offs, mask=offs < num_rays)\n    far_buffer = tl.load(far + offs, mask=offs < num_rays)\n    effective_num_samples_buffer = tl.zeros((1,), dtype=tl.int32)\n    depth = near_buffer\n    seed_buffer = tl.load(inject_noise_seed + offs, mask=offs < num_rays)\n    sample_index_buffer = tl.arange(0, BLOCK_SIZE\n        ) * tot_num_samples + pid * BLOCK_SIZE * tot_num_samples + 1\n    expected_depth_buffer = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    expected_features_buffer = tl.zeros((BLOCK_SIZE, OUT_C), dtype=tl.float32)\n    prev_transmittance = tl.full((BLOCK_SIZE,), 1.0, dtype=tl.float32)\n    negative_log_transmittance_buffer = tl.zeros((BLOCK_SIZE,), dtype=tl.\n        float32)\n    w1, w2, wr, wo, wc, b1, b2, br, bo, bc, w2c, b2c = _load_mlp_weights(\n        weights, biases, weight_opacity, bias_opacity, weight_color,\n        bias_color, C, OUT_C, BLOCK_SIZE)\n    rays_encoding_buffer = tl.load(rays_encoding + pid * BLOCK_SIZE * C + C *\n        tl.arange(0, BLOCK_SIZE)[:, None] + tl.arange(0, C)[None, :])\n    transmittance = tl.exp(-negative_log_transmittance_buffer)\n    zero_value = tl.zeros((BLOCK_SIZE,), tl.float32)\n    zero_color = tl.zeros((BLOCK_SIZE, OUT_C), tl.float32)\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = _depth_lin(near_buffer, far_buffer, num_samples, step)\n            depth_prev = _depth_lin(near_buffer, far_buffer, num_samples, \n                step - 1)\n        else:\n            depth = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n            depth_prev = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples - 1)\n        delta = depth - depth_prev\n        if tl.sum(transmittance > transmittance_thr, axis=0):\n            sample_x = center_x + depth * ray_x\n            sample_y = center_y + depth * ray_y\n            sample_z = center_z + depth * ray_z\n            if contract_coords:\n                sample_x, sample_y, sample_z = _contract_pi(sample_x,\n                    sample_y, sample_z, contract_perc_foreground)\n            vec = _sample_grid_rep(xy, yz, zx, batch_index, sample_x,\n                sample_y, sample_z, batch_size, C, D, H, W, BLOCK_SIZE,\n                shape_representation)\n            if mask_out_of_bounds_samples:\n                in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z,\n                    W, H, D, C, BLOCK_SIZE)\n                vec = vec * in_bounds_mask\n            vec = tl.maximum(tl.dot(vec, w1, allow_tf32=ALLOW_TF32) + b1, 0.0)\n            vec = tl.maximum(tl.dot(vec, w2, allow_tf32=ALLOW_TF32) + b2, 0.0)\n            value = tl.view(tl.sum(wo * vec, axis=1), (BLOCK_SIZE,)) + bo\n            if inject_noise:\n                r = _int_to_randn(sample_index_buffer, sample_index_buffer +\n                    num_rays * tot_num_samples, seed_buffer)\n                value = value + r * inject_noise_sigma\n            if activation_fun == 0:\n                value_act = _softplus(value)\n            else:\n                value_act = tl.maximum(value, 0.0)\n            value = delta * gain * value_act\n            if use_separate_color_rep:\n                vec_color = _sample_grid_rep(xy_color, yz_color, zx_color,\n                    batch_index, sample_x, sample_y, sample_z, batch_size,\n                    C, D, H, W, BLOCK_SIZE, shape_representation)\n                vec_color = vec_color + rays_encoding_buffer\n                if mask_out_of_bounds_samples:\n                    in_bounds_mask = _is_in_bounds(sample_x, sample_y,\n                        sample_z, W, H, D, C, BLOCK_SIZE)\n                    vec_color = vec_color * in_bounds_mask\n                vec_color1 = tl.maximum(tl.dot(vec_color, wr, allow_tf32=\n                    ALLOW_TF32) + br, 0.0)\n                vec_color2 = tl.maximum(tl.dot(vec_color1, w2c, allow_tf32=\n                    ALLOW_TF32) + b2c, 0.0)\n                log_color = tl.dot(vec_color2, wc, allow_tf32=ALLOW_TF32) + bc\n            else:\n                vecr = tl.maximum(tl.dot(vec, wr, allow_tf32=ALLOW_TF32) +\n                    br + rays_encoding_buffer, 0.0)\n                log_color = tl.dot(vecr, wc, allow_tf32=ALLOW_TF32) + bc\n            color = _color_activation(log_color)\n            effective_ns_increment = 1\n        else:\n            value = zero_value\n            color = zero_color\n            effective_ns_increment = 0\n        negative_log_transmittance_buffer = (\n            negative_log_transmittance_buffer + value)\n        transmittance = tl.exp(-negative_log_transmittance_buffer)\n        render_weights = prev_transmittance - transmittance\n        expected_depth_buffer = expected_depth_buffer + render_weights * depth\n        render_weights_bcast = tl.broadcast_to(prev_transmittance[:, None],\n            (BLOCK_SIZE, OUT_C)) - tl.broadcast_to(transmittance[:, None],\n            (BLOCK_SIZE, OUT_C))\n        feature_render = color * render_weights_bcast\n        expected_features_buffer = expected_features_buffer + feature_render\n        prev_transmittance = transmittance\n        sample_index_buffer = sample_index_buffer + 1\n        effective_num_samples_buffer = (effective_num_samples_buffer +\n            effective_ns_increment)\n    tl.store(negative_log_transmittance + offs,\n        negative_log_transmittance_buffer, mask=offs < num_rays)\n    tl.store(expected_depth + offs, expected_depth_buffer, mask=offs < num_rays\n        )\n    tl.store(effective_num_samples + pid + tl.arange(0, 1),\n        effective_num_samples_buffer)\n    tl.store(expected_features + offs_features, expected_features_buffer,\n        mask=offs_features_mask)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "53b58763-5d63-46ce-9f72-df2ce1772125"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_inter(q, k, gi, ge, A, s_k_h, s_k_t,\n    s_k_d, scale, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr',\n    BC: 'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_i, i_j = i_c // NC, i_c % NC\n    if i_i <= i_j:\n        return\n    if i_t * BT + i_i * BC >= T:\n        return\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gq = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (K, T), (s_k_d, s_k_t),\n            (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_gq - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A += tl.dot(b_qg, b_kg)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT +\n        i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "7e6a592d-be55-4f9f-aae8-26bc88eb9c4f"
  },
  {
    "input": "@triton.jit\ndef ninth_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST000 = 1.93163963757558\n    CONST001 = 2.65478475211798\n    CONST002 = 1.72771101506082\n    CONST004 = 1.59908344719522\n    CONST005 = 6.39633378878088\n    CONST006 = 6.39633378878088\n    CONST007 = 8.63855507530412\n    CONST008 = 9.59450068317133\n    CONST009 = 4.35889894354067\n    CONST010 = 10.7269778688696\n    CONST011 = 10.7269778688696\n    CONST012 = 6.39633378878088\n    CONST013 = 15.0007324039945\n    CONST014 = 13.0937127087774\n    CONST016 = 14.45506743704\n    CONST017 = 14.45506743704\n    CONST018 = 13.3827919767794\n    CONST019 = 13.5214774630291\n    CONST020 = 23.8930627690618\n    CONST021 = 27.0429549260581\n    CONST022 = 29.2403830344269\n    CONST023 = 29.2403830344269\n    CONST024 = 30.001464807989\n    CONST025 = -480.023436927823\n    CONST026 = -480.023436927823\n    CONST029 = 42.9079114754785\n    CONST030 = -462.562157985281\n    CONST032 = -967.518168434061\n    CONST034 = 57.8202697481601\n    CONST035 = 58.9217071894985\n    CONST036 = 58.9217071894985\n    CONST037 = 62.4530292249704\n    CONST038 = 1081.71819704233\n    CONST039 = 64.3618672132178\n    CONST040 = 578.202697481601\n    CONST044 = 600.029296159779\n    CONST045 = -936.795438374555\n    CONST047 = 96.7518168434061\n    CONST049 = 115.64053949632\n    CONST051 = -392.811381263323\n    CONST053 = 137.14955340795\n    CONST055 = 150.007324039945\n    CONST056 = -343.263291803828\n    CONST058 = 11.2632978048796\n    CONST061 = -315.37233853663\n    CONST062 = -314.249105010659\n    CONST063 = 205.957975082297\n    CONST065 = -294.608535947493\n    CONST066 = 240.011718463912\n    CONST068 = 241.879542108515\n    CONST069 = 255.853351551235\n    CONST070 = 255.853351551235\n    CONST071 = -241.879542108515\n    CONST072 = -240.011718463912\n    CONST073 = -241.879542108515\n    CONST074 = 788.430846341574\n    CONST075 = 1.72771101506082\n    CONST076 = -1.93163963757558\n    CONST077 = -1249.06058449941\n    CONST078 = -223.00191917791\n    CONST080 = -216.343639408465\n    CONST081 = 300.01464807989\n    CONST082 = -204.682681240988\n    CONST083 = -204.682681240988\n    CONST084 = -204.682681240988\n    CONST086 = -196.405690631662\n    CONST087 = -191.890013663426\n    CONST088 = -191.890013663427\n    CONST089 = -187.359087674911\n    CONST090 = -693.843236977922\n    CONST091 = 334.502878766866\n    CONST092 = -176.765121568496\n    CONST093 = -150.007324039945\n    CONST094 = -144.5506743704\n    CONST095 = 374.718175349822\n    CONST096 = 374.718175349822\n    CONST097 = -649.030918225395\n    CONST099 = -630.744677073259\n    CONST100 = -115.64053949632\n    CONST101 = -114.421097267943\n    CONST102 = -115.64053949632\n    CONST103 = -104.74970167022\n    CONST104 = 411.915950164594\n    CONST105 = -95.5722510762473\n    CONST106 = -90.106382439037\n    CONST107 = -90.0043944239669\n    CONST109 = -80.2967518606762\n    CONST110 = -78.4601809837321\n    CONST111 = 435.383175795327\n    CONST112 = -589.217071894985\n    CONST113 = -78.4601809837321\n    CONST114 = 435.383175795328\n    CONST115 = -68.5747767039748\n    CONST116 = -63.9633378878088\n    CONST117 = -63.9633378878088\n    CONST118 = -62.4530292249704\n    CONST119 = -58.9217071894985\n    CONST120 = -1081.71819704233\n    CONST121 = -57.8202697481601\n    CONST122 = -57.8202697481601\n    CONST123 = -58.9217071894985\n    CONST124 = -54.0859098521163\n    CONST125 = 462.562157985281\n    CONST127 = -48.3759084217031\n    CONST128 = -48.375908421703\n    CONST129 = -38.6327927515116\n    CONST130 = -30.9062342012093\n    CONST131 = 483.759084217031\n    CONST132 = -30.001464807989\n    CONST133 = -30.001464807989\n    CONST134 = -27.0429549260581\n    CONST135 = -24.1879542108515\n    CONST136 = -24.1879542108515\n    CONST137 = -1.63671408859718\n    CONST138 = -15.0007324039945\n    CONST139 = -13.5214774630291\n    CONST140 = -13.8216881204866\n    CONST141 = -13.0937127087774\n    CONST142 = -13.3827919767794\n    CONST143 = -9.82028453158308\n    CONST144 = -4.91014226579154\n    CONST145 = 511.706703102471\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR01 = VAR07 * VAR07 * VAR07\n    VAR02 = VAR06 * VAR06\n    VAR03 = VAR06 * VAR07\n    VAR04 = VAR07 * VAR07\n    VAR05 = VAR07 * VAR08\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR10 = VAR16 * VAR16 * VAR16\n    VAR11 = VAR15 * VAR15\n    VAR12 = VAR15 * VAR16\n    VAR13 = VAR16 * VAR16\n    VAR14 = VAR16 * VAR17\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR19 = VAR25 * VAR25 * VAR25\n    VAR20 = VAR24 * VAR24\n    VAR21 = VAR24 * VAR25\n    VAR22 = VAR25 * VAR25\n    VAR23 = VAR25 * VAR26\n    Y00 = (CONST001 * VAR01 + CONST020 * VAR20 * x + CONST078 * VAR07 *\n        VAR22 + CONST091 * VAR05 * VAR24 + CONST105 * VAR03 * VAR26)\n    Y01 = y * (-CONST099 * VAR05 * VAR25 + CONST099 * VAR07 * VAR23 + \n        CONST106 * VAR03 * z - CONST106 * VAR21 * x)\n    Y02 = CONST000 * VAR01 + VAR03 * (CONST129 * VAR26 + CONST130 * VAR17\n        ) + VAR05 * (CONST021 * VAR24 - CONST097 * VAR17 * VAR26) + VAR07 * (\n        CONST120 * VAR17 * VAR24 - CONST124 * VAR22) + x * (-CONST080 *\n        VAR17 * VAR22 + CONST139 * VAR20)\n    Y03 = VAR16 * (CONST077 * VAR07 * VAR25 + CONST095 * VAR05 * z + \n        CONST096 * VAR23 * x) + y * (-CONST089 * VAR05 * VAR25 - CONST089 *\n        VAR07 * VAR23 + CONST109 * VAR03 * z + CONST109 * VAR21 * x)\n    Y04 = (CONST002 * VAR01 + CONST007 * VAR20 * x + CONST135 * VAR05 *\n        VAR24 + CONST140 * VAR03 * VAR26 + VAR15 * (CONST032 * VAR07 *\n        VAR26 + CONST047 * VAR05 + CONST131 * VAR24 * x) + VAR17 * (-\n        CONST071 * VAR07 * VAR24 + CONST071 * VAR22 * x + CONST111 * VAR05 *\n        VAR26 + CONST127 * VAR03))\n    Y05 = VAR14 * (CONST030 * VAR07 * z - CONST030 * VAR25 * x) + VAR16 * (\n        CONST030 * VAR23 * x + CONST125 * VAR05 * z) + y * (CONST034 *\n        VAR07 * VAR23 + CONST121 * VAR05 * VAR25 - CONST121 * VAR21 * x + \n        CONST122 * VAR03 * z)\n    Y06 = CONST119 * VAR03 * VAR17 - CONST137 * VAR01 + VAR05 * (CONST035 *\n        VAR17 * VAR26 - CONST086 * VAR15 + CONST143 * VAR24) + VAR07 * (\n        CONST051 * VAR15 * VAR26 - CONST065 * VAR17 * VAR24 + CONST103 *\n        VAR13 + CONST141 * VAR22) + x * (-CONST062 * VAR13 * VAR26 - \n        CONST092 * VAR17 * VAR22 + CONST112 * VAR15 * VAR24 + CONST144 * VAR20)\n    Y07 = CONST132 * VAR03 * y * z + VAR05 * (CONST081 * VAR16 * z + \n        CONST107 * VAR25 * y) + VAR07 * (CONST026 * VAR14 * z + CONST044 *\n        VAR16 * VAR25 + CONST107 * VAR23 * y) + x * (CONST025 * VAR14 *\n        VAR25 + CONST053 * VAR12 * z + CONST081 * VAR16 * VAR23 + CONST132 *\n        VAR21 * y)\n    Y08 = CONST004 * VAR01 + VAR03 * (CONST006 * VAR26 + CONST116 * VAR17\n        ) + VAR05 * (CONST008 * VAR24 + CONST069 * VAR15 + CONST087 * VAR17 *\n        VAR26) + VAR07 * (CONST005 * VAR22 + CONST083 * VAR13 + CONST087 *\n        VAR17 * VAR24 + CONST145 * VAR15 * VAR26) + x * (CONST004 * VAR20 +\n        CONST022 * VAR11 + CONST069 * VAR15 * VAR24 + CONST082 * VAR13 *\n        VAR26 + CONST116 * VAR17 * VAR22)\n    Y09 = CONST009 * VAR10 + VAR12 * (CONST110 * VAR26 + CONST113 * VAR08\n        ) + VAR14 * (CONST063 * VAR06 + CONST063 * VAR24 + CONST104 * VAR08 *\n        VAR26) + VAR16 * (CONST056 * VAR06 * VAR26 + CONST056 * VAR08 *\n        VAR24 + CONST101 * VAR04 + CONST101 * VAR22) + y * (CONST010 *\n        VAR20 + CONST011 * VAR02 + CONST029 * VAR04 * VAR26 + CONST029 *\n        VAR08 * VAR22 + CONST039 * VAR06 * VAR24)\n    Y10 = CONST004 * VAR19 + VAR21 * (CONST005 * VAR08 + CONST117 * VAR17\n        ) + VAR23 * (CONST008 * VAR06 + CONST070 * VAR15 + CONST088 * VAR08 *\n        VAR17) + VAR25 * (CONST012 * VAR04 + CONST082 * VAR13 + CONST087 *\n        VAR06 * VAR17 + CONST145 * VAR08 * VAR15) + z * (CONST004 * VAR02 +\n        CONST023 * VAR11 + CONST070 * VAR06 * VAR15 + CONST084 * VAR08 *\n        VAR13 + CONST117 * VAR04 * VAR17)\n    Y11 = VAR12 * (CONST115 * VAR08 - CONST115 * VAR26) + VAR14 * (CONST066 *\n        VAR06 + CONST072 * VAR24) + VAR16 * (CONST055 * VAR08 * VAR24 + \n        CONST093 * VAR04 + CONST093 * VAR06 * VAR26 - CONST093 * VAR22) + y * (\n        CONST013 * VAR02 + CONST024 * VAR04 * VAR26 + CONST133 * VAR08 *\n        VAR22 + CONST138 * VAR20)\n    Y12 = CONST036 * VAR17 * VAR21 + CONST137 * VAR19 + VAR23 * (CONST086 *\n        VAR15 + CONST123 * VAR08 * VAR17 - CONST143 * VAR06) + VAR25 * (\n        CONST014 * VAR04 - CONST051 * VAR08 * VAR15 + CONST065 * VAR06 *\n        VAR17 - CONST103 * VAR13) + z * (CONST062 * VAR08 * VAR13 + \n        CONST092 * VAR04 * VAR17 - CONST112 * VAR06 * VAR15 - CONST144 * VAR02)\n    Y13 = VAR14 * (CONST049 * VAR06 + CONST049 * VAR24 + CONST090 * VAR08 *\n        VAR26) + VAR16 * (CONST040 * VAR06 * VAR26 + CONST040 * VAR08 *\n        VAR24 + CONST100 * VAR22 + CONST102 * VAR04) + y * (CONST016 *\n        VAR20 + CONST017 * VAR02 + CONST094 * VAR06 * VAR24 + CONST121 *\n        VAR04 * VAR26 + CONST122 * VAR08 * VAR22)\n    Y14 = (CONST007 * VAR02 * z + CONST075 * VAR19 + CONST136 * VAR06 *\n        VAR23 + CONST140 * VAR08 * VAR21 + VAR15 * (CONST032 * VAR08 *\n        VAR25 + CONST047 * VAR23 + CONST131 * VAR06 * z) + VAR17 * (\n        CONST068 * VAR06 * VAR25 + CONST073 * VAR04 * z + CONST114 * VAR08 *\n        VAR23 + CONST128 * VAR21))\n    Y15 = VAR16 * (CONST037 * VAR22 - CONST045 * VAR06 * VAR26 + CONST045 *\n        VAR08 * VAR24 + CONST118 * VAR04) + y * (CONST018 * VAR02 + \n        CONST089 * VAR04 * VAR26 - CONST089 * VAR08 * VAR22 + CONST142 * VAR20)\n    Y16 = (CONST019 * VAR02 * z + CONST076 * VAR19 + CONST124 * VAR04 *\n        VAR25 - CONST129 * VAR08 * VAR21 + CONST134 * VAR06 * VAR23 + VAR17 *\n        (CONST038 * VAR06 * VAR25 + CONST080 * VAR04 * z + CONST097 * VAR08 *\n        VAR23 - CONST130 * VAR21))\n    Y17 = y * (CONST058 * VAR02 + CONST058 * VAR20 + CONST061 * VAR04 *\n        VAR26 + CONST061 * VAR08 * VAR22 + CONST074 * VAR06 * VAR24)\n    Y18 = (CONST001 * VAR19 + CONST020 * VAR02 * z + CONST078 * VAR04 *\n        VAR25 + CONST091 * VAR06 * VAR23 + CONST105 * VAR08 * VAR21)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y07, mask=\n        output_row_offset + 7 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y08, mask=\n        output_row_offset + 8 < output_numel)\n    tl.store(output_ptr + output_row_offset + 9, Y09, mask=\n        output_row_offset + 9 < output_numel)\n    tl.store(output_ptr + output_row_offset + 10, Y10, mask=\n        output_row_offset + 10 < output_numel)\n    tl.store(output_ptr + output_row_offset + 11, Y11, mask=\n        output_row_offset + 11 < output_numel)\n    tl.store(output_ptr + output_row_offset + 12, Y12, mask=\n        output_row_offset + 12 < output_numel)\n    tl.store(output_ptr + output_row_offset + 13, Y13, mask=\n        output_row_offset + 13 < output_numel)\n    tl.store(output_ptr + output_row_offset + 14, Y14, mask=\n        output_row_offset + 14 < output_numel)\n    tl.store(output_ptr + output_row_offset + 15, Y15, mask=\n        output_row_offset + 15 < output_numel)\n    tl.store(output_ptr + output_row_offset + 16, Y16, mask=\n        output_row_offset + 16 < output_numel)\n    tl.store(output_ptr + output_row_offset + 17, Y17, mask=\n        output_row_offset + 17 < output_numel)\n    tl.store(output_ptr + output_row_offset + 18, Y18, mask=\n        output_row_offset + 18 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "7ebabfcf-c0d6-44c7-bf5f-6f3a7c847562"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}, num_warps=4),\n    triton.Config({'BLOCK_SIZE': 1024}, num_warps=8)], key=['n_elements'],\n    restore_value=['p_ptr', 'exp_avg_ptr'])\n@triton.jit\ndef update_fn_kernel(p_ptr, grad_ptr, exp_avg_ptr, lr, wd, beta1, beta2,\n    n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    offset_p_ptr = p_ptr + offsets\n    offset_grad_ptr = grad_ptr + offsets\n    offset_exp_avg_ptr = exp_avg_ptr + offsets\n    p = tl.load(offset_p_ptr, mask=mask)\n    grad = tl.load(offset_grad_ptr, mask=mask)\n    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)\n    p = p * (1 - lr * wd)\n    diff = exp_avg - grad\n    update = diff * beta1 + grad\n    can_update = update != 0\n    update_sign = tl.where(update > 0, -lr, lr)\n    p = p + update_sign * can_update\n    exp_avg = diff * beta2 + grad\n    tl.store(offset_p_ptr, p, mask=mask)\n    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "gradient accumulation",
    "uuid": "b81333dc-1770-491c-bfba-7dfb791ecee7"
  },
  {
    "input": "@triton.jit\ndef _silu_and_mul_kernel(input_ptr, output_ptr, stride_input_m,\n    stride_input_n, stride_output_m, stride_output_n, size_m, size_n,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    stride_input_m = stride_input_m\n    stride_output_m = stride_output_m\n    tid = tl.program_id(0)\n    input_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    output_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    pid = tl.program_id(1)\n    input_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    up_offsets = input_m_offsets[:, None] * stride_input_m + (input_n_offsets\n        [None, :] + size_n) * stride_input_n\n    gate_offsets = input_m_offsets[:, None] * stride_input_m + input_n_offsets[\n        None, :] * stride_input_n\n    res_offsets = output_m_offsets[:, None\n        ] * stride_output_m + output_n_offsets[None, :] * stride_output_n\n    up = tl.load(input_ptr + up_offsets, mask=(input_n_offsets < size_n)[\n        None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = tl.load(input_ptr + gate_offsets, mask=(input_n_offsets < size_n\n        )[None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = gate / (1 + tl.exp(-gate))\n    gate = gate\n    tl.store(output_ptr + res_offsets, up * gate, mask=(output_n_offsets <\n        size_n)[None, :] * (output_m_offsets < size_m)[:, None])\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "38c9ada0-471d-4c8e-a005-3e7e614d6fda"
  },
  {
    "input": "@triton.jit\ndef scaled_index_add_bwd_kernel(grad_output_ptr, grad_source_ptr,\n    grad_scaling_ptr, source_ptr, scaling_ptr, index_ptr, alpha,\n    num_inp_indices, num_src_indices, num_rows, num_cols, stride0, stride1,\n    stride2, BLOCK_SIZE_INDEX: 'tl.constexpr', BLOCK_SIZE_ROW:\n    'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr', HAS_SCALING: 'tl.constexpr'\n    ):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    pid2 = tl.program_id(axis=2)\n    rows = pid1 * BLOCK_SIZE_ROW + tl.arange(0, BLOCK_SIZE_ROW)\n    cols = pid2 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    source_offsets = source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    source_mask = (source_indices[:, None, None] < num_src_indices) & (rows\n        [None, :, None] < num_rows) & (cols[None, None, :] < num_cols)\n    source = tl.load(source_offsets, mask=source_mask)\n    grad_output_indices = tl.load(index_ptr + source_indices, mask=\n        source_indices < num_src_indices)\n    grad_output_offsets = (grad_output_ptr + grad_output_indices * stride0 +\n        rows[None, :, None] * stride1 + cols[None, None, :] * stride2)\n    grad_output = tl.load(grad_output_offsets, mask=source_mask)\n    grad_source_offsets = grad_source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    if HAS_SCALING:\n        scaling = tl.load(scaling_ptr + cols[None, None, :] * stride2, mask\n            =cols[None, None, :] < num_cols)\n        tl.store(grad_source_offsets, alpha * grad_output * scaling, mask=\n            source_mask)\n        grad_scaling_offsets = grad_scaling_ptr + source_indices[:, None, None\n            ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n            ] * stride2\n        tl.store(grad_scaling_offsets, alpha * grad_output * source, mask=\n            source_mask)\n    else:\n        tl.store(grad_source_offsets, alpha * grad_output, mask=source_mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "90835780-476e-40b9-ae2c-17157b848f4a"
  },
  {
    "input": "@triton.jit\ndef welford_combine(mean_1, m2_1, weight_1, mean_2, m2_2, weight_2):\n    delta = mean_2 - mean_1\n    new_weight = weight_1 + weight_2\n    w2_over_w = tl.where(new_weight == 0.0, 0.0, weight_2 / new_weight)\n    return (mean_1 + delta * w2_over_w, m2_1 + m2_2 + delta * delta *\n        weight_1 * w2_over_w, new_weight)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "c9cb4e70-09e3-4c3e-b762-d9c599b40c94"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_asm, stride_bk, stride_bn, stride_bsn, stride_cm,\n    stride_cn, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', SPLIT_K:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    c = accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "2631a89b-e689-4a19-b794-d3c1d394d6d2"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU)\"\"\"\n    return x * 0.5 * (1.0 + tl.math.erf(x / sqrt2))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "b065dc34-ba16-4a02-9a39-d5aca2bc15df"
  },
  {
    "input": "@triton.jit\ndef leaky_relu(x):\n    \"\"\"\n    LeakyReLU_ activation\n\n    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html\n    \"\"\"\n    scale = 0.01\n    scale = scale\n    return tl.where(x >= 0, x, scale * x)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "84d6045f-d128-430d-9e81-046ecd3f3595"
  },
  {
    "input": "@triton.jit\ndef layer_norm_kernel(x, mean, var, gamma, beta, epsilon, stride_xm,\n    stride_xn, stride_gamma, stide_beta, n, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Triton kernel for layer normalization.\n\n    Parameters:\n    x - Input tensor.\n    mean - Tensor to store computed means.\n    var - Tensor to store computed variances.\n    gamma - Scale tensor.\n    beta - Shift tensor.\n    epsilon - A small value to avoid division by zero.\n    stride_xm, stride_xn - Strides for the input tensor.\n    stride_gamma, stride_beta - Strides for Gamma and Beta tensors.\n    n - Size of the last dimension of the input tensor.\n    BLOCK_SIZE - Size of the block for Triton computation.\n    \"\"\"\n    row = tl.program_id(0)\n    x_ptrs = x + row * stride_xm\n    mean_ptrs = mean + row\n    var_ptrs = var + row\n    gamma_ptrs = gamma\n    beta_ptrs = beta\n    x = tl.load(x_ptrs, mask=tl.arange(0, BLOCK_SIZE) < n, other=0)\n    mean = tl.sum(x, axis=0) / n\n    tl.store(mean_ptrs, mean)\n    x_centered = x - mean\n    var = tl.sum(x_centered * x_centered, axis=0) / n\n    tl.store(var_ptrs, var)\n    std = tl.sqrt(var + epsilon)\n    y = x_centered / std * tl.load(gamma_ptrs, mask=tl.arange(0, BLOCK_SIZE\n        ) < n, other=1) + tl.load(beta_ptrs, mask=tl.arange(0, BLOCK_SIZE) <\n        n, other=0)\n    tl.store(x_ptrs, y, mask=tl.arange(0, BLOCK_SIZE) < n)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "2f703915-9abd-4a85-9d5c-c43ddd28e89b"
  },
  {
    "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: bool(args[\n    'DO_SOFTCAPPING']), 'DO_LOGIT_SCALING': lambda args: bool(args[\n    'DO_LOGIT_SCALING'])})\n@triton.jit\ndef _chunked_cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE, N_CHUNKS, BLOCK_SIZE:\n    'tl.constexpr', DO_SOFTCAPPING, SOFTCAP, DO_LOGIT_SCALING, LOGIT_SCALE):\n    \"\"\"\n        256K vocab divided in 4 chunks\n\n        |-65536-| |-65536-| |-65536-| |-65536-|\n        |-------| |-------| |-------| |-------|\n        |-------| |-------| |-------| |-------|\n\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        Notice we can do logsumexp for each chunk and then\n        logsumexp[chunk_sum(logsumexp)] == logsumexp\n\n        chunk_sum = log[chunk_sum(logsumexp)]\n                  = log[exp(logsumexp(a)) + ... + exp(logsumexp(z))]\n                  = log[exp(log[sum(exp(a))]) + ... + exp(log[sum(exp(z))])]\n                  = log[sum(exp(a)) + ... + sum(exp(z))]\n                  = logsumexp(x)\n\n        This means we can perform a logsumexp for each chunk, then do a\n        final logsumexp reduction!\n\n        Ie do: logsumexp(chunked_logsumexp) - x\n    \"\"\"\n    row_idx = tl.program_id(0)\n    chunk_idx = tl.program_id(1)\n    logits_ptr += row_idx * triton_cast(logits_row_stride, tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\n    labels_ptr += row_idx\n    col_offsets = chunk_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if chunk_idx == 0:\n        if label_idx != -100:\n            x = tl.load(logits_ptr + label_idx)\n            if DO_LOGIT_SCALING:\n                x = LOGIT_SCALE * x\n            if DO_SOFTCAPPING:\n                x = SOFTCAP * triton_tanh(x / SOFTCAP)\n            loss = -1.0 * x\n        else:\n            loss = 0.0\n        tl.store(loss_ptr, loss)\n    pass\n    tl.store(logsumexp_ptr, logsumexp)\n",
    "category": "Attention Mechanisms",
    "subcategory": "cross attention",
    "uuid": "a64ff357-4194-46bd-a3b5-e98e1d99df25"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd(Q, K, V, sm_scale, DO, DQ, DK, DV, M, D, stride_z, stride_h,\n    stride_tok, stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1:\n    'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr',\n    BLK_SLICE_FACTOR: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr'):\n    LN2: 'tl.constexpr' = 0.6931471824645996\n    bhid = tl.program_id(2)\n    off_chz = bhid * N_CTX\n    adj = stride_h * (bhid % H) + stride_z * (bhid // H)\n    pid = tl.program_id(0)\n    Q += adj\n    K += adj\n    V += adj\n    DO += adj\n    DQ += adj\n    DK += adj\n    DV += adj\n    M += off_chz\n    D += off_chz\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    start_n = pid * BLOCK_N1\n    start_m = start_n\n    MASK_BLOCK_M1: 'tl.constexpr' = BLOCK_M1 // BLK_SLICE_FACTOR\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    dv = tl.zeros([BLOCK_N1, BLOCK_DMODEL], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N1, BLOCK_DMODEL], dtype=tl.float32)\n    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, MASK_BLOCK_M1, BLOCK_N1, BLOCK_DMODEL, start_n,\n        start_m, num_steps, MASK=True)\n    start_m += num_steps * MASK_BLOCK_M1\n    num_steps = (N_CTX - start_m) // BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, BLOCK_M1, BLOCK_N1, BLOCK_DMODEL, start_n,\n        start_m, num_steps, MASK=False)\n    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dv_ptrs, dv)\n    dk *= sm_scale\n    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dk_ptrs, dk)\n    start_m = pid * BLOCK_M2\n    end_n = start_m + BLOCK_M2\n    MASK_BLOCK_N2: 'tl.constexpr' = BLOCK_N2 // BLK_SLICE_FACTOR\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    dq = tl.zeros([BLOCK_M2, BLOCK_DMODEL], dtype=tl.float32)\n    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n        )\n    m = tl.load(M + offs_m)\n    m = m[:, None]\n    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, MASK_BLOCK_N2, BLOCK_DMODEL, start_m, end_n - num_steps *\n        MASK_BLOCK_N2, num_steps, MASK=True)\n    end_n -= num_steps * MASK_BLOCK_N2\n    num_steps = end_n // BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, BLOCK_N2, BLOCK_DMODEL, start_m, end_n - num_steps *\n        BLOCK_N2, num_steps, MASK=False)\n    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    dq *= LN2\n    tl.store(dq_ptrs, dq)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "0909268e-5c84-41d5-a3af-1cd15c5d1cd5"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 1024}, num_warps=8),\n    triton.Config({'BLOCK_SIZE': 512}, num_warps=8), triton.Config({\n    'BLOCK_SIZE': 256}, num_warps=8), triton.Config({'BLOCK_SIZE': 128},\n    num_warps=8), triton.Config({'BLOCK_SIZE': 64}, num_warps=8), triton.\n    Config({'BLOCK_SIZE': 32}, num_warps=8), triton.Config({'BLOCK_SIZE': \n    16}, num_warps=8), triton.Config({'BLOCK_SIZE': 1024}, num_warps=4),\n    triton.Config({'BLOCK_SIZE': 512}, num_warps=4), triton.Config({\n    'BLOCK_SIZE': 256}, num_warps=4), triton.Config({'BLOCK_SIZE': 128},\n    num_warps=4), triton.Config({'BLOCK_SIZE': 64}, num_warps=4), triton.\n    Config({'BLOCK_SIZE': 32}, num_warps=4), triton.Config({'BLOCK_SIZE': \n    16}, num_warps=4)], key=['num_rows', 'num_cols'])\n@triton.jit\ndef layernorm_kernel(a_ptr, batch_stride, row_stride, col_stride, num_rows,\n    num_cols, weight_ptr, bias_ptr, eps, out_ptr, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    IDEA 1: Merge batch and seq len dimension into 1\n    IDEA 2: Use tiled row approach\n    \"\"\"\n    batch_idx = tl.program_id(axis=0)\n    row_idx = tl.program_id(axis=1)\n    batch_offset = batch_idx * batch_stride\n    row_offset = row_idx * row_stride\n    local_sum = 0.0\n    for offset in range(0, num_cols, BLOCK_SIZE):\n        local_offset = batch_offset + row_offset + offset + tl.arange(0,\n            BLOCK_SIZE)\n        mask = offset + tl.arange(0, BLOCK_SIZE) < num_cols\n        data = tl.load(a_ptr + local_offset, mask=mask, other=0.0)\n        local_sum += tl.sum(data)\n    mean = local_sum / num_cols\n    local_std = 0.0\n    for offset in range(0, num_cols, BLOCK_SIZE):\n        local_offset = batch_offset + row_offset + offset + tl.arange(0,\n            BLOCK_SIZE)\n        mask = offset + tl.arange(0, BLOCK_SIZE) < num_cols\n        data = tl.load(a_ptr + local_offset, mask=mask, other=mean)\n        x = data - mean\n        x = x * x\n        local_std += tl.sum(x)\n    std = local_std / num_cols + eps\n    std = tl.sqrt(std)\n    for offset in range(0, num_cols, BLOCK_SIZE):\n        local_offset = offset + tl.arange(0, BLOCK_SIZE)\n        mask = local_offset < num_cols\n        w = tl.load(weight_ptr + local_offset, mask=mask, other=0.0)\n        b = tl.load(bias_ptr + local_offset, mask=mask, other=0.0)\n        local_offset += row_offset + batch_offset\n        mask = offset + tl.arange(0, BLOCK_SIZE) < num_cols\n        x = tl.load(a_ptr + local_offset, mask=mask, other=0.0)\n        norm = w * ((x - mean) / std) + b\n        tl.store(out_ptr + local_offset, norm, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "99b944f2-75b5-4d97-a5d1-282fa018f4bc"
  },
  {
    "input": "@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkv(q, k, v, h, g, do, dh, dq, dk, dv,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False)\n    p_g = g + i_bh * T + i_t * BT + tl.arange(0, BT)\n    b_g = tl.load(p_g)\n    b_g_last = tl.load(g + i_bh * T + i_t * BT + BT - 1)\n    mask = tl.math.exp2(b_g[None, :] - b_g[:, None])\n    mask = tl.where(o_i[:, None] <= o_i[None, :], mask * scale, 0)\n    b_s = b_s * mask\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * tl.math.exp2(-b_g +\n            b_g_last)[:, None] + tl.dot(b_s, b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_dq = b_dq * tl.math.exp2(b_g)[:, None]\n    b_dk = b_dk * tl.math.exp2(-b_g + b_g_last)[:, None]\n    b_ds = b_ds * tl.trans(mask)\n    b_ds = b_ds\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "de9667cb-c29f-4bd3-b813-08976a80e318"
  },
  {
    "input": "@triton.jit\ndef kl_div_kernel(logits, target_logits, loss, s_logits, s_loss, reduction:\n    'tl.constexpr', N: 'tl.constexpr', V: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_n = tl.program_id(0)\n    logits += i_n * s_logits\n    target_logits += i_n * s_logits\n    sm, tm = float('-inf'), float('-inf')\n    sd, td = 0.0, 0.0\n    NV = tl.cdiv(V, BV)\n    for iv in range(0, NV):\n        o_x = iv * BV + tl.arange(0, BV)\n        b_sl = tl.load(logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_sm = tl.max(b_sl)\n        m_new = tl.maximum(sm, b_sm)\n        sd = sd * tl.exp(sm - m_new) + tl.sum(tl.exp(b_sl - m_new))\n        sm = m_new\n        b_tl = tl.load(target_logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_tm = tl.max(b_tl)\n        m_new = tl.maximum(tm, b_tm)\n        td = td * tl.exp(tm - m_new) + tl.sum(tl.exp(b_tl - m_new))\n        tm = m_new\n    b_loss = 0.0\n    for iv in range(0, NV):\n        o_x = iv * BV + tl.arange(0, BV)\n        b_sl = tl.load(logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_tl = tl.load(target_logits + o_x, mask=o_x < V, other=float('-inf'))\n        b_sp_log = b_sl - sm - tl.log(sd)\n        b_tp_log = b_tl - tm - tl.log(td)\n        b_sp = tl.exp(b_sp_log)\n        b_tp = tl.exp(b_tp_log)\n        b_kl = tl.where(o_x < V, b_tp * (b_tp_log - b_sp_log), 0)\n        b_dl = -b_tp + b_sp\n        b_loss += tl.sum(b_kl)\n        if reduction == 'batchmean':\n            b_dl = b_dl / N\n        tl.store(logits + o_x, b_dl, mask=o_x < V)\n    if reduction == 'batchmean':\n        b_loss = b_loss / N\n    tl.store(loss + i_n * s_loss, b_loss)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "b8771d35-b2d1-42ae-9d97-8af75a8ca06c"
  },
  {
    "input": "@triton.jit\ndef _gemma_rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    output = normed * (W_row + 1.0)\n    tl.store(Y + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "ffb37c35-24a6-44f9-a79f-943b3fcc7e0b"
  },
  {
    "input": "@triton.autotune(configs=AUTOTUNE_CONFIGS, key=['M', 'N', 'K'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    tl.device_assert(K % BLOCK_SIZE_K == 0)\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_ak = tl.arange(0, BLOCK_SIZE_K)\n    offs_bk = tl.arange(0, BLOCK_SIZE_K // 2)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_ak[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_ak[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        b = tl.load(b_ptrs)\n        tl.static_assert(b.dtype == tl.int8)\n        _4_i8 = tl.full((1,), 4, dtype=tl.int8)\n        b_lo = b << _4_i8 >> _4_i8\n        b_hi = b >> _4_i8\n        b_f16 = tl.join(b_lo, b_hi).permute(0, 2, 1).reshape(BLOCK_SIZE_K,\n            BLOCK_SIZE_N)\n        accumulator += tl.dot(a, b_f16)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk // 2\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "2c2abd14-4da7-4382-84f0-afe289a56512"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'K_LOAD_MASK_NEEDED': lambda args: args['K'] % (args[\n    'BLOCK_K'] * args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_fma(C, A, B, bias, dtype: 'tl.constexpr', M, N, K, CACHE_KEY_M,\n    CACHE_KEY_N, CACHE_KEY_K, output_m_stride, output_n_stride, a_m_stride,\n    a_k_stride, b_n_stride, b_k_stride, BLOCK_M: 'tl.constexpr', GROUP_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr',\n    SPLIT_K: 'tl.constexpr', K_LOAD_MASK_NEEDED: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Bias has shape (N,)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n\n    This kernel will consolidate over K\n    \"\"\"\n    program_idx = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_idx = program_idx // width\n    group_size = min(grid_m - group_idx * GROUP_M, GROUP_M)\n    block_m_idx = group_idx * GROUP_M + program_idx % group_size\n    block_n_idx = program_idx % width // group_size\n    m_offs_untagged = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)\n    n_offs_untagged = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)\n    m_offs = tl.max_contiguous(tl.multiple_of(m_offs_untagged % M, BLOCK_M),\n        BLOCK_M)\n    n_offs = tl.max_contiguous(tl.multiple_of(n_offs_untagged % N, BLOCK_N),\n        BLOCK_N)\n    k_range_offs = tl.arange(0, BLOCK_K)\n    A = A + (m_offs[:, None] * a_m_stride + k_range_offs[None, :] * a_k_stride)\n    B = B + (k_range_offs[:, None] * b_k_stride + n_offs[None, :] * b_n_stride)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    if HAS_BIAS:\n        bias = tl.load(bias + n_offs, mask=n_offs < N, other=0.0)\n        acc += bias[None, :]\n    for k in range(K, 0, -BLOCK_K):\n        if K_LOAD_MASK_NEEDED:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=k_range_offs[None, :] < k, other=0.0)\n            b = tl.load(B, mask=k_range_offs[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * a_k_stride\n        B += BLOCK_K * b_k_stride\n    if ACTIVATION:\n        acc = silu(acc)\n    acc = acc\n    C = C + m_offs[:, None] * output_m_stride + n_offs[None, :\n        ] * output_n_stride\n    c_ptr_mask = (m_offs < M)[:, None] & (n_offs < N)[None, :]\n    tl.store(C, acc, mask=c_ptr_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "bf57d5c9-3bba-4ba4-9fad-1b3a165a6cc4"
  },
  {
    "input": "@autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_K': 64,\n    'BLOCK_SIZE_N': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 256, 'BLOCK_SIZE_N':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 128, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_K': 32, 'BLOCK_SIZE_N': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_K': 64, 'BLOCK_SIZE_N': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'K'],\n    nearest_power_of_two=True)\n@triton.jit\ndef trans_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr,\n    g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn,\n    stride_cm, stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, N) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, K) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_k = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_bk[:, None] // infearure_per_bits * stride_bk + \n        offs_n[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_bk\n    g_idx = tl.load(g_ptrs)\n    scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n    zeros_ptrs = zeros_ptr + offs_n[None, :] // infearure_per_bits + g_idx[\n        :, None] * stride_zeros\n    shifter = offs_bk % infearure_per_bits * bits\n    zeros_shifter = offs_n % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for k in range(0, num_pid_n):\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        b = tl.trans(b)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_N\n        b_ptrs += BLOCK_SIZE_N\n        scales_ptrs += BLOCK_SIZE_N\n        zeros_ptrs += BLOCK_SIZE_N // infearure_per_bits\n    c = accumulator\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8b6512cb-cdc6-44d8-bd04-268dc120d362"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_linear_attn_fwd_kernel(q, k, v, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "170d3849-65ed-4ab7-96d7-a670b04fa97e"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_retention_fwd_kernel(q, k, v, o, h0, ht, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr', H: 'tl.constexpr',\n    T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b, d_o, d_h = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b\n        ), tl.math.exp2((BT - o_i - 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            if i == NT - 1 and T % BT != 0:\n                d_b = tl.math.exp2(T % BT * b_b)\n                d_h = tl.math.exp2((T % BT - o_i - 1) * b_b)\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "390cecbd-ecb2-463d-a447-efcae810e1ff"
  },
  {
    "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "085cca96-ce88-4c36-a069-252c926fd800"
  },
  {
    "input": "@triton.jit\ndef _rotary_kernel(Q, K, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_kbs, stride_kh, stride_kd, stride_cosbs, stride_cosd,\n    stride_sinbs, stride_sind, max_total_len, HEAD_Q, HEAD_K, rot_dim,\n    head_dim, BLOCK_HEAD: 'tl.constexpr', BLOCK_SEQ: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr'):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL)\n    dim_range1 = rot_dim + tl.arange(0, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range0[None, None, :] < rot_dim), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range1[None, None, :] < head_dim), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range0[None, None, :] < rot_dim))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_Q) & (\n        dim_range1[None, None, :] < head_dim))\n    off_k0 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range0[None, None, :] * stride_kd\n    off_k1 = cur_seq_range[:, None, None] * stride_kbs + cur_head_range[\n        None, :, None] * stride_kh + dim_range1[None, None, :] * stride_kd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    k0 = tl.load(K + off_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range0[None, None, :] < rot_dim), other=0.0)\n    k1 = tl.load(K + off_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range1[None, None, :] < head_dim), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out_k0 = k0 * cos - k1 * sin\n    out_k1 = k0 * sin + k1 * cos\n    tl.store(K + off_k0, out_k0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range0[None, None, :] < rot_dim))\n    tl.store(K + off_k1, out_k1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < HEAD_K) & (\n        dim_range1[None, None, :] < head_dim))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "93463baa-bc44-42cc-8d4d-ff4747f33c28"
  },
  {
    "input": "@triton.jit\ndef __scan_col_4_compute(NON_ZERO_PIXELS, stride_nzp_n, stride_nzp_d,\n    PIXEL_INDICES, stride_pixel_n, stride_pixel_m, V_STARTS, stride_vs_tdst,\n    stride_vs_tm, V_ENDS, stride_ve_tdst, stride_ve_tm, COL_INDICES,\n    stride_col_n, stride_col_z, N, M, H, T_M, TARGET_WIDTH_MAX,\n    MAX_INTER_PADDED: 'tl.constexpr', MAX_INTERP, NZR_N, NZR_D,\n    BLOCK_N_ZERO: 'tl.constexpr'):\n    pid_nzp = tl.program_id(0)\n    i_nzp_n = pid_nzp * BLOCK_N_ZERO + tl.arange(0, BLOCK_N_ZERO)\n    mask_i_nzp = i_nzp_n < NZR_N\n    is_batch = tl.load(NON_ZERO_PIXELS + i_nzp_n * stride_nzp_n + 0 *\n        stride_nzp_d, mask=mask_i_nzp)\n    is_col = tl.load(NON_ZERO_PIXELS + i_nzp_n * stride_nzp_n + 1 *\n        stride_nzp_d, mask=mask_i_nzp)\n    idx_tdst = is_col // (H * T_M)\n    idx_h = is_col % (H * T_M) // T_M\n    idx_tm = is_col % T_M\n    v_start = tl.load(V_STARTS + idx_tdst * stride_vs_tdst + idx_tm *\n        stride_vs_tm, mask=mask_i_nzp)\n    v_end = tl.load(V_ENDS + idx_tdst * stride_ve_tdst + idx_tm *\n        stride_ve_tm, mask=mask_i_nzp)\n    col_start = tl.load(PIXEL_INDICES + is_batch * stride_pixel_n + (is_col -\n        1) * stride_pixel_m, mask=(is_col - 1 >= 0 and is_col < M) and\n        mask_i_nzp, other=0)\n    col_end = tl.load(PIXEL_INDICES + is_batch * stride_pixel_n + is_col *\n        stride_pixel_m, mask=(is_col >= 0 and is_col < M) and mask_i_nzp)\n    col_len = col_end - col_start\n    range_start = v_start + idx_h * TARGET_WIDTH_MAX\n    range_end = v_end + idx_h * TARGET_WIDTH_MAX\n    tl.store(COL_INDICES + is_batch[:, None] * stride_col_n + (tl.arange(0,\n        MAX_INTER_PADDED)[None, :] + col_start[:, None]) * stride_col_z, \n        range_end[:, None] - tl.arange(0, MAX_INTER_PADDED)[None, :] * ((\n        range_end[:, None] - range_start[:, None]) / col_len[:, None]) - 1,\n        mask=(tl.arange(0, MAX_INTER_PADDED)[None, :] < col_len[:, None] and\n        tl.arange(0, MAX_INTER_PADDED)[None, :] < MAX_INTERP) and\n        mask_i_nzp[:, None])\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "d2c3ec19-1b96-4797-809d-e5a6d544bbc7"
  },
  {
    "input": "@triton.jit\ndef fused_moe_kernel(a_ptr, b_ptr, c_ptr, topk_weights_ptr,\n    sorted_token_ids_ptr, expert_ids_ptr, num_tokens_post_padded_ptr, N, K,\n    EM, num_valid_tokens, stride_am, stride_ak, stride_be, stride_bk,\n    stride_bn, stride_cm, stride_cn, stride_weight, stride_token_id,\n    block_m: 'tl.constexpr', block_n: 'tl.constexpr', block_k:\n    'tl.constexpr', MUL_ROUTED_WEIGHT: 'tl.constexpr', top_k:\n    'tl.constexpr', compute_type: 'tl.constexpr'):\n    \"\"\"\n    Implements the fused computation for a Mixture of Experts (MOE) using token and expert matrices.\n\n    Key Parameters:\n    - A: The input tensor representing tokens with shape (*, K), where '*' can be any shape representing batches and K is the feature dimension of each token.\n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is the number of experts, K is the input feature dimension, and N is the output feature dimension.\n    - C: The output cache tensor with shape (M, topk, N), where M is the total number of tokens post padding, topk is the number of times each token is repeated,\n        and N is the output feature dimension.\n    - sorted_token_ids: A tensor containing the sorted indices of tokens, repeated topk times and arranged by the expert index they are assigned to.\n    - expert_ids: A tensor containing the indices of the expert for each block. It determines which expert matrix from B should be used for each block in A.\n    This kernel performs the multiplication of a token by its corresponding expert matrix as determined by `expert_ids`. The sorting of `sorted_token_ids`\n    by expert index and padding ensures divisibility by block_m, which is necessary to maintain consistency in block matrix multiplication across different blocks processed by the same expert.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_m, pid_n = col_major(pid, EM, N, block_m, block_n)\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * block_m >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * block_m + tl.arange(0, block_m)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * block_n + tl.arange(0, block_n)) % N\n    offs_k = tl.arange(0, block_k)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am + offs_k[\n        None, :] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    accumulator = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, block_k)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * block_k), other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * block_k, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += block_k * stride_ak\n        b_ptrs += block_k * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token * stride_weight,\n            mask=token_mask, other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    accumulator = accumulator\n    offs_cn = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "7d348424-4e80-42ab-9e78-08eeede7bc6b"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "cd7b86b4-12ab-420d-98ed-07b900cf0c67"
  },
  {
    "input": "@triton.jit\ndef _voxel_grid_sample(feature_grid, feature_grid_size, batch_index, ix_in,\n    iy_in, iz_in, C: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    out_val = tl.zeros((BLOCK_SIZE, C), dtype=tl.float32)\n    feature_grid_offs = tl.zeros((1,), dtype=tl.int32)\n    for gi in range(NUM_GRIDS):\n        offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n        ID = tl.load(feature_grid_size + offs + 1)\n        IH = tl.load(feature_grid_size + offs + 2)\n        IW = tl.load(feature_grid_size + offs + 3)\n        ID_ = tl.sum(ID, axis=0) // BLOCK_SIZE\n        IH_ = tl.sum(IH, axis=0) // BLOCK_SIZE\n        IW_ = tl.sum(IW, axis=0) // BLOCK_SIZE\n        voxel_grid = (ID_ - 1) * (IH_ - 1) * (IW_ - 1)\n        if voxel_grid > 0:\n            sampled, grid_numel = _voxel_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, ix_in,\n                iy_in, iz_in, ID, IH, IW, C, BLOCK_SIZE,\n                mask_out_of_bounds_samples)\n        elif ID_ == 1:\n            sampled, grid_numel = _plane_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, ix_in,\n                iy_in, IH, IW, C, BLOCK_SIZE, mask_out_of_bounds_samples)\n        elif IH_ == 1:\n            sampled, grid_numel = _plane_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, ix_in,\n                iz_in, ID, IW, C, BLOCK_SIZE, mask_out_of_bounds_samples)\n        else:\n            sampled, grid_numel = _plane_grid_sample_one(gi, feature_grid +\n                feature_grid_offs, feature_grid_size, batch_index, iy_in,\n                iz_in, ID, IH, C, BLOCK_SIZE, mask_out_of_bounds_samples)\n        out_val += sampled\n        feature_grid_offs += grid_numel\n    if mask_out_of_bounds_samples:\n        in_bounds_mask = is_in_bounds(ix_in, iy_in, iz_in, C, BLOCK_SIZE)\n        out_val *= in_bounds_mask\n    return out_val\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "00ba1db1-c3ef-4377-9b5f-0ae7b4d74d4c"
  },
  {
    "input": "@triton.jit\ndef _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n    col_offsets, n_cols, is_bf16: 'tl.constexpr'):\n    output_row = tl.load(output_ptrs, mask=col_offsets < n_cols, other=float(0)\n        )\n    d_output_row = tl.load(d_output_ptrs, mask=col_offsets < n_cols, other=\n        float(0))\n    if is_bf16:\n        output_row = output_row\n        d_output_row = d_output_row\n    row_sum = tl.sum(output_row * d_output_row, axis=0)\n    d_softmax_output = (d_output_row - row_sum) * output_row\n    tl.store(d_input_ptrs, d_softmax_output, mask=col_offsets < n_cols)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "6eb7cde7-7ca1-403b-a40d-956a99823f7d"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_flex(x, y, x_layout: 'tl.constexpr', y_layout:\n    'tl.constexpr', operation: 'tl.constexpr', onebyone: 'tl.constexpr', BC:\n    'tl.constexpr', BH: 'tl.constexpr', BW: 'tl.constexpr', DC:\n    'tl.constexpr', DH: 'tl.constexpr', DW: 'tl.constexpr', NH:\n    'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    HWRoute0 = i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    HWRoute1 = i_w * BW * DH + tl.arange(0, BW)[None, :\n        ] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    HWRoute2 = (NH - i_h - 1) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]\n        ) * DW + (NW - i_w - 1) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (\n        DH - NH * BH) * DW + (DW - NW * BW)\n    HWRoute3 = (NW - i_w - 1) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]\n        ) * DH + (NH - i_h - 1) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (\n        DH - NH * BH) + (DW - NW * BW) * DH\n    _tmp1 = DC * DH * DW\n    y_ptr_base = y + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if y_layout == 0\n         else i_c * BC)\n    if y_layout == 0:\n        p_y1 = y_ptr_base + HWRoute0\n        p_y2 = y_ptr_base + _tmp1 + HWRoute1\n        p_y3 = y_ptr_base + 2 * _tmp1 + HWRoute2\n        p_y4 = y_ptr_base + 3 * _tmp1 + HWRoute3\n    else:\n        p_y1 = y_ptr_base + HWRoute0 * 4 * DC\n        p_y2 = y_ptr_base + DC + HWRoute1 * 4 * DC\n        p_y3 = y_ptr_base + 2 * DC + HWRoute2 * 4 * DC\n        p_y4 = y_ptr_base + 3 * DC + HWRoute3 * 4 * DC\n    if onebyone == 0:\n        x_ptr_base = x + i_b * _tmp1 + (i_c * BC * DH * DW if x_layout == 0\n             else i_c * BC)\n        if x_layout == 0:\n            p_x = x_ptr_base + HWRoute0\n        else:\n            p_x = x_ptr_base + HWRoute0 * DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _x = tl.load(p_x + _idx_x, mask=_mask_hw)\n                tl.store(p_y1 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, _x, mask=_mask_hw)\n        elif operation == 1:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _y1 = tl.load(p_y1 + _idx_y, mask=_mask_hw)\n                _y2 = tl.load(p_y2 + _idx_y, mask=_mask_hw)\n                _y3 = tl.load(p_y3 + _idx_y, mask=_mask_hw)\n                _y4 = tl.load(p_y4 + _idx_y, mask=_mask_hw)\n                tl.store(p_x + _idx_x, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n    else:\n        x_ptr_base = x + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if x_layout ==\n            0 else i_c * BC)\n        if x_layout == 0:\n            p_x1 = x_ptr_base + HWRoute0\n            p_x2 = p_x1 + _tmp1\n            p_x3 = p_x2 + _tmp1\n            p_x4 = p_x3 + _tmp1\n        else:\n            p_x1 = x_ptr_base + HWRoute0 * 4 * DC\n            p_x2 = p_x1 + DC\n            p_x3 = p_x2 + DC\n            p_x4 = p_x3 + DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_y1 + _idx_y, tl.load(p_x1 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, tl.load(p_x2 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, tl.load(p_x3 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, tl.load(p_x4 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n        else:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_x1 + _idx_x, tl.load(p_y1 + _idx_y), mask=_mask_hw)\n                tl.store(p_x2 + _idx_x, tl.load(p_y2 + _idx_y), mask=_mask_hw)\n                tl.store(p_x3 + _idx_x, tl.load(p_y3 + _idx_y), mask=_mask_hw)\n                tl.store(p_x4 + _idx_x, tl.load(p_y4 + _idx_y), mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "ae4a2336-b1f3-4ef3-94fb-e0640a08a204"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_reversed_cumsum_vector_kernel(s, z, offsets, T:\n    'tl.constexpr', H: 'tl.constexpr', S: 'tl.constexpr', BT:\n    'tl.constexpr', BS: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr',\n    USE_OFFSETS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b), tl.load(offsets + i_b + 1)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n        else:\n            p_s = tl.make_block_ptr(s + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "a3e23734-44c9-44a3-8cc6-cbfb694ddf65"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "832671d2-c65f-4eeb-b7c1-0b48f29ad8e9"
  },
  {
    "input": "@triton.jit\ndef update_position(a, b, position_a, position_b):\n    tmp = a - b\n    return tl.where(tmp > 0, position_a, position_b)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "9b597b5f-a2a7-4f93-ba3d-7a667da3d7eb"
  },
  {
    "input": "@triton.jit\ndef sixth_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_7 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_8 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    g_9 = tl.load(sph_grad_ptr + output_row_offset + 9, mask=\n        output_row_offset + 9 < output_numel)\n    g_10 = tl.load(sph_grad_ptr + output_row_offset + 10, mask=\n        output_row_offset + 10 < output_numel)\n    g_11 = tl.load(sph_grad_ptr + output_row_offset + 11, mask=\n        output_row_offset + 11 < output_numel)\n    g_12 = tl.load(sph_grad_ptr + output_row_offset + 12, mask=\n        output_row_offset + 12 < output_numel)\n    CONST000 = 2.0\n    CONST002 = 4.0\n    CONST003 = 3.0\n    CONST004 = 6.53117523880657\n    CONST006 = 8.94318001328386\n    CONST007 = 8.38944649544891\n    CONST008 = 10.3266947761614\n    CONST009 = 9.79676285820985\n    CONST013 = 16.3279380970164\n    CONST014 = 17.8863600265677\n    CONST015 = 16.5227116418583\n    CONST016 = 20.6533895523229\n    CONST017 = 20.2812259244849\n    CONST018 = 21.6333076527839\n    CONST020 = 17.8863600265677\n    CONST022 = 29.3902885746295\n    CONST024 = 35.7727200531355\n    CONST026 = 40.5624518489699\n    CONST028 = 41.9472324772445\n    CONST029 = 48.9838142910493\n    CONST030 = 51.6334738808072\n    CONST035 = 71.5454401062709\n    CONST037 = 81.1249036979398\n    CONST039 = 82.6135582092915\n    CONST040 = -3.26558761940328\n    CONST042 = 117.561154298518\n    CONST046 = 208.99760764181\n    CONST048 = -251.683394863467\n    CONST049 = -214.636320318813\n    CONST050 = -214.636320318813\n    CONST051 = 16.5227116418583\n    CONST052 = -167.788929908978\n    CONST053 = -156.748205731358\n    CONST054 = -145.309475774982\n    CONST055 = -123.920337313937\n    CONST056 = -117.561154298518\n    CONST057 = 3.26558761940328\n    CONST058 = -108.16653826392\n    CONST059 = -107.318160159406\n    CONST060 = -104.498803820905\n    CONST061 = -104.498803820905\n    CONST062 = -83.8944649544891\n    CONST063 = -82.6135582092915\n    CONST064 = -78.3741028656788\n    CONST065 = -72.6547378874909\n    CONST066 = -71.5454401062709\n    CONST067 = -58.7805771492591\n    CONST068 = -54.0832691319598\n    CONST069 = -52.2494019104525\n    CONST070 = -52.2494019104525\n    CONST071 = -48.9838142910492\n    CONST072 = -41.3067791046458\n    CONST073 = -39.1870514328394\n    CONST074 = -35.7727200531355\n    CONST075 = -29.3902885746295\n    CONST076 = -27.0416345659799\n    CONST077 = -26.1247009552263\n    CONST078 = -26.1247009552263\n    CONST079 = -19.5935257164197\n    CONST080 = -14.5309475774982\n    CONST081 = -13.52081728299\n    CONST082 = -10.7318160159406\n    CONST083 = -9.79676285820985\n    CONST084 = -7.15454401062709\n    CONST085 = -6.76040864149498\n    CONST086 = -3.38020432074749\n    CONST087 = -1.63279380970164\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR05 = VAR07 * VAR08\n    VAR06 = VAR08 * VAR08\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR14 = VAR16 * VAR17\n    VAR15 = VAR17 * VAR17\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR23 = VAR25 * VAR26\n    VAR24 = VAR26 * VAR26\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += g_0 * (CONST054 * VAR08 * VAR25 - CONST065 * VAR06 * z - \n        CONST080 * VAR23) + g_1 * y * (CONST028 * VAR06 + CONST028 * VAR24 +\n        CONST048 * VAR08 * VAR26) + g_10 * (CONST000 * x * (CONST006 *\n        VAR24 + CONST059 * VAR17 * VAR26) + CONST002 * VAR07 * (CONST006 *\n        VAR26 + CONST014 * VAR17) + CONST082 * VAR05) + g_11 * y * (-\n        CONST052 * VAR07 * z + CONST052 * VAR25 * x) + g_12 * (-CONST054 *\n        VAR07 * VAR26 + CONST065 * VAR24 * x + CONST080 * VAR05) + g_2 * (-\n        CONST074 * VAR06 * z + CONST084 * VAR23 + VAR17 * (CONST049 * VAR08 *\n        z - CONST066 * VAR25)) + g_3 * (VAR16 * (CONST064 * VAR08 - \n        CONST064 * VAR26) + y * (CONST029 * VAR06 + CONST067 * VAR08 *\n        VAR26 + CONST075 * VAR24)) + g_4 * (CONST003 * VAR08 * (CONST004 *\n        VAR25 + CONST069 * VAR17 * z) + CONST013 * VAR06 * z - CONST040 *\n        VAR23 - CONST070 * VAR15 * z + CONST070 * VAR17 * VAR25) + g_5 * (\n        CONST003 * VAR08 * (CONST016 * VAR26 * y + CONST072 * VAR16) + \n        CONST008 * VAR24 * y + CONST015 * VAR14 + CONST030 * VAR06 * y + \n        CONST072 * VAR16 * VAR26) + g_6 * (CONST000 * x * (CONST026 * VAR17 *\n        VAR26 + CONST076 * VAR15 + CONST086 * VAR24) + CONST002 * VAR07 * (\n        CONST017 * VAR17 + CONST086 * VAR26) + CONST085 * VAR05) + g_7 * (-\n        CONST072 * VAR25 * x * y + z * (CONST063 * VAR16 * x - CONST072 *\n        VAR07 * y)) + g_8 * (CONST000 * x * (CONST077 * VAR15 - CONST087 *\n        VAR24) + CONST002 * VAR07 * (-CONST077 * VAR17 + CONST087 * VAR26) +\n        CONST083 * VAR05) + g_9 * (CONST053 * VAR16 * x * z + y * (CONST042 *\n        VAR07 * z - CONST073 * VAR25 * x))\n    g_y += CONST000 * g_2 * y * (CONST066 * VAR07 * z - CONST066 * VAR25 * x\n        ) + g_1 * (CONST007 * VAR05 + CONST028 * VAR24 * x + CONST062 *\n        VAR07 * VAR26) + g_10 * (CONST024 * VAR06 * y + CONST050 * VAR08 *\n        VAR26 * y - CONST074 * VAR24 * y) + g_11 * (CONST007 * VAR23 + \n        CONST028 * VAR06 * z + CONST062 * VAR08 * VAR25) + g_3 * (CONST003 *\n        VAR17 * (-CONST064 * VAR26 * x + CONST078 * VAR07) + CONST009 *\n        VAR05 + CONST075 * VAR24 * x + CONST079 * VAR07 * VAR26) + g_4 * (\n        CONST061 * VAR07 * y * z + x * (CONST046 * VAR16 * z + CONST060 *\n        VAR25 * y)) + g_5 * (CONST008 * VAR05 + VAR07 * (CONST016 * VAR26 +\n        CONST055 * VAR17) + x * (CONST008 * VAR24 + CONST055 * VAR17 *\n        VAR26 - CONST063 * VAR15)) + g_6 * (CONST018 * VAR14 + CONST026 *\n        VAR06 * y + CONST026 * VAR24 * y + CONST058 * VAR16 * VAR26 + VAR08 *\n        (CONST037 * VAR26 * y + CONST058 * VAR16)) + g_7 * (CONST008 *\n        VAR23 + VAR25 * (CONST016 * VAR08 + CONST055 * VAR17) + z * (\n        CONST008 * VAR06 + CONST039 * VAR15 + CONST055 * VAR08 * VAR17)\n        ) + g_8 * (CONST060 * VAR08 * VAR16 - CONST060 * VAR16 * VAR26 + \n        CONST069 * VAR24 * y - CONST070 * VAR06 * y) + g_9 * (CONST003 *\n        VAR17 * (CONST064 * VAR08 * z - CONST077 * VAR25) + CONST022 *\n        VAR06 * z - CONST079 * VAR08 * VAR25 + CONST083 * VAR23)\n    g_z += g_0 * (CONST054 * VAR07 * VAR26 - CONST065 * VAR24 * x - \n        CONST080 * VAR05) + g_1 * y * (CONST052 * VAR07 * z - CONST052 *\n        VAR25 * x) + g_10 * (CONST020 * VAR06 * z + CONST035 * VAR17 *\n        VAR25 + CONST082 * VAR23 + VAR08 * (CONST050 * VAR17 * z - CONST074 *\n        VAR25)) + g_11 * y * (CONST028 * VAR06 + CONST028 * VAR24 + \n        CONST048 * VAR08 * VAR26) + g_12 * (CONST054 * VAR08 * VAR25 - \n        CONST065 * VAR06 * z - CONST080 * VAR23) + g_2 * (CONST074 * VAR24 *\n        x - CONST084 * VAR05 + VAR17 * (-CONST049 * VAR26 * x + CONST066 *\n        VAR07)) + g_3 * (-CONST053 * VAR16 * x * z + y * (CONST056 * VAR25 *\n        x + CONST073 * VAR07 * z)) + g_4 * (CONST057 * VAR05 + VAR07 * (\n        CONST069 * VAR17 - CONST079 * VAR26) + x * (CONST013 * VAR24 + \n        CONST053 * VAR17 * VAR26 - CONST070 * VAR15)) + g_5 * (-CONST072 *\n        VAR07 * y * z + x * (CONST063 * VAR16 * z - CONST072 * VAR25 * y)\n        ) + g_6 * (CONST037 * VAR17 * VAR25 + CONST068 * VAR15 * z + \n        CONST085 * VAR06 * z + CONST085 * VAR23 + VAR08 * (CONST037 * VAR17 *\n        z + CONST081 * VAR25)) + g_7 * (CONST003 * VAR26 * (CONST016 *\n        VAR08 * y + CONST072 * VAR16) + CONST008 * VAR06 * y + CONST030 *\n        VAR24 * y + CONST051 * VAR14 + CONST072 * VAR08 * VAR16) + g_8 * (\n        CONST004 * VAR08 * VAR25 + CONST040 * VAR06 * z + CONST061 * VAR17 *\n        VAR25 - CONST070 * VAR15 * z - CONST083 * VAR23) + g_9 * (VAR16 * (\n        CONST064 * VAR08 - CONST064 * VAR26) + y * (CONST022 * VAR06 - \n        CONST067 * VAR08 * VAR26 + CONST071 * VAR24))\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "23adb7fa-e9f6-4e10-98e0-259915f305c5"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128, 'NUM_WARPS': 4}\n    ), triton.Config({'BLOCK_SIZE': 256, 'NUM_WARPS': 8}), triton.Config({\n    'BLOCK_SIZE': 512, 'NUM_WARPS': 16}), triton.Config({'BLOCK_SIZE': 1024,\n    'NUM_WARPS': 16}), triton.Config({'BLOCK_SIZE': 2048, 'NUM_WARPS': 32}),\n    triton.Config({'BLOCK_SIZE': 4096, 'NUM_WARPS': 32}), triton.Config({\n    'BLOCK_SIZE': 8192, 'NUM_WARPS': 48})], key=['n_cols'])\n@triton.jit\ndef _rms_layernorm_backward(dY, dY_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, dX, dX_row_stride, dW, n_cols, eps,\n    BLOCK_SIZE: 'tl.constexpr', NUM_WARPS: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pids = tl.num_programs(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY_ptr = dY + pid * dY_row_stride + col_offsets\n    X_ptr = X + pid * X_row_stride + col_offsets\n    dX_ptr = dX + pid * dX_row_stride + col_offsets\n    dY_row = tl.load(dY_ptr, mask=mask, other=0)\n    X_row = tl.load(X_ptr, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    rms = tl.load(r + pid)\n    X_norm = X_row * rms\n    dY_W = dY_row * W_row\n    sum_dY_X = tl.sum(dY_W * X_norm, axis=0)\n    dX = rms * (dY_W - X_norm * (sum_dY_X / n_cols))\n    dW_row = dY_row * X_norm\n    tl.atomic_add(dW + col_offsets, dW_row, mask=mask)\n    tl.store(dX_ptr, dX, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "3d7bc8f2-f3a8-499d-ad70-accdb6b8d7c3"
  },
  {
    "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(4096, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_2d_kernel(output_ptr, input_ptr, bs, size_inp_0, batch_stride_inp,\n    stride_inp_0, batch_stride_out, stride_out_0, BATCH_STRIDE_INP_IS_1:\n    'tl.constexpr', STRIDE_INP_0_IS_1: 'tl.constexpr',\n    BATCH_STRIDE_OUT_IS_1: 'tl.constexpr', STRIDE_OUT_0_IS_1:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    pid_m = pid\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + rm * (1 if STRIDE_INP_0_IS_1 else stride_inp_0)\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + rm * (1 if STRIDE_OUT_0_IS_1 else stride_out_0)\n    mask = rm < size_inp_0\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "a23bad93-3963-4962-95f5-e6a23f613b94"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "ed29e407-e231-46f6-b7d1-0835303eae41"
  },
  {
    "input": "@triton.autotune(configs=_config_grouping(), key=['K'])\n@triton.heuristics({'NO_K_MASK': lambda args: args['K'] % args['BLOCK_K'] == 0}\n    )\n@triton.jit\ndef _group(src_ptr, stride_sn, stride_sk, has_coeff: 'tl.constexpr',\n    coeff_ptr, FAN_OUT: 'tl.constexpr', tgt_ptr, stride_tn, stride_ti,\n    grouped_idx_ptr, N, K: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K:\n    'tl.constexpr', NO_K_MASK: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    N_block_id = pid\n    N_blk = N_block_id * BLOCK_N + tl.arange(0, BLOCK_N)\n    N_mask = N_blk < N\n    N_blk = tl.max_contiguous(tl.multiple_of(N_blk % N, BLOCK_N), BLOCK_N)\n    N_idx = tl.load(grouped_idx_ptr + N_blk, mask=N_mask, other=0)\n    K_blk = tl.arange(0, BLOCK_K)\n    src_blk_ptrs = src_ptr + (N_idx // FAN_OUT)[:, None] * stride_sn + K_blk[\n        None, :] * stride_sk\n    tgt_blk_ptrs = tgt_ptr + N_blk[:, None] * stride_tn + K_blk[None, :\n        ] * stride_ti\n    if has_coeff:\n        c = tl.load(coeff_ptr + N_idx, mask=N_mask)[:, None]\n    iters = tl.cdiv(K, BLOCK_K)\n    for i in range(0, iters):\n        if NO_K_MASK or i < iters - 1:\n            block = tl.load(src_blk_ptrs, mask=N_mask[:, None])\n            if has_coeff:\n                block *= c\n            tl.store(tgt_blk_ptrs, block, mask=N_mask[:, None])\n        else:\n            K_mask = i * BLOCK_K + K_blk < K\n            mask = N_mask[:, None] & K_mask[None, :]\n            block = tl.load(src_blk_ptrs, mask=mask)\n            if has_coeff:\n                block *= c\n            tl.store(tgt_blk_ptrs, block, mask=mask)\n        src_blk_ptrs += BLOCK_K * stride_sk\n        tgt_blk_ptrs += BLOCK_K * stride_ti\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "b3880a20-4b89-4bed-a2bb-b10243cb4f9d"
  },
  {
    "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd,\n    stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row,\n    stride_dz_row, stride_dw_row, stride_db_row, M, N, eps,\n    rows_per_program, NORM_BEFORE_GATE: 'tl.constexpr', IS_RMS_NORM:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    RECOMPUTE_OUTPUT: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        elif RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask\n        =mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db,\n            mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "bc316ccb-7e1d-444b-8d40-d3523b0b68f7"
  },
  {
    "input": "@triton.jit\ndef _bwd_kv_kernel(Q1, K1, Q2, K2, V, sm_scale, DO, DK1, DK2, DV, L, D,\n    stride_q1z, stride_q1h, stride_q1m, stride_q1k, stride_k1z, stride_k1h,\n    stride_k1n, stride_k1k, stride_q2z, stride_q2h, stride_q2m, stride_q2k,\n    stride_k2z, stride_k2h, stride_k2n, stride_k2k, stride_vz, stride_vh,\n    stride_vn, stride_vk, stride_doz, stride_doh, stride_dom, stride_dok,\n    stride_dk1z, stride_dk1h, stride_dk1n, stride_dk1k, stride_dk2z,\n    stride_dk2h, stride_dk2n, stride_dk2k, stride_dvz, stride_dvh,\n    stride_dvn, stride_dvk, Z, H, M, N, P_SEQ, w: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    CAUSAL: 'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N:\n    'tl.constexpr'):\n    input_dtype = Q1.dtype.element_ty\n    start_n = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q1 += off_z * stride_q1z + off_h * stride_q1h\n    Q2 += off_z * stride_q2z + off_h * stride_q2h\n    K1 += off_z * stride_k1z + off_h * stride_k1h\n    K2 += off_z * stride_k2z + off_h * stride_k2h\n    V += off_z * stride_vz + off_h * stride_vh\n    DO += off_z * stride_doz + off_h * stride_doh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    DK1 += off_z * stride_dk1z + off_h * stride_dk1h\n    DK2 += off_z * stride_dk2z + off_h * stride_dk2h\n    DV += off_z * stride_dvz + off_h * stride_dvh\n    if CAUSAL:\n        lo = tl.maximum(start_n * BLOCK_N - P_SEQ, 0)\n        lo = lo // BLOCK_M * BLOCK_M\n    else:\n        lo = 0\n    offs_m_init = lo + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q1_ptrs = Q1 + (offs_m_init[:, None] * stride_q1m + offs_k[None, :] *\n        stride_q1k)\n    q2_ptrs = Q2 + (offs_m_init[:, None] * stride_q2m + offs_k[None, :] *\n        stride_q2k)\n    k1_ptrs = K1 + (offs_k[:, None] * stride_k1k + offs_n[None, :] * stride_k1n\n        )\n    k2_ptrs = K2 + (offs_k[:, None] * stride_k2k + offs_n[None, :] * stride_k2n\n        )\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n    do_ptrs = DO + (offs_m_init[:, None] * stride_dom + offs_k[None, :] *\n        stride_dok)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_k[None, :] * stride_dvk\n        )\n    dk1_ptrs = DK1 + (offs_n[:, None] * stride_dk1n + offs_k[None, :] *\n        stride_dk1k)\n    dk2_ptrs = DK2 + (offs_n[:, None] * stride_dk2n + offs_k[None, :] *\n        stride_dk2k)\n    if DIVISIBLE_N:\n        k1 = tl.load(k1_ptrs)\n        k2 = tl.load(k2_ptrs)\n        v = tl.load(v_ptrs)\n    else:\n        mask_n = offs_n < N\n        k1 = tl.load(k1_ptrs, mask=mask_n[None, :])\n        k2 = tl.load(k2_ptrs, mask=mask_n[None, :])\n        v = tl.load(v_ptrs, mask=mask_n[:, None])\n    dk1 = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dk2 = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    for start_m in range(lo, M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m = start_m + offs_m_base\n        if DIVISIBLE_M:\n            q1 = tl.load(q1_ptrs)\n            q2 = tl.load(q2_ptrs)\n            do = tl.load(do_ptrs)\n            delta = tl.load(D + offs_m)\n            l = tl.load(L + offs_m)\n        else:\n            mask_m = offs_m < M\n            q1 = tl.load(q1_ptrs, mask=mask_m[:, None])\n            q2 = tl.load(q2_ptrs, mask=mask_m[:, None])\n            do = tl.load(do_ptrs, mask=mask_m[:, None])\n            delta = tl.load(D + offs_m, mask=mask_m)\n            l = tl.load(L + offs_m, mask=mask_m)\n        piecewise_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :] + w\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.where(piecewise_mask, tl.dot(q2, k2), tl.dot(q1, k1))\n        p = tl.math.exp2(s * qk_scale - l[:, None] * log2e)\n        if not DIVISIBLE_M:\n            valid_mask = mask_m[:, None]\n            p = tl.where(valid_mask, p, 0.0)\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(causal_mask, p, 0.0)\n        dv += tl.dot(tl.trans(p), do)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_M:\n            ds = tl.where(valid_mask, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        ds2 = tl.where(piecewise_mask, ds, 0.0)\n        ds1 = tl.where(piecewise_mask, 0.0, ds)\n        dk1 += tl.dot(tl.trans(ds1), q1)\n        dk2 += tl.dot(tl.trans(ds2), q2)\n        q1_ptrs += BLOCK_M * stride_q1m\n        q2_ptrs += BLOCK_M * stride_q2m\n        do_ptrs += BLOCK_M * stride_dom\n    dk1 *= sm_scale\n    dk2 *= sm_scale\n    if DIVISIBLE_N:\n        tl.store(dk1_ptrs, dk1)\n        tl.store(dk2_ptrs, dk2)\n        tl.store(dv_ptrs, dv)\n    else:\n        tl.store(dk1_ptrs, dk1, mask=mask_n[:, None])\n        tl.store(dk2_ptrs, dk2, mask=mask_n[:, None])\n        tl.store(dv_ptrs, dv, mask=mask_n[:, None])\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "b647e6fd-8792-4cc9-a7f8-a28ea42fc493"
  },
  {
    "input": "@triton.jit\ndef glu(input1, input2, param, act_func: 'tl.constexpr'):\n    \"\"\"\n    Applies the gated linear unit with an arbitrary activation function\n    to the input.\n\n    Args:\n        input1: First half of input to gate.\n            The first half must be of the same shape as the second half.\n        input2: Second half of input to gate.\n            The second half must be of the same shape as the first half.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        param: Parameter in the case of parameterized activation functions.\n\n    Args:\n        Input transformed by the gated linear unit\n        with an arbitrary activation function.\n    \"\"\"\n    return input1 * apply_act_func(input2, None, None, None, param,\n        act_func, False)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "f9fd21b4-aa52-4b59-9036-76d6ce996e12"
  },
  {
    "input": "@triton.jit\ndef _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds + b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "dbc88004-2985-4e94-b37d-8b83d53e553e"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(Q, K, V, sm_scale, qk_scale, Out, DO, DQ, DK,\n    DV, L, D, Q_block_ptr, K_block_ptr, V_block_ptr, DO_block_ptr,\n    DQ_block_ptr, DK_block_ptr, DV_block_ptr, stride_dqa, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, Z, H, N_CTX,\n    off_h, off_z, off_hz, start_n, num_block, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', CAUSAL: 'tl.constexpr', MMA_V3:\n    'tl.constexpr'):\n    if CAUSAL:\n        lo = start_n * BLOCK_M\n    else:\n        lo = 0\n    Q_offset = (off_z * stride_qz + off_h * stride_qh) // stride_qm\n    DQ_offset = off_z * stride_qz + off_h * stride_qh\n    K_offset = (off_z * stride_kz + off_h * stride_kh) // stride_kn\n    V_offset = (off_z * stride_vz + off_h * stride_vh) // stride_vn\n    if SEQUENCE_PARALLEL:\n        DQ_offset += stride_dqa * start_n\n    DQ_offset = DQ_offset // stride_qm\n    Q_block_ptr = tl.advance(Q_block_ptr, (lo + Q_offset, 0))\n    K_block_ptr = tl.advance(K_block_ptr, (start_n * BLOCK_M + K_offset, 0))\n    V_block_ptr = tl.advance(V_block_ptr, (start_n * BLOCK_M + V_offset, 0))\n    DO_block_ptr = tl.advance(DO_block_ptr, (lo + Q_offset, 0))\n    DQ_block_ptr = tl.advance(DQ_block_ptr, (lo + DQ_offset, 0))\n    DK_block_ptr = tl.advance(DK_block_ptr, (start_n * BLOCK_M + K_offset, 0))\n    DV_block_ptr = tl.advance(DV_block_ptr, (start_n * BLOCK_M + V_offset, 0))\n    offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_m = tl.arange(0, BLOCK_N)\n    D_ptrs = D + off_hz * N_CTX\n    l_ptrs = L + off_hz * N_CTX\n    dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    k = tl.load(K_block_ptr)\n    v = tl.load(V_block_ptr)\n    for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n        offs_m_curr = start_m + offs_m\n        q = tl.load(Q_block_ptr)\n        if CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], float(\n                0.0), float('-inf'))\n        else:\n            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        qk *= qk_scale\n        l_i = tl.load(l_ptrs + offs_m_curr)\n        p = tl.math.exp2(qk - l_i[:, None])\n        do = tl.load(DO_block_ptr)\n        dv += tl.dot(tl.trans(p), do)\n        Di = tl.load(D_ptrs + offs_m_curr)\n        dp = tl.dot(do, tl.trans(v))\n        ds = p * (dp - Di[:, None]) * sm_scale\n        dk += tl.dot(tl.trans(ds), q)\n        if not SEQUENCE_PARALLEL:\n            dq = tl.load(DQ_block_ptr)\n            dq += tl.dot(ds, k)\n            tl.store(DQ_block_ptr, dq)\n        elif SEQUENCE_PARALLEL:\n            if MMA_V3:\n                dq = tl.dot(ds, k)\n            else:\n                dq = tl.trans(tl.dot(tl.trans(k), tl.trans(ds)))\n            tl.store(DQ_block_ptr, dq)\n        DQ_block_ptr = tl.advance(DQ_block_ptr, (BLOCK_M, 0))\n        Q_block_ptr = tl.advance(Q_block_ptr, (BLOCK_M, 0))\n        DO_block_ptr = tl.advance(DO_block_ptr, (BLOCK_M, 0))\n    tl.store(DV_block_ptr, dv)\n    tl.store(DK_block_ptr, dk)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "67415ad7-8676-47a2-90de-4d21b41dc81d"
  },
  {
    "input": "@triton.jit\ndef get_sample_randn(pid, step, n_rays, n_steps, BLOCK_SIZE, seed_buffer):\n    offs = pid * BLOCK_SIZE * n_steps + 1\n    i1 = offs + step + tl.arange(0, BLOCK_SIZE) * n_steps\n    i2 = n_rays * n_steps + i1\n    return int_to_randn(i1, i2, seed_buffer)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "9465d055-b687-47f6-9de4-43d76389d18b"
  },
  {
    "input": "@triton.jit\ndef jagged_reduce_sum(seq_offsets, Jagged, Out, D, stride_jn, stride_ob,\n    BLOCK_D: 'tl.constexpr'):\n    \"\"\"\n    Computing Out = Jagged + Dense\n    JaggedA has shape (sum_B(N_i), D), Dense has shape (B, D), and Out has shape (sum_B(N_i), D)\n    \"\"\"\n    off_b = tl.program_id(0)\n    off_d = tl.program_id(1) * BLOCK_D\n    seq_start = tl.load(seq_offsets + off_b)\n    seq_end = tl.load(seq_offsets + off_b + 1)\n    seq_len = seq_end - seq_start\n    Jagged += seq_start * stride_jn\n    Out += off_b * stride_ob\n    offs_d = off_d + tl.arange(0, BLOCK_D)\n    jagged_ptrs = Jagged + offs_d\n    out_ptrs = Out + offs_d\n    accumulator = tl.zeros((BLOCK_D,), dtype=tl.float32)\n    for _ in range(0, seq_len):\n        jg = tl.load(jagged_ptrs, mask=offs_d < D)\n        accumulator += jg\n        jagged_ptrs += stride_jn\n    out = accumulator\n    tl.store(out_ptrs, out, mask=offs_d < D)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "06e48080-1527-43dc-8bb4-b5cd64bbe443"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd(Q, K, V, sm_scale, DO, DQ, DK, DV, M, D, stride_z, stride_h,\n    stride_tok, stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1:\n    'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr',\n    BLK_SLICE_FACTOR: 'tl.constexpr', HEAD_DIM: 'tl.constexpr'):\n    LN2: 'tl.constexpr' = 0.6931471824645996\n    bhid = tl.program_id(2)\n    off_chz = bhid * N_CTX\n    adj = stride_h * (bhid % H) + stride_z * (bhid // H)\n    pid = tl.program_id(0)\n    Q += adj\n    K += adj\n    V += adj\n    DO += adj\n    DQ += adj\n    DK += adj\n    DV += adj\n    M += off_chz\n    D += off_chz\n    offs_k = tl.arange(0, HEAD_DIM)\n    start_n = pid * BLOCK_N1\n    start_m = start_n\n    MASK_BLOCK_M1: 'tl.constexpr' = BLOCK_M1 // BLK_SLICE_FACTOR\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, MASK_BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n,\n        start_m, num_steps, MASK=True)\n    start_m += num_steps * MASK_BLOCK_M1\n    num_steps = (N_CTX - start_m) // BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n, start_m,\n        num_steps, MASK=False)\n    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dv_ptrs, dv)\n    dk *= sm_scale\n    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dk_ptrs, dk)\n    start_m = pid * BLOCK_M2\n    end_n = start_m + BLOCK_M2\n    MASK_BLOCK_N2: 'tl.constexpr' = BLOCK_N2 // BLK_SLICE_FACTOR\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n        )\n    m = tl.load(M + offs_m)\n    m = m[:, None]\n    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, MASK_BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps *\n        MASK_BLOCK_N2, num_steps, MASK=True)\n    end_n -= num_steps * MASK_BLOCK_N2\n    num_steps = end_n // BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps * BLOCK_N2,\n        num_steps, MASK=False)\n    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    dq *= LN2\n    tl.store(dq_ptrs, dq)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "187dc7d4-7df8-40cb-8aaa-f1a9053e4764"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        h += _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dq += DK\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "9d811f96-53bd-4d71-a19b-f5b0dd1d2391"
  },
  {
    "input": "@triton.jit\ndef _voxel_grid_sample_one(gi, feature_grid, feature_grid_size, batch_index,\n    ix_in, iy_in, iz_in, ID, IH, IW, C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    ix, iy, iz, ix0, iy0, iz0, grid_numel = _get_voxel_grid_sample_info(gi,\n        ix_in, iy_in, iz_in, ID, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    (V000x, V000y, V000z, V100x, V100y, V100z, V010x, V010y, V010z, V001x,\n        V001y, V001z, V101x, V101y, V101z, V011x, V011y, V011z, V110x,\n        V110y, V110z, V111x, V111y, V111z, x, y, z\n        ) = _get_voxel_grid_sample_locs_weights(ix, iy, iz, ix0, iy0, iz0)\n    sampled = _sample_3d(feature_grid, (1 - x) * (1 - y) * (1 - z),\n        batch_index, V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(feature_grid, (1 - x) * (1 - y) * z, batch_index,\n        V100x, V100y, V100z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(\n        feature_grid, (1 - x) * y * (1 - z), batch_index, V010x, V010y,\n        V010z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(feature_grid, x * (1 -\n        y) * (1 - z), batch_index, V001x, V001y, V001z, ID, IH, IW, C,\n        BLOCK_SIZE) + _sample_3d(feature_grid, x * (1 - y) * z, batch_index,\n        V101x, V101y, V101z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(\n        feature_grid, x * y * (1 - z), batch_index, V011x, V011y, V011z, ID,\n        IH, IW, C, BLOCK_SIZE) + _sample_3d(feature_grid, (1 - x) * y * z,\n        batch_index, V110x, V110y, V110z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(feature_grid, x * y * z, batch_index, V111x, V111y,\n        V111z, ID, IH, IW, C, BLOCK_SIZE)\n    return sampled, grid_numel\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "83876eec-5567-40c2-b501-bf5459afc2f0"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_K': 16}, num_stages=2,\n    num_warps=4), triton.Config({'BLOCK_SIZE_K': 32}, num_stages=2,\n    num_warps=4), triton.Config({'BLOCK_SIZE_K': 64}, num_stages=2,\n    num_warps=4), triton.Config({'BLOCK_SIZE_K': 16}, num_stages=2,\n    num_warps=4), triton.Config({'BLOCK_SIZE_K': 32}, num_stages=2,\n    num_warps=2), triton.Config({'BLOCK_SIZE_K': 16}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_K': 32}, num_stages=3,\n    num_warps=2), triton.Config({'BLOCK_SIZE_K': 16}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=2), triton.Config({'BLOCK_SIZE_K': 128}, num_stages=2,\n    num_warps=2), triton.Config({'BLOCK_SIZE_K': 128}, num_stages=1,\n    num_warps=4)], key=['B', 'M', 'N'])\n@triton.jit\ndef matmul_quant_kernel(b_ptr, c_ptr, res_ptr, output_scale, B, M:\n    'tl.constexpr', N: 'tl.constexpr', np2_M: 'tl.constexpr', np2_N:\n    'tl.constexpr', stride_bb, stride_bk, stride_bn, stride_ck, stride_cn,\n    stride_resb, stride_resm, stride_resn, BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"\n    Quant(b @ c)\n\n    b [B, M, N]\n    c [N, N]\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    batch_id = tl.program_id(axis=1) + tl.program_id(axis=2) * tl.num_programs(\n        axis=1)\n    pid_m = pid\n    offs_bm = (pid_m * M + tl.arange(0, np2_M)) % M\n    offs_cn = tl.arange(0, np2_N) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    b_ptrs = b_ptr + batch_id * stride_bb + (offs_bm[:, None] * stride_bk +\n        offs_k[None, :] * stride_bn)\n    c_ptrs = c_ptr + (offs_k[:, None] * stride_ck + offs_cn[None, :] *\n        stride_cn)\n    accumulator = tl.zeros((np2_M, np2_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(N, BLOCK_SIZE_K)):\n        b = tl.load(b_ptrs, mask=offs_k[None, :] < N - k * BLOCK_SIZE_K,\n            other=0.0)\n        c = tl.load(c_ptrs, mask=offs_k[:, None] < N - k * BLOCK_SIZE_K,\n            other=0.0)\n        accumulator = tl.dot(b, c, accumulator)\n        b_ptrs += BLOCK_SIZE_K * stride_bn\n        c_ptrs += BLOCK_SIZE_K * stride_ck\n    abs_src_val = tl.abs(accumulator)\n    max_src_val = tl.max(abs_src_val)\n    scale = max_src_val / 7.0\n    quant_val = libdevice.llrint(accumulator / scale)\n    quant_val = max(-8, min(quant_val, 7))\n    quant_val = quant_val.reshape(np2_M, np2_N // 2, 2, can_reorder=False)\n    quant_val_even, quant_val_odd = quant_val.split()\n    quant_val_odd = quant_val_odd << 4\n    res = tl.zeros((np2_M, np2_N // 2), dtype=tl.int8)\n    res = res | quant_val_odd & 240\n    res = res | quant_val_even & 15\n    offs_resm = pid_m * M + tl.arange(0, np2_M)\n    offs_resn = tl.arange(0, np2_N // 2)\n    res_ptrs = res_ptr + stride_resb * batch_id + stride_resm * offs_resm[:,\n        None] + stride_resn * offs_resn[None, :]\n    res_mask = (offs_resm[:, None] < M) & (offs_resn[None, :] < N // 2)\n    tl.store(res_ptrs, res, mask=res_mask)\n    tl.store(output_scale + batch_id, scale)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "373ab11d-6bbb-43fb-8caa-91179759dcd8"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra_split(q, k, g, A, scale, B:\n    'tl.constexpr', T: 'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr',\n    BT: 'tl.constexpr', BC: 'tl.constexpr', BK: 'tl.constexpr', NC:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_tc // NC, i_tc % NC\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_i = tl.arange(0, BC)\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    if HEAD_FIRST:\n        o_A = (i_k * B * H + i_bh) * T * BC + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BC\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * T * K + (i_t * BT +\n            i_j * BC) * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_j * BC) * K + o_k, BK), BK)\n    else:\n        o_A = (i_k * B + i_b) * T * H * BC + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * H * BC + i_h * BC\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        b_k = tl.load(p_k, mask=m_k, other=0)\n        b_gk = tl.load(p_gk, mask=m_k, other=0)\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K if HEAD_FIRST else H * K\n        p_gk += K if HEAD_FIRST else H * K\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "2062738a-56a9-44a0-9c1c-22f826f77d4c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_stages=stages, num_warps=\n    warps) for stages in [0, 1, 3, 4] for warps in [4, 8, 16]], key=[\n    'QUERY_GROUP_SIZE', 'HEAD_SIZE', 'KV_BLOCK_SIZE'])\n@triton.jit\ndef _paged_attn_w_mma_kernel(m_i_ptr, l_i_ptr, out_ptr, q_ptr, k_cache_ptr,\n    v_cache_ptr, context_lens_ptr, block_tables_ptr, attn_scale, stride_bt0,\n    stride_bt1, stride_q0, stride_q1, stride_q2, stride_kv0, stride_kv1,\n    stride_kv2, stride_kv3, stride_o0, stride_o1, stride_o2, stride_o3,\n    stride_o4, HEAD_SIZE: 'tl.constexpr', QUERY_GROUP_SIZE: 'tl.constexpr',\n    PADDED_QUERY_GROUP_SIZE: 'tl.constexpr', NUM_KV_HEADS: 'tl.constexpr',\n    KV_BLOCK_SIZE: 'tl.constexpr', PARTITION_SIZE: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    part_idx = tl.program_id(2)\n    max_num_partitions = tl.num_programs(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    USE_PARTITIONING = PARTITION_SIZE > 0\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    if USE_PARTITIONING:\n        context_start_idx = part_idx * PARTITION_SIZE\n        if context_start_idx >= context_len:\n            return\n        context_end_idx = tl.minimum(context_start_idx + PARTITION_SIZE,\n            context_len)\n        num_blocks = tl.cdiv(context_end_idx - context_start_idx, KV_BLOCK_SIZE\n            )\n    else:\n        num_blocks = tl.cdiv(context_len, KV_BLOCK_SIZE)\n    block_offset = tl.arange(0, KV_BLOCK_SIZE)\n    head_offset = tl.arange(0, HEAD_SIZE)\n    padding_group_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)\n    kv_offset = kv_head_idx * stride_kv1 + block_offset[:, None\n        ] * stride_kv2 + head_offset[None, :] * stride_kv3\n    q_offset = seq_idx * stride_q0 + (kv_head_idx * QUERY_GROUP_SIZE +\n        padding_group_offset[:, None]) * stride_q1 + head_offset[None, :\n        ] * stride_q2\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n    q = tl.load(q_ptr + q_offset, mask=group_mask, other=0.0)\n    q = q * attn_scale\n    m_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32)\n    acc = tl.zeros([PADDED_QUERY_GROUP_SIZE, HEAD_SIZE], dtype=tl.float32)\n    num_prev_blocks = part_idx * (PARTITION_SIZE // KV_BLOCK_SIZE)\n    for i in range(num_blocks):\n        block_idx = num_prev_blocks + i\n        block_number = tl.load(block_tables_ptr + seq_idx * stride_bt0 + \n            block_idx * stride_bt1)\n        kv_block_offset = block_number * stride_kv0 + kv_offset\n        mask_offset = block_idx * KV_BLOCK_SIZE + block_offset\n        kv_mask = mask_offset[:, None] < context_len\n        k = tl.load(k_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            qk = tl.sum(q[:, None, :] * k[None, :, :], axis=2)\n        else:\n            qk = tl.dot(q, k.T, out_dtype=tl.float32)\n        qk = tl.where(mask_offset < context_len, qk, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n        p = tl.math.exp2((qk - m_i_new[:, None]) * log2e)\n        alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n        acc *= alpha[:, None]\n        v = tl.load(v_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n        if PADDED_QUERY_GROUP_SIZE == 1:\n            acc += tl.sum(p.T[:, :, None] * v[:, None, :], axis=0)\n        else:\n            p = p\n            acc += tl.dot(p, v, out_dtype=tl.float32)\n        l_i = l_i * alpha + tl.sum(p, axis=1)\n        m_i = m_i_new\n    acc = acc / l_i[:, None]\n    if USE_PARTITIONING:\n        part_offset = ((seq_idx * NUM_KV_HEADS + kv_head_idx) *\n            max_num_partitions * QUERY_GROUP_SIZE + part_idx *\n            QUERY_GROUP_SIZE + padding_group_offset)\n        mask = padding_group_offset < QUERY_GROUP_SIZE\n        tl.store(m_i_ptr + part_offset, m_i, mask=mask)\n        tl.store(l_i_ptr + part_offset, l_i, mask=mask)\n    out_offset = seq_idx * stride_o0\n    if USE_PARTITIONING:\n        out_offset += kv_head_idx * stride_o1\n    else:\n        out_offset += kv_head_idx * QUERY_GROUP_SIZE * stride_o1\n    out_offset += part_idx * stride_o2 + padding_group_offset[:, None\n        ] * stride_o3 + head_offset[None, :] * stride_o4\n    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\n    tl.store(out_ptr + out_offset, acc, mask=group_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "ea70ef24-0a90-490f-8f96-c5c29d2bd9e0"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, Mean, Rstd, Lock,\n    stride: 'tl.constexpr', N: 'tl.constexpr', eps, GROUP_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    c2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    partial_db = dy\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "5024643b-c55e-4ec2-b1a8-b54f6afcaf0b"
  },
  {
    "input": "@triton.jit\ndef _triton_rope(q_ptr, q_row_stride, k_ptr, k_row_stride, cos,\n    cos_row_stride, sin, sin_row_stride, sl, bs: 'tl.constexpr', n_qh:\n    'tl.constexpr', n_kh: 'tl.constexpr', hd: 'tl.constexpr', pad_n_qh:\n    'tl.constexpr', pad_n_kh: 'tl.constexpr', pad_hd: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', BACKWARD_PASS: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n    cos_row_idx = pid % sl\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        )\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        )\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "e7d8d1d3-6c7b-4f97-9fe2-345f54f2bc11"
  },
  {
    "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=['D'])\n@triton.jit\ndef _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, N, D, BLOCK_N:\n    'tl.constexpr', BLOCK_D: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)\n    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    db = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = (rows[:, None] < N) & (cols[None, :] < D)\n        offs = rows[:, None] * D + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < D)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < D)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "5fdb4e83-2bfa-4a4d-a4b6-c95fe0e3dff0"
  },
  {
    "input": "@triton.jit\ndef sum_kernel(y_ptr, x_ptr, size, block_size: 'tl.constexpr'):\n    offsets = tl.arange(0, block_size)\n    mask = offsets < size\n    x = tl.load(x_ptr + offsets, mask)\n    y = tl.reduce(x, 0, combine_add)\n    tl.store(y_ptr, y)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "5328c4a4-ecce-40ee-a77f-c0f079e952c7"
  },
  {
    "input": "@triton.jit\ndef liger_cross_entropy_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr,\n    z_loss_ptr, loss_stride, n_cols, n_non_ignore, ignore_index,\n    lse_square_scale: 'tl.constexpr', label_smoothing: 'tl.constexpr',\n    reduction: 'tl.constexpr', softcap, RETURN_Z_LOSS: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', HAS_SOFTCAPPING: 'tl.constexpr'):\n    \"\"\"\n    This kernel computes both cross entropy loss and the gradient of the input.\n    We only consider hard label + mean reduction for now. Please refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for the math.\n\n    Parameters:\n    X_ptr: Pointer to input tensor.\n    X_stride (int): The stride of the input tensor.\n    Y_ptr: Pointer to target tensor.\n    Y_stride (int): The stride of the target tensor.\n    loss_ptr: Pointer to tensor to store the loss.\n    z_loss_ptr: Pointer to tensor to store the z loss. No operation if RETURN_Z_LOSS is 0.\n    loss_stride (int): The stride of the loss tensor.\n    n_cols (int): The number of columns in the input tensor.\n    n_non_ignore (int): The number of non-ignored elements in the batch.\n    ignore_index (int): The index to ignore in the target.\n    label_smoothing (float): The amount of smoothing when computing the loss, where 0.0 means no smoothing.\n    lse_square_scale (float): The scaler of (logsumexp(_input)) ^ 2 adding to the loss for the stability of training.\n    RETURN_Z_LOSS (int): The boolean value to decide whether storing z loss to z_loss_ptr or not. It must be 0 or 1.\n    reduction (str): The string for the reduction to apply\n    softcap (float): The upper threshold for scaling logits to the range (-softcap, +softcap).\n    BLOCK_SIZE (int): The block size for Triton operations.\n    HAS_SOFTCAPPING (bool): The boolean value to determine whether applying soft-capping or not.\n    \"\"\"\n    program_id = tl.program_id(0)\n    Y_ptr += program_id * Y_stride\n    y = tl.load(Y_ptr)\n    X_ptr += program_id * X_stride\n    if y == ignore_index:\n        for i in range(0, n_cols, BLOCK_SIZE):\n            X_offsets = i + tl.arange(0, BLOCK_SIZE)\n            tl.store(X_ptr + X_offsets, 0.0, mask=X_offsets < n_cols)\n        return\n    loss_ptr += program_id * loss_stride\n    z_loss_ptr += program_id * loss_stride\n    m = float('-inf')\n    d = 0.0\n    ori_X_y = tl.load(X_ptr + y).cast(tl.float32)\n    if HAS_SOFTCAPPING:\n        ori_X_y = softcap * tanh(ori_X_y / softcap)\n    scaled_x_sum = 0.0\n    eps = label_smoothing / n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf')).cast(tl.float32)\n        if HAS_SOFTCAPPING:\n            X_block = softcap * tanh(X_block / softcap)\n        block_max = tl.max(X_block)\n        if label_smoothing > 0:\n            scaled_x_sum += tl.sum(tl.where(X_offsets < n_cols, -eps *\n                X_block, 0.0))\n        m_new = tl.maximum(m, block_max)\n        d = d * tl.exp(m - m_new) + tl.sum(tl.exp(X_block - m_new))\n        m = m_new\n    lse = m + tl.log(d)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf')).cast(tl.float32)\n        if HAS_SOFTCAPPING:\n            intermediate = tanh(X_block / softcap)\n            X_block = softcap * intermediate\n        X_block = tl.exp(X_block - m) / d\n        X_block += 2 * lse_square_scale * lse * X_block\n        X_block += -eps\n        X_block = tl.where(X_offsets != y, X_block, X_block - (1 -\n            label_smoothing))\n        if reduction == 'mean':\n            X_block = X_block / n_non_ignore\n        if HAS_SOFTCAPPING:\n            X_block = X_block * (1 - intermediate * intermediate)\n        tl.store(X_ptr + X_offsets, X_block, mask=X_offsets < n_cols)\n    tl.debug_barrier()\n    loss = lse - ori_X_y\n    if label_smoothing > 0:\n        smooth_loss = scaled_x_sum + label_smoothing * lse\n        loss = loss * (1 - label_smoothing) + smooth_loss\n    z_loss = lse_square_scale * lse * lse\n    loss += z_loss\n    if reduction == 'mean':\n        z_loss = z_loss / n_non_ignore\n        loss = loss / n_non_ignore\n    tl.store(loss_ptr, loss)\n    if RETURN_Z_LOSS == _TRUE:\n        tl.store(z_loss_ptr, z_loss)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "e700b66b-7ca5-4f70-95ae-0ca39b8f4e0b"
  },
  {
    "input": "@triton.autotune(list(filter(keep, configs)), key=['N_CTX', 'HEAD_DIM'])\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, HEAD_DIM: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', STAGE: 'tl.constexpr'):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    v_order: 'tl.constexpr' = (0, 1\n        ) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, HEAD_DIM), order=v_order)\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(HEAD_DIM,\n        N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape\n        =(HEAD_DIM, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    q = tl.load(Q_block_ptr)\n    if STAGE & 1:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 4 -\n            STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5)\n    if STAGE & 2:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 2,\n            offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5)\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "3fc4d57f-a631-42ba-bcd1-ad114ef56671"
  },
  {
    "input": "@triton.jit\ndef flashatt_kernel(q_ptr, k_ptr, v_ptr, z_ptr, N0, T, B0: 'tl.constexpr',\n    B1: 'tl.constexpr'):\n    block_id_i = tl.program_id(0)\n    log2_e = 1.44269504\n    off_i = block_id_i * B0 + tl.arange(0, B0)\n    mask_i = off_i < N0\n    inf = 1000000.0\n    q = tl.load(q_ptr + off_i, mask=mask_i)\n    exp_sum = tl.zeros((B0,), dtype=tl.float32)\n    qk_max = tl.full((B0,), -inf, dtype=tl.float32)\n    z = tl.zeros((B0,), dtype=tl.float32)\n    for id_j in tl.range(0, T, B1):\n        off_j = id_j + tl.arange(0, B1)\n        mask_j = off_j < T\n        mask_ij = mask_i[:, None] & mask_j[None, :]\n        k = tl.load(k_ptr + off_j, mask=mask_j)\n        qk = q[:, None] * k[None, :] + tl.where(mask_ij, 0, -1000000.0)\n        new_max = tl.maximum(tl.max(qk, axis=1), qk_max)\n        qk_exp = tl.exp2(log2_e * (qk - new_max[:, None]))\n        factor = tl.exp2(log2_e * (qk_max - new_max))\n        new_exp_sum = exp_sum * factor + tl.sum(qk_exp, axis=1)\n        v = tl.load(v_ptr + off_j, mask=mask_j, other=0.0)\n        z = z * factor + tl.sum(qk_exp * v[None, :], axis=1)\n        qk_max = new_max\n        exp_sum = new_exp_sum\n    z = z / exp_sum\n    tl.store(z_ptr + off_i, z, mask=mask_i)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "1ef59610-55a3-44da-ad28-75806c879162"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS,\n    NUM_STAGES)], key=['M'])\n@triton.jit\ndef triton_jagged_mean_kernel_simple_fused_buffer_then_sum(input_ptr_values,\n    input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_b), tl.load(\n        input_ptr_offsets + (pid_b + 1))\n    ragged_len = ragged_end - ragged_start\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer_sum = tl.sum(buffer, axis=0)\n    buffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n    output_offsets = offsets_m + pid_b * M\n    output_mask = output_offsets < M * (pid_b + 1)\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "8051db50-d57b-4406-821b-95f87d98eced"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8), triton.Config({'BT': 128}, num_warps=\n    2), triton.Config({'BT': 128}, num_warps=4), triton.Config({'BT': 128},\n    num_warps=8), triton.Config({'BT': 256}, num_warps=2), triton.Config({\n    'BT': 256}, num_warps=4), triton.Config({'BT': 256}, num_warps=8)], key\n    =['D'])\n@triton.jit\ndef logsigmoid_fwd_kernel(x, y, T: 'tl.constexpr', D: 'tl.constexpr', BT:\n    'tl.constexpr'):\n    i = tl.program_id(0)\n    o_i = i * BT + tl.arange(0, BT)\n    p_x = x + o_i\n    p_y = y + o_i\n    mask = o_i < T\n    b_x = tl.load(p_x, mask=mask, other=0.0)\n    b_m = tl.minimum(0.0, b_x)\n    b_z = 1.0 + tl.exp(-tl.abs(b_x))\n    b_y = b_m - tl.log(b_z)\n    tl.store(p_y, b_y, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "a3ef12c9-06cf-4635-8890-60266887a7c6"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, TMP, L, M, Out, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh,\n    stride_om, stride_on, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(q_ptrs)\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + start_n * stride_kn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        qk *= sm_scale\n        qk += tl.where(offs_m[:, None] >= start_n + offs_n[None, :], 0,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale)\n        acc_scale = tl.load(t_ptrs)\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + start_n * stride_vk)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "754fc17a-f80d-44c0-9154-a6e485b1a7c0"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, N_CTX, sliding_window_offset, sliding_window_size, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    if SLIDING_WINDOW and not COMPLEMENT_SLIDING_WINDOW:\n        if COMPLEMENT_SLIDING_WINDOW:\n            lo = 0\n            hi = ((start_m + 1) * BLOCK_M + sliding_window_offset -\n                sliding_window_size + BLOCK_N - 1) // BLOCK_N * BLOCK_N\n        else:\n            lo = (start_m * BLOCK_M + sliding_window_offset -\n                sliding_window_size + 1) // BLOCK_N * BLOCK_N\n            hi = ((start_m + 1) * BLOCK_M - 1 + sliding_window_offset + BLOCK_N\n                ) // BLOCK_N * BLOCK_N\n            if lo < 0:\n                lo = 0\n            if hi > N_CTX:\n                hi = N_CTX\n            lo = tl.multiple_of(lo, BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n            V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    else:\n        lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if IS_EVEN_N:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m * BLOCK_M - start_n + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n            qk = tl.where(mask, qk, float('-inf'))\n        if not IS_EVEN_N:\n            qk = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :\n                ], qk, float('-inf'))\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :],\n                p, 0)\n        l_ij = tl.sum(p, 1)\n        tmp = m_i - m_ij\n        alpha_mask = tmp != tmp\n        alpha = tl.math.exp2(tmp)\n        alpha = tl.where(alpha_mask, 1.0, alpha)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        if IS_EVEN_N:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        acc += tl.dot(p, v)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "07168755-cb8b-407b-82f4-fda920967757"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_fwd_kernel_o(q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_hh, s_ht, H, T, TD, scale, DK, DV, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    for i_v in range(0, tl.cdiv(DV, BV)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i_t * BT, 0), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (0, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (TD, DV), (s_ht, 1), (i_t *\n            DK, i_v * BV), (BK, BV), (1, 0))\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_s = tl.zeros([BT, BT], dtype=tl.float32)\n        for _ in range(0, tl.cdiv(DK, BK)):\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_q = b_q * scale\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_o += tl.dot(b_q * d_i[:, None], b_h, allow_tf32=False)\n            b_s += tl.dot(b_q, b_k, allow_tf32=False)\n            p_q = tl.advance(p_q, (0, BK))\n            p_k = tl.advance(p_k, (BK, 0))\n            p_h = tl.advance(p_h, (BK, 0))\n        b_s *= d_s\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "ad0bc1aa-a40d-4f75-bb4c-d21bf82bf643"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_retention_bwd_kernel(q, k, v, do, dq, dk, dv, initial_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT -\n        o_i - 1) * b_b)\n    d_b = tl.math.exp2(BT * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    d_s = tl.trans(d_s)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "cf9b5ad1-d16a-4e0f-b106-86d20c3524ac"
  },
  {
    "input": "@triton.jit\ndef tl_tanh(a: 'tl.tensor') ->tl.tensor:\n    return tl_libdevice.tanh(a)\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "29a051f9-f121-4de7-b992-cf5a5488c1a0"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: 'tl.constexpr',\n    HAS_SMOOTHING: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - smooth_positive,\n            probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "54755a16-3c1c-44ee-97e2-0169fde54403"
  },
  {
    "input": "@triton.jit\ndef gelu_approx_grad(x):\n    tanh_out = tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n    return 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + \n        0.1070322243 * x * x)) + 0.5 * (1 + tanh_out)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "56f81b3c-119f-4486-8f9f-9ec03fea53e7"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_bwd(dY, dX, dW, X, W, Rstd, stride, N, BLOCK_SIZE: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    X += row * stride\n    dY += row * stride\n    dX += row * stride\n    dW += row * stride\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    dy = tl.load(dY + cols, mask=mask, other=0.0)\n    x = tl.load(X + cols, mask=mask, other=0.0)\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    rstd = tl.load(Rstd + row)\n    m = dy * w\n    dx = rstd * m\n    dx += rstd * -(1 / N) * rstd * rstd * tl.sum(m * x, axis=0) * x\n    dw = dy * (x * rstd)\n    tl.store(dX + cols, dx, mask=mask)\n    tl.store(dW + cols, dw, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "b3b5ba8c-a20f-48b0-8c28-173d88bd18a4"
  },
  {
    "input": "@triton.jit\ndef parallel_simple_gla_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n    dk, dv, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        i_v * BV), (BT, BV), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    b_gk = tl.load(p_gk, boundary_check=(0,))\n    NTS = tl.cdiv(T, BS)\n    b_kg = b_k * tl.exp(tl.load(g + i_bh * T + min(i_t * BT + BT, T) - 1) -\n        b_gk)[:, None]\n    for i_s in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_s, i_v * BV), (BS, BV), (1, 0))\n        p_gq = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0,))\n        b_gp = tl.load(g + i_bh * T + min(i_s + BS, T) - 1)\n        b_gn = tl.load(g + i_bh * T + i_s - 1) if i_s % BT > 0 else 0.0\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_gq - b_gn)[:, None]\n        b_dk *= tl.exp(b_gp - b_gn)\n        b_dv *= tl.exp(b_gp - b_gn)\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_s = tl.dot(b_kg, tl.trans(b_q), allow_tf32=False)\n        b_dk += tl.dot(b_ds, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, b_do, allow_tf32=False)\n    b_dk *= tl.exp(tl.load(g + i_bh * T + min(T, i_t * BT + BT) - 1) - b_gk)[\n        :, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q = i_t * BT + tl.arange(0, BS)\n    o_k = i_t * BT + tl.arange(0, BT)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_s, i_v * BV), (BS, BV), (1, 0))\n        p_gq = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0,))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.exp(-b_gk[:, None] + b_gq[None, :]), 0) * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False) * d_s\n        b_s = tl.dot(b_k, tl.trans(b_q), allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, b_do, allow_tf32=False)\n        o_q += BS\n    p_dk = tl.make_block_ptr(dk + (i_v * B * H + i_bh) * s_k_h, (T, K), (\n        s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_k * B * H + i_bh) * s_v_h, (T, V), (\n        s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (i_v * B * H + i_bh) * T, (T,), (1,), (\n        i_t * BT,), (BT,), (0,))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_dg = tl.load(p_dg, boundary_check=(0,))\n    b_dg -= tl.sum(b_dk * b_k, 1)\n    tl.store(p_dg, b_dg, boundary_check=(0,))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "28c76011-3652-44c8-8dca-59bea775d531"
  },
  {
    "input": "@triton.jit\ndef _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds + b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "734878c0-b270-42d1-b9bf-d3bb7619112c"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_h(k, v, z, h, h0, ht, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NORMK: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    if NORMK:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_k *\n            BK,), (BK,), (0,))\n    else:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_v *\n            BV,), (BV,), (0,))\n    b_zp = tl.load(p_z0)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if NORMK:\n            p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,),\n                ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[:, None]\n            b_k = tl.exp(b_k - b_zc[:, None])\n        else:\n            p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,),\n                ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[None, :]\n            b_v = tl.exp(b_v - b_zc[None, :])\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "be0657cf-3c18-4fe4-9758-8c8611c4b8b4"
  },
  {
    "input": "@triton.jit\ndef is_in_bounds(x, y, z, C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    in_bounds = (tl.abs(x) <= 1) * (tl.abs(y) <= 1) * (tl.abs(z) <= 1)\n    if C == 1:\n        in_bounds_mask = tl.view(in_bounds, (BLOCK_SIZE,))\n    else:\n        in_bounds_mask = tl.broadcast_to(in_bounds[:, None], (BLOCK_SIZE, C))\n    return in_bounds_mask\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "7a8b4b75-507b-49dc-9f08-9a7662b6f0c3"
  },
  {
    "input": "@triton.jit\ndef _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "8eb9ddeb-23fa-40f1-bd9b-8e7dca526dcc"
  },
  {
    "input": "@triton.jit\ndef _splat_2d(to_splat, grad_image, w, batch_index, ix, iy, IH:\n    'tl.constexpr', IW: 'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    w = tl.view((w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW)))[:,\n        None] * channel_bcast, (BLOCK_SIZE, C))\n    offs = tl.view((batch_index * IW * IH * C + iy_ * IW * C + ix_ * C)[:,\n        None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "0fd27eb3-b226-49dd-80e6-35936a1a52ec"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, h0, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr', H: 'tl.constexpr',\n    T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_k_h + ((i + 1) * BT - 1\n            ) * s_k_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[None, :] + tl.dot(b_v, b_k, allow_tf32=\n                False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[None, :] + tl.dot(b_v, b_k, allow_tf32=\n                False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_k_h + (T - (i - 1) * BT - 1\n            ) * s_k_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_db = tl.load(p_db, mask=mask, other=0)\n        if CHECK and i == 1:\n            b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh = b_dh * tl.exp(b_db)[:, None] + tl.dot(b_q, b_do,\n                allow_tf32=False)\n        else:\n            b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False))\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh = b_dh * tl.exp(b_db)[:, None] + tl.dot(b_q, b_do,\n                allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "4df5ec23-3390-47fd-a4f5-794c3e540c77"
  },
  {
    "input": "@triton.jit\ndef cross_entropy_loss_kernel(logits_ptr, targets_ptr, loss_ptr, n_classes,\n    n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    targets = tl.load(targets_ptr + offsets, mask=mask, other=-1)\n    row_max = tl.full([BLOCK_SIZE], float('-inf'), dtype=tl.float32)\n    row_sum = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for i in range(n_classes):\n        col_offset = offsets * n_classes + i\n        logit = tl.load(logits_ptr + col_offset, mask=mask, other=float('-inf')\n            )\n        row_max = tl.maximum(row_max, logit)\n    loss = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for i in range(n_classes):\n        col_offset = offsets * n_classes + i\n        logit = tl.load(logits_ptr + col_offset, mask=mask, other=float('-inf')\n            )\n        exp_logit = tl.exp(logit - row_max)\n        row_sum += exp_logit\n        loss = tl.where(targets == i, loss - logit + row_max, loss)\n    loss += tl.log(row_sum)\n    tl.store(loss_ptr + offsets, loss, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "bc8b6da1-da69-4c7e-b27c-623f08c288f8"
  },
  {
    "input": "@triton.jit\ndef demo3(z_ptr):\n    range = tl.arange(0, 8)\n    z = tl.store(z_ptr + range, 10, range < 5)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "903a9e96-0e01-4ddb-87d6-fba6be6cd22e"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_V(k, v, z, h, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    b_zc = tl.load(p_zc, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k = tl.exp(b_k - b_zc[None, :])\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n        if i_k == 0:\n            b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n        b_do = b_do * scale\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n    p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_zp = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_p * K +\n        i_k * BK,), (BK,), (0,))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_z = tl.exp(b_zp[None, :] - b_z)\n    b_dq = b_dq * b_z\n    b_dk = b_dk * b_k\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_dA = tl.where(m_s, b_dA, 0.0)\n    if i_k == 0:\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d293c88f-fb75-4557-87b5-489b29fe4b09"
  },
  {
    "input": "@triton.jit\ndef _rms_layer_norm_bwd_dx_fused(DX, DY, DW, X, W, RMS, Lock, stride, N,\n    GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    rms = tl.load(RMS + row)\n    xhat = x * rms\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    dx = (wdy - xhat * c1) * rms\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "1d80dad5-808a-4075-9870-6993170f0b89"
  },
  {
    "input": "@triton_autotune(configs=_add_embeddings_bwd_configs(), key=[\n    'AUTOTUNE_MAX_SEQ_LEN', 'AUTOTUNE_B', 'D'])\n@triton.jit\ndef _add_embeddings_bwd_kernel(In, KeyInds, ValueInds, Out,\n    AUTOTUNE_MAX_SEQ_LEN, B, AUTOTUNE_B, D, jagged_size, stride_in,\n    stride_on, BLOCK_D: 'tl.constexpr', BLOCK: 'tl.constexpr'):\n    off_block = tl.program_id(0)\n    offs_d = tl.arange(0, BLOCK_D)\n    mask_d = offs_d < D\n    key_ind = -1\n    key_ind = key_ind\n    accumulator = tl.zeros((BLOCK_D,), dtype=In.dtype.element_ty)\n    for off_i in range(0, BLOCK):\n        off = off_block * BLOCK + off_i\n        if off < jagged_size:\n            value_ind = tl.load(ValueInds + off)\n            in_offset = In + value_ind * stride_in\n            jagged_in = tl.load(in_offset + offs_d, mask=mask_d)\n            key_ind_new = tl.load(KeyInds + off)\n            if key_ind == key_ind_new:\n                accumulator += jagged_in\n            else:\n                if key_ind >= 0:\n                    out_offset = Out + key_ind * stride_on\n                    tl.atomic_add(out_offset + offs_d, accumulator, mask=\n                        mask_d, sem='relaxed')\n                key_ind = key_ind_new\n                accumulator = jagged_in\n    if key_ind >= 0:\n        out_offset = Out + key_ind * stride_on\n        tl.atomic_add(out_offset + offs_d, accumulator, mask=mask_d, sem=\n            'relaxed')\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d1ecf2f7-c807-46e9-b543-c0f047d340f4"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "c3993473-84bd-484d-a746-904a1199f85e"
  },
  {
    "input": "@triton.jit\ndef tanh(input):\n    \"\"\"\n    Applies tanh to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by tanh.\n    \"\"\"\n    return 2 * sigmoid(2 * input) - 1\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "b368c89c-8aa4-44c0-a001-05264f0c92d6"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr', fp8_v: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        qk = tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr)\n        if fp8_v:\n            p = p\n        else:\n            p = p\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "00d66e81-8bb2-4eed-a4c7-297b8c67a54d"
  },
  {
    "input": "@triton.jit\ndef _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "94e333f5-ab73-40f9-9dcd-4095cf723e6c"
  },
  {
    "input": "@triton.jit\ndef prepare_qg_kg(q, k, g, qg, kg, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale,\n    BT: 'tl.constexpr', BK: 'tl.constexpr', DK: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_g = g + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_qg = qg + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_kg = kg + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    mask = i_k * BK + tl.arange(0, BK) < DK\n    last_decay = tl.load(g + i_bh * s_qk_h + (i_c * BT + BT - 1) * DK + i_k *\n        BK + tl.arange(0, BK))\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0)\n        _k = tl.load(p_k, mask=mask, other=0)\n        _g = tl.load(p_g, mask=mask, other=0)\n        _q *= tl.math.exp2(_g) * scale\n        _k *= tl.math.exp2(last_decay - _g)\n        tl.store(p_kg, _k, mask=mask)\n        tl.store(p_qg, _q, mask=mask)\n        p_q += DK\n        p_g += DK\n        p_k += DK\n        p_kg += DK\n        p_qg += DK\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "94ced8a9-16a8-414a-9666-7912390f7969"
  },
  {
    "input": "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD_SIZED, key=['BLOCK_DMODEL',\n    'max_seqlen_q', 'max_seqlen_k'])\n@triton.jit\ndef sized_tuned_bwd_kernel_dq(Q, K, V, B, sm_scale, Out, DO, DQ, DB, L, D,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n    stride_om, stride_ok, stride_dqz, stride_dqh, stride_dqm, stride_dqk,\n    stride_dbz, stride_dbh, stride_dbm, stride_dbn, cu_seqlens_q,\n    cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim,\n    dropout_p, philox_seed, philox_offset_base, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    bare_bwd_kernel_dq(Q, K, V, B, sm_scale, Out, DO, DQ, DB, L, D,\n        stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n        stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n        stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n        stride_om, stride_ok, stride_dqz, stride_dqh, stride_dqm,\n        stride_dqk, stride_dbz, stride_dbh, stride_dbm, stride_dbn,\n        cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k,\n        head_dim, dropout_p, philox_seed, philox_offset_base, BLOCK_M,\n        BLOCK_DMODEL, BLOCK_N, CAUSAL, ENABLE_DROPOUT, PADDED_HEAD=\n        PADDED_HEAD, BIAS_TYPE=BIAS_TYPE)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "336b140d-f856-4151-8b4d-981da7804a06"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_apply_penalty(Logits, presence_penalty, freqency_penalty,\n    repetition_penalty, p_token_ids, p_token_counts, p_cumsum_seq_len,\n    stride_logit_b, stride_logit_s, BLOCK_P: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_repetition = tl.load(repetition_penalty + cur_batch)\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset <\n        cur_batch_end_index, other=0.0)\n    rep_logits = tl.where(cur_logits > 0, cur_logits / cur_repetition, \n        cur_logits * cur_repetition)\n    freq_logits = rep_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset <\n        cur_batch_end_index)\n    return\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "863bfabf-13b5-4edf-a3a1-9bcc4d652faf"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=4),\n    triton.Config({}, num_stages=2, num_warps=2), triton.Config({},\n    num_stages=3, num_warps=4), triton.Config({}, num_stages=3, num_warps=2\n    ), triton.Config({}, num_stages=4, num_warps=4), triton.Config({},\n    num_stages=4, num_warps=2)], key=['B', 'M', 'N'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, res_ptr, output_scale, B, M:\n    'tl.constexpr', N: 'tl.constexpr', np2_M: 'tl.constexpr', np2_N:\n    'tl.constexpr', stride_am, stride_ak, stride_bb, stride_bk, stride_bn,\n    stride_ck, stride_cn, stride_resb, stride_resm, stride_resn,\n    BLOCK_SIZE_M: 'tl.constexpr', is_split: 'tl.constexpr'):\n    \"\"\"\n    a @ b @ c\n\n    a [M, M]\n    b [B, M, N]\n    c [N, N]\n\n    now only supports BLOCK_SIZE_M == triton.next_power_of_2(BLOCK_SIZE_M)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    batch_id = tl.program_id(axis=1) + tl.program_id(axis=2) * tl.num_programs(\n        axis=1)\n    pid_m = pid\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = tl.arange(0, np2_N) % N\n    offs_k = tl.arange(0, np2_M)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + batch_id * stride_bb + (offs_k[:, None] * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, np2_N), dtype=tl.float32)\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < M, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < M, other=0.0)\n    accumulator += tl.dot(a, b)\n    tmp_ab = accumulator\n    offs_cn = tl.arange(0, np2_N) % N\n    offs_k = tl.arange(0, np2_N)\n    c_ptrs = c_ptr + (offs_k[:, None] * stride_ck + offs_cn[None, :] *\n        stride_cn)\n    c = tl.load(c_ptrs, mask=offs_k[:, None] < N, other=0.0)\n    accumulator = 0\n    accumulator += tl.dot(tmp_ab, c)\n    if is_split:\n        res = accumulator\n        offs_resm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_resn = tl.arange(0, np2_N)\n        res_ptrs = res_ptr + stride_resb * batch_id + stride_resm * offs_resm[\n            :, None] + stride_resn * offs_resn[None, :]\n        res_mask = (offs_resm[:, None] < M) & (offs_resn[None, :] < N)\n        tl.store(res_ptrs, res, mask=res_mask)\n    else:\n        abs_src_val = tl.abs(accumulator)\n        max_src_val = tl.max(abs_src_val)\n        scale = max_src_val / 7.0\n        quant_val = libdevice.llrint(accumulator / scale)\n        quant_val = max(-8, min(quant_val, 7))\n        quant_val = quant_val.reshape(BLOCK_SIZE_M, np2_N // 2, 2,\n            can_reorder=False)\n        quant_val_even, quant_val_odd = quant_val.split()\n        quant_val_odd = quant_val_odd << 4\n        res = tl.zeros((BLOCK_SIZE_M, np2_N // 2), dtype=tl.int8)\n        res = res | quant_val_odd & 240\n        res = res | quant_val_even & 15\n        offs_resm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_resn = tl.arange(0, np2_N // 2)\n        res_ptrs = res_ptr + stride_resb * batch_id + stride_resm * offs_resm[\n            :, None] + stride_resn * offs_resn[None, :]\n        res_mask = (offs_resm[:, None] < M) & (offs_resn[None, :] < N // 2)\n        tl.store(res_ptrs, res, mask=res_mask)\n        tl.store(output_scale + batch_id, scale)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "c75b0e2c-7607-43d3-ac74-c9ab9760c7a1"
  },
  {
    "input": "@triton.jit\ndef _fwd_diag_kernel(Q, K, V, Out, S, b: 'tl.constexpr', h: 'tl.constexpr',\n    n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK: 'tl.constexpr',\n    NUM_CBLOCK: 'tl.constexpr'):\n    off = tl.program_id(0)\n    off_bh = off // NUM_BLOCK\n    off_block = off % NUM_BLOCK\n    off_cblock = tl.program_id(1)\n    off_h = off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK\n    qk_block_offset = block_offset * d\n    v_block_offset = block_offset * e\n    o_block_offset = block_offset * e\n    cblock_offset = off_cblock * CBLOCK\n    q_cblock_offset = cblock_offset * d\n    o_cblock_offset = cblock_offset * e\n    Q_block_ptr = (Q + qk_offset + qk_block_offset + q_cblock_offset + tl.\n        arange(0, CBLOCK)[:, None] * d + tl.arange(0, d)[None, :])\n    K_trans_block_ptr = K + qk_offset + qk_block_offset + tl.arange(0, CBLOCK)[\n        None, :] * d + tl.arange(0, d)[:, None]\n    V_block_ptr = V + v_offset + v_block_offset + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    O_block_ptr = (Out + o_offset + o_block_offset + o_cblock_offset + tl.\n        arange(0, CBLOCK)[:, None] * e + tl.arange(0, e)[None, :])\n    S_block_ptr = S + off_h\n    s = tl.load(S_block_ptr)\n    i = off_cblock\n    q_index = tl.arange(0, CBLOCK) + i * CBLOCK\n    q = tl.load(Q_block_ptr, mask=q_index[:, None] < n, other=0.0)\n    qkv = tl.zeros([CBLOCK, e], dtype=tl.float32)\n    for j in range(i + 1):\n        kv_index = tl.arange(0, CBLOCK) + j * CBLOCK\n        diff = q_index[:, None] - kv_index[None, :]\n        s_index = s * diff\n        s_index = tl.where(diff >= 0, -s_index, float('-inf'))\n        decay = tl.exp(s_index)\n        k_trans = tl.load(K_trans_block_ptr, mask=kv_index[None, :] < n,\n            other=0.0)\n        v = tl.load(V_block_ptr, mask=kv_index[:, None] < n, other=0.0)\n        qk = tl.dot(q, k_trans) * decay\n        qkv += tl.dot(qk, v)\n        K_trans_block_ptr += CBLOCK * d\n        V_block_ptr += CBLOCK * e\n    tl.store(O_block_ptr, qkv, mask=q_index[:, None] < n)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "d605acf4-0982-4111-ae85-e47c5b3c3e10"
  },
  {
    "input": "@triton.jit\ndef _save_into_alpha_d(alpha_d, x, stride_alpha_d1, stride_alpha_d2,\n    stride_alpha_d3, stride_alpha_d4, stride_alpha_d5, stride_merge1,\n    stride_merge2, stride_merge3, B, L, w, r, BLOCK_RD: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    if b_idx >= B:\n        return\n    span_length_left = tl.program_id(1) + 1\n    tid = tl.program_id(2)\n    start = 0\n    mask = tl.arange(0, BLOCK_RD) < r\n    to_save = tl.load(x + b_idx * stride_merge1 + tl.program_id(1) *\n        stride_merge2 + tid * stride_merge3 + tl.arange(0, BLOCK_RD), mask=\n        mask, other=0)\n    while tid >= L - w - start:\n        tid -= L - w - start\n        start += 1\n    gap_start = start + span_length_left\n    gap_end = gap_start + (tid + 1)\n    end = gap_end + (w - span_length_left)\n    tl.store(alpha_d + b_idx * stride_alpha_d1 + start * stride_alpha_d2 + \n        gap_start * stride_alpha_d3 + gap_end * stride_alpha_d4 + end *\n        stride_alpha_d5 + tl.arange(0, BLOCK_RD), to_save, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "37bfd501-0941-4e62-b481-d94bd9066b53"
  },
  {
    "input": "@triton.jit\ndef _flash_decoding_stage1_kernel(Q, K, V, sm_scale, actual_seq_len, Mid_O,\n    Mid_O_LogExpSum, q_bs_stride, q_heads_stride, q_dim_stride, k_bs_stride,\n    k_heads_stride, k_dim_stride, v_bs_stride, v_heads_stride, v_dim_stride,\n    mido_batch_stride, mido_heads_stride, mido_partitions_stride,\n    mido_dim_stride, mido_les_batch_stride, mido_les_heads_stride,\n    mido_les_partitions_stride, BLOCK_SEQ: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr'):\n    \"\"\"Flash Attention Stage1 Triton Kernel\"\"\"\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    seq_block_idx = tl.program_id(2)\n    cur_batch_start_loc = batch_idx * actual_seq_len\n    cur_batch_partition_start_index = seq_block_idx * BLOCK_SEQ\n    cur_batch_partition_end_index = tl.minimum(actual_seq_len, \n        cur_batch_partition_start_index + BLOCK_SEQ)\n    num_blocks = (cur_batch_partition_end_index -\n        cur_batch_partition_start_index + BLOCK_N - 1) // BLOCK_N\n    offs_n = cur_batch_partition_start_index + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    q_offs = (batch_idx * q_bs_stride + head_idx * q_heads_stride + offs_d *\n        q_dim_stride)\n    k_offs = (cur_batch_start_loc + offs_n[:, None]\n        ) * k_bs_stride + head_idx * k_heads_stride + offs_d[None, :\n        ] * k_dim_stride\n    v_offs = (cur_batch_start_loc + offs_n[:, None]\n        ) * v_bs_stride + head_idx * v_heads_stride + offs_d[None, :\n        ] * v_dim_stride\n    q_ptrs = Q + q_offs\n    k_ptrs = K + k_offs\n    v_ptrs = V + v_offs\n    q = tl.load(q_ptrs)\n    d_i = 0.0\n    m_i = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(num_blocks):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        k_mask = offs_n_new < cur_batch_partition_end_index\n        k = tl.load(k_ptrs, mask=k_mask[:, None], other=0.0)\n        v = tl.load(v_ptrs, mask=k_mask[:, None], other=0.0)\n        qk = tl.sum(q * k, axis=1)\n        qk = qk * sm_scale\n        qk = tl.where(k_mask, qk, float('-inf'))\n        current_max = tl.max(qk)\n        m_ij = tl.maximum(m_i, current_max)\n        qk = qk - m_ij\n        p = tl.exp(qk)\n        alpha = tl.exp(m_i - m_ij)\n        d_i = d_i * alpha + tl.sum(p)\n        acc = acc * alpha + tl.sum(p[:, None] * v, axis=0)\n        m_i = m_ij\n        k_ptrs += BLOCK_N * k_bs_stride\n        v_ptrs += BLOCK_N * v_bs_stride\n    need_store = num_blocks > 0\n    off_mid_o = (batch_idx * mido_batch_stride + head_idx *\n        mido_heads_stride + seq_block_idx * mido_partitions_stride + offs_d *\n        mido_dim_stride)\n    off_mid_o_les = (batch_idx * mido_les_batch_stride + head_idx *\n        mido_les_heads_stride + seq_block_idx * mido_les_partitions_stride)\n    part_atten_out = acc / d_i\n    logexpsum = m_i + tl.log(d_i)\n    part_atten_out = tl.where(need_store, part_atten_out, 0.0)\n    logexpsum = tl.where(need_store, logexpsum, float('-inf'))\n    tl.store(Mid_O + off_mid_o, part_atten_out, mask=need_store)\n    tl.store(Mid_O_LogExpSum + off_mid_o_les, logexpsum, mask=need_store)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "642286f8-7985-4422-953a-115a0ee20cb0"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_gla_bwd_kernel(q, k, v, gk, gv, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', REVERSE:\n    'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * DK if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if REVERSE else 0)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[:, None] & mask_bv[None, :]\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            h = h * _gk[:, None]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            h = h * _gv[None, :]\n        h += _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += -DK if REVERSE else DK\n        p_v += -DV if REVERSE else DV\n        p_q += -DK if REVERSE else DK\n        p_do += -DV if REVERSE else DV\n        p_dq += -DK if REVERSE else DK\n        if USE_GK:\n            p_gk += -DK if REVERSE else DK\n        if USE_GV:\n            p_gv += -DV if REVERSE else DV\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        not REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        not REVERSE else 0)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        not REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        not REVERSE else 0)\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * DK if not REVERSE else 0)\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * DV if not REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if not REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if not REVERSE else 0)\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            d_h *= _gk[:, None]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            d_h *= _gv[None, :]\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do += DV if REVERSE else -DV\n        p_q += DK if REVERSE else -DK\n        p_k += DK if REVERSE else -DK\n        p_v += DV if REVERSE else -DV\n        p_dk += DK if REVERSE else -DK\n        p_dv += DV if REVERSE else -DV\n        if USE_GK:\n            p_gk += DK if REVERSE else -DK\n        if USE_GV:\n            p_gv += DV if REVERSE else -DV\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "107ccbdf-a100-4048-9ace-7ace1bf69a9d"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_fused_multi_pass(output_ptr, a_ptr, weight_ptr,\n    bias_ptr, mean_ptr, rstd_ptr, output_row_stride, output_col_stride,\n    a_row_stride, a_col_stride, N_SIZE, eps, IS_RMSNORM: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Implementation from triton tutorial:\n    https://github.com/openai/triton/blob/master/python/tutorials/05-layer-norm.py\n    It requires multiple passes on the data to compute mean and variance, it is slower than the single pass version.\n    -> only used in benchmarks\n    \"\"\"\n    row_idx = tl.program_id(0)\n    row_off = row_idx * a_row_stride\n    block_range_offs = tl.arange(0, BLOCK_N_SIZE)\n    mean_acc = tl.zeros((BLOCK_N_SIZE,), dtype=tl.float32)\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        cols_offs = block_n_start_idx + block_range_offs\n        a = tl.load(a_ptr + row_off + cols_offs * a_col_stride, mask=\n            cols_offs < N_SIZE, other=0.0, eviction_policy='evict_last')\n        mean_acc += a\n    mean = tl.sum(mean_acc, axis=0) / N_SIZE\n    var_acc = tl.zeros((BLOCK_N_SIZE,), dtype=tl.float32)\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        cols_offs = block_n_start_idx + block_range_offs\n        a = tl.load(a_ptr + row_off + cols_offs * a_col_stride, mask=\n            cols_offs < N_SIZE, other=0.0, eviction_policy='evict_last')\n        a = tl.where(cols_offs < N_SIZE, a - mean, 0.0)\n        var_acc += a * a\n    var = tl.sum(var_acc, axis=0) / N_SIZE\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(mean_ptr + row_idx, mean)\n    tl.store(rstd_ptr + row_idx, rstd)\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        cols_offs = block_n_start_idx + tl.arange(0, BLOCK_N_SIZE)\n        mask_ptr = cols_offs < N_SIZE\n        weight = tl.load(weight_ptr + cols_offs, mask=mask_ptr)\n        bias = tl.load(bias_ptr + cols_offs, mask=mask_ptr)\n        a = tl.load(a_ptr + row_off + cols_offs * a_col_stride, mask=\n            mask_ptr, other=0.0, eviction_policy='evict_first')\n        a_hat = (a - mean) * rstd\n        output = a_hat * weight + bias\n        tl.store(output_ptr + row_idx * output_row_stride + cols_offs *\n            output_col_stride, output, mask=mask_ptr)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "81613844-73ea-4962-8554-ef7b15eab650"
  },
  {
    "input": "@triton.jit\ndef blockwise_barrier(signal_pad_ptrs, block_id, RANK: 'tl.constexpr',\n    WORLD_SIZE: 'tl.constexpr'):\n    \"\"\"\n    A general purpose, efficient, CUDA graph-friendly multi-device barrier.\n\n    CUDA graph friendliness:\n\n        This barrier operates through atomic operations on a zero-filled signal\n        pad, which resets to a zero-filled state after each successful\n        synchronization. This design eliminates the need for incrementing a\n        flag from host.\n\n    Memory consistency:\n\n        The barrier ensures a causality order between memory operations issued\n        before the calling kernel across all devices and those issued after the\n        barrier by all threads within the calling kernel. It achieves this\n        through fine-grained acquire and release semantics, which is more\n        efficient than __threadfence_system().\n\n        To additionally ensure writes issued within the calling kernel across\n        all devices are visible by all threads in the calling kernel after the\n        barrier, a __threadfence_block() is required before the barrier.\n\n    Psuedo code:\n\n        if (warpid == 0 && laneid < world_size) {\n            int remote_rank = laneid;\n            cas &signal_pad_ptrs[remote_rank][block_id * world_size + rank] from 0 to 1 until succeed;\n            cas &signal_pad_ptrs[rank][block_id * world_size + remote_rank] from 1 to 0 until succeed;\n        }\n        __syncthread()\n\n        for (int remote_rank = 0; remote_rank < world_size; ++remote_rank) {\n            acquire &signal_pad_ptrs[remote_rank][blockIdx.x * world_size + rank];\n        }\n    \"\"\"\n    if block_id is None:\n        block_id = tl.program_id(2) * tl.num_programs(1) * tl.num_programs(0\n            ) + tl.program_id(1) * tl.num_programs(0) + tl.program_id(0)\n    flat_tid = get_flat_tid()\n    remote_ranks = tl.arange(0, WORLD_SIZE)\n    signal_pad_ptrs = signal_pad_ptrs\n    remote_signal_pad_addrs = tl.load(signal_pad_ptrs + remote_ranks)\n    send_addrs = remote_signal_pad_addrs + block_id * WORLD_SIZE + RANK\n    local_signal_pad_addr = tl.load(signal_pad_ptrs + RANK)\n    wait_addrs = local_signal_pad_addr + block_id * WORLD_SIZE + remote_ranks\n    if flat_tid < WORLD_SIZE:\n        tl.inline_asm_elementwise(\n            \"\"\"\n            {\n                .reg .u32   %tmp32_<1>;\n                .reg .pred  %p<1>;\n\n                send_signal:\n                    atom.global.release.sys.cas.b32 %tmp32_0, [$1], 0, 1;\n                    setp.eq.u32 %p0, %tmp32_0, 0;\n                    @!%p0 bra send_signal;\n\n                wait_signal:\n                    // No need to acquire here since all threads will\n                    // acquire this location after the barrier.\n                    atom.global.sys.cas.b32 %tmp32_0, [$2], 1, 0;\n                    setp.eq.u32 %p0, %tmp32_0, 1;\n                    @!%p0 bra wait_signal;\n\n                barrier_end:\n            }\n            \"\"\"\n            , '=r, l, l', [send_addrs, wait_addrs], dtype=tl.int32, is_pure\n            =False, pack=1)\n    tl.inline_asm_elementwise('bar.sync 0;', '=r', [], dtype=tl.int32,\n        is_pure=False, pack=1)\n    for remote_rank in range(WORLD_SIZE):\n        tl.inline_asm_elementwise('ld.acquire.sys.global.u32 $0, [$1];',\n            '=r, l', [local_signal_pad_addr + remote_rank], dtype=tl.int32,\n            is_pure=False, pack=1)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "00ac31cb-5255-4959-a6da-7586b53b434d"
  },
  {
    "input": "@triton.jit\ndef _weighted_rms_norm_bwd_dx(DX, DY, DW, X, W, Rstd, Lock, stride_dx,\n    stride_dy, stride_x, D, eps, GROUP_N, BLOCK_D: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    X += row * stride_x\n    DY += row * stride_dy\n    DX += row * stride_dx\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    rstd = tl.load(Rstd + row)\n    xhat = x * rstd\n    w = tl.load(W + cols, mask=mask)\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / D\n    dx = (wdy - xhat * c1) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    lock_id = row % GROUP_N\n    Lock += lock_id\n    Count = Lock + GROUP_N\n    DW = DW + lock_id * D + cols\n    partial_dw = dy * xhat\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "25829083-999b-4007-8faa-bfd0440b0f45"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['hdim', 'dstate', 'chunk_size'])\n@triton.jit\ndef _chunk_state_fwd_kernel(x_ptr, b_ptr, states_ptr, dt_ptr, dA_cumsum_ptr,\n    seq_idx_ptr, hdim, dstate, chunk_size, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head,\n    stride_b_dstate, stride_states_batch, stride_states_chunk,\n    stride_states_head, stride_states_hdim, stride_states_dstate,\n    stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    HAS_SEQ_IDX: 'tl.constexpr', REVERSE: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim + offs_k[None, :] *\n        stride_x_seqlen)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate + offs_k[:, None] *\n        stride_b_seqlen)\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    if not REVERSE:\n        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) *\n            stride_dA_cs_csize)\n    else:\n        dA_cs_last = tl.load(dA_cumsum_ptr)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    if HAS_SEQ_IDX:\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :\n            ] < chunk_size_limit - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) &\n            (offs_n[None, :] < dstate), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size_limit -\n            k, other=0.0)\n        if HAS_SEQ_IDX:\n            seq_idx_k = tl.load(seq_idx_ptrs, mask=offs_k < \n                chunk_size_limit - k, other=-1)\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0)\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_last - dA_cs_k) * dt_k\n        else:\n            scale = tl.where(seq_idx_k == seq_idx_last, tl.exp(dA_cs_last -\n                dA_cs_k) * dt_k, 0.0)\n        b *= scale[:, None]\n        b = b\n        acc += tl.dot(x, b)\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n        if HAS_SEQ_IDX:\n            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\n    states = acc\n    states_ptr += (pid_b * stride_states_batch + pid_c *\n        stride_states_chunk + pid_h * stride_states_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    states_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim + \n        offs_n[None, :] * stride_states_dstate)\n    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\n    tl.store(states_ptrs, states, mask=c_mask)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "8215c9f6-4c50-4719-9a84-6fe4156faa05"
  },
  {
    "input": "@triton.jit\ndef leaky_relu(x):\n    \"\"\"\n    LeakyReLU_ activation\n\n    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html\n    \"\"\"\n    scale = 0.01 + 0.0\n    scale = scale\n    return tl.where(x >= 0, x, scale * x)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "33cc8e4c-7149-4b82-aefa-abe853a1a96c"
  },
  {
    "input": "@triton.jit\ndef srms_norm_fw(X, Y, V, stride, N, eps, BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    x_ptrs = X + row * stride + cols\n    x = tl.load(x_ptrs, mask=mask, other=0.0)\n    x_zm = tl.where(mask, x, 0.0)\n    x_var = tl.sum(x_zm * x_zm, axis=0) / N\n    rstd = 1.0 / tl.sqrt(x_var + eps)\n    y = x_zm * rstd\n    tl.store(V + row, rstd)\n    y_ptrs = Y + row * stride + cols\n    tl.store(y_ptrs, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "21d5adad-a168-440a-b143-501a692aaa43"
  },
  {
    "input": "@triton.jit\ndef dot_kernel(x_ptr, y_ptr, z_ptr, N0, N1, N2, MID, B0: 'tl.constexpr', B1:\n    'tl.constexpr', B2: 'tl.constexpr', B_MID: 'tl.constexpr'):\n    block_id_j = tl.program_id(0)\n    block_id_k = tl.program_id(1)\n    block_id_i = tl.program_id(2)\n    off_i = block_id_i * B2 + tl.arange(0, B2)\n    off_j = block_id_j * B0 + tl.arange(0, B0)\n    off_k = block_id_k * B1 + tl.arange(0, B1)\n    mask_i = off_i < N2\n    mask_j = off_j < N0\n    mask_k = off_k < N1\n    z = tl.zeros((B2, B0, B1), dtype=tl.float32)\n    off_z = off_i[:, None, None] * N0 * N1 + off_j[None, :, None] * N1 + off_k[\n        None, None, :]\n    mask_z = mask_i[:, None, None] & mask_j[None, :, None] & mask_k[None,\n        None, :]\n    for l in tl.range(0, MID, B_MID):\n        off_l = l + tl.arange(0, B_MID)\n        mask_l = off_l < MID\n        off_x = off_i[:, None, None] * N0 * N1 + off_j[None, :, None\n            ] * MID + off_l[None, None, :]\n        off_y = off_i[:, None, None] * N0 * N1 + off_l[None, :, None\n            ] * N1 + off_k[None, None, :]\n        mask_x = mask_i[:, None, None] & mask_j[None, :, None] & mask_l[\n            None, None, :]\n        mask_y = mask_i[:, None, None] & mask_l[None, :, None] & mask_k[\n            None, None, :]\n        x = tl.load(x_ptr + off_x, mask=mask_x)\n        y = tl.load(y_ptr + off_y, mask=mask_y)\n        z += tl.dot(x, y)\n    tl.store(z_ptr + off_z, z, mask=mask_z)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "3beb0800-fbae-47f8-82a5-47513091ffb9"
  },
  {
    "input": "@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'\n    ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    None\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "35ba598e-84c3-4eee-82f8-446ed408711e"
  },
  {
    "input": "@triton.jit\ndef tr2(X, dX, dY):\n    r = tl.arange(0, 16)\n    r2 = tl.arange(0, 16)[:, None]\n    x = tl.load(X + r)\n    dy = tl.load(dY + 16 * r2 + r)\n    tl.static_print('shape', dy.shape)\n    dx = dcomp2dx(x, dy)\n    tl.static_print('shape', dx.shape)\n    tl.store(dX + r, dx)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d775459d-7564-4635-a710-f8c9598298ae"
  },
  {
    "input": "@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'\n    ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "4cb7e72c-65f5-4829-b5aa-0d62dae3b606"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ff3a57bb-cc27-48a4-bad1-69363213bc95"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size},\n    num_warps=num_warps) for block_size, num_warps in itertools.product([32,\n    64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=[\n    'length', 'kernel_size', 'stride', 'n_frames'])\n@triton.jit\ndef unfold_kernel(input_ptr, output_ptr, length, kernel_size, stride,\n    n_frames, BLOCK_SIZE: 'tl.constexpr'):\n    batch_idx = tl.program_id(0)\n    frame_idx = tl.program_id(1) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = frame_idx < n_frames\n    input_pos = frame_idx * stride\n    for i in range(kernel_size):\n        in_bounds = mask & (input_pos + i < length)\n        val = tl.where(in_bounds, tl.load(input_ptr + batch_idx * length +\n            input_pos + i, mask=in_bounds), 0)\n        out_idx = (batch_idx * n_frames * kernel_size + frame_idx *\n            kernel_size + i)\n        tl.store(output_ptr + out_idx, val, mask=in_bounds)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "717a9a9e-e526-4f19-b3ee-ccb82d3e009e"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_K(q, k, h, g, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_A += tl.dot(b_q, b_k, allow_tf32=False)\n    p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o = b_o * tl.exp(b_g)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "17992de1-b176-4f88-9db1-0ac54efbfc8e"
  },
  {
    "input": "@triton.jit\ndef _rotary_kernel(Q, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_cosbs, stride_cosd, stride_sinbs, stride_sind, max_total_len, H,\n    BLOCK_HEAD: 'tl.constexpr', BLOCK_SEQ: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr'):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2)\n    dim_range1 = tl.arange(BLOCK_DMODEL // 2, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    return\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "461b160c-47e7-410f-83a9-f4b4e0ad824e"
  },
  {
    "input": "@triton.jit\ndef __flat_csr_sdbmm_tch_compute(CROW_INDICES, stride_crow_n, stride_crow_r,\n    COL_INDICES, stride_col_n, stride_col_z, VALUES, stride_v_n, stride_v_z,\n    OTHER, stride_other_n, stride_other_h, stride_other_t, stride_other_d,\n    OUTPUT, stride_output_n, stride_output_h, stride_output_t,\n    stride_output_d, TEMP_COUNT_HEAD, stride_tch_n, stride_tch_r,\n    stride_tch_h, N, R, Z, H, T_DST, T_SRC, HID, MAX_ROW_Z: 'tl.constexpr',\n    MAX_ROW_T: 'tl.constexpr', BLOCK_HID: 'tl.constexpr', BLOCK_H:\n    'tl.constexpr', BLOCK_R: 'tl.constexpr', BLOCK_COL_HEAD: 'tl.constexpr',\n    GRID_COL_HEAD: 'tl.constexpr'):\n    n = tl.program_id(0)\n    pid_ir = tl.program_id(1)\n    grid_ir = tl.num_programs(1)\n    for _ir in range(BLOCK_R):\n        ir = _ir * grid_ir + pid_ir\n        ir_mask = ir < R\n        crow_start = tl.load(CROW_INDICES + n * stride_crow_n + ir *\n            stride_crow_r, mask=ir_mask)\n        crow_end = tl.load(CROW_INDICES + n * stride_crow_n + (ir + 1) *\n            stride_crow_r, mask=ir_mask)\n        count_heads_sum = tl.zeros((BLOCK_H,), dtype=tl.int32)\n        for i in range(GRID_COL_HEAD):\n            _col_indices = tl.load(COL_INDICES + n * stride_col_n + (tl.\n                arange(0, BLOCK_COL_HEAD) + crow_start + i * BLOCK_COL_HEAD\n                ) * stride_col_z, mask=(tl.arange(0, BLOCK_COL_HEAD) +\n                crow_start + i * BLOCK_COL_HEAD < crow_end) & ir_mask,\n                other=T_SRC * BLOCK_H * 2)\n            t = _col_indices // T_SRC\n            count_heads_sum += tl.sum(t[None, :] == tl.arange(0, BLOCK_H)[:,\n                None], axis=1)\n        count_heads_cumsum = tl.cumsum(count_heads_sum)\n        tl.store(TEMP_COUNT_HEAD + n * stride_tch_n + ir * stride_tch_r + \n            tl.arange(0, BLOCK_H) * stride_tch_h, value=count_heads_cumsum,\n            mask=(tl.arange(0, BLOCK_H) < H) & ir_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "5c339c9a-7721-4ef6-91ad-a7f7edb33d5e"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, Mean, Rstd, Lock,\n    stride, N, eps, GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    c2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    partial_db = dy\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "5934be1b-e073-480d-896d-f77a3fa602e5"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_fwd(C, ACT_INPUT, A, B, bias, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bn, stride_bk,\n    BLOCK_M: 'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', A_ROWMAJOR: 'tl.constexpr', B_COLMAJOR:\n    'tl.constexpr', BIAS: 'tl.constexpr', SAVE_ACT_INPUT: 'tl.constexpr',\n    ACTIVATION: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Bias has shape (N,)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    if A_ROWMAJOR:\n        A = A + (ram[:, None] * stride_am + rk[None, :])\n    else:\n        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    if B_COLMAJOR:\n        B = B + (rk[:, None] + rbn[None, :] * stride_bn)\n    else:\n        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        if A_ROWMAJOR:\n            A += BLOCK_K\n        else:\n            A += BLOCK_K * stride_ak\n        if B_COLMAJOR:\n            B += BLOCK_K\n        else:\n            B += BLOCK_K * stride_bk\n    if BIAS:\n        bias = tl.load(bias + rn, mask=rn < N, other=0.0)\n        acc += bias[None, :]\n    if SAVE_ACT_INPUT:\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        tl.store(act_in_ptrs, acc)\n    if ACTIVATION == 'gelu':\n        acc = gelu(acc)\n    elif ACTIVATION == 'gelu_approx':\n        acc = gelu_approx(acc)\n    elif ACTIVATION == 'squared_relu':\n        acc = squared_relu(acc)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "360f9993-9354-4c20-a7ee-37d248fbdeb4"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_delta_rule_bwd_kernel(q, k, v, beta, h0, dh0, dht, do,\n    dq, dk, dv, db, scale, B: 'tl.constexpr', T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NK: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', IS_BETA_HEADWISE: 'tl.constexpr',\n    USE_DH0: 'tl.constexpr', USE_DHT: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_dk = dk + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK\n            ) + (T - 1) * K\n        p_dv = dv + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV\n            ) + (T - 1) * V\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1\n                ) * V\n            p_dbeta = db + (i_v * NK * B * H + i_k * B * H + i_bh\n                ) * T * V + tl.arange(0, BV) + (T - 1) * V\n        else:\n            p_beta = beta + i_bh * T + T - 1\n            p_dbeta = db + (i_v * B * H + i_bh) * T + T - 1\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T\n             - 1) * H * V\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (\n            T - 1) * H * V\n        p_dk = dk + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T - 1\n            ) * H * K\n        p_dv = dv + (i_k * B + i_b\n            ) * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T - 1\n            ) * H * V\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(\n                0, BV) + (T - 1) * H * V\n            p_dbeta = db + (i_v * NK * B + i_k * B + i_b\n                ) * T * H * V + i_h * V + tl.arange(0, BV) + (T - 1) * H * V\n        else:\n            p_beta = beta + i_b * T * H + (T - 1) * H + i_h\n            p_dbeta = db + (i_v * B + i_b) * T * H + (T - 1) * H + i_h\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        d_h += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_do = tl.load(p_do, mask=mask_v, other=0)\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0)\n        else:\n            b_beta = tl.load(p_beta)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * (b_v * b_beta)[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n        d_beta = d_v * b_v if IS_BETA_HEADWISE else tl.sum(d_v * b_v)\n        d_v = d_v * b_beta\n        tl.store(p_dk, d_k, mask=mask_k)\n        tl.store(p_dv, d_v, mask=mask_v)\n        if IS_BETA_HEADWISE:\n            tl.store(p_dbeta, d_beta, mask=mask_v)\n        else:\n            tl.store(p_dbeta, d_beta)\n        d_h -= b_k[:, None] * d_v[None, :]\n        p_q -= K if HEAD_FIRST else H * K\n        p_k -= K if HEAD_FIRST else H * K\n        p_v -= V if HEAD_FIRST else H * V\n        p_do -= V if HEAD_FIRST else H * V\n        p_dk -= K if HEAD_FIRST else H * K\n        p_dv -= V if HEAD_FIRST else H * V\n        p_dbeta -= (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n        p_beta -= (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n    if USE_DH0:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, d_h, mask=mask_k[:, None] & mask_v[None, :])\n    tl.debug_barrier()\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        else:\n            p_beta = beta + i_bh * T\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK)\n        p_dk = dk + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK)\n        p_dv = dv + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(\n                0, BV)\n        else:\n            p_beta = beta + i_b * T * H + i_h\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_dk = dk + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_dv = dv + (i_k * B + i_b\n            ) * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n    if USE_INITIAL_STATE:\n        mask_h = mask_k[:, None] & mask_v[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_h, other=0)\n    for _ in range(0, T):\n        d_k = tl.load(p_dk, mask=mask_k, other=0)\n        d_v = tl.load(p_dv, mask=mask_v, other=0)\n        d_k -= tl.sum(d_v[None, :] * b_h, axis=1)\n        tl.store(p_dk, d_k, mask=mask_k)\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_do = tl.load(p_do, mask=mask_v, other=0)\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0)\n        else:\n            b_beta = tl.load(p_beta)\n        b_v *= b_beta\n        b_h += b_k[:, None] * b_v[None, :]\n        b_dq = b_h * b_do[None, :]\n        d_q = tl.sum(b_dq, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_k)\n        p_k += K if HEAD_FIRST else H * K\n        p_v += V if HEAD_FIRST else H * V\n        p_do += V if HEAD_FIRST else H * V\n        p_dq += K if HEAD_FIRST else H * K\n        p_dk += K if HEAD_FIRST else H * K\n        p_dv += V if HEAD_FIRST else H * V\n        p_beta += (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "a00554a8-99dd-4d46-91a6-6cadc16f6670"
  },
  {
    "input": "@triton.jit\ndef _ragged_hstu_attn_bwd_one_col_block(start_n, seq_len, n_targets, Q, K,\n    V, TS, TW, PW, Bias, DOut, DQ, DK, DV, DBias, DTW, DPW, LOCK, stride_qm,\n    stride_kn, stride_vn, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n    alpha, MAX_SEQ_LEN, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, MAX_ATTN_LEN: 'tl.constexpr',\n    INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL: 'tl.constexpr', BUCKET_FN:\n    'tl.constexpr', ATTN_BIAS_TYPE: 'tl.constexpr', USE_TIME_BIAS:\n    'tl.constexpr', USE_POS_BIAS: 'tl.constexpr', FUSED_BIAS_BWD:\n    'tl.constexpr', HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS:\n    'tl.constexpr', CONTEXTUAL_SEQ_LEN: 'tl.constexpr', ALLOW_TF32:\n    'tl.constexpr', BLOCK_D_Q: 'tl.constexpr', BLOCK_D_V: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', UNROLL:\n    'tl.constexpr', ATOMIC_ADD: 'tl.constexpr'):\n    if INVALID_MASK_TYPE == 'lower_triangular':\n        if HAS_MULTIPLE_TARGETS:\n            low = start_n\n            if MAX_ATTN_LEN > 0:\n                high = start_n + MAX_ATTN_LEN + BLOCK_N\n                high = high if high + n_targets < seq_len else seq_len\n            else:\n                high = seq_len\n        else:\n            low = start_n\n            if MAX_ATTN_LEN > 0:\n                high = start_n + MAX_ATTN_LEN + BLOCK_N\n                high = high if high < seq_len else seq_len\n            else:\n                high = seq_len\n        if CONTEXTUAL_SEQ_LEN > 0:\n            contextual_block_end = tl.cdiv(CONTEXTUAL_SEQ_LEN, BLOCK_M\n                ) * BLOCK_M\n            if low < contextual_block_end:\n                low = contextual_block_end\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        low = 0\n        high = start_n + BLOCK_N\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_qk_d = tl.arange(0, BLOCK_D_Q)\n    offs_v_d = tl.arange(0, BLOCK_D_V)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    q_ptrs_trans = Q + (offs_m[None, :] * stride_qm + offs_qk_d[:, None])\n    dq_ptrs_trans = DQ + (offs_m[None, :] * stride_dqm + offs_qk_d[:, None])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_qk_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_v_d[None, :])\n    mask_n = offs_n < seq_len\n    ts_0_ptrs = None\n    ts_1_ptrs = None\n    ts_1 = None\n    off_bias_trans = None\n    bias_ptrs_trans = None\n    dbias_ptrs_trans = None\n    if ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS:\n        ts_0_ptrs = TS + offs_m\n        ts_1_ptrs = TS + offs_n\n        if CAUSAL:\n            ts_1 = tl.load(ts_1_ptrs, mask=mask_n)\n        else:\n            ts_1 = tl.load(ts_1_ptrs + 1, mask=mask_n)\n    elif ATTN_BIAS_TYPE == 'separate':\n        off_bias_trans = offs_m[None, :] * seq_len + offs_n[:, None]\n        bias_ptrs_trans = Bias + off_bias_trans\n        dbias_ptrs_trans = DBias + off_bias_trans\n    do_ptrs = DOut + (offs_m[:, None] * stride_dom + offs_v_d[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_D_V], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_D_Q], dtype=tl.float32)\n    k = tl.load(k_ptrs, mask=mask_n[:, None], other=0.0)\n    v = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0)\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            pos_offs_n = tl.where(offs_n < seq_len - n_targets, offs_n, \n                seq_len - n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            pos_offs_n = tl.where(offs_n > n_targets - 1, offs_n, n_targets - 1\n                )\n    else:\n        pos_offs_n = offs_n\n    if CONTEXTUAL_SEQ_LEN > 0 and INVALID_MASK_TYPE == 'lower_triangular':\n        for start_m in range(0, CONTEXTUAL_SEQ_LEN, BLOCK_M):\n            start_m = tl.multiple_of(start_m, BLOCK_M)\n            dk, dv = _ragged_hstu_attn_bwd_one_block(start_m=start_m,\n                offs_n=offs_n, offs_m=offs_m, q_ptrs_trans=q_ptrs_trans,\n                dq_ptrs_trans=dq_ptrs_trans, mask_n=mask_n, ts_0_ptrs=\n                ts_0_ptrs, ts_1=ts_1, bias_ptrs_trans=bias_ptrs_trans,\n                dbias_ptrs_trans=dbias_ptrs_trans, do_ptrs=do_ptrs, dk=dk,\n                dv=dv, k=k, v=v, pos_offs_n=pos_offs_n, seq_len=seq_len,\n                n_targets=n_targets, TW=TW, PW=PW, DTW=DTW, DPW=DPW, LOCK=\n                LOCK, stride_qm=stride_qm, stride_dom=stride_dom,\n                stride_dqm=stride_dqm, alpha=alpha, MAX_SEQ_LEN=MAX_SEQ_LEN,\n                num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n                MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n                time_bucket_incr, time_bucket_div=time_bucket_div,\n                time_delta=time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, FUSED_BIAS_BWD=FUSED_BIAS_BWD,\n                HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n                HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n                ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,\n                ATOMIC_ADD=ATOMIC_ADD)\n    for start_m in tl.range(low, high, BLOCK_M, loop_unroll_factor=UNROLL):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        dk, dv = _ragged_hstu_attn_bwd_one_block(start_m=start_m, offs_n=\n            offs_n, offs_m=offs_m, q_ptrs_trans=q_ptrs_trans, dq_ptrs_trans\n            =dq_ptrs_trans, mask_n=mask_n, ts_0_ptrs=ts_0_ptrs, ts_1=ts_1,\n            bias_ptrs_trans=bias_ptrs_trans, dbias_ptrs_trans=\n            dbias_ptrs_trans, do_ptrs=do_ptrs, dk=dk, dv=dv, k=k, v=v,\n            pos_offs_n=pos_offs_n, seq_len=seq_len, n_targets=n_targets, TW\n            =TW, PW=PW, DTW=DTW, DPW=DPW, LOCK=LOCK, stride_qm=stride_qm,\n            stride_dom=stride_dom, stride_dqm=stride_dqm, alpha=alpha,\n            MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=num_buckets, max_pos_ind=\n            max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n            time_bucket_incr, time_bucket_div=time_bucket_div, time_delta=\n            time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL,\n            BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n            USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=USE_POS_BIAS,\n            FUSED_BIAS_BWD=FUSED_BIAS_BWD, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n            HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=\n            CONTEXTUAL_SEQ_LEN, ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N, ATOMIC_ADD=ATOMIC_ADD)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_v_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_qk_d[None, :])\n    dk = dk * alpha\n    tl.store(dv_ptrs, dv, mask=mask_n[:, None])\n    tl.store(dk_ptrs, dk, mask=mask_n[:, None])\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "bca7cdc6-31a1-4fb1-885d-cfd4bcdf9c21"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256,\n    'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE':\n    64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({\n    'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.\n    Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}),\n    triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE':\n    512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64,\n    'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256,\n    'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({\n    'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton\n    .Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})],\n    key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])\n@triton.jit\ndef linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, losses_ptr,\n    lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V:\n    'tl.constexpr', N: 'tl.constexpr', H: 'tl.constexpr', V_BLOCK_SIZE:\n    'tl.constexpr', N_BLOCK_SIZE: 'tl.constexpr', H_BLOCK_SIZE: 'tl.constexpr'\n    ):\n    idx = tl.program_id(axis=0)\n    tl.static_assert(N % N_BLOCK_SIZE == 0)\n    tl.static_assert(V % V_BLOCK_SIZE == 0)\n    tl.static_assert(H % H_BLOCK_SIZE == 0)\n    x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(\n        stride_x_N, stride_x_H), offsets=(idx * N_BLOCK_SIZE, 0),\n        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(\n        stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE,\n        V_BLOCK_SIZE), order=(1, 0))\n    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + offsets)\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(1000000.0)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n    for _ in range(V // V_BLOCK_SIZE):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        local_x_block_ptr = x_block_ptr\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(local_x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE]\n                )\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n        mask = y[:, None] == v_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n        m = m_new\n        A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H //\n            H_BLOCK_SIZE), V_BLOCK_SIZE])\n        v_range = v_range + V_BLOCK_SIZE\n    lse = m + tl.log(s)\n    loss += tl.sum(lse) / N\n    tl.store(losses_ptr + idx, loss)\n    tl.store(lse_ptr + offsets, lse)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "14388fd5-f419-4fbc-ae28-9de1aaa714a8"
  },
  {
    "input": "@triton.jit\ndef gelu(input):\n    \"\"\"\n    Applies GELU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by GELU.\n    \"\"\"\n    cdf = 0.5 * (1 + tl.math.erf(0.707106781 * input))\n    return cdf * input\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "cb11adbf-8a4a-415b-b071-89ace2670721"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['BLOCK_A', 'MAX_NCOLS'])\n@triton.jit\ndef __compact_cols_compute(NCOLS_CS, stride_ncols_cs_n, stride_ncols_cs_a,\n    N, A, COL_INDICES, stride_col_indices_n, stride_col_indices_a,\n    stride_col_indices_mz, OUT_COL_INDICES, stride_out_col_indices_n,\n    stride_out_col_indices_z, MAX_NCOLS: 'tl.constexpr', BLOCK_A:\n    'tl.constexpr'):\n    n = tl.program_id(0)\n    pid_a = tl.program_id(1)\n    for ia in range(BLOCK_A):\n        a = pid_a * BLOCK_A + ia\n        mask_a = a < A\n        cs_start = tl.load(NCOLS_CS + n * stride_ncols_cs_n + a *\n            stride_ncols_cs_a, mask=mask_a)\n        cs_end = tl.load(NCOLS_CS + n * stride_ncols_cs_n + (a + 1) *\n            stride_ncols_cs_a, mask=mask_a)\n        cs_len = cs_end - cs_start\n        col_indices = tl.load(COL_INDICES + n * stride_col_indices_n + a *\n            stride_col_indices_a + tl.arange(0, MAX_NCOLS), mask=(tl.arange\n            (0, MAX_NCOLS) < cs_len) & mask_a)\n        tl.store(OUT_COL_INDICES + n * stride_out_col_indices_n + (tl.\n            arange(0, MAX_NCOLS) + cs_start) * stride_out_col_indices_z,\n            col_indices, mask=(tl.arange(0, MAX_NCOLS) < cs_len) & mask_a)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "ff6e5ba9-322c-430d-bdc1-3c580dd8c746"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dw(DW, FINAL_DW, M, N, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "799d95bb-022a-4a4b-84fb-52faf756aab1"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, M_in, Lse_in, O_in, Lse, M_out, TMP,\n    softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh,\n    stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh,\n    stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k,\n    seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K,\n    BIAS_TYPE: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lin_ptrs = Lse_in + off_hb * seqlen_q_rounded + offs_m\n    acc_o_ptrs = O_in + off_b * stride_qb + off_h * stride_qh + (offs_m[:,\n        None] * stride_qm + offs_d[None, :])\n    lse_i = tl.load(lin_ptrs)\n    m_ptrs = M_in + off_hb * seqlen_q_rounded + offs_m\n    m_i = tl.load(m_ptrs)\n    acc_o = tl.load(acc_o_ptrs)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    m_ptrs = M_out + off_hb * seqlen_q_rounded + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "9deef46f-d958-4429-a4db-8450a0856b26"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_dquant_kernel(X, Y, W, B, out, scale, stride, N, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    out += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    _max_x = 0.0\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        _norm = (x - mean) * rstd * w + b\n        tl.store(out + cols, _norm, mask=mask)\n        _max_x = tl.maximum(_max_x, tl.max(tl.abs(_norm), axis=0))\n    scale_x = _max_x / 127.0\n    tl.store(scale + row, scale_x)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        _norm = tl.load(out + cols, mask=mask, other=0.0)\n        _norm = _norm / scale_x + 0.5\n        tl.store(Y + cols, _norm, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "46b0fd13-c9d6-445a-b64b-9856f0bbfd07"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'NUM_SM': 84}), triton.Config(\n    {'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'NUM_SM':\n    128}), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32, 'NUM_SM': 84}), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'NUM_SM': 128})], key=[\n    'group_size'])\n@triton.jit\ndef grouped_matmul_kernel(group_a_ptrs, group_b_ptrs, group_c_ptrs,\n    group_gemm_sizes, g_lds, group_size, NUM_SM: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    tile_idx = tl.program_id(0)\n    last_problem_end = 0\n    for g in range(group_size):\n        gm = tl.load(group_gemm_sizes + g * 3)\n        gn = tl.load(group_gemm_sizes + g * 3 + 1)\n        gk = tl.load(group_gemm_sizes + g * 3 + 2)\n        num_m_tiles = tl.cdiv(gm, BLOCK_SIZE_M)\n        num_n_tiles = tl.cdiv(gn, BLOCK_SIZE_N)\n        num_tiles = num_m_tiles * num_n_tiles\n        while (tile_idx >= last_problem_end and tile_idx < last_problem_end +\n            num_tiles):\n            k = gk\n            lda = tl.load(g_lds + g * 3)\n            ldb = tl.load(g_lds + g * 3 + 1)\n            ldc = tl.load(g_lds + g * 3 + 2)\n            a_ptr = tl.load(group_a_ptrs + g)\n            b_ptr = tl.load(group_b_ptrs + g)\n            c_ptr = tl.load(group_c_ptrs + g)\n            tile_idx_in_gemm = tile_idx - last_problem_end\n            tile_m_idx = tile_idx_in_gemm // num_n_tiles\n            tile_n_idx = tile_idx_in_gemm % num_n_tiles\n            offs_am = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_bn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            offs_k = tl.arange(0, BLOCK_SIZE_K)\n            a_ptrs = a_ptr + offs_am[:, None] * lda + offs_k[None, :]\n            b_ptrs = b_ptr + offs_k[:, None] * ldb + offs_bn[None, :]\n            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.\n                float32)\n            for kk in range(0, tl.cdiv(k, BLOCK_SIZE_K)):\n                tl.multiple_of(a_ptrs, [16, 16])\n                tl.multiple_of(b_ptrs, [16, 16])\n                a = tl.load(a_ptrs)\n                b = tl.load(b_ptrs)\n                accumulator += tl.dot(a, b)\n                a_ptrs += BLOCK_SIZE_K\n                b_ptrs += BLOCK_SIZE_K * ldb\n            c = accumulator\n            offs_cm = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            offs_cn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n            c_ptrs = c_ptr + ldc * offs_cm[:, None] + offs_cn[None, :]\n            tl.store(c_ptrs, c)\n            tile_idx += NUM_SM\n        last_problem_end = last_problem_end + num_tiles\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4f7cf632-113c-437d-8ac0-3da804d86cfb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "faf20891-2bd0-463f-a226-6d374f12b5c9"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64,\n    'BLOCK_DMODEL': 64}, num_stages=3, num_warps=4)], key=['num_queries'])\n@triton.jit\ndef triton_tem_fused_with_exp2(arg_Q, arg_K, arg_V, out_ptr0, num_queries:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr'):\n    SCORE_MOD_IS_LINEAR: 'tl.constexpr' = False\n    ROWS_GUARANTEED_SAFE: 'tl.constexpr' = False\n    Q = arg_Q\n    K = arg_K\n    V = arg_V\n    stride_qz = 4194304\n    stride_qh = 262144\n    stride_qm = 64\n    stride_qk = 1\n    stride_kz = 4194304\n    stride_kh = 262144\n    stride_kn = 64\n    stride_kk = 1\n    stride_vz = 4194304\n    stride_vh = 262144\n    stride_vk = 64\n    stride_vn = 1\n    Z = 16\n    H = 16\n    N_CTX = 4096\n    qk_scale = 1.0\n    MATMUL_PRECISION = Q.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    qkv_offset = off_hz * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qkv_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qkv_offset, shape=(\n        BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n        ), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qkv_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(Q_block_ptr)\n    if SCORE_MOD_IS_LINEAR:\n        qk_scale *= 1.44269504\n    q = q * qk_scale\n    lo = 0\n    hi = N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.dot(q, k, acc=qk)\n        tmp0 = tl.full([1], 1024, tl.int64)\n        tmp1 = offs_m[:, None] <= tmp0\n        tmp2 = start_n + offs_n[None, :] <= tmp0\n        tmp3 = tmp1 & tmp2\n        tmp4 = offs_m[:, None] >= start_n + offs_n[None, :]\n        tmp5 = tmp3 | tmp4\n        tmp6 = float('-inf')\n        tmp7 = tmp6\n        tmp8 = tl.where(tmp5, qk, tmp7)\n        qk = tmp8\n        if not SCORE_MOD_IS_LINEAR:\n            qk *= 1.44269504\n        row_max = tl.max(qk, 1)\n        m_i_new = tl.maximum(m_i, row_max)\n        masked_out_rows = m_i_new == float('-inf')\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        if not ROWS_GUARANTEED_SAFE:\n            alpha = tl.where(masked_out_rows, 0, alpha)\n            p = tl.where(masked_out_rows[:, None], 0, p)\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc = tl.dot(p, v, acc)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    idx_z = tl.program_id(1) // H\n    idx_h = tl.program_id(1) % H\n    idx_m = offs_m[:, None]\n    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]\n    mask = (idx_m != -1) & (idx_d != -1)\n    xindex = idx_d + 64 * idx_m + 262144 * idx_h + 4194304 * idx_z\n    tl.store(out_ptr0 + xindex, acc, None)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "d63d19c4-5132-4dab-b5af-c49002f4b439"
  },
  {
    "input": "@triton.jit\ndef _approx_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    s = 0.7978845608028654\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (tl.math.tanh(s * e_row * (1.0 + 0.044715 * e_row *\n        e_row)) + 1.0)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "exponential",
    "uuid": "6c7f48a9-d171-4e42-852f-7f828a15dc75"
  },
  {
    "input": "@triton.jit\ndef standardize(input, mean, inv_std, weight, bias):\n    \"\"\"\n    Standardizes the input given its mean and inverse standard deviation,\n    multiplies the result by weights, and adds a bias vector.\n\n    Args:\n        input: Input to standardize.\n        mean: Mean of input.\n        inv_std: Inverse standard deviation of input.\n        weight: Weight multiplied by the standardized input.\n        bias: Bias added to the result of the weight multiplication.\n\n    Returns:\n        Standardized input.\n    \"\"\"\n    return weight * inv_std * (input - mean) + bias\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "47dc22f7-284c-45e5-8985-fb937862af29"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=2), triton.Config({},\n    num_warps=4), triton.Config({}, num_warps=8)], key=['S'])\n@triton.jit\ndef softmax_bwd_kernel(p, dp, ds, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_p = tl.make_block_ptr(p + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    p_dp = tl.make_block_ptr(dp + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT, 0), (BT, S), (1, 0))\n    p_ds = tl.make_block_ptr(ds + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT, 0), (BT, S), (1, 0))\n    b_p = tl.load(p_p, boundary_check=(0, 1))\n    b_dp = tl.load(p_dp, boundary_check=(0, 1))\n    b_pp = tl.sum(b_p * b_dp, 1)\n    b_ds = b_p * b_dp - b_p * b_pp[:, None]\n    tl.store(p_ds, b_ds, boundary_check=(0, 1))\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "12593047-1f6d-404c-b9f6-233715268f8b"
  },
  {
    "input": "@triton.jit\ndef _matmul_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', GROUP_M: 'tl.constexpr', epilogue_alpha=None,\n    epilogue_beta=None, epilogue_source=None, acc_dtype: 'tl.constexpr'=tl.\n    float32, allow_tf32: 'tl.constexpr'=True, fp8_fast_accum:\n    'tl.constexpr'=True, AB_DTYPE: 'tl.constexpr'=None, EPILOGUE:\n    'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        if AB_DTYPE is not None:\n            a = a\n            b = b\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        else:\n            acc += tl.dot(a, b, out_dtype=acc_dtype, allow_tf32=allow_tf32)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    if EPILOGUE:\n        if epilogue_alpha is not None:\n            acc = epilogue_alpha * acc\n        if epilogue_source is not None:\n            epilogue_src = tl.load(epilogue_source + rm[:, None] *\n                stride_cm + rn[None, :] * stride_cn)\n            if epilogue_beta is not None:\n                epilogue_src = epilogue_src * epilogue_beta\n            acc = acc + epilogue_src\n    acc = acc\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "86328446-bac7-49c0-b814-41ed1152ab29"
  },
  {
    "input": "@triton.jit\ndef leaky_relu_grad(x):\n    min_grad = 0.01\n    max_grad = 1\n    min_grad = min_grad\n    max_grad = max_grad\n    return tl.where(x >= 0, max_grad, min_grad)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "6be8d2b1-4eaa-4dfb-b419-99924e85a0b0"
  },
  {
    "input": "@triton.jit\ndef demo4(x_ptr):\n    pid = tl.program_id(0)\n    range = tl.arange(0, 8) + pid * 8\n    x = tl.load(x_ptr + range, range < 20)\n    None\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "0c8cb36a-f978-4102-bc4b-6969247f9edb"
  },
  {
    "input": "@triton.jit\ndef maximum_scalar_kernel(x_ptr, scalar, output_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.maximum(x, scalar)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "799444e7-455b-4bff-8908-3366e665316f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8), triton.Config({'BT': 128}, num_warps=\n    2), triton.Config({'BT': 128}, num_warps=4), triton.Config({'BT': 128},\n    num_warps=8), triton.Config({'BT': 256}, num_warps=2), triton.Config({\n    'BT': 256}, num_warps=4), triton.Config({'BT': 256}, num_warps=8)], key\n    =['D'])\n@triton.jit\ndef logsigmoid_bwd_kernel(x, dx, dy, T: 'tl.constexpr', D: 'tl.constexpr',\n    BT: 'tl.constexpr'):\n    i = tl.program_id(0)\n    o_i = i * BT + tl.arange(0, BT)\n    p_x = x + o_i\n    p_dx = dx + o_i\n    p_dy = dy + o_i\n    mask = o_i < T\n    b_x = tl.load(p_x, mask=mask, other=0.0)\n    b_dy = tl.load(p_dy, mask=mask, other=0.0)\n    b_dx = b_dy * (1.0 - tl.sigmoid(b_x))\n    tl.store(p_dx, b_dx, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "31664c39-dbcb-4957-9374-db58426d19ec"
  },
  {
    "input": "@triton.jit\ndef chunk_rwkv6_bwd_kernel_dh(q, g, gs, do, dh, dh0, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        o_t = min(i_t * BT + BT, T)\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_gs = tl.make_block_ptr(gs + i_bh * s_k_h, (K, T), (s_k_d, s_k_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            o_t - 1) * K + i_k * BK,), (BK,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        b_dh *= tl.exp(b_gn)[:, None]\n        b_gs = tl.load(p_gs, boundary_check=(0, 1))\n        b_q = b_q * tl.exp(b_gs)\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n    if USE_INITIAL_STATE:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0b3a4aa3-b7f8-4551-b6db-dcaf360b9eb5"
  },
  {
    "input": "@triton.jit\ndef rmsnorm_triton(x_ptr, rms_w_ptr, out_ptr, stride_x_batch, stride_x_m,\n    stride_x_k, stride_rms_w, stride_out_batch, stride_out_m, stride_out_k,\n    N_SIZE: 'tl.constexpr', eps: 'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr'):\n    pid_batch = tl.program_id(0)\n    pid_m = tl.program_id(1)\n    offset_m = pid_batch * stride_x_batch + pid_m * stride_x_m\n    block_n_size = tl.arange(0, BLOCK_N_SIZE)\n    var = tl.zeros((BLOCK_N_SIZE,), tl.float32)\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=\n            x_ptr_mask, other=0.0)\n        xf = x\n        var += xf * xf\n    var = tl.sum(var, axis=0) / N_SIZE\n    std = tl.sqrt(var + eps)\n    for block_n_strart_ptr in range(0, N_SIZE, BLOCK_N_SIZE):\n        offset_n = block_n_strart_ptr + block_n_size\n        x_ptr_mask = offset_n < N_SIZE\n        rms_w_offset = tl.load(rms_w_ptr + offset_n * stride_rms_w, mask=\n            x_ptr_mask)\n        x = tl.load(x_ptr + offset_m + offset_n * stride_x_k, mask=\n            x_ptr_mask, other=0.0)\n        x_new = x / std\n        out = x_new * rms_w_offset\n        out_offset = (pid_batch * stride_out_batch + pid_m * stride_out_m +\n            offset_n * stride_out_k)\n        tl.store(out_ptr + out_offset, out, mask=x_ptr_mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "be0b6da4-67ee-4bd7-949a-4c282b2ad5c1"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    G, eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_RESIDUAL: 'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr',\n    HAS_WEIGHT: 'tl.constexpr', HAS_BIAS: 'tl.constexpr'):\n    row = tl.program_id(0)\n    group = row % G\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + group * stride_x_row + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + group * stride_x_row + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w if HAS_WEIGHT else x_hat\n    if HAS_BIAS:\n        y = y + b\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "413b6954-2dc8-42a7-86a0-04c7318439fb"
  },
  {
    "input": "@triton.jit\ndef _voxel_grid_splat_one(gi, to_splat, grad_feature_grid,\n    feature_grid_size, batch_index, ix_in, iy_in, iz_in, IH, IW, ID, C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr'):\n    ix, iy, iz, ix0, iy0, iz0, grid_numel = _get_voxel_grid_sample_info(gi,\n        ix_in, iy_in, iz_in, ID, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    (V000x, V000y, V000z, V100x, V100y, V100z, V010x, V010y, V010z, V001x,\n        V001y, V001z, V101x, V101y, V101z, V011x, V011y, V011z, V110x,\n        V110y, V110z, V111x, V111y, V111z, x, y, z\n        ) = _get_voxel_grid_sample_locs_weights(ix, iy, iz, ix0, iy0, iz0)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * (1 - y) * (1 - z),\n        batch_index, V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * (1 - y) * z,\n        batch_index, V100x, V100y, V100z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * y * (1 - z),\n        batch_index, V010x, V010y, V010z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * (1 - y) * (1 - z),\n        batch_index, V001x, V001y, V001z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * (1 - y) * z, batch_index,\n        V101x, V101y, V101z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * y * (1 - z), batch_index,\n        V011x, V011y, V011z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, (1 - x) * y * z, batch_index,\n        V110x, V110y, V110z, ID, IH, IW, C, BLOCK_SIZE)\n    _splat_3d(to_splat, grad_feature_grid, x * y * z, batch_index, V111x,\n        V111y, V111z, ID, IH, IW, C, BLOCK_SIZE)\n    return grid_numel\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "51fa45cb-d7c6-4629-82c8-4062780dfbb0"
  },
  {
    "input": "@triton.jit\ndef _contract_pi(x, y, z, perc_foreground):\n    n = tl.maximum(tl.maximum(tl.abs(x), tl.abs(y)), tl.abs(z))\n    x_c = _contract_pi_one(x, n, perc_foreground)\n    y_c = _contract_pi_one(y, n, perc_foreground)\n    z_c = _contract_pi_one(z, n, perc_foreground)\n    return x_c, y_c, z_c\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "1829c40b-6dae-4d26-8fba-d48818be4bd8"
  },
  {
    "input": "@triton.jit\ndef _rope_fwd(q_ptr, k_ptr, f_ptr, oq_ptr, ok_ptr, stride, d, BLOCK_SIZE:\n    'tl.constexpr'):\n    bh_idx = tl.program_id(0)\n    s_idx = tl.program_id(1)\n    q_start_ptr = q_ptr + bh_idx * stride\n    k_start_ptr = k_ptr + bh_idx * stride\n    oq_start_ptr = oq_ptr + bh_idx * stride\n    ok_start_ptr = ok_ptr + bh_idx * stride\n    d_half = d // 2\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_offsets2 = tl.arange(0, BLOCK_SIZE * 2)\n    f0_ptrs = f_ptr + s_idx * d * 2 + col_offsets2 * 2\n    f1_ptrs = f_ptr + s_idx * d * 2 + col_offsets2 * 2 + 1\n    f0 = tl.load(f0_ptrs, mask=col_offsets2 < d, other=0.0).reshape(BLOCK_SIZE,\n        2)\n    f1 = tl.load(f1_ptrs, mask=col_offsets2 < d, other=0.0).reshape(BLOCK_SIZE,\n        2)\n    q0_ptrs = q_start_ptr + s_idx * d + col_offsets * 2\n    q1_ptrs = q_start_ptr + s_idx * d + col_offsets * 2 + 1\n    q0 = tl.load(q0_ptrs, mask=col_offsets < d_half, other=0.0).reshape(\n        BLOCK_SIZE, 1)\n    q1 = tl.load(q1_ptrs, mask=col_offsets < d_half, other=0.0).reshape(\n        BLOCK_SIZE, 1)\n    k0_ptrs = k_start_ptr + s_idx * d + col_offsets * 2\n    k1_ptrs = k_start_ptr + s_idx * d + col_offsets * 2 + 1\n    k0 = tl.load(k0_ptrs, mask=col_offsets < d_half, other=0.0).reshape(\n        BLOCK_SIZE, 1)\n    k1 = tl.load(k1_ptrs, mask=col_offsets < d_half, other=0.0).reshape(\n        BLOCK_SIZE, 1)\n    oq = f0 * q0 + f1 * q1\n    ok = f0 * k0 + f1 * k1\n    oq_ptrs = oq_start_ptr + s_idx * d + col_offsets2\n    ok_ptrs = ok_start_ptr + s_idx * d + col_offsets2\n    tl.store(oq_ptrs, oq.reshape(BLOCK_SIZE * 2), mask=col_offsets2 < d)\n    tl.store(ok_ptrs, ok.reshape(BLOCK_SIZE * 2), mask=col_offsets2 < d)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "f27aa799-3d6f-42e3-a278-f1d9a509e10f"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f7c3ea6c-4ad9-484f-9cfa-0748e7a5a7a7"
  },
  {
    "input": "@triton.jit\ndef fifth_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST000 = 1.73430461568895\n    CONST001 = 2.32681380862329\n    CONST002 = 1.60565407233314\n    CONST003 = 3.21130814466628\n    CONST004 = 3.3166247903554\n    CONST005 = 6.21867148191637\n    CONST006 = 6.21867148191637\n    CONST007 = 1.60565407233314\n    CONST009 = 11.6340690431164\n    CONST010 = 12.8452325786651\n    CONST011 = 12.4373429638327\n    CONST012 = 12.8452325786651\n    CONST013 = 13.8744369255116\n    CONST017 = 33.9852909359329\n    CONST018 = 7.35803132638072\n    CONST020 = -44.1481879582843\n    CONST021 = -41.6233107765348\n    CONST022 = -29.4321253055229\n    CONST023 = -23.2681380862329\n    CONST024 = -19.2678488679977\n    CONST025 = -19.2678488679977\n    CONST026 = -16.9926454679664\n    CONST027 = -16.9926454679664\n    CONST028 = -13.8744369255116\n    CONST029 = -16.583123951777\n    CONST030 = 3.4686092313779\n    CONST031 = -8.49632273398321\n    CONST032 = -5.20291384706685\n    CONST033 = -3.4686092313779\n    CONST034 = -1.73430461568895\n    VAR05 = x * x * x * x * x\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR14 = y * y * y * y * y\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR23 = z * z * z * z * z\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    Y00 = CONST001 * VAR05 + CONST009 * VAR24 * x + CONST023 * VAR07 * VAR26\n    Y01 = y * (CONST022 * VAR07 * z - CONST022 * VAR25 * x)\n    Y02 = CONST000 * VAR05 + VAR07 * (CONST028 * VAR17 + CONST033 * VAR26\n        ) + x * (-CONST021 * VAR17 * VAR26 + CONST032 * VAR24)\n    Y03 = CONST027 * VAR07 * y * z + x * (CONST017 * VAR16 * z + CONST026 *\n        VAR25 * y)\n    Y04 = CONST002 * VAR05 + VAR07 * (CONST003 * VAR26 + CONST025 * VAR17\n        ) + x * (CONST002 * VAR24 + CONST010 * VAR15 + CONST024 * VAR17 * VAR26\n        )\n    Y05 = CONST004 * VAR14 + VAR16 * (CONST029 * VAR08 + CONST029 * VAR26\n        ) + y * (CONST005 * VAR06 + CONST006 * VAR24 + CONST011 * VAR08 * VAR26\n        )\n    Y06 = CONST002 * VAR23 + VAR25 * (CONST003 * VAR08 + CONST024 * VAR17\n        ) + z * (CONST007 * VAR06 + CONST012 * VAR15 + CONST024 * VAR08 * VAR17\n        )\n    Y07 = VAR16 * (CONST026 * VAR08 - CONST026 * VAR26) + y * (-CONST031 *\n        VAR06 + CONST031 * VAR24)\n    Y08 = CONST034 * VAR23 + VAR25 * (CONST013 * VAR17 + CONST030 * VAR08\n        ) + z * (CONST021 * VAR08 * VAR17 - CONST032 * VAR06)\n    Y09 = y * (CONST018 * VAR06 + CONST018 * VAR24 + CONST020 * VAR08 * VAR26)\n    Y10 = CONST001 * VAR23 + CONST009 * VAR06 * z + CONST023 * VAR08 * VAR25\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y07, mask=\n        output_row_offset + 7 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y08, mask=\n        output_row_offset + 8 < output_numel)\n    tl.store(output_ptr + output_row_offset + 9, Y09, mask=\n        output_row_offset + 9 < output_numel)\n    tl.store(output_ptr + output_row_offset + 10, Y10, mask=\n        output_row_offset + 10 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "e63e92aa-e8b7-4fbe-b5d7-a6a315d8c832"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size',\n    'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_kernel(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m *\n        stride_ddA_cs_csize_m)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None,\n        :] * stride_cb_csize_n)\n    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr, 0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M\n    for start_n in range(lo, hi, BLOCK_SIZE_N):\n        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit - start_n), other=0.0)\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= dt_n\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n\n            [None, :] < chunk_size - start_n), other=0.0)\n        acc *= cb\n        dA_cs_n = tl.load(dA_cumsum_ptr + (start_n + offs_n) *\n            stride_dA_cs_csize, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=offs_n < \n            chunk_size - start_n - 1)\n        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n        tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N\n            ,), dtype=tl.float32), mask=offs_n < chunk_size - start_n - 1)\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "46af2ca8-550c-4429-be89-6b9babbdb5ee"
  },
  {
    "input": "@triton.jit\ndef _is_in_bounds(x, y, z, W: 'tl.constexpr', H: 'tl.constexpr', D:\n    'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (x + 1) / 2 * W - 0.5\n    iy = (y + 1) / 2 * H - 0.5\n    iz = (z + 1) / 2 * D - 0.5\n    in_bounds = (iy >= 0) * (iy < H) * (ix >= 0) * (ix < W) * (iz >= 0) * (iz <\n        D)\n    in_bounds_mask = tl.broadcast_to(in_bounds[:, None], (BLOCK_SIZE, C))\n    return in_bounds_mask\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "ee3e4a18-4ff5-4a82-992a-7eb5986884a4"
  },
  {
    "input": "@triton.jit\ndef _rms_kernel_bwd_dwdb():\n    pass\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "94c8194a-89c0-4c60-a7f3-50f4a009c8a2"
  },
  {
    "input": "@triton.jit\ndef _fwd_preprocess_cumsum_gv(V, GV, GV_cumsum, GV_exp, V_reduce,\n    GV_last_exp, NUM_CHUNK, L, D_MODEL_V: 'tl.constexpr', CHUNK_SIZE:\n    'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    GV_ptr = (GV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_last_exp_ptr = (GV_last_exp + offset_bh * NUM_CHUNK * D_MODEL_V + \n        offset_c * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_cumsum_ptr = (GV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_exp_ptr = (GV_exp + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    cumsum = tl.zeros([D_MODEL_V], dtype=tl.float32)\n    for _ in range(CHUNK_SIZE):\n        gv = tl.load(GV_ptr)\n        cumsum += gv\n        tl.store(GV_cumsum_ptr, cumsum)\n        tl.store(GV_exp_ptr, tl.exp(cumsum))\n        GV_cumsum_ptr += D_MODEL_V\n        GV_exp_ptr += D_MODEL_V\n        GV_ptr += D_MODEL_V\n    tl.store(GV_last_exp_ptr, tl.exp(cumsum))\n    tl.debug_barrier()\n    V_ptr = (V + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_cumsum_ptr = (GV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    V_reduce_ptr = (V_reduce + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    for _ in range(CHUNK_SIZE):\n        v = tl.load(V_ptr)\n        gv = tl.load(GV_cumsum_ptr)\n        v_reduce = v * tl.exp(cumsum - gv)\n        tl.store(V_reduce_ptr, v_reduce)\n        V_ptr += D_MODEL_V\n        V_reduce_ptr += D_MODEL_V\n        GV_cumsum_ptr += D_MODEL_V\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "29697a8e-db82-48fa-84c6-3712f0e40f70"
  },
  {
    "input": "@triton.jit\ndef max_kernel(x_ptr, output_ptr, M, N, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    m_mask = m_offset < M\n    out = tl.full((BLOCK_M,), -float('inf'), dtype=tl.float32)\n    for start in range(0, N, BLOCK_N):\n        n_offset = start + tl.arange(0, BLOCK_N)\n        offset = m_offset[:, None] * N + n_offset[None, :]\n        n_mask = n_offset < N\n        mask = m_mask[:, None] & n_mask[None, :]\n        inp = tl.load(x_ptr + offset, mask=mask, other=-float('inf'))\n        out = tl.maximum(out, tl.max(inp, axis=1))\n    tl.store(output_ptr + m_offset, out, mask=m_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "f3875329-de23-47fa-8adf-fe04843383a4"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_fwd_kernel(q, k, v, alpha, beta, o, ha, h0, ht, s_k_h,\n    s_v_h, scale, B, H, T, K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_alpha = alpha + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_beta = beta + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_ha = ha + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_alpha = tl.load(p_alpha, mask=mask_bk, other=0)\n        b_beta = tl.load(p_beta, mask=mask_bk, other=0)\n        tmp = tl.sum(h * b_alpha[None, :], axis=1)\n        h += tmp[:, None] * b_beta[None, :] + b_k[None, :] * b_v[:, None]\n        _o = h * b_q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        tl.store(p_ha, tmp, mask=mask_bv)\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n        p_ha += V\n        p_alpha += K\n        p_beta += K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "1b4f71b5-c49f-4216-84e4-219d19832379"
  },
  {
    "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    nheads, rotary_dim, seqlen_ro, CACHE_KEY_SEQLEN, stride_out_batch,\n    stride_out_seqlen, stride_out_nheads, stride_out_headdim,\n    stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim,\n    BLOCK_K: 'tl.constexpr', IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr',\n    IS_VARLEN: 'tl.constexpr', INTERLEAVED: 'tl.constexpr', CONJUGATE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "5c499268-5434-4bad-af35-45b6c55b6fb6"
  },
  {
    "input": "@triton.jit\ndef _single_query_cached_kv_attention_v1(out, q, k_cache, v_cache,\n    head_mapping, scale, block_tables, seq_lens, max_num_blocks_per_seq,\n    stride_qm, stride_qn, stride_om, stride_on, stride_km, stride_kn,\n    stride_kk, SLOT_SIZE: 'tl.constexpr', HEAD_SIZE: 'tl.constexpr'):\n    head_idx = tl.program_id(axis=0)\n    token_idx = tl.program_id(axis=1)\n    kv_head_idx = tl.load(head_mapping + head_idx)\n    offs_q = token_idx * stride_qm + head_idx * stride_qn + tl.arange(0,\n        HEAD_SIZE)\n    q = tl.load(q + offs_q)\n    q = q * scale\n    seq_len = tl.load(seq_lens + token_idx)\n    qkv = tl.zeros([SLOT_SIZE, HEAD_SIZE], dtype=tl.float32)\n    m_prev = tl.zeros([1, 1], tl.float32) - float('inf')\n    d_prev = tl.zeros([1, 1], tl.float32)\n    slot_offs = tl.arange(0, SLOT_SIZE)\n    head_size_offs = tl.arange(0, HEAD_SIZE)\n    block_base_ptrs = block_tables + token_idx * max_num_blocks_per_seq\n    kv_base_offs = kv_head_idx * stride_kn + slot_offs[:, None\n        ] * stride_kk + head_size_offs[None, :]\n    for i in range(0, tl.cdiv(seq_len, SLOT_SIZE)):\n        block_idx = tl.load(block_base_ptrs + i)\n        mask = (slot_offs[:, None] < seq_len - i * SLOT_SIZE) & (head_size_offs\n            [None, :] < HEAD_SIZE)\n        kv_offs = block_idx * stride_km + kv_base_offs\n        k = tl.load(k_cache + kv_offs, mask=mask, other=0.0)\n        v = tl.load(v_cache + kv_offs, mask=mask, other=0.0)\n        x_i = tl.sum(q[None, :] * k, axis=1)[:, None]\n        x_i = tl.where(slot_offs[:, None] < seq_len - i * SLOT_SIZE, x_i,\n            float('-inf'))\n        m_i = tl.maximum(m_prev, tl.max(x_i, axis=0))\n        d_i = d_prev * tl.exp(m_prev - m_i) + tl.sum(tl.exp(x_i - m_i), axis=0)\n        qkv = qkv * (d_prev * tl.exp(m_prev - m_i) / d_i) + tl.exp(x_i - m_i\n            ) / d_i * v\n        m_prev = m_i\n        d_prev = d_i\n    offs_q = token_idx * stride_om + head_idx * stride_on + tl.arange(0,\n        HEAD_SIZE)\n    tl.store(out + offs_q, tl.sum(qkv, axis=0))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "5ef5c673-46fe-4e8f-83d8-8a02678ba193"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, x_stride0, x_stride1, y_stride0, y_stride1,\n    N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * y_stride0\n    X += row * x_stride0\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols * x_stride1, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols * y_stride1, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "7aa37815-bee6-40aa-b895-2c3ef25ff960"
  },
  {
    "input": "@triton.jit\ndef hello_triton():\n    tl.device_print('Hello Triton!')\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "fd5aef01-e936-4a32-883f-f796c71e8d8e"
  },
  {
    "input": "@triton.jit\ndef calc_mean_and_inv_std(input, last_dim, eps, last_dim_mask: 'tl.constexpr'):\n    \"\"\"\n    Calculates the mean and inverse standard deviation of the input\n    along the last dimension.\n\n    Args:\n        input: Input whose mean and inverse standard deviation are calculated.\n            The input must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        last_dim: Size of the last dimension of input.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        last_dim_mask: Mask for the last dimension indicating\n            which elements should be included in the calculations.\n            The mask must be of shape [BLOCK_SIZE2].\n\n    Returns:\n        Mean and inverse standard deviation of the input.\n    \"\"\"\n    input = input\n    mean = tl.sum(input, axis=1) / last_dim\n    diff = tl.where(last_dim_mask[None, :], input - mean[:, None], 0)\n    inv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / last_dim + eps)\n    return mean, inv_std\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "12db18ab-d3b1-4c7b-8291-d929edff15b4"
  },
  {
    "input": "@triton.jit\ndef _bwd_ds_kernel(O, Q, S, Z, DO, DQ, DS, DZ, stride_qk_bh, stride_qk_l,\n    stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh,\n    stride_s_dk, stride_s_dv, stride_z_bh, stride_dz_bh, B: 'tl.constexpr',\n    H: 'tl.constexpr', L: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', BL: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    start_kv, start_m, off_bs_head = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = start_kv // NV\n    i_v = start_kv % NV\n    O_block_ptr = tl.make_block_ptr(base=O + off_bs_head * stride_vo_bh,\n        shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_m *\n        BL, i_v * BV), block_shape=(BL, BV), order=(1, 0))\n    Q_block_ptr = tl.make_block_ptr(base=Q + off_bs_head * stride_qk_bh,\n        shape=(L, DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_m *\n        BL, i_k * BK), block_shape=(BL, BK), order=(1, 0))\n    DO_block_ptr = tl.make_block_ptr(base=DO + off_bs_head * stride_vo_bh,\n        shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_m *\n        BL, i_v * BV), block_shape=(BL, BV), order=(1, 0))\n    S_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh,\n        shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(i_k *\n        BK, i_v * BV), block_shape=(BK, BV), order=(1, 0))\n    Z_block_ptr = Z + off_bs_head * stride_z_bh + i_k * BK + tl.arange(0, BK)\n    ds = tl.zeros([BK, BV], dtype=tl.float32)\n    dz = tl.zeros([BL], dtype=tl.float32)\n    dq = tl.zeros([BL, BK], dtype=tl.float32)\n    do = tl.load(DO_block_ptr, boundary_check=(0, 1))\n    o = tl.load(O_block_ptr, boundary_check=(0, 1))\n    q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n    s = tl.load(S_block_ptr, boundary_check=(0, 1))\n    z = tl.load(Z_block_ptr, mask=i_k * BK + tl.arange(0, BK) < DK)\n    z = tl.sum(q * z[None, :], axis=1, keep_dims=True) + 1e-06\n    ds += tl.dot(tl.trans(q / z), do, allow_tf32=False)\n    dz -= tl.sum(o * do / z, axis=1)\n    dq += tl.dot(do, tl.trans(s), allow_tf32=False) / z\n    DS_block_ptr = tl.make_block_ptr(base=DS + (off_bs_head + B * H *\n        start_m) * stride_s_bh, shape=(BK, BV), strides=(stride_s_dk,\n        stride_s_dv), offsets=(i_k * BK, i_v * BV), block_shape=(BK, BV),\n        order=(1, 0))\n    tl.store(DS_block_ptr, ds, boundary_check=(0, 1))\n    DQ_block_ptr = tl.make_block_ptr(base=DQ + off_bs_head * stride_vo_bh,\n        shape=(L, BK), strides=(stride_vo_l, stride_vo_d), offsets=(start_m *\n        BL, i_k * BK), block_shape=(BL, BK), order=(1, 0))\n    tl.store(DQ_block_ptr, dq, boundary_check=(0, 1))\n    DZ_block_ptr = DZ + off_bs_head * stride_dz_bh + start_m * BL + tl.arange(\n        0, BL)\n    tl.store(DZ_block_ptr, dz, mask=start_m * BL + tl.arange(0, BL) < L)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "452750c2-5140-4738-b647-b8865aea3797"
  },
  {
    "input": "@triton.jit\ndef _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\n    h = f * up\n\n    df/de (with help of Wolfram :)\n    df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)\n\n    Reuse via\n    f =        1/2 * (1 + erf(1/sqrt(2) * e)) * e\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_partial_row = 0.5 * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_partial_row * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    t = 0.3989422804014327\n    df_de = f_partial_row + t * e_row * tl.exp(-0.5 * e_row * e_row)\n    de_row = dg_row * df_de\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "eb2b2854-9660-4d67-957c-43ae47e3dffd"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_bwd_kernel_dhu(q, k, d, dht, dh0, do, dh, dv, dv2,\n    scale, T: 'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if STORE_FINAL_STATE:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_dh_tmp = tl.zeros([BK, BV], dtype=tl.float32)\n        for i_c in range(tl.cdiv(BT, BC) - 1, -1, -1):\n            if HEAD_FIRST:\n                p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (\n                    i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n                p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n                p_d = tl.make_block_ptr(d + i_bh * T * K, (K, T), (1, K), (\n                    i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n                p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1),\n                    (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n                p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1),\n                    (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n                p_dv2 = tl.make_block_ptr(dv2 + i_bh * T * V, (T, V), (V, 1\n                    ), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n            else:\n                p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK,\n                    BC), (0, 1))\n                p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_c * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_d = tl.make_block_ptr(d + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK,\n                    BC), (0, 1))\n                p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC,\n                    BV), (1, 0))\n                p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC,\n                    BV), (1, 0))\n                p_dv2 = tl.make_block_ptr(dv2 + i_b * T * H * V + i_h * V,\n                    (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (\n                    BC, BV), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_q = b_q * scale\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_d = tl.load(p_d, boundary_check=(0, 1))\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_dv = tl.load(p_dv, boundary_check=(0, 1))\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            tl.store(p_dv2, b_dv, boundary_check=(0, 1))\n            b_dh_tmp += tl.dot(b_q, b_do, allow_tf32=False)\n            b_dh_tmp -= tl.dot(b_d, b_dv, allow_tf32=False)\n        b_dh += b_dh_tmp\n    if USE_INITIAL_STATE:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "60e8c867-b038-4c1f-8a73-a8504b13a96c"
  },
  {
    "input": "@triton.jit\ndef _fwd_preprocess_cumsum_gk(Q, K, GK, GK_cumsum, Q_exp, K_reduce,\n    GK_last_exp, NUM_CHUNK, L, D_MODEL_K: 'tl.constexpr', D_BLOCK_K:\n    'tl.constexpr', CHUNK_SIZE: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    offset_nk = tl.program_id(2)\n    Q_ptr = (Q + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    Q_exp_ptr = (Q_exp + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_ptr = (GK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_cumsum_ptr = (GK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    GK_last_exp_ptr = (GK_last_exp + offset_bh * NUM_CHUNK * D_MODEL_K + \n        offset_c * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    cumsum = tl.zeros([D_BLOCK_K], dtype=tl.float32)\n    mask = D_BLOCK_K * offset_nk + tl.arange(0, D_BLOCK_K) < D_MODEL_K\n    for _ in range(CHUNK_SIZE):\n        gk = tl.load(GK_ptr, mask=mask, other=0)\n        cumsum += gk\n        tl.store(GK_cumsum_ptr, cumsum, mask=mask)\n        cumsum_exp = tl.exp(cumsum)\n        q = tl.load(Q_ptr, mask=mask, other=0)\n        q_exp = q * cumsum_exp\n        tl.store(Q_exp_ptr, q_exp, mask=mask)\n        Q_ptr += D_MODEL_K\n        Q_exp_ptr += D_MODEL_K\n        GK_ptr += D_MODEL_K\n        GK_cumsum_ptr += D_MODEL_K\n    tl.store(GK_last_exp_ptr, tl.exp(cumsum), mask=mask)\n    tl.debug_barrier()\n    GK_cumsum_ptr = (GK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    K_ptr = (K + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    K_reduce_ptr = (K_reduce + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    for _ in range(CHUNK_SIZE):\n        gk_cumsum = tl.load(GK_cumsum_ptr, mask=mask, other=0)\n        k = tl.load(K_ptr, mask=mask, other=0)\n        k_reduce = k * tl.exp(cumsum - gk_cumsum)\n        tl.store(K_reduce_ptr, k_reduce, mask=mask)\n        K_ptr += D_MODEL_K\n        GK_cumsum_ptr += D_MODEL_K\n        K_reduce_ptr += D_MODEL_K\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "eb01339d-6317-44a1-8c7a-2fef14e153b8"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_retention_bwd_kernel(q, k, v, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.pow(2, -5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        h = b_b * h + _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dq += DK\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        d_h *= b_b\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c3c59ff4-fcbf-4bcd-a664-8cefabde06f2"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_bwd_dwdb(DW, FINAL_DW, M, N, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b28a2415-bc8e-4af9-b11c-5d1b76d88ecd"
  },
  {
    "input": "@triton.jit\ndef parallel_rebased_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "991791e8-f250-4272-8590-85d9563fe5ae"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)],\n    key=['M'])\n@triton.jit\ndef triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer(\n    input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_b), tl.load(\n        input_ptr_offsets + (pid_b + 1))\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED\n        ):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n        buffer += tl.sum(input, axis=0)\n    buffer_view = buffer.reshape((BLOCK_SIZE_M,))\n    output_offsets = offsets_m + pid_b * M\n    output_mask = output_offsets < M * (pid_b + 1)\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "fd8425e8-20e6-400a-8551-695596a7c5b7"
  },
  {
    "input": "@triton.jit\ndef parallel_retention_fwd_kernel(q, k, v, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    o_k = tl.arange(0, BTS)\n    d_h = tl.math.exp2((BTS - o_k) * b_b)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_h[None, :]\n        b_o = b_o * tl.math.exp2(b_b * BTS)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    d_q = tl.math.exp2(tl.arange(0, BTL) * b_b)\n    b_o *= d_q[:, None]\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "2b658ffa-e0eb-4276-a7a2-10a41c699c5b"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dx(dk_ptrs, dk, offs_n, offs_d, seqlen_k, headdim,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n        _bwd_store_dx(dv_ptrs, dv, offs_n, offs_d, seqlen_k, headdim,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, tl.trans(k))\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(tl.trans(p), do)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, tl.trans(v))\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(tl.trans(ds), q)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dx(dk_ptrs, dk, offs_n, offs_d, seqlen_k, headdim, EVEN_M=\n        EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n    _bwd_store_dx(dv_ptrs, dv, offs_n, offs_d, seqlen_k, headdim, EVEN_M=\n        EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "dfe60ba8-07da-4892-88a2-eada31cbbd23"
  },
  {
    "input": "@triton.jit\ndef tl_logaddexp(a, b) ->tl.tensor:\n    minx = tl.minimum(a, b)\n    mx = tl.maximum(a, b)\n    return tl_log1p(tl.exp(minx - mx)) + mx\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "77aec88a-96cc-4056-acb3-1c816eea748a"
  },
  {
    "input": "@triton.jit\ndef int32_to_float01(x):\n    x_01 = (x + MAX_INT_32_F + MAX_UINT_32_F_EPS) / (MAX_UINT_32_F +\n        MAX_UINT_32_F_EPS)\n    return x_01\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "27d7c0ea-e768-4b6f-8415-b8a9584c7fb6"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dk(q, k, rk, ck, ds, dk, dsk, s_qk_h, s_qk_t,\n    s_qk_d, s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BM: 'tl.constexpr', DK: 'tl.constexpr', DM:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        (NT - 1) * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, (NT - 1) * BT), (BK, BT), (0, 1))\n    p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_ck = tl.make_block_ptr(ck + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_ds = tl.make_block_ptr(ds + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t),\n        (i_m * BM, (NT - 1) * BT), (BM, BT), (0, 1))\n    p_dk = tl.make_block_ptr(dk + (i_m * n_bh + i_bh) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), ((NT - 1) * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dsk = tl.make_block_ptr(dsk + (i_k * n_bh + i_bh) * s_sk_h, (T, DM),\n        (s_sk_t, s_sk_m), ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s, m_t = o_i[:, None] <= o_i[None, :], o_i[:, None] >= o_i[None, :]\n    b_dhk = tl.zeros([BM, BK], dtype=tl.float32)\n    for i in range(NT):\n        p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (\n            s_sk_m,), ((NT - i) % NT * DM + i_m * BM,), (BM,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_rk = tl.load(p_rk, boundary_check=(0,))\n        b_ck = tl.load(p_ck, boundary_check=(0, 1))\n        b_ds = tl.load(p_ds, boundary_check=(0, 1))\n        b_inter = tl.dot(b_ck * b_rk[None, :], b_dhk, allow_tf32=False)\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_ck, b_ds, allow_tf32=False),\n            0.0), b_q, allow_tf32=False)\n        b_dk = b_inter + b_intra\n        b_inter = tl.dot(b_dhk, b_k, allow_tf32=False) * b_rk[:, None]\n        b_intra = tl.dot(b_ds, tl.where(m_t, tl.dot(b_q, b_k, allow_tf32=\n            False), 0.0), allow_tf32=False)\n        b_dsk = b_ck * tl.trans(b_inter + b_intra)\n        b_dhk = b_dhk * b_rk[:, None] + tl.dot(b_ds, b_q, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dsk, b_dsk, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (-BT, 0))\n        p_k = tl.advance(p_k, (0, -BT))\n        p_ck = tl.advance(p_ck, (-BT, 0))\n        p_ds = tl.advance(p_ds, (0, -BT))\n        p_dk = tl.advance(p_dk, (-BT, 0))\n        p_dsk = tl.advance(p_dsk, (-BT, 0))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "f21b4ceb-565c-4957-98d2-bd61e1622ec9"
  },
  {
    "input": "@triton.jit\ndef blora_fwd_kernel(x_ptr, x_stride_bsk, x_stride_m, x_stride_h, o_ptr,\n    o_stride_bsk, o_stride_m, o_stride_hout, blA_ptr, blA_stride_bsk,\n    blA_stride_h, blA_stride_r, blB_ptr, blB_stride_bsk, blB_stride_r,\n    blB_stride_hout, h: 'tl.constexpr', hout: 'tl.constexpr', m:\n    'tl.constexpr', r: 'tl.constexpr', block_size_h: 'tl.constexpr',\n    block_size_hout: 'tl.constexpr', block_size_m: 'tl.constexpr',\n    block_size_r: 'tl.constexpr'):\n    block_idx_bsk = tl.program_id(0)\n    block_idx_hout = tl.program_id(1)\n    offsets_h = tl.arange(0, block_size_h)\n    offsets_hout = block_idx_hout * block_size_hout + tl.arange(0,\n        block_size_hout)\n    offsets_m = tl.arange(0, block_size_m)\n    offsets_r = tl.arange(0, block_size_r)\n    block_mask_m_col = offsets_m[:, None] < m\n    block_mask_r_row = offsets_r[None, :] < r\n    block_mask_r_col = offsets_r[:, None] < r\n    block_mask_hout_row = offsets_hout[None, :] < hout\n    x_ptrs = x_ptr + block_idx_bsk * x_stride_bsk + (offsets_m[:, None] *\n        x_stride_m + offsets_h[None, :] * x_stride_h)\n    o_ptrs = o_ptr + block_idx_bsk * o_stride_bsk + (offsets_m[:, None] *\n        o_stride_m + offsets_hout[None, :] * o_stride_hout)\n    blA_ptrs = blA_ptr + block_idx_bsk * blA_stride_bsk + (offsets_h[:,\n        None] * blA_stride_h + offsets_r[None, :] * blA_stride_r)\n    blB_ptrs = blB_ptr + block_idx_bsk * blB_stride_bsk + (offsets_r[:,\n        None] * blB_stride_r + offsets_hout[None, :] * blB_stride_hout)\n    olA = tl.zeros((block_size_m, block_size_r), dtype=tl.float32)\n    for block_idx_h in range(0, tl.cdiv(h, block_size_h)):\n        block_offs_h = block_idx_h * block_size_h + offsets_h\n        block_mask_h_row = block_offs_h[None, :] < h\n        block_mask_h_col = block_offs_h[:, None] < h\n        x = tl.load(x_ptrs, mask=block_mask_m_col & block_mask_h_row, other=0.0\n            )\n        blA = tl.load(blA_ptrs, mask=block_mask_h_col & block_mask_r_row,\n            other=0.0)\n        olA += tl.dot(x, blA)\n        x_ptrs += block_size_h * x_stride_h\n        blA_ptrs += block_size_h * blA_stride_h\n    blB = tl.load(blB_ptrs, mask=block_mask_r_col & block_mask_hout_row,\n        other=0.0)\n    olB = tl.dot(olA, blB)\n    tl.store(o_ptrs, olB, mask=block_mask_m_col & block_mask_hout_row)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d80288a6-003f-431f-b39b-73e928a7ee20"
  },
  {
    "input": "@triton.jit\ndef rms_norm_fwd_fused(X, Y, W, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "6b249f2b-63dc-4cb8-85b6-5863fb144c52"
  },
  {
    "input": "@triton.jit\ndef blockwise_barrier_double_tree(signal_pad_ptrs, block_id, send1_rank,\n    send2_rank, wait1_rank, wait2_rank, RANK: 'tl.constexpr', WORLD_SIZE:\n    'tl.constexpr'):\n    if block_id is None:\n        block_id = tl.program_id(2) * tl.num_programs(1) * tl.num_programs(0\n            ) + tl.program_id(1) * tl.num_programs(0) + tl.program_id(0)\n    flat_tid = get_flat_tid()\n    remote_ranks = tl.cat(tl.full((1,), send1_rank, tl.int32), tl.full((1,),\n        send2_rank, tl.int32))\n    signal_pad_ptrs = signal_pad_ptrs\n    remote_signal_pad_addrs = tl.load(signal_pad_ptrs + remote_ranks)\n    send_addrs = remote_signal_pad_addrs + block_id * WORLD_SIZE + RANK\n    remote_ranks = tl.cat(tl.full((1,), wait1_rank, tl.int32), tl.full((1,),\n        wait2_rank, tl.int32))\n    local_signal_pad_addr = tl.load(signal_pad_ptrs + RANK)\n    wait_addrs = local_signal_pad_addr + block_id * WORLD_SIZE + remote_ranks\n    if flat_tid < WORLD_SIZE:\n        tl.inline_asm_elementwise(\n            \"\"\"\n            {\n                .reg .u32   %tmp32_<1>;\n                .reg .pred  %p<1>;\n\n                send_signal:\n                    atom.global.release.sys.cas.b32 %tmp32_0, [$1], 0, 1;\n                    setp.eq.u32 %p0, %tmp32_0, 0;\n                    @!%p0 bra send_signal;\n\n                wait_signal:\n                    // No need to acquire here since all threads will\n                    // acquire this location after the barrier.\n                    atom.global.sys.cas.b32 %tmp32_0, [$2], 1, 0;\n                    setp.eq.u32 %p0, %tmp32_0, 1;\n                    @!%p0 bra wait_signal;\n\n                barrier_end:\n            }\n            \"\"\"\n            , '=r, l, l', [send_addrs, wait_addrs], dtype=tl.int32, is_pure\n            =False, pack=1)\n    tl.inline_asm_elementwise('bar.sync 0;', '=r', [], dtype=tl.int32,\n        is_pure=False, pack=1)\n    tl.inline_asm_elementwise('ld.acquire.sys.global.u32 $0, [$1];',\n        '=r, l', [local_signal_pad_addr + send1_rank], dtype=tl.int32,\n        is_pure=False, pack=1)\n    tl.inline_asm_elementwise('ld.acquire.sys.global.u32 $0, [$1];',\n        '=r, l', [local_signal_pad_addr + send2_rank], dtype=tl.int32,\n        is_pure=False, pack=1)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "b9b77671-d23d-4a80-b407-fd6aadee2bb3"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "6d8449a0-3048-447a-9fdc-d320f369ac59"
  },
  {
    "input": "@triton.jit\ndef fused_moe_kernel(a_ptr, b_ptr, c_ptr, a_scale_ptr, b_scale_ptr,\n    topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr,\n    num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am,\n    stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr',\n    MUL_ROUTED_WEIGHT: 'tl.constexpr', top_k: 'tl.constexpr', compute_type:\n    'tl.constexpr', use_fp8: 'tl.constexpr'):\n    \"\"\"\n    Implements the fused computation for a Mixture of Experts (MOE) using\n    token and expert matrices.\n\n    Key Parameters:\n    - A: The input tensor representing tokens with shape (*, K), where '*' can\n        be any shape representing batches and K is the feature dimension of\n        each token.\n    - B: The stacked MOE weight tensor with shape (E, N, K), where E is\n        the number of experts, K is the input feature dimension, and N is\n        the output feature dimension.\n    - C: The output cache tensor with shape (M, topk, N), where M is the\n        total number of tokens post padding, topk is the number of times\n        each token is repeated, and N is the output feature dimension.\n    - sorted_token_ids: A tensor containing the sorted indices of tokens,\n        repeated topk times and arranged by the expert index they are\n        assigned to.\n    - expert_ids: A tensor containing the indices of the expert for each\n        block. It determines which expert matrix from B should be used for\n        each block in A.\n    This kernel performs the multiplication of a token by its corresponding\n    expert matrix as determined by `expert_ids`. The sorting of\n    `sorted_token_ids` by expert index and padding ensures divisibility by\n    BLOCK_SIZE_M, which is necessary to maintain consistency in block matrix\n    multiplication across different blocks processed by the same expert.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_token[:, None] // top_k * stride_am + offs_k[\n        None, :] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = b_ptr + off_experts * stride_be + (offs_k[:, None] * stride_bk +\n        offs_bn[None, :] * stride_bn)\n    if use_fp8:\n        a_scale = tl.load(a_scale_ptr)\n        b_scale = tl.load(b_scale_ptr + off_experts)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        if use_fp8:\n            accumulator = tl.dot(a, b, acc=accumulator)\n        else:\n            accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    if use_fp8:\n        accumulator = accumulator * a_scale * b_scale\n    else:\n        accumulator = accumulator\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_token[:, None] + stride_cn * offs_cn[\n        None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "0296abc0-f958-4287-b62b-cb6a948c588a"
  },
  {
    "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: bool(args[\n    'DO_SOFTCAPPING']), 'DO_LOGIT_SCALING': lambda args: bool(args[\n    'DO_LOGIT_SCALING'])})\n@triton.jit\ndef _cross_entropy_backward(logits_ptr, logits_row_stride, dloss_ptr,\n    dloss_row_stride, logsumexp_ptr, labels_ptr, VOCAB_SIZE, BLOCK_SIZE:\n    'tl.constexpr', DO_SOFTCAPPING, SOFTCAP, DO_LOGIT_SCALING, LOGIT_SCALE):\n    \"\"\"\n        CE_i = -y log(P) = y * (log[sum(exp(x))] - x)\n        dC/dx = d/dx (y * log[sum(exp(x))] - x * y)\n\n        From https://en.wikipedia.org/wiki/LogSumExp\n        d/dx logsumexp = exp(x) / sum(exp(x)) = softmax(x)\n\n        dC/dx = y * exp(x) / sum(exp(x)) - d/dx (x * y)\n        dC/dx = y * exp[ log[exp(x) / sum(exp(x))] ] using x = exp(log(x)) trick\n        dC/dx = y * exp[x - logsumexp] - d/dx (x * y)\n\n        If y == 0: dC/dx = 0\n        If y == 1 and x == label: dC/dlabel = exp[x - logsumexp] - 1\n        If y == 1 and x != label: dC/dx     = exp[x - logsumexp]\n    \"\"\"\n    row_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n    logits_ptr += row_idx * triton_cast(logits_row_stride, tl.int64)\n    dloss_ptr += row_idx * dloss_row_stride\n    col_offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != -100:\n        dloss = tl.load(dloss_ptr)\n    else:\n        dloss = 0.0\n    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    if DO_LOGIT_SCALING:\n        x = x * LOGIT_SCALE\n    pass\n    partial = x\n    if DO_SOFTCAPPING:\n        partial = triton_tanh(x / SOFTCAP)\n        x = SOFTCAP * partial\n    pass\n    logsumexp = tl.load(logsumexp_ptr + row_idx)\n    y = tl.exp(x - logsumexp)\n    y = tl.where(col_offsets == label_idx, y - 1.0, y)\n    if DO_LOGIT_SCALING:\n        y = y * LOGIT_SCALE\n    pass\n    if DO_SOFTCAPPING:\n        y = y * (1.0 - partial * partial)\n    pass\n    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f9da3799-5931-4218-9eae-4450da98fdbf"
  },
  {
    "input": "@triton.jit\ndef div_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'\n    ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x / y\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "4c758deb-247e-4ae2-8599-b888906bbf8c"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_bwd_kernel_dh(q, do, dh, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, s_hh, s_ht, H, T, scale, DK, DV, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_hh, ((i + 1) * DK, DV), (\n            s_ht, 1), (i * DK + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, b_do * d_i[:, None], allow_tf32=False)\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "fb8abe4f-2b95-4556-a9b0-8e3e2aa7bf6e"
  },
  {
    "input": "@triton.jit\ndef triton_cross_entropy_forward_backward_kernel(logits_ptr, labels_ptr,\n    grad_logits_ptr, losses_ptr, grad_losses, n_cols, logits_stride_0,\n    grad_logits_stride_0, logits_scale_factor: 'tl.constexpr', block_size:\n    'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, block_size)\n    logits_ptr = logits_ptr + block_idx * logits_stride_0\n    mask = col_offsets < n_cols\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    if logits_scale_factor != 1.0:\n        logits *= logits_scale_factor\n    max_logits = tl.max(logits, 0)\n    exp_logits = tl.exp(logits - max_logits)\n    sum_exp_logits = tl.sum(exp_logits, 0)\n    label_idx = tl.load(labels_ptr + block_idx)\n    label_logits = tl.load(logits_ptr + label_idx)\n    if label_idx < 0:\n        loss = 0.0\n    else:\n        loss = tl.log(sum_exp_logits) + max_logits - label_logits\n    tl.store(losses_ptr + block_idx, loss)\n    grad_logits_ptr = grad_logits_ptr + block_idx * grad_logits_stride_0\n    col_offsets = tl.arange(0, block_size)\n    label_idx = tl.load(labels_ptr + block_idx)\n    exp_logits = exp_logits / sum_exp_logits\n    if logits_scale_factor != 1.0:\n        exp_logits *= logits_scale_factor\n    if label_idx < 0:\n        grad_losses = 0.0\n    grad_logits = grad_losses * tl.where(col_offsets == label_idx, \n        exp_logits - 1.0, exp_logits)\n    tl.store(grad_logits_ptr + col_offsets, grad_logits, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "3ff77eef-4ee8-4675-8cea-ee7596d918ba"
  },
  {
    "input": "@triton.jit\ndef _inner_paged_attn_unroll_2_kernel(q, k_cache, v_cache, stride_km,\n    block_base_ptrs, base_offs_kv, alibi_slope, block_offs, seq_len, qkv,\n    qk_max, exp_sum, BLOCK_SIZE: 'tl.constexpr', LO: 'tl.constexpr', HI:\n    'tl.constexpr'):\n    for block_idx in range(LO, HI, 2):\n        offs_kv_0 = tl.load(block_base_ptrs + block_idx + 0\n            ) * stride_km + base_offs_kv\n        offs_kv_1 = tl.load(block_base_ptrs + block_idx + 1\n            ) * stride_km + base_offs_kv\n        k_0 = tl.load(k_cache + offs_kv_0)\n        k_1 = tl.load(k_cache + offs_kv_1)\n        v_0 = tl.load(v_cache + offs_kv_0)\n        v_1 = tl.load(v_cache + offs_kv_1)\n        _qk_0 = tl.sum(q[None, :] * k_0, axis=1)\n        _qk_1 = tl.sum(q[None, :] * k_1, axis=1)\n        if alibi_slope is not None:\n            _qk_0 += alibi_slope * ((block_idx + 0) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_1 += alibi_slope * ((block_idx + 1) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n        _qk_max = tl.maximum(tl.max(_qk_0, axis=0), qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_1, axis=0), _qk_max)\n        exp_tmp = tl.exp(_qk_0 - _qk_max) + tl.exp(_qk_1 - _qk_max)\n        _exp_sum = exp_sum * tl.exp(qk_max - _qk_max) + tl.sum(exp_tmp, axis=0)\n        qkv_sum_tmp = tl.exp(_qk_0[:, None] - _qk_max) * v_0 + tl.exp(_qk_1\n            [:, None] - _qk_max) * v_1\n        qkv = (qkv * (exp_sum * tl.exp(qk_max - _qk_max)) + qkv_sum_tmp\n            ) / _exp_sum\n        qk_max = _qk_max\n        exp_sum = _exp_sum\n    return qkv, qk_max, exp_sum\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "5ac79967-359a-46d3-a8cc-44da7b811750"
  },
  {
    "input": "@triton.jit\ndef _int_to_randn_kernel(x1, x2, out, N: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', seed: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) < N\n    x1_buffer = tl.load(x1 + offs, mask=offs_mask)\n    x2_buffer = tl.load(x2 + offs, mask=offs_mask)\n    seed_buffer = tl.full((BLOCK_SIZE,), seed, dtype=tl.int64)\n    r = _int_to_randn(x1_buffer, x2_buffer, seed_buffer)\n    tl.store(out + offs, r, mask=offs_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "bd170e7a-0bfa-49d0-9ac1-cf9ba682ff8c"
  },
  {
    "input": "@triton.jit\ndef dropout_mask(philox_seed, philox_offset, dropout_p, m, n, stride):\n    rng_output = dropout_rng(philox_seed, philox_offset, dropout_p, m, n,\n        stride)\n    rng_keep = rng_output > dropout_p\n    return rng_keep\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "e2e4bf6d-4ddd-4fde-8066-0dc8b9a9dca0"
  },
  {
    "input": "@triton.jit\ndef offset_2d(offs0, offs1, stride0, stride1=1):\n    return tl.expand_dims(offs0, 1) * stride0 + tl.expand_dims(offs1, 0\n        ) * stride1\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "c60fe5df-223a-48ce-9dcf-459578ea4e38"
  },
  {
    "input": "@triton.jit\ndef _class_probs_forward(LOGITS, PROBS, IDX, LOSS, weight, N, WEIGHT_BUFFER,\n    smoothing_factor, log_size_logits, WEIGHTS: 'tl.constexpr',\n    CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING: 'tl.constexpr',\n    IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE: 'tl.constexpr', BLOCK:\n    'tl.constexpr'):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK)\n    logit_start_ptrs = LOGITS + row * N\n    logit_ptrs = logit_start_ptrs + cols\n    m_prev = -float('inf')\n    l_prev = 0.0\n    m_prev = m_prev\n    l_prev = l_prev\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=-float('inf'))\n        m_curr = tl.maximum(tl.max(row_logits, 0), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(row_logits - m_curr)\n        l_curr = tl.sum(p, 0) + l_prev\n        l_prev = l_curr\n        m_prev = m_curr\n        logit_ptrs += BLOCK\n    logit_ptrs = logit_start_ptrs + cols\n    output_ptrs = PROBS + row * N + cols\n    WRIT_PROBS = PROBS + row * N + cols\n    sum_total = 0.0\n    weights_total = 0.0\n    sum_total = sum_total\n    weights_total = weights_total\n    idx_ptr = IDX + row * N + cols\n    if WEIGHTS:\n        weight_ptr = weight + cols\n    l_prev_log = tl.log(l_prev)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=l_prev_log + m_prev)\n        idx = tl.load(idx_ptr, mask=cols < N - start_n * BLOCK, other=0.0)\n        full_weights_val = (1.0 - smoothing_factor\n            ) * idx + smoothing_factor / N\n        if WEIGHTS:\n            weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            full_weights_val = weights_val * full_weights_val\n        else:\n            full_weights_val = tl.where(cols < N - start_n * BLOCK,\n                full_weights_val, 0.0)\n        weights_total += tl.sum(full_weights_val, 0)\n        row_minus_max = row_logits - m_prev\n        log_softmax = l_prev_log - row_minus_max\n        log_softmax *= full_weights_val\n        sum_total += tl.sum(log_softmax, 0)\n        tl.store(WRIT_PROBS, log_softmax, mask=cols < N - start_n * BLOCK)\n        logit_ptrs += BLOCK\n        WRIT_PROBS += BLOCK\n        idx_ptr += BLOCK\n        if WEIGHTS:\n            weight_ptr += BLOCK\n    tl.store(WEIGHT_BUFFER + row, weights_total)\n    probs = sum_total\n    tl.store(LOSS + row, probs)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "2957c00f-eea7-4f0f-9cd7-0fe07f079322"
  },
  {
    "input": "@triton.jit\ndef _grid_sample(image, batch_index, ix, iy, N: 'tl.constexpr', C:\n    'tl.constexpr', IH: 'tl.constexpr', IW: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    ix_nw = ix - ix % 1\n    iy_nw = iy - iy % 1\n    ix_ne = ix_nw + 1\n    iy_ne = iy_nw\n    ix_sw = ix_nw\n    iy_sw = iy_nw + 1\n    ix_se = ix_nw + 1\n    iy_se = iy_nw + 1\n    nw = (ix_se - ix) * (iy_se - iy)\n    ne = (ix - ix_sw) * (iy_sw - iy)\n    sw = (ix_ne - ix) * (iy - iy_ne)\n    se = (ix - ix_nw) * (iy - iy_nw)\n    out_val = _sample_2d(image, nw, batch_index, ix_nw, iy_nw, IH, IW, C,\n        BLOCK_SIZE) + _sample_2d(image, ne, batch_index, ix_ne, iy_ne, IH,\n        IW, C, BLOCK_SIZE) + _sample_2d(image, se, batch_index, ix_se,\n        iy_se, IH, IW, C, BLOCK_SIZE) + _sample_2d(image, sw, batch_index,\n        ix_sw, iy_sw, IH, IW, C, BLOCK_SIZE)\n    return out_val\n",
    "category": "Helper Functions",
    "subcategory": "shape manipulation",
    "uuid": "aff462d5-c827-4d13-bf6e-6e46349e4609"
  },
  {
    "input": "@triton.jit\ndef nop_with_args_kernel(t1, t2, t3, t4, t5, i1, i2, i3, i4, i5, i6, i7, i8,\n    i9, c1: 'tl.constexpr', c2: 'tl.constexpr', c3: 'tl.constexpr', c4:\n    'tl.constexpr', c5: 'tl.constexpr'):\n    pass\n",
    "category": "Helper Functions",
    "subcategory": "",
    "uuid": "91008597-d25c-44c6-b3a0-0c54daab2521"
  },
  {
    "input": "@triton.jit\ndef _bwd_kv_kernel(Q, K, V, B, sm_scale, DO, DK, DV, DS, L, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_bz,\n    stride_bh, stride_bm, stride_bn, stride_doz, stride_doh, stride_dom,\n    stride_dok, stride_dkz, stride_dkh, stride_dkn, stride_dkk, stride_dvz,\n    stride_dvh, stride_dvn, stride_dvk, Z, H, M, N, P_SEQ, lock, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    CAUSAL: 'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', RETURN_DS: 'tl.constexpr',\n    IS_BATCH_REDUCED: 'tl.constexpr', GROUP_SIZE_BIAS: 'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_n = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_h * stride_kh\n    V += off_z * stride_vz + off_h * stride_vh\n    if HAS_BIAS:\n        if IS_BATCH_REDUCED:\n            B += off_h * stride_bh\n        else:\n            B += off_z * stride_bz + off_h * stride_bh\n    DO += off_z * stride_doz + off_h * stride_doh\n    DK += off_z * stride_dkz + off_h * stride_dkh\n    DV += off_z * stride_dvz + off_h * stride_dvh\n    if RETURN_DS:\n        if IS_BATCH_REDUCED:\n            lock_id = off_z % GROUP_SIZE_BIAS\n            lock += lock_id\n            count = lock + GROUP_SIZE_BIAS\n            DS += lock_id * stride_bz + off_h * stride_bh\n        else:\n            DS += off_z * stride_bz + off_h * stride_bh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    if CAUSAL:\n        lo = tl.maximum(start_n * BLOCK_N - P_SEQ, 0)\n        lo = lo // BLOCK_M * BLOCK_M\n    else:\n        lo = 0\n    offs_m_init = lo + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m_init[:, None] * stride_qm + offs_k[None, :] *\n        stride_qk)\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n    do_ptrs = DO + (offs_m_init[:, None] * stride_dom + offs_k[None, :] *\n        stride_dok)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_k[None, :] * stride_dvk\n        )\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_k[None, :] * stride_dkk\n        )\n    if HAS_BIAS:\n        bias_ptrs = B + (offs_m_init[:, None] * stride_bm + offs_n[None, :] *\n            stride_bn)\n    if RETURN_DS:\n        ds_ptrs = DS + (offs_m_init[:, None] * stride_bm + offs_n[None, :] *\n            stride_bn)\n    mask_n = offs_n < N\n    if DIVISIBLE_N:\n        v = tl.load(v_ptrs)\n        k = tl.load(k_ptrs)\n    else:\n        v = tl.load(v_ptrs, mask=mask_n[:, None])\n        k = tl.load(k_ptrs, mask=mask_n[:, None])\n    dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    for start_m in range(lo, M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m = start_m + offs_m_base\n        causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n        mask_m = offs_m < M\n        if DIVISIBLE_M:\n            q = tl.load(q_ptrs)\n        else:\n            valid_mask = mask_m[:, None]\n            q = tl.load(q_ptrs, mask=mask_m[:, None])\n        if HAS_BIAS:\n            if DIVISIBLE_M and DIVISIBLE_N:\n                b = tl.load(bias_ptrs)\n            else:\n                b = tl.load(bias_ptrs, mask=mask_m[:, None] & mask_n[None, :])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k)) * sm_scale\n        if HAS_BIAS:\n            s += b\n        if DIVISIBLE_M:\n            l = tl.load(L + offs_m)\n        else:\n            l = tl.load(L + offs_m, mask=mask_m)\n        p = tl.math.exp2((s - l[:, None]) * log2e)\n        if not DIVISIBLE_M:\n            p = tl.where(valid_mask, p, 0.0)\n        if CAUSAL:\n            p = tl.where(causal_mask, p, 0.0)\n        if DIVISIBLE_M:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=mask_m[:, None])\n        dv += tl.dot(tl.trans(p), do)\n        if DIVISIBLE_M:\n            delta = tl.load(D + offs_m)\n        else:\n            delta = tl.load(D + offs_m, mask=mask_m)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_M:\n            ds = tl.where(valid_mask, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        ds = ds\n        dk += tl.dot(tl.trans(ds), q)\n        if RETURN_DS:\n            if IS_BATCH_REDUCED:\n                while tl.atomic_cas(lock, 0, 1) == 1:\n                    pass\n                counter = tl.load(count)\n                if counter == 0:\n                    tl.atomic_xchg(count, 1)\n                elif DIVISIBLE_M and DIVISIBLE_N:\n                    ds += tl.load(ds_ptrs)\n                else:\n                    ds += tl.load(ds_ptrs, mask=mask_m[:, None] & mask_n[\n                        None, :])\n                if DIVISIBLE_M and DIVISIBLE_N:\n                    tl.store(ds_ptrs, ds)\n                else:\n                    tl.store(ds_ptrs, ds, mask=mask_m[:, None] & mask_n[\n                        None, :])\n                tl.atomic_xchg(lock, 0)\n            elif DIVISIBLE_M and DIVISIBLE_N:\n                tl.store(ds_ptrs, ds)\n            else:\n                tl.store(ds_ptrs, ds, mask=mask_m[:, None] & mask_n[None, :])\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if HAS_BIAS:\n            bias_ptrs += BLOCK_M * stride_bm\n        if RETURN_DS:\n            ds_ptrs += BLOCK_M * stride_bm\n    dk *= sm_scale\n    if DIVISIBLE_N:\n        tl.store(dk_ptrs, dk)\n        tl.store(dv_ptrs, dv)\n    else:\n        tl.store(dk_ptrs, dk, mask=mask_n[:, None])\n        tl.store(dv_ptrs, dv, mask=mask_n[:, None])\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "2bd8e714-ee95-4567-b137-201b85f62fbf"
  },
  {
    "input": "@triton.jit\ndef gelu_kernel(x_ptr, y_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    offsets = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    sqrt_2_over_pi = 0.7978845608028654\n    coeff = sqrt_2_over_pi * (1 + 0.044715 * x * x)\n    y = 0.5 * x * (1 + x * coeff / (1 + tl.abs(x * coeff)))\n    tl.store(y_ptr + offsets, y, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "4713c829-fcc0-409d-b078-2578a8cd57a1"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config(\n    {'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config\n    ({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.\n    Config({'BLOCK_SIZE': 2048})], key=['dim'])\n@triton.jit\ndef _state_passing_bwd_kernel(dout_ptr, out_ptr, dA_cs_ptr,\n    dfinal_states_ptr, seq_idx_ptr, dstates_ptr, ddA_cs_ptr,\n    dinitstates_ptr, states_converted_ptr, dim, nchunks, seqlen, chunk_size,\n    stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_dim,\n    stride_out_batch, stride_out_chunk, stride_out_head, stride_out_dim,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dfinal_states_batch, stride_dfinal_states_head,\n    stride_dfinal_states_dim, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    stride_dstates_batch, stride_dstates_chunk, stride_dstates_head,\n    stride_dstates_dim, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_dinitstates_batch, stride_dinitstates_head,\n    stride_dinitstates_dim, CONVERT_STATES: 'tl.constexpr',\n    HAS_DFINAL_STATES: 'tl.constexpr', HAS_DINITSTATES: 'tl.constexpr',\n    HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_h *\n        stride_dstates_head + (nchunks - 1) * stride_dstates_chunk)\n    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head + (\n        nchunks - 1) * stride_dA_cs_chunk\n    ddA_cs_ptr += pid_b * stride_ddA_cs_batch + pid_h * stride_ddA_cs_head + (\n        nchunks - 1) * stride_ddA_cs_chunk + pid_m\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head + (nchunks -\n        1) * stride_out_chunk\n    dout_ptr += pid_b * stride_dout_batch + pid_h * stride_dout_head + (nchunks\n         - 1) * stride_dout_chunk\n    if CONVERT_STATES:\n        states_converted_ptr += (pid_b * stride_out_batch + pid_h *\n            stride_out_head + (nchunks - 1) * stride_out_chunk)\n    if HAS_DFINAL_STATES:\n        dfinal_states_ptr += (pid_b * stride_dfinal_states_batch + pid_h *\n            stride_dfinal_states_head)\n    if HAS_DINITSTATES:\n        dinitstates_ptr += (pid_b * stride_dinitstates_batch + pid_h *\n            stride_dinitstates_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    dout_ptrs = dout_ptr + offs_m * stride_dout_dim\n    if CONVERT_STATES:\n        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim\n    if HAS_DFINAL_STATES:\n        dstates = tl.load(dfinal_states_ptr + offs_m *\n            stride_dfinal_states_dim, mask=offs_m < dim, other=0.0)\n    else:\n        dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n    if HAS_SEQ_IDX:\n        seq_idx = tl.load(seq_idx_ptr + (seqlen - 1) * stride_seq_idx_seqlen)\n    dstates_ptrs -= stride_dstates_chunk\n    for c in range(nchunks - 1):\n        dA_cs = tl.load(dA_cs_ptr)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            seq_idx_new = tl.load(seq_idx_ptr + ((nchunks - c - 1) *\n                chunk_size - 1) * stride_seq_idx_seqlen)\n            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n            seq_idx = seq_idx_new\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0)\n        if CONVERT_STATES:\n            tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n        ddA = tl.sum(out * dstates) * scale\n        tl.store(ddA_cs_ptr, ddA)\n        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0)\n        dstates = scale * dstates + dout\n        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n        dout_ptrs -= stride_dout_chunk\n        dstates_ptrs -= stride_dstates_chunk\n        dA_cs_ptr -= stride_dA_cs_chunk\n        ddA_cs_ptr -= stride_ddA_cs_chunk\n        out_ptrs -= stride_out_chunk\n        if CONVERT_STATES:\n            states_converted_ptrs -= stride_out_chunk\n    if CONVERT_STATES:\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0)\n        tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n    if not HAS_DINITSTATES:\n        tl.store(ddA_cs_ptr, 0.0)\n    else:\n        dA_cs = tl.load(dA_cs_ptr)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            scale = tl.where(seq_idx == 0, scale, 0.0)\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0)\n        ddA = tl.sum(out * dstates) * scale\n        tl.store(ddA_cs_ptr, ddA)\n        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0)\n        dstates = scale * dstates + dout\n        tl.store(dinitstates_ptr + offs_m * stride_dinitstates_dim, dstates,\n            mask=offs_m < dim)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "7f8c254e-6a96-4525-86df-eb7df749eb76"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_bwd_dx_fused(DX, DY, DW, X, W, Rstd, Lock, stride, N, eps,\n    GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    rstd = tl.load(Rstd + row)\n    xhat = x * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    dx = (wdy - xhat * c1) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "3df5aeab-6539-46cd-b391-136efb1cb2a8"
  },
  {
    "input": "@triton.heuristics({'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _fwd_attention_kernel(Q, K, V, B, softmax_scale: 'tl.constexpr',\n    stride_qb, stride_qh, stride_qg, stride_qm, stride_kb, stride_kh,\n    stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh,\n    stride_bg, stride_bm, stride_bn, stride_ob, stride_oh, stride_og,\n    stride_om, stride_lb, stride_lh, stride_lg, headdim: 'tl.constexpr',\n    num_kv_heads: 'tl.constexpr', num_groups: 'tl.constexpr', CQL:\n    'tl.constexpr', CKL: 'tl.constexpr', seqlen_q, seqlen_k, O, L,\n    HAVE_BIAS: 'tl.constexpr', BIAS_SINGLE_HEAD: 'tl.constexpr',\n    BLOCK_HEADDIM: 'tl.constexpr', EVEN_N: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m, off_bh, off_gp = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    off_h = off_bh % num_kv_heads\n    off_b = off_bh // num_kv_heads\n    if not EVEN_N:\n        offs_n = tl.arange(0, BLOCK_N)\n    Q_Block_ptr = tl.make_block_ptr(base=Q + (off_b * stride_qb + off_h *\n        stride_qh + off_gp * stride_qg), shape=(seqlen_q, headdim),\n        block_shape=(BLOCK_M, BLOCK_HEADDIM), strides=(stride_qm, 1),\n        offsets=(start_m * BLOCK_M, 0), order=(0, 1))\n    O_Block_ptr = tl.make_block_ptr(base=O + (off_b * stride_ob + off_h *\n        stride_oh + off_gp * stride_og), shape=(seqlen_q, headdim),\n        block_shape=(BLOCK_M, BLOCK_HEADDIM), strides=(stride_om, 1),\n        offsets=(start_m * BLOCK_M, 0), order=(0, 1))\n    L_Block_ptr = tl.make_block_ptr(base=L + (off_b * stride_lb + off_h *\n        stride_lh + off_gp * stride_lg), shape=(seqlen_q,), strides=(1,),\n        offsets=(start_m * BLOCK_M,), block_shape=(BLOCK_M,), order=(0,))\n    kv_stride = off_b * stride_kb + off_h * stride_kh\n    K_Block_ptr = tl.make_block_ptr(base=K + kv_stride, shape=(headdim,\n        seqlen_k), block_shape=(BLOCK_HEADDIM, BLOCK_N), strides=(1,\n        stride_kn), offsets=(0, 0), order=(1, 0))\n    V_Block_ptr = tl.make_block_ptr(base=V + kv_stride, shape=(seqlen_k,\n        headdim), block_shape=(BLOCK_N, BLOCK_HEADDIM), strides=(stride_vn,\n        1), offsets=(0, 0), order=(0, 1))\n    q = tl.load(Q_Block_ptr, boundary_check=(0, 1))\n    softmax_scale = softmax_scale\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = (0 if BIAS_SINGLE_HEAD else off_h *\n            stride_bh + off_gp * stride_bg)\n        B_Block_ptr = tl.make_block_ptr(base=B + (off_b * stride_bb +\n            bias_h_pos), shape=(seqlen_q, seqlen_k), block_shape=(BLOCK_M,\n            BLOCK_N), strides=(stride_bm, stride_bn), offsets=(start_m *\n            BLOCK_M, 0), order=(0, 1))\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    max_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    for j in range(0, seqlen_k, BLOCK_N):\n        j = tl.multiple_of(j, BLOCK_N)\n        k = tl.load(K_Block_ptr, boundary_check=(0, 1))\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k) * softmax_scale\n        if not EVEN_N:\n            qk += tl.where((j + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n        if HAVE_BIAS:\n            b = tl.load(B_Block_ptr, boundary_check=(0, 1))\n            B_Block_ptr = tl.advance(B_Block_ptr, (0, BLOCK_N))\n            qk = qk + b\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        else:\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(max_i - max_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v = tl.load(V_Block_ptr, boundary_check=(0, 1))\n        acc_o += tl.dot(p, v)\n        max_i = max_ij\n        lse_i = max_ij + tl.log(tl.exp(lse_i - max_ij) + l_ij)\n        K_Block_ptr = tl.advance(K_Block_ptr, (0, BLOCK_N))\n        V_Block_ptr = tl.advance(V_Block_ptr, (BLOCK_N, 0))\n    o_scale = tl.exp(max_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    tl.store(L_Block_ptr, lse_i, boundary_check=(0,))\n    tl.store(O_Block_ptr, acc_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "85af7d05-2438-48d0-84c6-4d639d2feba4"
  },
  {
    "input": "@triton.jit\ndef int_to_randn_kernel(x1, x2, out, N: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', seed: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) < N\n    x1_buffer = tl.load(x1 + offs, mask=offs_mask)\n    x2_buffer = tl.load(x2 + offs, mask=offs_mask)\n    seed_buffer = tl.full((BLOCK_SIZE,), seed, dtype=tl.int64)\n    r = int_to_randn(x1_buffer, x2_buffer, seed_buffer)\n    tl.store(out + offs, r, mask=offs_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "7e449691-e876-4cea-8508-19be23eb8193"
  },
  {
    "input": "@triton.jit\ndef _kernel_inside_merge_discontinuous_v2(alpha_c, alpha_d, tmp_merge,\n    tmp_merge_normalized, tmp_normalizer, w, batch, L, stride_alpha_c1,\n    stride_alpha_c2, stride_alpha_c3, stride_alpha_d1, stride_alpha_d2,\n    stride_alpha_d3, stride_alpha_d4, stride_alpha_d5, stride_tmp_merge1,\n    stride_tmp_merge2, stride_tmp_merge3, stride_tmp_normalizer1,\n    stride_tmp_normalizer2, r1, r2, r3, r4, BLOCK_R3: 'tl.constexpr',\n    BLOCK_R4: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    if b_idx >= batch:\n        return\n    span_length_left = tl.program_id(1) + 1\n    tid = tl.program_id(2)\n    start = 0\n    while tid >= L - w - start:\n        tid -= L - w - start\n        start += 1\n    gap_start = start + span_length_left\n    gap_end = gap_start + (tid + 1)\n    end = gap_end + (w - span_length_left)\n    acc2 = tl.zeros((BLOCK_R4,), dtype=tl.float32) - 1000000000.0\n    alpha_c_ptr = (alpha_c + b_idx * stride_alpha_c1 + 2 * r1 + r2 + 2 * r3 +\n        tl.arange(0, BLOCK_R4))\n    alpha_d_ptr = alpha_d + b_idx * stride_alpha_d1 + r2 + tl.arange(0,\n        BLOCK_R4)\n    mask = tl.arange(0, BLOCK_R4) < r4\n    for split in range(start + 1, gap_start):\n        c_ptr = alpha_c_ptr + start * stride_alpha_c2 + split * stride_alpha_c3\n        d_ptr = (alpha_d_ptr + split * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        child_c = tl.load(c_ptr, mask, other=-1000000000.0)\n        child_d = tl.load(d_ptr, mask, other=-1000000000.0)\n        acc2 = logaddexp(acc2, child_c + child_d)\n        c_ptr = (alpha_c_ptr + split * stride_alpha_c2 + gap_start *\n            stride_alpha_c3 + r4)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + split *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        child_c = tl.load(c_ptr, mask, other=-1000000000.0)\n        child_d = tl.load(d_ptr, mask, other=-1000000000.0)\n        acc2 = logaddexp(acc2, child_c + child_d)\n    for split in range(gap_end + 1, end):\n        c_ptr = (alpha_c_ptr + gap_end * stride_alpha_c2 + split *\n            stride_alpha_c3 + 2 * r4)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + split * stride_alpha_d4 + end * stride_alpha_d5)\n        child_c = tl.load(c_ptr, mask, other=-1000000000.0)\n        child_d = tl.load(d_ptr, mask, other=-1000000000.0)\n        acc2 = logaddexp(acc2, child_c + child_d)\n        c_ptr = (alpha_c_ptr + split * stride_alpha_c2 + end *\n            stride_alpha_c3 + 3 * r4)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + split *\n            stride_alpha_d5)\n        child_c = tl.load(c_ptr, mask, other=-1000000000.0)\n        child_d = tl.load(d_ptr, mask, other=-1000000000.0)\n        acc2 = logaddexp(acc2, child_c + child_d)\n    acc_max = tl.max(acc2, 0)\n    tl.store(tmp_normalizer + b_idx * stride_tmp_normalizer1 + tl.\n        program_id(1) * stride_tmp_normalizer2 + tl.program_id(2), acc_max)\n    ptr = b_idx * stride_tmp_merge1 + tl.program_id(1\n        ) * stride_tmp_merge2 + tl.program_id(2\n        ) * stride_tmp_merge3 + tl.arange(0, BLOCK_R4)\n    out = tl.exp(acc2 - acc_max)\n    tl.store(tmp_merge + ptr, acc2, mask=mask)\n    tl.store(tmp_merge_normalized + ptr, out, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "cb768904-5d8f-4104-8abc-98f464851796"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, DO, Delta, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    D_HEAD: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_hz = tl.program_id(1)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(O + off_hz * D_HEAD * N_CTX + off_m[:, None] * D_HEAD +\n        off_n[None, :])\n    do = tl.load(DO + off_hz * D_HEAD * N_CTX + off_m[:, None] * D_HEAD +\n        off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "76f5721e-9d3b-44c3-81b7-865bf8610d39"
  },
  {
    "input": "@triton.jit\ndef breakpoint_if(conds):\n    \"\"\"Stop kernel, if condition on pids is fulfilled\"\"\"\n    if test_pid_conds(conds):\n        set_trace()\n",
    "category": "Helper Functions",
    "subcategory": "",
    "uuid": "5064ae81-cd22-47c4-942c-54ee022c3092"
  },
  {
    "input": "@triton.jit\ndef fwd_recurrence(A, B, C, Dt, X, Y, H, start, initial_state, T:\n    'tl.constexpr', D: 'tl.constexpr', K: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    i_v = tl.program_id(1)\n    dt_ptr = Dt + i_bh * T * D + i_v * BV + tl.arange(0, BV)\n    u_ptr = X + i_bh * T * D + i_v * BV + tl.arange(0, BV)\n    o_ptr = Y + i_bh * T * D + i_v * BV + tl.arange(0, BV)\n    start_ptr = start + i_bh * T\n    h = tl.zeros([BV, K], dtype=tl.float32)\n    b_ptr = B + i_bh * T * K + tl.arange(0, K)\n    A = A + (i_v * BV + tl.arange(0, BV)[:, None]) * K + tl.arange(0, K)[\n        None, :]\n    _A = tl.load(A)\n    H_ptr = H + i_bh * T * D * K + (i_v * BV + tl.arange(0, BV)[:, None]\n        ) * K + tl.arange(0, K)[None, :]\n    h += tl.load(initial_state + i_bh * D * K + (i_v * BV + tl.arange(0, BV\n        )[:, None]) * K + tl.arange(0, K)[None, :])\n    for i in range(T):\n        b = tl.load(b_ptr)\n        dt = tl.load(dt_ptr)\n        start_flag = tl.load(start_ptr)\n        u = tl.load(u_ptr)\n        x_dt = u * dt\n        x_dt_b = x_dt[:, None] * b[None, :]\n        dt_a = tl.exp(dt[:, None] * _A) * (1 - start_flag)\n        h = h * dt_a + x_dt_b\n        tl.store(H_ptr, h)\n        b_ptr += K\n        dt_ptr += D\n        start_ptr += 1\n        u_ptr += D\n        o_ptr += D\n        H_ptr += D * K\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "72a574f2-5cd1-42c2-8813-bcd3b2dbb995"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'\n    ):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_0o = tl.zeros([BV], dtype=tl.float32)\n    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_0o = 0\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT])\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1)) * scale\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_z = tl.zeros([BT], dtype=tl.float32)\n        b_o += b_h_0o\n        b_z += k_0o\n        b_o += tl.dot(b_q, b_h_1o, allow_tf32=False)\n        b_z += tl.sum(b_q * k_1o, axis=1)\n        b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK])\n        b_o += tl.dot(b_q_2o, b_h_2o, allow_tf32=False) * 0.5\n        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n        k_1o += tl.sum(b_k, axis=1)[None, :]\n        k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n        k_0o += BT\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        tl.store(p_z, b_z, mask=i * BT + tl.arange(0, BT) < T)\n        b_h_2o = b_h_2o + tl.dot(b_k_2o, b_v, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_z += BT\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e7d64a30-c369-40b2-99cb-41a91514c1a9"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128,\n    'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, fpb_ptr, K, N, stride_bk,\n    stride_bn, stride_fpbk, stride_fpbn, BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "6b7de17e-b5dd-4193-bdc4-6f71439c3921"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef logcumsumexp_fwd_kernel(s, z, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_mp = tl.full([S], float('-inf'), dtype=tl.float32)\n    b_zp = tl.zeros([S], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_mc = tl.max(b_s, 0)\n        if i_t > 0:\n            b_mc = tl.maximum(b_mp, b_mc)\n        b_zp = b_zp * tl.exp(b_mp - b_mc)\n        b_s = tl.exp(b_s - b_mc)\n        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp\n        b_zc = tl.max(b_z, 0)\n        b_mp = b_mc\n        b_zp = b_zc\n        b_z = tl.log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc\n        tl.store(p_z, b_z, boundary_check=(0, 1))\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "dfd753d9-24d6-45c4-8fee-464e6bcb5606"
  },
  {
    "input": "@triton.jit\ndef _contract_pi_one(x, n):\n    x_c = tl.where(n <= 1.0, x, tl.where(tl.abs(tl.abs(x) - n) <= 1e-08, (2 -\n        1 / tl.abs(x)) * (x / tl.abs(x)), x / n))\n    x_c = x_c * 0.5\n    return x_c\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "e8e1ab27-4caf-43a4-ba18-2850428f809c"
  },
  {
    "input": "@triton.jit\ndef _sample_grid_rep(xy, yz, zx, batch_index, sample_x, sample_y, sample_z,\n    batch_size: 'tl.constexpr', C: 'tl.constexpr', D: 'tl.constexpr', H:\n    'tl.constexpr', W: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    shape_representation: 'tl.constexpr'):\n    if shape_representation == 0:\n        a = _grid_sample(xy, batch_index, sample_x, sample_y, batch_size, C,\n            H, W, BLOCK_SIZE)\n        b = _grid_sample(yz, batch_index, sample_y, sample_z, batch_size, C,\n            D, H, BLOCK_SIZE)\n        c = _grid_sample(zx, batch_index, sample_z, sample_x, batch_size, C,\n            W, D, BLOCK_SIZE)\n        vec = a + b + c\n    else:\n        vec = _voxel_grid_sample(xy, batch_index, sample_x, sample_y,\n            sample_z, batch_size, C, D, H, W, BLOCK_SIZE)\n    vec = tl.view(vec, (BLOCK_SIZE, C))\n    return vec\n",
    "category": "Helper Functions",
    "subcategory": "shape manipulation",
    "uuid": "c2e6560d-e0d4-45b5-a7a8-59402de60d2f"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, B, Rstd, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n    Kernel invocation for forward pass of RMS normalization with fused operations\n\n    Params:\n        - X (tensor): Input tensor\n        - Y (tensor): Output tensor where the normalized results will be written\n        - W (tensor): Scale tensor applied to the normalized input\n        - B (tensor): Bias tensor added to the scaled input\n        - Rstd (tensor): Reciprocal of the standard deviation used for normalization\n        - stride (int): Stride to be applied when accessing elements in the input and output tensors\n        - N (int): Number of elements in the input tensor\n        - eps (float): Small epsilon value added to the variance to prevent division by zero\n        - BLOCK_SIZE (constexpr): Size of the block for computation, provided as a compile-time constant\n\n    Return:\n        - None\n\n    Usage:\n        _rms_norm_fwd_fused[grid, block](X, Y, W, B, Rstd, stride, N, eps, BLOCK_SIZE)\n    \"\"\"\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _rms = 0\n    _rms = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _rms += a * a\n    rms = tl.sqrt(tl.sum(_rms) / N + eps)\n    tl.store(Rstd + row, rms)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first')\n        x_hat = x / rms\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "7cbbe93e-870c-434c-a14b-03f88556a03b"
  },
  {
    "input": "@triton.jit\ndef _bwd_kv_bias_kernel(Q, K, V, BW, sm_scale, DO, DK, DV, DB, L, D,\n    cu_seqlens_q, cu_seqlens_k, nid_batch, nid_start, stride_qz, stride_qh,\n    stride_qk, stride_kz, stride_kh, stride_kk, stride_vz, stride_vh,\n    stride_vk, stride_doz, stride_doh, stride_dok, stride_dkz, stride_dkh,\n    stride_dkk, stride_dvz, stride_dvh, stride_dvk, stride_bw, Z, H, M, N,\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', HAS_BIAS: 'tl.constexpr',\n    NUM_BUCKETS: 'tl.constexpr', MAX_DISTANCE: 'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    start_z = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_b = tl.load(nid_batch + start_z)\n    off_n = tl.load(nid_start + start_z)\n    q_start = tl.load(cu_seqlens_q + off_b)\n    q_end = tl.load(cu_seqlens_q + off_b + 1)\n    k_start = tl.load(cu_seqlens_k + off_b)\n    k_end = tl.load(cu_seqlens_k + off_b + 1)\n    lM = q_end - q_start\n    lN = k_end - k_start\n    P_SEQ = lM - lN\n    D += q_start * H + off_h\n    L += q_start * H + off_h\n    if CAUSAL:\n        lo = tl.maximum(off_n - k_start - P_SEQ, 0)\n        lo = lo // BLOCK_M * BLOCK_M\n    else:\n        lo = 0\n    offs_m_init = lo + tl.arange(0, BLOCK_M) + q_start\n    offs_n = tl.arange(0, BLOCK_N) + off_n\n    offs_n_relative = offs_n - k_start\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m_init[:, None] * stride_qz + offs_k[None, :] *\n        stride_qk + off_h * stride_qh)\n    k_ptrs = K + (offs_n[:, None] * stride_kz + offs_k[None, :] * stride_kk +\n        off_h * stride_kh)\n    v_ptrs = V + (offs_n[:, None] * stride_vz + offs_k[None, :] * stride_vk +\n        off_h * stride_vh)\n    do_ptrs = DO + (offs_m_init[:, None] * stride_doz + offs_k[None, :] *\n        stride_dok + off_h * stride_doh)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvz + offs_k[None, :] *\n        stride_dvk + off_h * stride_dvh)\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkz + offs_k[None, :] *\n        stride_dkk + off_h * stride_dkh)\n    mask_n = offs_n < k_end\n    v = tl.load(v_ptrs, mask=mask_n[:, None])\n    k = tl.load(k_ptrs, mask=mask_n[:, None])\n    dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    for start_m in range(lo, lM, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m = start_m + offs_m_base\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n        mask_m = offs_m < lM\n        valid_mask = mask_m[:, None]\n        q = tl.load(q_ptrs, mask=mask_m[:, None])\n        if HAS_BIAS:\n            relative_positions = offs_n_relative[None, :] - offs_m[:, None]\n            relative_buckets = tl.zeros_like(relative_positions)\n            num_buckets = NUM_BUCKETS\n            if not CAUSAL:\n                num_buckets //= 2\n                relative_buckets += (relative_positions > 0) * num_buckets\n                relative_positions = tl.abs(relative_positions)\n            else:\n                relative_positions = tl.maximum(-relative_positions, tl.\n                    zeros_like(relative_positions))\n            max_exact = num_buckets // 2\n            is_small = relative_positions < max_exact\n            relative_position_if_large = max_exact + tl.log(\n                relative_positions.to(tl.float32) / max_exact) / tl.log(\n                MAX_DISTANCE / max_exact) * (num_buckets - max_exact)\n            relative_position_if_large = tl.minimum(relative_position_if_large,\n                num_buckets - 1)\n            relative_buckets += tl.where(is_small, relative_positions,\n                relative_position_if_large)\n            bucket_offs = relative_buckets * stride_bw + off_h\n            bias_ptrs = BW + bucket_offs\n            b = tl.load(bias_ptrs, mask=mask_m[:, None] & mask_n[None, :])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k)) * sm_scale\n        if HAS_BIAS:\n            s += b\n        l = tl.load(L + offs_m * H, mask=mask_m)\n        p = tl.math.exp2((s - l[:, None]) * log2e)\n        p = tl.where(valid_mask, p, 0.0)\n        if CAUSAL:\n            p = tl.where(causal_mask, p, 0.0)\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n        dv += tl.dot(tl.trans(p), do)\n        delta = tl.load(D + offs_m * H, mask=mask_m)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        ds = tl.where(valid_mask, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        ds = ds\n        dk += tl.dot(tl.trans(ds), q)\n        if HAS_BIAS:\n            db_ptrs = DB + bucket_offs\n            tl.atomic_add(db_ptrs, ds, mask=relative_buckets < NUM_BUCKETS)\n        q_ptrs += BLOCK_M * stride_qz\n        do_ptrs += BLOCK_M * stride_doz\n    dk *= sm_scale\n    tl.store(dk_ptrs, dk, mask=mask_n[:, None])\n    tl.store(dv_ptrs, dv, mask=mask_n[:, None])\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "83ae08ad-06d8-48a5-b21d-9cecfe26210f"
  },
  {
    "input": "@triton.jit\ndef print_tensor_dim(tensor, str_name):\n    if tl.program_id(0) == 0 and tl.program_id(1) == 0:\n        tl.static_print(str_name, ' ', tensor.shape, ' ', tensor.dtype)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "60a103b8-d72a-4e26-80cd-17e1d3a9e475"
  },
  {
    "input": "@triton.jit\ndef _triton_logits_processor_kernel(scores, penalty, input_ids_ptr,\n    input_ids_length, num_tokens: 'tl.constexpr', vocab_size:\n    'tl.constexpr', max_ids_length: 'tl.constexpr', power_2_of_vocab_size:\n    'tl.constexpr', power_2_of_max_ids_length: 'tl.constexpr', penalty_ty:\n    'tl.constexpr'):\n    token_id = tl.program_id(0)\n    penalty_val = tl.load(penalty + token_id)\n    if tl.abs(penalty_val - 1.0) > 1e-09:\n        input_ids_address = tl.load(input_ids_ptr + token_id)\n        current_input_ids_length = tl.load(input_ids_length + token_id)\n        ids_offs = tl.arange(0, power_2_of_max_ids_length)\n        ids = tl.load(input_ids_address + ids_offs, mask=ids_offs <\n            current_input_ids_length, other=vocab_size)\n        ori_scores = tl.load(scores + token_id * vocab_size + ids[None, :],\n            mask=ids[None, :] < vocab_size, other=0.0)\n        tl.debug_barrier()\n        if penalty_ty == 'REPETITION':\n            new_scores = tl.where(ori_scores <= 0, ori_scores * penalty_val,\n                ori_scores / penalty_val)\n        elif penalty_ty == 'PRESENCE':\n            new_scores = ori_scores - penalty_val\n        tl.store(scores + token_id * vocab_size + ids[None, :], new_scores,\n            mask=ids[None, :] < vocab_size)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "a14da276-4a8a-4f86-91f9-acb2f722e0bc"
  },
  {
    "input": "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] ==\n    args['V_BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_permuted_block_diagonal_kernel(Q, K, V, q_sort_idx, k_sort_idx, DO,\n    DQ, DK, DV, LSE, D, softmax_scale, stride_qb, stride_qh, stride_qm,\n    stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn,\n    stride_q_sort_idxb, stride_q_sort_idxh, stride_q_sort_idxm,\n    stride_k_sort_idxb, stride_k_sort_idxh, stride_k_sort_idxn, stride_dob,\n    stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm, stride_dkb,\n    stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn, nheads,\n    seqlen_q, block_size, headdim, v_headdim, smooth_block,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BLOCK_HEADDIM: 'tl.constexpr',\n    V_BLOCK_HEADDIM: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    EVEN_V_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    Q_idx = (q_sort_idx + off_b * stride_q_sort_idxb + off_h *\n        stride_q_sort_idxh)\n    K_idx = (k_sort_idx + off_b * stride_k_sort_idxb + off_h *\n        stride_k_sort_idxh)\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    D += off_hb * seqlen_q\n    LSE += off_hb * seqlen_q\n    start_n = tl.program_id(0)\n    _bwd_blocked_kernel_one_col(start_n=start_n, Q=Q, K=K, V=V, Q_idx=Q_idx,\n        K_idx=K_idx, DO=DO, DQ=DQ, DK=DK, DV=DV, LSE=LSE, D=D,\n        softmax_scale=softmax_scale, stride_qm=stride_qm, stride_kn=\n        stride_kn, stride_vn=stride_vn, stride_dom=stride_dom, stride_dqm=\n        stride_dqm, stride_dkn=stride_dkn, stride_dvn=stride_dvn,\n        stride_q_idxm=stride_q_sort_idxm, stride_k_idxn=stride_k_sort_idxn,\n        seqlen_q=seqlen_q, block_size=block_size // BLOCK_N, headdim=\n        headdim, v_headdim=v_headdim, smooth_block=smooth_block,\n        BLOCK_HEADDIM=BLOCK_HEADDIM, V_BLOCK_HEADDIM=V_BLOCK_HEADDIM,\n        EVEN_HEADDIM=EVEN_HEADDIM, EVEN_V_HEADDIM=EVEN_V_HEADDIM, BLOCK_M=\n        BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "825a32bb-ef75-46ec-beee-e6d131b875f0"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_flex(x: 'tl.tensor', y: 'tl.tensor', x_layout:\n    'tl.constexpr', y_layout: 'tl.constexpr', operation: 'tl.constexpr',\n    onebyone: 'tl.constexpr', scans: 'tl.constexpr', BC: 'tl.constexpr', BH:\n    'tl.constexpr', BW: 'tl.constexpr', DC: 'tl.constexpr', DH:\n    'tl.constexpr', DW: 'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'\n    ):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    pos_h = i_h * BH + tl.arange(0, BH)[:, None]\n    pos_w = i_w * BW + tl.arange(0, BW)[None, :]\n    neg_h = DH - i_h * BH - 1 - tl.arange(0, BH)[:, None]\n    neg_w = DW - i_w * BW - 1 - tl.arange(0, BW)[None, :]\n    if scans == 0:\n        HWRoute0 = pos_h * DW + pos_w\n        HWRoute1 = pos_w * DH + pos_h\n        HWRoute2 = neg_h * DW + neg_w\n        HWRoute3 = neg_w * DH + neg_h\n    elif scans == 1:\n        HWRoute0 = pos_h * DW + pos_w\n        HWRoute1 = HWRoute0\n        HWRoute2 = HWRoute0\n        HWRoute3 = HWRoute0\n    elif scans == 2:\n        HWRoute0 = pos_h * DW + pos_w\n        HWRoute1 = HWRoute0\n        HWRoute2 = neg_h * DW + neg_w\n        HWRoute3 = HWRoute2\n    _tmp1 = DC * DH * DW\n    y_ptr_base = y + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if y_layout == 0\n         else i_c * BC)\n    if y_layout == 0:\n        p_y1 = y_ptr_base + HWRoute0\n        p_y2 = y_ptr_base + _tmp1 + HWRoute1\n        p_y3 = y_ptr_base + 2 * _tmp1 + HWRoute2\n        p_y4 = y_ptr_base + 3 * _tmp1 + HWRoute3\n    else:\n        p_y1 = y_ptr_base + HWRoute0 * 4 * DC\n        p_y2 = y_ptr_base + DC + HWRoute1 * 4 * DC\n        p_y3 = y_ptr_base + 2 * DC + HWRoute2 * 4 * DC\n        p_y4 = y_ptr_base + 3 * DC + HWRoute3 * 4 * DC\n    if onebyone == 0:\n        x_ptr_base = x + i_b * _tmp1 + (i_c * BC * DH * DW if x_layout == 0\n             else i_c * BC)\n        if x_layout == 0:\n            p_x = x_ptr_base + HWRoute0\n        else:\n            p_x = x_ptr_base + HWRoute0 * DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _x = tl.load(p_x + _idx_x, mask=_mask_hw)\n                tl.store(p_y1 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, _x, mask=_mask_hw)\n        elif operation == 1:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _y1 = tl.load(p_y1 + _idx_y, mask=_mask_hw)\n                _y2 = tl.load(p_y2 + _idx_y, mask=_mask_hw)\n                _y3 = tl.load(p_y3 + _idx_y, mask=_mask_hw)\n                _y4 = tl.load(p_y4 + _idx_y, mask=_mask_hw)\n                tl.store(p_x + _idx_x, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n    else:\n        x_ptr_base = x + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if x_layout ==\n            0 else i_c * BC)\n        if x_layout == 0:\n            p_x1 = x_ptr_base + HWRoute0\n            p_x2 = p_x1 + _tmp1\n            p_x3 = p_x2 + _tmp1\n            p_x4 = p_x3 + _tmp1\n        else:\n            p_x1 = x_ptr_base + HWRoute0 * 4 * DC\n            p_x2 = p_x1 + DC\n            p_x3 = p_x2 + DC\n            p_x4 = p_x3 + DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_y1 + _idx_y, tl.load(p_x1 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, tl.load(p_x2 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, tl.load(p_x3 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, tl.load(p_x4 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n        else:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_x1 + _idx_x, tl.load(p_y1 + _idx_y), mask=_mask_hw)\n                tl.store(p_x2 + _idx_x, tl.load(p_y2 + _idx_y), mask=_mask_hw)\n                tl.store(p_x3 + _idx_x, tl.load(p_y3 + _idx_y), mask=_mask_hw)\n                tl.store(p_x4 + _idx_x, tl.load(p_y4 + _idx_y), mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "fbfd3fad-ea07-48b5-969a-da900b9ac2a1"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "78594206-5e67-4d08-b314-374ac4edd595"
  },
  {
    "input": "@triton.jit\ndef _bwd_none_diag_kernel(Q, K, V, S, DO, DQ, DK, DV, DKV, b:\n    'tl.constexpr', h: 'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr',\n    e: 'tl.constexpr', BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr',\n    CBLOCK: 'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_h = off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    kv_offset = off_bh * d * e\n    block_offset = off_block * BLOCK\n    qk_block_offset = block_offset * d\n    v_block_offset = block_offset * e\n    o_block_offset = block_offset * e\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK\n    qk_block_offset = block_offset * d\n    v_block_offset = block_offset * e\n    o_block_offset = block_offset * e\n    S_block_ptr = S + off_h\n    s = tl.load(S_block_ptr)\n    block_decay = tl.exp(-s * BLOCK)\n    DQ_block_ptr = DQ + qk_offset + qk_block_offset + tl.arange(0, CBLOCK)[\n        :, None] * d + tl.arange(0, d)[None, :]\n    K_block_ptr = K + qk_offset + qk_block_offset + tl.arange(0, CBLOCK)[:,\n        None] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + v_block_offset + tl.arange(0, CBLOCK)[\n        None, :] * e + tl.arange(0, e)[:, None]\n    DO_block_ptr = DO + o_offset + o_block_offset + tl.arange(0, CBLOCK)[:,\n        None] * e + tl.arange(0, e)[None, :]\n    DKV_block_ptr = DKV + kv_offset + tl.arange(0, d)[:, None] * e + tl.arange(\n        0, e)[None, :]\n    c_array = tl.arange(0, CBLOCK)\n    kv_trans = tl.zeros([e, d], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        for j in range(NUM_CBLOCK):\n            q_decay = tl.exp(-s * (j * CBLOCK + c_array[:, None]))\n            do = tl.load(DO_block_ptr)\n            dq_none_diag = tl.dot(do, kv_trans) * q_decay\n            dq = dq_none_diag + tl.load(DQ_block_ptr)\n            tl.store(DQ_block_ptr, dq)\n            DQ_block_ptr += CBLOCK * d\n            DO_block_ptr += CBLOCK * e\n        kv_trans_current = tl.zeros([e, d], dtype=tl.float32)\n        for j in range(NUM_CBLOCK):\n            v_trans = tl.load(V_trans_block_ptr)\n            k = tl.load(K_block_ptr)\n            k_decay = tl.exp(-s * (BLOCK - (j * CBLOCK + c_array[:, None])))\n            kv_trans_current += tl.dot(v_trans, k * k_decay)\n            K_block_ptr += CBLOCK * d\n            V_trans_block_ptr += CBLOCK * e\n        kv_trans = block_decay * kv_trans + kv_trans_current\n    Q_trans_block_ptr = Q + qk_offset + qk_block_offset + n * d + tl.arange(\n        0, CBLOCK)[None, :] * d + tl.arange(0, d)[:, None]\n    K_block_ptr = K + qk_offset + qk_block_offset + n * d + tl.arange(0, CBLOCK\n        )[:, None] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + v_block_offset + n * e + tl.arange(0,\n        CBLOCK)[None, :] * e + tl.arange(0, e)[:, None]\n    DK_trans_block_ptr = DK + qk_offset + qk_block_offset + n * d + tl.arange(\n        0, CBLOCK)[None, :] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + v_block_offset + n * e + tl.arange(0, CBLOCK\n        )[:, None] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + o_block_offset + n * e + tl.arange(0, CBLOCK\n        )[:, None] * e + tl.arange(0, e)[None, :]\n    dkv = tl.zeros([d, e], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            K_block_ptr -= CBLOCK * d\n            V_trans_block_ptr -= CBLOCK * e\n            DK_trans_block_ptr -= CBLOCK * d\n            DV_block_ptr -= CBLOCK * e\n            k = tl.load(K_block_ptr)\n            v_trans = tl.load(V_trans_block_ptr)\n            k_decay_trans = tl.exp(-s * (BLOCK - (j * CBLOCK + c_array[None,\n                :])))\n            k_decay = tl.exp(-s * (BLOCK - (j * CBLOCK + c_array[:, None])))\n            dk_none_diag_trans = tl.dot(dkv, v_trans) * k_decay_trans\n            dv_none_diag = tl.dot(k, dkv) * k_decay\n            dk_trans = dk_none_diag_trans + tl.load(DK_trans_block_ptr)\n            dv = dv_none_diag + tl.load(DV_block_ptr)\n            tl.store(DK_trans_block_ptr, dk_trans)\n            tl.store(DV_block_ptr, dv)\n        dkv_current = tl.zeros([d, e], dtype=tl.float32)\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            DO_block_ptr -= CBLOCK * e\n            Q_trans_block_ptr -= CBLOCK * d\n            do = tl.load(DO_block_ptr)\n            q_trans = tl.load(Q_trans_block_ptr)\n            q_decay_trans = tl.exp(-s * (j * CBLOCK + c_array[None, :]))\n            dkv_current += tl.dot(q_trans * q_decay_trans, do)\n        dkv = block_decay * dkv + dkv_current\n    tl.store(DKV_block_ptr, dkv)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "89e08de0-134f-4aba-8a65-6729e73125ae"
  },
  {
    "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, L, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX, sliding_window_offset,\n    sliding_window_size, IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', END: 'tl.constexpr', INIT: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    v_offset = off_z * stride_vz + off_hkv * stride_vh\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + v_offset, shape=(NKV_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(ROUND_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    m_ptrs = M + off_hz * ROUND_CTX + offs_m\n    l_ptrs = L + off_hz * ROUND_CTX + offs_m\n    if INIT:\n        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    else:\n        m_i = tl.load(m_ptrs)\n        l_i = tl.load(l_ptrs)\n        acc = tl.load(O_block_ptr)\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n        V_block_ptr, start_m, qk_scale, NKV_CTX, sliding_window_offset,\n        sliding_window_size, BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW,\n        IS_EVEN_M, IS_EVEN_N, COMPLEMENT_SLIDING_WINDOW)\n    if END:\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n    else:\n        tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "6298f77d-9dfc-45a8-8c0a-29a3994828b5"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_intra_KV(v, g, o, A, do, dv, dg, T: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BV:\n    'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr', OVERWRITE_DG:\n    'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n    if i_t * BT + i_i * BC > T:\n        return\n    p_gv = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT + \n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + min(i_t * BT +\n        i_i * BC + BC, T) * V - V + o_v, BV), BV)\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT +\n            i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (i_i *\n            BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g - b_gn[None, :])\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do)\n    b_dv *= tl.exp(b_gn[None, :] - b_gv)\n    o_i = tl.arange(0, BC)\n    o_c = i_i * BC + tl.arange(0, BC)\n    p_g = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t * BT + \n        i_i * BC) * V + o_v, BV), BV)\n    p_A = tl.max_contiguous(tl.multiple_of(A + i_bh * T * BT + (i_t * BT + \n        i_i * BC) * BT + o_c, BC), BC)\n    p_do = tl.max_contiguous(tl.multiple_of(do + i_bh * T * V + (i_t * BT +\n        i_i * BC) * V + o_v, BV), BV)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.load(p_A)\n        b_g = tl.load(p_g, mask=m_v, other=0)\n        b_do = tl.load(p_do, mask=m_v, other=0)\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(m_i, tl.exp(b_g[None, :] - b_gv) * b_A[:, None] *\n            b_do[None, :], 0.0)\n        p_g += V\n        p_A += BT\n        p_do += V\n    p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT + \n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t * BT + \n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT +\n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (i_t * BT +\n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_dg = tl.make_block_ptr(dg + i_bh * T * V, (T, V), (V, 1), (i_t * BT +\n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_o = tl.load(p_o, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = b_dv + tl.load(p_dv, boundary_check=(0, 1))\n    b_dg = b_o * b_do - b_v * b_dv\n    if not OVERWRITE_DG:\n        b_dg = b_dg + tl.load(p_dg, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "2e69cd3f-6759-450c-9da3-b2fe6e066760"
  },
  {
    "input": "@triton.autotune(DEFAULT_DEQUANT_CONFIGS, key=['numels'])\n@triton.jit\ndef dequant_kernel_248(g_idx_ptr, scales_ptr, qweight_ptr, qzeros_ptr,\n    out_ptr, numels, maxq: 'tl.constexpr', bits: 'tl.constexpr',\n    outfeatures: 'tl.constexpr', num_groups: 'tl.constexpr', X_BLOCK:\n    'tl.constexpr'):\n    xoffset = tl.program_id(0) * X_BLOCK\n    x_index = xoffset + tl.arange(0, X_BLOCK)\n    xmask = x_index < numels\n    row_idx = x_index // outfeatures\n    col_idx = x_index % outfeatures\n    elements_per_feature: 'tl.constexpr' = 32 // bits\n    g_idx = tl.load(g_idx_ptr + row_idx, None, eviction_policy='evict_last')\n    qweights = tl.load(qweight_ptr + (col_idx + outfeatures * (row_idx //\n        elements_per_feature)), None)\n    wf_weights = row_idx % elements_per_feature * bits\n    wf_zeros = col_idx % elements_per_feature * bits\n    tmp1 = g_idx + num_groups\n    tmp2 = g_idx < 0\n    tl.device_assert(g_idx >= 0, 'index out of bounds: 0 <= tmp0 < 0')\n    groups = tl.where(tmp2, tmp1, g_idx)\n    scales = tl.load(scales_ptr + (col_idx + outfeatures * groups), None)\n    weights = qweights >> wf_weights\n    weights = weights & maxq\n    qzero_ncols: 'tl.constexpr' = outfeatures // elements_per_feature\n    qzeros = tl.load(qzeros_ptr + (qzero_ncols * groups + col_idx //\n        elements_per_feature), None, eviction_policy='evict_last')\n    zeros = qzeros >> wf_zeros\n    zeros = zeros & maxq\n    zeros = zeros + 1\n    weights = weights - zeros\n    weights = weights\n    weights = scales * weights\n    tl.store(out_ptr + x_index, weights, mask=xmask)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "8e37dd6d-79ec-427c-b1ef-70260387467b"
  },
  {
    "input": "@triton.jit\ndef triton_softmax(X_ptr, Y_ptr, M, N, BLOCK_SIZE):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = tl.arange(0, BLOCK_SIZE)\n    idx = block_start + offsets\n    mask = idx < M\n    x_row = tl.load(X_ptr + idx * N, mask=mask)\n    x_max = tl.max(x_row)\n    x_shifted = x_row - x_max\n    exp_x = tl.exp(x_shifted)\n    sum_x = tl.sum(exp_x)\n    softmax_ret = exp_x / sum_x\n    tl.store(Y_ptr + idx * N, softmax_ret, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "4a7d7e14-c847-4100-8113-7726b9d60f7c"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride, BLOCK_SIZE:\n    'tl.constexpr', HAS_SMOOTHING: 'tl.constexpr', SPLIT: 'tl.constexpr',\n    PRECOMPUTED_LSE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    sum_logits = 0.0\n    if not PRECOMPUTED_LSE:\n        m_i = -float('inf')\n        l_i = 0.0\n        for col_offset in range(0, n_cols, BLOCK_SIZE):\n            cols = col_offset + tl.arange(0, BLOCK_SIZE)\n            logits = tl.load(logits_ptr + cols, mask=cols < n_cols, other=-\n                float('inf')) * logit_scale\n            if HAS_SMOOTHING:\n                sum_logits += tl.sum(tl.where(cols < n_cols, logits, 0.0))\n            m_i_new = tl.maximum(m_i, tl.max(logits))\n            l_i = tl.exp(m_i - m_i_new) * l_i + tl.sum(tl.exp(logits - m_i_new)\n                )\n            m_i = m_i_new\n        lse = tl.log(l_i) + m_i\n        tl.store(lse_ptr + row_idx, lse)\n    else:\n        lse = tl.load(lse_ptr + row_idx)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= 0 and label_idx < n_cols:\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - smoothing * sum_logits / total_classes - (1 - smoothing\n                    ) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits /\n                total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + row_idx, z_loss)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "8eea5a6e-7858-41d8-b527-e721188e2c1f"
  },
  {
    "input": "@triton.jit\ndef __flat_csr_softmax_compute(CROW_INDICES, stride_crow_n, stride_crow_r,\n    COL_INDICES, stride_col_n, stride_col_z, IN_VALUES, stride_in_n,\n    stride_in_z, OUT_VALUES, stride_out_n, stride_out_z, N, R, H, T_SRC,\n    BLOCK_Z: 'tl.constexpr', BLOCK_R: 'tl.constexpr'):\n    n = tl.program_id(0)\n    pid_ir = tl.program_id(1)\n    for i in range(BLOCK_R):\n        ir = pid_ir * BLOCK_R + i\n        ir_mask = ir < R\n        crow_start = tl.load(CROW_INDICES + n * stride_crow_n + ir *\n            stride_crow_r, mask=ir_mask)\n        crow_end = tl.load(CROW_INDICES + n * stride_crow_n + (ir + 1) *\n            stride_crow_r, mask=ir_mask)\n        row_mask = tl.arange(0, BLOCK_Z) + crow_start < crow_end\n        row = tl.load(IN_VALUES + n * stride_in_n + (tl.arange(0, BLOCK_Z) +\n            crow_start) * stride_in_z, mask=row_mask & ir_mask, other=-\n            float('inf'))\n        col_idx = tl.load(COL_INDICES + n * stride_col_n + (tl.arange(0,\n            BLOCK_Z) + crow_start) * stride_col_z, mask=row_mask & ir_mask,\n            other=0)\n        head_idx = col_idx // T_SRC\n        output = tl.zeros_like(row)\n        for ih in range(H):\n            head_mask = head_idx == ih\n            row_per_head = tl.where(head_mask, row, -float('inf'))\n            row_max = tl.max(row_per_head)\n            row_minus_max = row_per_head - row_max\n            numerator = tl.exp(row_minus_max)\n            denominator = tl.sum(numerator)\n            softmax_result = numerator / denominator\n            output += tl.where(head_mask, softmax_result, 0)\n        tl.store(OUT_VALUES + n * stride_out_n + (tl.arange(0, BLOCK_Z) +\n            crow_start) * stride_out_z, output, mask=row_mask & ir_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "030e65d1-6925-4004-bc82-6e794f6caf2c"
  },
  {
    "input": "@triton.jit\ndef _rms_kernel_bwd_dx():\n    pass\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "87c74184-add8-4a94-a169-777a3daeb3a2"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_K(v, g, o, A, s_v_h, s_v_t, s_v_d, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_gv = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = b_v * tl.exp(b_gn[None, :] - b_gv)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, b_vg, allow_tf32=False)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        p_gv = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n        b_v = tl.load(p_v, boundary_check=(0,))\n        b_gv = tl.load(p_gv, boundary_check=(0,))\n        b_vg = b_v[None, :] * tl.exp(b_g - b_gv[None, :])\n        m_i = o_i[:, None] >= j\n        b_o += tl.where(m_i, b_A[:, None] * b_vg, 0.0)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "7c2281ea-9d54-4c4d-a7c1-c939e5872fd6"
  },
  {
    "input": "@triton.autotune(configs=_get_configs(), key=['N_CTX', 'H', 'Z'])\n@triton.heuristics({'EVEN_CTX': lambda args: args['N_CTX'] % args['BLOCK_M'\n    ] == 0})\n@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, qkv_scale_ptr, out_scale_ptr, Out,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, EVEN_CTX:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(\n        BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n        ), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qkv_scale = tl.load(qkv_scale_ptr)\n    qk_scale = qkv_scale * qkv_scale * sm_scale * 1.44269504\n    if EVEN_CTX:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n    for start_n in range(0, N_CTX, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_CTX:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(1,), padding_option='zero'\n                )\n        qk = tl.dot(q, k, allow_tf32=False, out_dtype=tl.int32)\n        qk_fp32 = qk * qk_scale\n        m_ij = tl.maximum(m_i, tl.max(qk_fp32, 1))\n        p = tl.math.exp2(qk_fp32 - m_ij[:, None])\n        alpha = tl.math.exp2(m_i - m_ij)\n        m_i = m_ij\n        if EVEN_CTX:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0,), padding_option='zero'\n                )\n        v = v * qkv_scale\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v, allow_tf32=True)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    out_scale = tl.load(out_scale_ptr)\n    acc = tl.math.llrint(acc / (l_i[:, None] * out_scale))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    if EVEN_CTX:\n        tl.store(O_block_ptr, acc)\n    else:\n        tl.store(O_block_ptr, acc, boundary_check=(0,))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "a27d5e82-1afb-4567-86ad-9df2a855d530"
  },
  {
    "input": "@triton.jit\ndef conv2d_kernel(x_ptr, k_ptr, z_ptr, N0, H, W, KH: 'tl.constexpr', KW:\n    'tl.constexpr', B0: 'tl.constexpr'):\n    block_id_i = tl.program_id(0)\n    off_i = block_id_i * B0 + tl.arange(0, B0)\n    mask_i = off_i < N0\n    off_h = tl.arange(0, KH)\n    off_w = tl.arange(0, KW)\n    off_hw = off_h[:, None] * KW + off_w[None, :]\n    k = tl.load(k_ptr + off_hw)\n    for j in tl.range(0, H):\n        for l in tl.range(0, W):\n            off_j_oj = j + off_h[None, :, None]\n            off_l_ol = l + off_w[None, None, :]\n            off_x = off_i * H * W + off_j_oj * W + off_l_ol\n            mask_x = (off_j_oj < H) & (off_l_ol < W)\n            x = tl.load(x_ptr + off_x, mask=mask_x)\n            z = tl.sum(x * k[None, :])\n            off_z = off_i * H * W + j * W + l\n            tl.store(z_ptr + off_z, z)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "5b444927-e988-42cf-bb2f-227ad6af0afc"
  },
  {
    "input": "@triton.jit\ndef cdiv_fn(x, y):\n    return (x + y - 1) // y\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "135fedf7-641d-4ff7-a611-7d2f4da43471"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2),\n    triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64},\n    num_warps=4)], key=[])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_cumsum_scalar_kernel(s, o, offsets, T: 'tl.constexpr', H:\n    'tl.constexpr', BT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr',\n    USE_OFFSETS: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b), tl.load(offsets + i_b + 1)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    b_z = tl.zeros([], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n            p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,),\n                (BT,), (0,))\n        else:\n            p_s = tl.make_block_ptr(s + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n            p_o = tl.make_block_ptr(o + start * H + i_h, (T,), (H,), (i_t *\n                BT,), (BT,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0,))\n        b_o = tl.cumsum(b_s, axis=0) + b_z[None]\n        b_z += tl.sum(b_s, axis=0)\n        tl.store(p_o, b_o, boundary_check=(0,))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "afaf07a4-c36e-440b-9f6f-3ee51101b331"
  },
  {
    "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _score_kernel(Q, K, M, sm_scale, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_oz,\n    stride_oh, stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX,\n    sliding_window_offset, sliding_window_size, SLIDING_WINDOW:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M:\n    'tl.constexpr', IS_EVEN_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, start_n *\n        BLOCK_N), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        m = tl.load(m_ptrs)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m - start_n * BLOCK_N + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_M) + start_m < N_CTX)[:, None],\n                p, 0)\n        o += tl.sum(p, axis=0)\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o, mask=o_range < NKV_CTX)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "10d45f98-b716-48d4-91a2-d109249b31ac"
  },
  {
    "input": "@triton.jit\ndef quantize(x, scale, qmin, qmax) ->tl.tensor:\n    \"\"\"Quantize the tensor given quantization scale and data type.\n\n    Args:\n        x (tl.tensor): floating-point tensor\n        scale (tl.tensor): quantization scale factor.\n        qmin (Number): quantization minimum range.\n        qmax (Number): quantization maximum range\n\n    Returns:\n        tl.tensor: rounded and clamped tensor.\n            Note: this is still in floating point as we can't pass dtype to function\n\n    Example:\n    \n        out = quantize(out, scale, -128, 127).to(tl.int8)\n    \"\"\"\n    return clamp(tl.math.round(x / scale), qmin, qmax)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "a8132e1c-568c-4f81-adc6-b6f02cfcd781"
  },
  {
    "input": "@triton.jit\ndef _fwd_split_kv_kernel(Q, K, V, sm_scale, L, O, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk, stride_oz, stride_oh,\n    stride_os, stride_om, stride_ok, Z, H, M, N, P_SEQ, N_SPLIT_SIZE, S,\n    num_groups, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr', LARGER_M:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_m = tl.program_id(0)\n    n_split_id = tl.program_id(1)\n    off_zh = tl.program_id(2)\n    off_h = off_zh % H\n    off_z = off_zh // H\n    off_hk = off_h // num_groups\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_hk * stride_kh\n    V += off_z * stride_vz + off_hk * stride_vh\n    O += off_z * stride_oz + off_h * stride_oh + n_split_id * stride_os\n    L += ((off_z * H + off_h) * S + n_split_id) * M\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_m = start_m * BLOCK_M + offs_m_base\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    o_ptrs = O + (offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)\n    l_ptrs = L + offs_m\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if DIVISIBLE_M:\n        q = tl.load(q_ptrs)\n    else:\n        mask_m = offs_m < M\n        q = tl.load(q_ptrs, mask=mask_m[:, None])\n    if BLOCK_DMODEL < 128:\n        I = tl.where(offs_k[:, None] == offs_k, tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 1.0, dtype=input_dtype), tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 0.0, dtype=input_dtype))\n        q = tl.dot(q, I)\n    N_LEFT = n_split_id * N_SPLIT_SIZE\n    N_RIGHT = tl.minimum(N_LEFT + N_SPLIT_SIZE, N)\n    if IS_CAUSAL:\n        hi = tl.minimum(N_RIGHT, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(N_LEFT, hi)\n    else:\n        hi = N_RIGHT\n    offs_n_init = N_LEFT + offs_n_base\n    k_ptrs = K + (offs_k[:, None] * stride_vk + offs_n_init[None, :] *\n        stride_vn)\n    v_ptrs = V + (offs_n_init[:, None] * stride_kn + offs_k[None, :] *\n        stride_kk)\n    for start_n in range(N_LEFT, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        if DIVISIBLE_N:\n            k = tl.load(k_ptrs, cache_modifier='.cg')\n            v = tl.load(v_ptrs, cache_modifier='.cg')\n        else:\n            mask_n = offs_n < N\n            k = tl.load(k_ptrs, mask=mask_n[None, :], cache_modifier='.cg')\n            v = tl.load(v_ptrs, mask=mask_n[:, None], cache_modifier='.cg')\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, k)\n        if not DIVISIBLE_N:\n            s = tl.where(mask_n[None, :], s, float('-inf'))\n        if IS_CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            s = tl.where(causal_mask, s, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(s, 1))\n        alpha = tl.math.exp2((m_i - m_i_new) * qk_scale)\n        p = tl.math.exp2(s * qk_scale - m_i_new[:, None] * qk_scale)\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vn\n    if IS_CAUSAL and LARGER_M:\n        is_empty_line = offs_m + P_SEQ < 0\n        acc = tl.where(is_empty_line[:, None], 0.0, acc * (1.0 / l_i[:, None]))\n        l = tl.where(is_empty_line, float('-inf'), m_i * sm_scale + tl.log(l_i)\n            )\n    else:\n        acc = acc * (1.0 / l_i[:, None])\n        l = m_i * sm_scale + tl.log(l_i)\n    if DIVISIBLE_M:\n        tl.store(l_ptrs, l, cache_modifier='.cg')\n        tl.store(o_ptrs, acc, cache_modifier='.cg')\n    else:\n        tl.store(l_ptrs, l, mask=mask_m, cache_modifier='.cg')\n        tl.store(o_ptrs, acc, mask=mask_m[:, None], cache_modifier='.cg')\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "11f85472-7819-4a40-96c0-8bd3e1cca675"
  },
  {
    "input": "@triton.jit\ndef layernorm_backward(dY, dY_row_stride, X, X_row_stride, W, b, r, mu,\n    n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    inv_var = tl.load(r)\n    mean = tl.load(mu)\n    normed = (X_row - mean) * inv_var\n    dY_W = dY_row * W_row\n    dX_row = dY_W - tl.sum(dY_W, axis=0) / n_cols - normed * tl.sum(dY_W *\n        normed, axis=0) / n_cols\n    dX_row = dX_row * inv_var\n    tl.store(dY + col_offsets, dX_row, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "5c22f7af-dd6f-486f-9742-6312f3212c4c"
  },
  {
    "input": "@triton.jit\ndef load_128(addrs, mask):\n    return tl.inline_asm_elementwise(\n        \"\"\"\n        {\n            .reg .pred %p0;\n            setp.eq.s32             %p0, $3, 1;\n            @%p0 ld.global.v2.u64   {$0, $1}, [$2];\n        }\n        \"\"\"\n        , '=l,=l,l,r', args=[addrs, mask], dtype=(tl.uint64, tl.uint64),\n        is_pure=True, pack=1)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "e95889c2-79b7-4127-830a-d97c6a8b92ec"
  },
  {
    "input": "@triton.jit\ndef d_linear(d_y, w, b, x):\n    d_x = tl.dot(d_y, tl.trans(w), allow_tf32=ALLOW_TF32)\n    d_w = tl.dot(tl.trans(d_y), x, allow_tf32=ALLOW_TF32)\n    d_b = tl.sum(d_y, axis=0)\n    return d_x, d_w, d_b\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "a530dce4-5bc0-42c7-adab-ef3d1a8742e4"
  },
  {
    "input": "@triton.jit\ndef dropout(x, p, seed, offset):\n    random = tl.rand(seed, offset)\n    return tl.where(random > p, x / (1 - p), 0.0)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "334d8294-5c0a-4e4c-a946-df047672d790"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignored_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: 'tl.constexpr',\n    HAS_SMOOTHING: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing),\n            probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "59a9a330-709b-4d9e-93eb-f978eda03636"
  },
  {
    "input": "@triton.jit\ndef tr1(X, Y):\n    r = tl.arange(0, 16)\n    x = tl.load(X + r)\n    y = comp2tt(x)\n    tl.store(Y + 16 * r[:, None] + r, y)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "21732c3a-b612-47ed-9d89-5eb023a373c2"
  },
  {
    "input": "@triton.jit\ndef chunk_hgrn_bwd_kernel_o(g, gc, o, dx, dg, s_b, s_t, s_d, T:\n    'tl.constexpr', D: 'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_g = tl.make_block_ptr(g + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT - 1, i_d * BD), (BT, BD), (1, 0))\n        p_dx = tl.make_block_ptr(dx + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        mask_t = mask & ((i_t + 1) * BT < T)\n        b_ht = tl.load(dx + i_b * T * D + (i_t + 1) * BT * D + o_d, mask=\n            mask_t, other=0)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_gc = tl.load(p_gc, boundary_check=(0, 1))\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_dx = tl.load(p_dx, boundary_check=(0, 1))\n        b_dx = b_dx + tl.exp(b_gc) * b_ht[None, :]\n        b_dg = b_o * b_dx * tl.exp(b_g)\n        tl.store(p_dx, b_dx, boundary_check=(0, 1))\n        tl.store(p_dg, b_dg, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ca0acbd4-deaf-4122-a9d0-77aee8156fc6"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_retention_fwd_kernel(q, k, v, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.pow(2, -5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        h = b_b * h + _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "7353b04f-29fd-4129-9675-682da4a3142f"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] >\n    0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, label_smoothing, logit_scale, lse_square_scale,\n    ignore_index, total_classes, class_start_idx, n_cols, n_rows,\n    logits_row_stride, BLOCK_SIZE: 'tl.constexpr', HAS_SMOOTHING:\n    'tl.constexpr', SPLIT: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf'))\n    logits = logits * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols,\n            (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - label_smoothing * sum_logits / total_classes - (1 -\n                    label_smoothing) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = label_smoothing * ((lse if not SPLIT else 0.0) - \n                sum_logits / total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n",
    "category": "Linear Operations",
    "subcategory": "",
    "uuid": "2ea091b3-c06f-40e3-b086-dd354abb23d4"
  },
  {
    "input": "@triton.jit\ndef bwd_sequential_scan_complex(grad_output_real, grad_output_imag, v_real,\n    v_imag, f_real, f_imag, hidden_real, hidden_imag, grad_detach, B, L, C,\n    BLOCK_M: 'tl.constexpr'):\n    offset_b = tl.program_id(0)\n    if offset_b >= B:\n        return\n    offset_n = tl.program_id(1)\n    ptr = tl.arange(0, BLOCK_M) + offset_b * L * C + (L - 1\n        ) * C + offset_n * BLOCK_M\n    grad_detach_ptr = grad_detach + offset_b * L + (L - 1)\n    grad_h_real = tl.zeros([BLOCK_M], dtype=tl.float32)\n    grad_h_imag = tl.zeros([BLOCK_M], dtype=tl.float32)\n    for time_step in range(L - 1, -1, -1):\n        grad_real = tl.load(grad_output_real + ptr)\n        grad_imag = tl.load(grad_output_imag + ptr)\n        grad_detach_item = tl.load(grad_detach_ptr)\n        grad_h_real = grad_h_real * (1 - grad_detach_item)\n        grad_h_imag = grad_h_imag * (1 - grad_detach_item)\n        grad_h_real += grad_real\n        grad_h_imag += grad_imag\n        decay_real = tl.load(f_real + ptr)\n        decay_imag = tl.load(f_imag + ptr)\n        h_real = tl.load(hidden_real + ptr)\n        h_imag = tl.load(hidden_imag + ptr)\n        grad_f_real = grad_h_real * h_real + grad_h_imag * h_imag\n        grad_f_imag = grad_h_imag * h_real - grad_h_real * h_imag\n        tl.store(f_real + ptr, grad_f_real)\n        tl.store(f_imag + ptr, grad_f_imag)\n        tl.store(v_real + ptr, grad_h_real)\n        tl.store(v_imag + ptr, grad_h_imag)\n        grad_h_real_new = grad_h_real * decay_real + grad_h_imag * decay_imag\n        grad_h_imag_new = grad_h_imag * decay_real - grad_h_real * decay_imag\n        grad_h_real = grad_h_real_new\n        grad_h_imag = grad_h_imag_new\n        ptr -= C\n        grad_detach_ptr -= 1\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "7d912c07-c52a-4be7-af07-d2c63953febf"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_bwd(C, ACT_INPUT, A, B, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bk, stride_bn,\n    BLOCK_M: 'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    if ACTIVATION != 'id':\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        act_input = tl.load(act_in_ptrs)\n    if ACTIVATION == 'gelu':\n        acc *= gelu_grad(act_input)\n    elif ACTIVATION == 'gelu_approx':\n        acc *= gelu_approx_grad(act_input)\n    elif ACTIVATION == 'squared_relu':\n        acc *= squared_relu_grad(act_input)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "c819f05c-b51e-49f2-aa2f-6bf269b5bd0c"
  },
  {
    "input": "@triton.jit\ndef pair_hash(x, h):\n    h = h ^ x\n    h = (h << 24) + h * 403\n    return h\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "df37eb8a-e5cb-4c7c-9c05-259c60e74e4e"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel(q, k, v, w, u, o, h0, ht, s_k_h, s_v_h,\n    scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr'):\n    TargetDType = tl.bfloat16\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_h = tl.zeros([BV, BK], dtype=TargetDType)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w.to(tl.float32))\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "acd6bf1b-3dd5-42f0-bab9-67c4edce5041"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr', fp8_v: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        qk = tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr)\n        if fp8_v:\n            p = p\n        else:\n            p = p\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "f200d470-707b-4694-80ad-6b201969df66"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(q, k, v, h, o, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "5ef33b6b-b23f-4b2a-b922-b445bc420d2d"
  },
  {
    "input": "@triton.jit\ndef _flash_decoding_fwd_reduce_kernel(mid_o, mid_o_lse, O, kv_seq_len,\n    q_len: 'tl.constexpr', batch_size, stride_mid_ot, stride_mid_oh,\n    stride_mid_ob, stride_mid_oqlen, stride_mid_od, stride_o_lset,\n    stride_o_lseh, stride_o_lseb, stride_o_lseqlen, stride_ot, stride_oh,\n    stride_oqlen, BLOCK_KV: 'tl.constexpr', HEAD_DIM: 'tl.constexpr'):\n    cur_token_idx = tl.program_id(0)\n    cur_head_idx = tl.program_id(1)\n    cur_q_idx = tl.program_id(2)\n    cur_kv_seq_len = tl.load(kv_seq_len + cur_token_idx)\n    offsets_dmodel = tl.arange(0, HEAD_DIM)\n    kv_split_num = (cur_kv_seq_len + BLOCK_KV - 1) // BLOCK_KV\n    m_i = float('-inf')\n    l_i = 0.0\n    acc = tl.zeros([HEAD_DIM], dtype=tl.float32)\n    offsets_mid_o = (cur_token_idx * stride_mid_ot + cur_head_idx *\n        stride_mid_oh + cur_q_idx * stride_mid_oqlen + offsets_dmodel)\n    offset_mid_lse = (cur_token_idx * stride_o_lset + cur_head_idx *\n        stride_o_lseh + cur_q_idx * stride_o_lseqlen)\n    for block_i in range(0, kv_split_num, 1):\n        mid_o_block = tl.load(mid_o + offsets_mid_o + block_i * stride_mid_ob)\n        lse = tl.load(mid_o_lse + offset_mid_lse + block_i * stride_o_lseb)\n        m_ij = tl.maximum(m_i, lse)\n        scale = tl.exp(m_i - m_ij)\n        acc = acc * scale\n        lse -= m_ij\n        exp_logic = tl.exp(lse)\n        acc += exp_logic * mid_o_block\n        l_i = scale * l_i + exp_logic\n        m_i = m_ij\n    acc = acc / l_i\n    offsets_O = (cur_token_idx * stride_ot + cur_head_idx * stride_oh + \n        cur_q_idx * stride_oqlen + offsets_dmodel)\n    tl.store(O + offsets_O, acc)\n    return l_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e99e1003-dc72-4e23-bac2-18eb094f0f0b"
  },
  {
    "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    nheads, rotary_dim, seqlen_ro, CACHE_KEY_SEQLEN, stride_out_batch,\n    stride_out_seqlen, stride_out_nheads, stride_out_headdim,\n    stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim,\n    BLOCK_K: 'tl.constexpr', IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr',\n    IS_VARLEN: 'tl.constexpr', INTERLEAVED: 'tl.constexpr', CONJUGATE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "52895e65-3759-47d6-90f3-bb58848ab79b"
  },
  {
    "input": "@triton.jit\ndef ub1(X, Y):\n    r = tl.arange(0, 16)\n    r2 = tl.arange(0, 32)\n    x = tl.load(X + 16 * r2[:, None] + r)\n    y = triton_unbroadcast(x, tl.arange(0, 16).shape)\n    tl.store(Y + r, y)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "a367fd7c-b846-4907-9e0d-b943c70aea93"
  },
  {
    "input": "@triton.jit\ndef _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "8b802961-3384-4d0f-b205-fe8b3466d812"
  },
  {
    "input": "@triton.jit\ndef _paged_attention_v2_reduce(out, exp_sums, max_logits, tmp_out,\n    context_lens, stride_exp_m, stride_exp_n, stride_out_m, stride_out_n,\n    stride_tmp_m, stride_tmp_n, stride_tmp_k, HEAD_SIZE: 'tl.constexpr',\n    NUM_PARTITIONS: 'tl.constexpr'):\n    seq_idx = tl.program_id(axis=1)\n    head_idx = tl.program_id(axis=0)\n    context_len = tl.load(context_lens + seq_idx)\n    num_partitions = tl.cdiv(context_len, PARTITION_SIZE)\n    exp_sum = 0.0\n    max_logit = float('-inf')\n    offs_logit = seq_idx * stride_exp_m + head_idx * stride_exp_n\n    head_size_offs = tl.arange(0, HEAD_SIZE)\n    tmp_out_ptr = seq_idx * stride_tmp_m + head_idx * stride_tmp_n\n    out_ptr = seq_idx * stride_out_m + head_idx * stride_out_n + head_size_offs\n    acc = tl.zeros([HEAD_SIZE], dtype=tl.float32)\n    global_exp_sum = tl.zeros([1], dtype=tl.float32)\n    logits = tl.load(max_logits + offs_logit + tl.arange(0, NUM_PARTITIONS),\n        mask=tl.arange(0, NUM_PARTITIONS) < num_partitions, other=float('-inf')\n        )\n    max_logit = tl.max(logits, axis=0)\n    exp_sum = tl.load(exp_sums + offs_logit + tl.arange(0, NUM_PARTITIONS),\n        mask=tl.arange(0, NUM_PARTITIONS) < num_partitions, other=0.0)\n    rescaled_exp_sum = exp_sum * tl.exp(logits - max_logit)\n    global_exp_sum += tl.sum(rescaled_exp_sum, axis=0)\n    tmp = tl.load(tmp_out + tmp_out_ptr + tl.arange(0, NUM_PARTITIONS)[:,\n        None] * stride_tmp_k + head_size_offs)\n    acc += tl.sum(tmp * rescaled_exp_sum[:, None], axis=0)\n    inv_sum = 1.0 / (global_exp_sum + 1e-06)\n    tl.store(out + out_ptr, acc * inv_sum)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "6cea4f3b-3017-4951-abd4-527d7382a8b2"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_h(k, v, g, h, h0, ht, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NORMK: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if NORMK:\n            p_g = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (s_k_d, s_k_t\n                ), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,),\n                ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            b_h *= tl.exp(b_gn)[:, None]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_k = b_k * tl.exp(b_gn[:, None] - b_g)\n        else:\n            p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d\n                ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,),\n                ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            b_h *= tl.exp(b_gn)[None, :]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_v = b_v * tl.exp(b_gn[None, :] - b_g)\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "589e261a-0931-4208-9f89-ea0da15d8cb1"
  },
  {
    "input": "@triton.jit\ndef blora_bp_kernel_without_mask(dr_ptr, dr_stride_bs, dx_ptr,\n    dx_stride_bsk, dx_stride_hm, lA_ptr, lA_stride_l, lA_stride_hm, lB_ptr,\n    lB_stride_l, lB_stride_r, sel_ptr, k: 'tl.constexpr', m: 'tl.constexpr',\n    r: 'tl.constexpr', rm: 'tl.constexpr', hm: 'tl.constexpr', hout:\n    'tl.constexpr', block_size_hout: 'tl.constexpr', block_size_hm:\n    'tl.constexpr', block_size_r: 'tl.constexpr'):\n    block_idx_bsk = tl.program_id(0)\n    block_idx_bs = block_idx_bsk // k\n    block_idx_hm = tl.program_id(1)\n    offsets_hout = tl.arange(0, block_size_hout)\n    offsets_hm = block_idx_hm * block_size_hm + tl.arange(0, block_size_hm)\n    offsets_r = tl.arange(0, block_size_r)\n    offsets_m = tl.arange(0, m)\n    offsets_rm = tl.arange(0, rm)\n    block_mask_hout = offsets_hout < hout\n    block_mask_hout_row = offsets_hout[None, :] < hout\n    block_mask_hm_col = offsets_hm[:, None] < hm\n    block_mask_rm_row = offsets_rm[None, :] < rm\n    block_mask_r_col = offsets_r[:, None] < r\n    block_mask_m_row = offsets_r[None, :] < m\n    sel_ptr += block_idx_bsk\n    sel_idx = tl.load(sel_ptr)\n    lA_block_ptrs = lA_ptr + sel_idx * lA_stride_l + (offsets_hm[:, None] *\n        lA_stride_hm + offsets_rm[None, :])\n    dr_block_ptrs = dr_ptr + block_idx_bs * dr_stride_bs + ([[0]] +\n        offsets_hout[None, :])\n    lA_block_ptrs = lA_ptr + sel_idx * lA_stride_l + (offsets_hm[:, None] *\n        lA_stride_hm + offsets_rm)\n    lB_block_ptrs = lB_ptr + sel_idx * lB_stride_l + (offsets_r[:, None] *\n        lB_stride_r + offsets_hout[None, :])\n    dx_block_ptrs = dx_ptr + block_idx_bsk * dx_stride_bsk + offsets_hm[:, None\n        ] * dx_stride_hm + offsets_m[None, :]\n    compute_olB_dtype = tl.float16\n    olB = tl.zeros((block_size_r, 1))\n    for _ in range(tl.cdiv(hout, block_size_hout)):\n        block_mask_hout_col = offsets_hout[:, None] < hout\n        block_mask_hout_row = offsets_hout[None, :] < hout\n        dr = tl.load(dr_block_ptrs, mask=block_mask_hout_row, other=0.0)\n        lB = tl.load(lB_block_ptrs, mask=block_mask_hout_row, others=0.0)\n        olB += tl.dot(lB, dr.T)\n        offsets_hout += block_size_hout\n        dr_block_ptrs += block_size_hout\n        lB_block_ptrs += block_size_hout\n    olB_r = torch.zeros(r * m, m, dtype=compute_olB_dtype, device=olB.device)\n    for i in range(m):\n        olB_r[i * r:(i + 1) * r, i] = olB\n    compute_olA_dtype = tl.float16\n    lA = tl.load(lA_block_ptrs, mask=block_mask_hm_col & block_mask_rm_row,\n        other=0.0)\n    olA = tl.dot(lA, olB)\n    tl.store(dx_block_ptrs, olA, mask=block_mask_hm_col & block_mask_m_row)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "61defd23-cb23-4baf-90b5-a5203c3b0e6b"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_init_att_window_info(b_seq_len, b_att_seq_len, batch_size,\n    sliding_window, BLOCK_SIZE: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    cur_start = cur_index * BLOCK_SIZE\n    offsets = cur_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < batch_size\n    cur_seq_len = tl.load(b_seq_len + offsets, mask=mask)\n    b_att_seq_len_data = tl.minimum(cur_seq_len, sliding_window)\n    tl.store(b_att_seq_len + offsets, b_att_seq_len_data, mask=mask)\n    return\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "d702c51b-61b8-4096-a8ae-860a4432605d"
  },
  {
    "input": "@triton.jit\ndef breakpoint_once():\n    breakpoint_if('=0,=0,=0')\n",
    "category": "Helper Functions",
    "subcategory": "",
    "uuid": "a1f8b767-e101-4268-b77e-34fb6016ff02"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen'] % args['BLOCK_M'] ==\n    0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})\n@triton.jit\ndef _angular_lsh_kernel(in_mat, proj_dir, perm, enc_vec, buckets,\n    stride_in_matb, stride_in_math, stride_in_matm, stride_proj_dirb,\n    stride_proj_dirh, stride_proj_dird, stride_bucketsb, stride_bucketsh,\n    nheads, seqlen, seqlen_rounded, headdim, NUM_PROJ_ROUNDED:\n    'tl.constexpr', num_projs: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, NUM_PROJ_ROUNDED)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    in_mat_ptrs = in_mat + off_b * stride_in_matb + off_h * stride_in_math + (\n        offs_m[:, None] * stride_in_matm + offs_d[None, :])\n    proj_dir_ptrs = (proj_dir + off_b * stride_proj_dirb + off_h *\n        stride_proj_dirh + (offs_d[:, None] * stride_proj_dird + offs_n[\n        None, :]))\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            mat = tl.load(in_mat_ptrs)\n        else:\n            mat = tl.load(in_mat_ptrs, mask=offs_d[None, :] < headdim,\n                other=0.0)\n    elif EVEN_HEADDIM:\n        mat = tl.load(in_mat_ptrs, mask=offs_m[:, None] < seqlen, other=0.0)\n    else:\n        mat = tl.load(in_mat_ptrs, mask=(offs_m[:, None] < seqlen) & (\n            offs_d[None, :] < headdim), other=0.0)\n    if EVEN_HEADDIM:\n        proj_dir_block = tl.load(proj_dir_ptrs, mask=offs_n[None, :] <\n            num_projs, other=0.0)\n    else:\n        proj_dir_block = tl.load(proj_dir_ptrs, mask=(offs_n[None, :] <\n            num_projs) & (offs_d[:, None] * stride_proj_dird < headdim),\n            other=0.0)\n    mask = tl.dot(mat, proj_dir_block)\n    mask = tl.where(mask > 0.0, 1.0, 0.0)\n    encoding_vectors = tl.load(enc_vec + offs_n, mask=offs_n < num_projs,\n        other=0.0)\n    bin_ids = tl.sum(mask * encoding_vectors[None, :], 1)\n    hash_buckets = tl.load(perm + bin_ids)\n    buckets_ptrs = (buckets + off_b * stride_bucketsb + off_h *\n        stride_bucketsh + offs_m)\n    if EVEN_M:\n        tl.store(buckets_ptrs, hash_buckets)\n    else:\n        tl.store(buckets_ptrs, hash_buckets, mask=offs_m < seqlen)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "ba99e4b0-d08c-4769-8bd0-f167f8102676"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({\n    'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({\n    'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({\n    'BLOCK_N': 1024})], key=['ncols'])\n@triton.jit\ndef _swiglu_fwd_kernel(X, Y, OUT, stride_x_row, stride_y_row,\n    stride_out_row, ncols, BLOCK_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    OUT += row * stride_out_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0)\n    out = x * tl.sigmoid(x) * y\n    tl.store(OUT + cols, out, mask=cols < ncols)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "dbaf55eb-22ef-4d12-b2e8-463e48fe9447"
  },
  {
    "input": "@triton.jit\ndef one_shot_all_reduce_kernel(buffer_ptrs, signal_pad_ptrs, output_ptr,\n    numel: 'tl.constexpr', rank: 'tl.constexpr', world_size: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', NUMEL_PER_THREAD: 'tl.constexpr'):\n    blockwise_barrier(signal_pad_ptrs, None, rank, world_size)\n    pid = tl.program_id(axis=0)\n    buffer_ptrs = buffer_ptrs\n    output_ptr = output_ptr\n    block_start = pid * BLOCK_SIZE\n    while block_start < numel // NUMEL_PER_THREAD:\n        offsets = block_start + tl.arange(0, BLOCK_SIZE)\n        mask = block_start + tl.arange(0, BLOCK_SIZE\n            ) < numel // NUMEL_PER_THREAD\n        acc = tl.zeros((BLOCK_SIZE,), tl.uint64)\n        for i in range(world_size):\n            buffer_ptr = tl.load(buffer_ptrs + i)\n            val = tl.load(buffer_ptr + offsets, mask=mask)\n            acc = add_v4_bf16(acc, val)\n        tl.store(output_ptr + offsets, acc, mask=mask)\n        block_start += tl.num_programs(axis=0) * BLOCK_SIZE\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "37f71386-a9b8-4806-beed-f9d786f52a51"
  },
  {
    "input": "@triton.jit\ndef rmsnorm_forward(Y, Y_row_stride, X, X_row_stride, W, r, n_cols, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr = Y + row_idx * Y_row_stride\n    X_ptr = X + row_idx * X_row_stride\n    r_ptr = r + row_idx\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    X_squared = X_row * X_row\n    mean_X_squared = tl.sum(X_squared, axis=0) / n_cols\n    rms = tl.math.rsqrt(mean_X_squared + eps)\n    tl.store(r_ptr, rms)\n    output = X_row * rms * W_row\n    tl.store(Y_ptr + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "3c34e423-7619-409d-b278-ff5d5d2a26ab"
  },
  {
    "input": "@triton.jit\ndef store_full_2d(vals, ptr, sz0: 'const', sz1: 'const', stride0=None,\n    stride1=1):\n    \"\"\"Store 2d block into matrix (defined by ptr)\"\"\"\n    stride0 = stride0 or sz1\n    offs = offset_2d(tl.arange(0, sz0), tl.arange(0, sz1), stride0, stride1)\n    mask = mask_2d(tl.arange(0, sz0), tl.arange(0, sz1), sz0, sz1)\n    tl.store(ptr + offs, vals, mask)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "502e7837-2345-4394-b274-5e6da6be8c91"
  },
  {
    "input": "@triton.jit\ndef _gemm_activation_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', activation: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a_data = tl.load(a_ptrs, mask=offs_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        b_data = tl.load(b_ptrs, mask=offs_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a_data, b_data)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    acc = activation(acc)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] *\n        stride_cn)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b96769e1-edb2-448b-ae6a-07825d1cc526"
  },
  {
    "input": "@triton.jit\ndef _sparse_attention_compute(INDICES, stride_indices_d, stride_indices_z,\n    VALUES, stride_values_z, V, stride_v_n, stride_v_tsrc, stride_v_hid,\n    CONTEXT, stride_context_n, stride_context_tdst, stride_context_hid, N,\n    TDST, TSRC, HID, BK, NUM_SINK, WINDOW_SIZE, BLOCK_HID: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr'):\n    idx_n = tl.program_id(0)\n    idx_tdst = tl.program_id(1)\n    idx_hid = tl.arange(0, BLOCK_HID)\n    mask_hid = idx_hid < HID\n    acc = tl.zeros((BLOCK_HID,), dtype=tl.float32)\n    for idx_bk in range(BK):\n        CACHE_SIZE = NUM_SINK + WINDOW_SIZE\n        idx_k = idx_bk * BLOCK_K + tl.arange(0, BLOCK_K)\n        mask_k = idx_k < CACHE_SIZE\n        idx_z = idx_n * TDST * CACHE_SIZE + idx_tdst * CACHE_SIZE + idx_k\n        mask_z = mask_k\n        idx_tsrc = tl.load(INDICES + 2 * stride_indices_d + idx_z *\n            stride_indices_z, mask=mask_z, other=0)\n        mask_tsrc = mask_z\n        score = tl.load(VALUES + idx_z * stride_values_z, mask=mask_z, other=0)\n        value = tl.load(V + idx_n * stride_v_n + idx_tsrc[:, None] *\n            stride_v_tsrc + idx_hid[None, :] * stride_v_hid, mask=mask_tsrc\n            [:, None] & mask_hid[None, :], other=0)\n        context = tl.sum(score[:, None] * value, axis=0)\n        acc += context\n    tl.store(CONTEXT + idx_n * stride_context_n + idx_tdst *\n        stride_context_tdst + idx_hid * stride_context_hid, mask=mask_hid,\n        value=acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "75b39aec-998f-4d01-9b57-110cb615cbe5"
  },
  {
    "input": "@triton.jit\ndef layer_norm_xformers(output_ptr, a_ptr, weight_ptr, bias_ptr, mean_ptr,\n    rstd_ptr, output_row_stride, output_col_stride, a_row_stride,\n    a_col_stride, N_SIZE, eps, HAS_BIAS: 'tl.constexpr', IS_RMSNORM:\n    'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr'):\n    \"\"\"\n    LayerNorm forward pass for a single feature.\n    Requires that a whole row of X is loaded into shared memory -> won't work for large tensors.\n    based on:\n    https://github.com/facebookresearch/xformers/blob/main/xformers/triton/k_layer_norm.py\n    (arg names modified to match other implementation)\n    -> only used in benchmarks\n    \"\"\"\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_N_SIZE)\n    mask = cols < N_SIZE\n    x_ptrs = a_ptr + row * a_row_stride + cols * a_col_stride\n    x = tl.load(x_ptrs, mask=mask, other=0.0, eviction_policy='evict_first')\n    w = tl.load(weight_ptr + cols, mask=mask, other=1.0)\n    b = tl.load(bias_ptr + cols, mask=mask, other=0.0)\n    mean = tl.sum(x, axis=0) / N_SIZE\n    x_zm = tl.where(mask, x - mean, 0.0)\n    tl.store(mean_ptr + row, mean)\n    x_var = tl.sum(x_zm * x_zm, axis=0) / N_SIZE\n    rstd = 1.0 / tl.sqrt(x_var + eps)\n    y = x_zm * rstd\n    tl.store(rstd_ptr + row, rstd)\n    y = y * w + b\n    y_ptrs = output_ptr + row * output_row_stride + cols * output_col_stride\n    tl.store(y_ptrs, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "4f6853b0-2bd3-4b87-8c03-d580b98f881a"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n    BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', start_m, start_n, num_steps, MASK: 'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    offs_n = start_n + tl.arange(0, BLOCK_N2)\n    offs_k = tl.arange(0, HEAD_DIM)\n    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    Di = tl.load(D + offs_m)\n    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n    curr_n = start_n\n    step_n = BLOCK_N2\n    for blk_idx in range(num_steps):\n        kT = tl.load(kT_ptrs)\n        vT = tl.load(vT_ptrs)\n        qk = tl.dot(q, kT)\n        p = tl.math.exp2(qk - m)\n        if MASK:\n            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(mask, p, 0.0)\n        dp = tl.dot(do, vT)\n        ds = p * (dp - Di[:, None])\n        ds = ds\n        dq += tl.dot(ds, tl.trans(kT))\n        curr_n += step_n\n        kT_ptrs += step_n * stride_tok\n        vT_ptrs += step_n * stride_tok\n    return dq\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "9638ae50-ad12-4a50-ae2c-72eeb159aa71"
  },
  {
    "input": "@triton.jit\ndef logcumsumexp_fwd_kernel(s, z, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_mp = tl.full([S], float('-inf'), dtype=tl.float32)\n    b_zp = tl.zeros([S], dtype=tl.float32)\n    for i_t in range(NT):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_mc = tl.max(b_s, 0)\n        if i_t > 0:\n            b_mc = tl.maximum(b_mp, b_mc)\n        b_zp = b_zp * tl.exp(b_mp - b_mc)\n        b_s = tl.exp(b_s - b_mc)\n        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp\n        b_zc = tl.max(b_z, 0)\n        b_mp = b_mc\n        b_zp = b_zc\n        b_z = tl.log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc\n        tl.store(p_z, b_z, boundary_check=(0, 1))\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "d3c6c3b3-c003-43b5-8b90-c2cdf57f530a"
  },
  {
    "input": "@triton.jit\ndef cast_uint32_to_half2(scale_shift):\n    \"\"\"Extract two float16 packed into one int32\"\"\"\n    scale = scale_shift & 65535\n    shift = scale_shift >> 16\n    scale = scale.to(tl.uint16)\n    shift = shift.to(tl.uint16)\n    return scale, shift\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "7d272218-bd36-4e36-8f59-79f7a04fc7b5"
  },
  {
    "input": "@triton.jit\ndef _kernel_bwd_merge_discontinuous_v1(alpha_c, tmp_merge_normalized,\n    tmp_merge_grad, w, batch, L, stride_alpha_c1, stride_alpha_c2,\n    stride_alpha_c3, stride_tmp_merge1, stride_tmp_merge2,\n    stride_tmp_merge3, r1, r2, r3, r4, BLOCK_R3: 'tl.constexpr', BLOCK_R4:\n    'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    if b_idx >= batch:\n        return\n    span_length_left = tl.program_id(1) + 1\n    tid = tl.program_id(2)\n    start = 0\n    while tid >= L - w - start:\n        tid -= L - w - start\n        start += 1\n    gap_start = start + span_length_left\n    gap_end = gap_start + (tid + 1)\n    end = gap_end + (w - span_length_left)\n    l_bwd_ptr = (alpha_c + b_idx * stride_alpha_c1 + gap_start *\n        stride_alpha_c2 + start * stride_alpha_c3 + 2 * r1 + r2 + tl.arange\n        (0, BLOCK_R3))\n    r_bwd_ptr = (alpha_c + b_idx * stride_alpha_c1 + end * stride_alpha_c2 +\n        gap_end * stride_alpha_c3 + 2 * r1 + r2 + r3 + tl.arange(0, BLOCK_R3))\n    mask = tl.arange(0, BLOCK_R3) < r3\n    do = tl.load(tmp_merge_normalized + b_idx * stride_tmp_merge1 + tl.\n        program_id(1) * stride_tmp_merge2 + tl.program_id(2) *\n        stride_tmp_merge3 + tl.arange(0, BLOCK_R3), mask=mask, other=0)\n    do *= tl.load(tmp_merge_grad + b_idx * stride_tmp_merge1 + tl.\n        program_id(1) * stride_tmp_merge2 + tl.program_id(2) *\n        stride_tmp_merge3 + tl.arange(0, BLOCK_R3), mask=mask, other=0)\n    tl.atomic_add(l_bwd_ptr, do, mask=mask)\n    tl.atomic_add(r_bwd_ptr, do, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "3b27d3df-6bc9-4c60-b2a6-6337e17f0b20"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dh(q, z, do, dh, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t,\n    s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NORMK: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    b_zp = tl.full([BK if NORMK else BV], float('inf'), dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        i_p = tl.maximum(i_t * BT - 1, 0)\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        if NORMK:\n            p_z = tl.make_block_ptr(z + i_bh * s_k_h, (K, T), (s_k_d, s_k_t\n                ), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,),\n                (i_p * K + i_k * BK,), (BK,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zc - b_zp), b_zc\n            b_z = tl.load(p_z, boundary_check=(0, 1))\n            b_q = b_q * tl.exp(b_zc[:, None] - b_z)\n            b_dh = b_dh * b_r[:, None]\n        else:\n            p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d\n                ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,),\n                (i_p * V + i_v * BV,), (BV,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zc - b_zp), b_zc\n            b_z = tl.load(p_z, boundary_check=(0,))\n            b_do = b_do * tl.exp(b_zc[None, :] - b_z)\n            b_dh = b_dh * b_r[None, :]\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "f64db1d7-b7b2-4695-a3a6-ec7ecfb3c78a"
  },
  {
    "input": "@triton.jit\ndef _kernel_bwd_merge_discontinuous_v2(alpha_c, alpha_d, tmp_merge,\n    tmp_merge_normalized, tmp_merge_grad, w, batch, L, stride_alpha_c1,\n    stride_alpha_c2, stride_alpha_c3, stride_alpha_d1, stride_alpha_d2,\n    stride_alpha_d3, stride_alpha_d4, stride_alpha_d5, stride_tmp_merge1,\n    stride_tmp_merge2, stride_tmp_merge3, r1, r2, r3, r4, BLOCK_R3:\n    'tl.constexpr', BLOCK_R4: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    if b_idx >= batch:\n        return\n    span_length_left = tl.program_id(1) + 1\n    tid = tl.program_id(2)\n    start = 0\n    while tid >= L - w - start:\n        tid -= L - w - start\n        start += 1\n    gap_start = start + span_length_left\n    gap_end = gap_start + (tid + 1)\n    end = gap_end + (w - span_length_left)\n    ptr = b_idx * stride_tmp_merge1 + tl.program_id(1\n        ) * stride_tmp_merge2 + tl.program_id(2\n        ) * stride_tmp_merge3 + tl.arange(0, BLOCK_R4)\n    alpha_c_ptr = (alpha_c + b_idx * stride_alpha_c1 + 2 * r1 + r2 + 2 * r3 +\n        tl.arange(0, BLOCK_R4))\n    alpha_d_ptr = alpha_d + b_idx * stride_alpha_d1 + r2 + tl.arange(0,\n        BLOCK_R4)\n    mask = tl.arange(0, BLOCK_R4) < r4\n    parent_score = tl.load(tmp_merge + ptr, mask=mask, other=0)\n    do = tl.load(tmp_merge_normalized + ptr, mask=mask, other=0) * tl.load(\n        tmp_merge_grad + ptr, mask=mask, other=0)\n    for split in range(start + 1, gap_start):\n        c_ptr = alpha_c_ptr + start * stride_alpha_c2 + split * stride_alpha_c3\n        d_ptr = (alpha_d_ptr + split * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        child_c = tl.load(c_ptr, mask=mask, other=0)\n        child_d = tl.load(d_ptr, mask=mask, other=0)\n        new_grad = tl.exp(child_c + child_d - parent_score) * do\n        c_bwd_ptr = (alpha_c_ptr + split * stride_alpha_c2 + start *\n            stride_alpha_c3)\n        d_bwd_ptr = (alpha_d_ptr + gap_start * stride_alpha_d2 + split *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        tl.atomic_add(c_bwd_ptr, new_grad)\n        tl.atomic_add(d_bwd_ptr, new_grad)\n        c_ptr = (alpha_c_ptr + split * stride_alpha_c2 + gap_start *\n            stride_alpha_c3 + r4)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + split *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        child_c = tl.load(c_ptr, mask=mask, other=0)\n        child_d = tl.load(d_ptr, mask=mask, other=0)\n        new_grad = tl.exp(child_c + child_d - parent_score) * do\n        c_bwd_ptr = (alpha_c_ptr + gap_start * stride_alpha_c2 + split *\n            stride_alpha_c3 + r4)\n        d_bwd_ptr = (alpha_d_ptr + split * stride_alpha_d2 + start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        tl.atomic_add(c_bwd_ptr, new_grad)\n        tl.atomic_add(d_bwd_ptr, new_grad)\n    for split in range(gap_end + 1, end):\n        c_ptr = (alpha_c_ptr + gap_end * stride_alpha_c2 + split *\n            stride_alpha_c3 + 2 * r4)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + split * stride_alpha_d4 + end * stride_alpha_d5)\n        child_c = tl.load(c_ptr, mask=mask, other=0)\n        child_d = tl.load(d_ptr, mask=mask, other=0)\n        new_grad = tl.exp(child_c + child_d - parent_score) * do\n        c_bwd_ptr = (alpha_c_ptr + split * stride_alpha_c2 + gap_end *\n            stride_alpha_c3 + 2 * r4)\n        d_bwd_ptr = (alpha_d_ptr + gap_start * stride_alpha_d2 + start *\n            stride_alpha_d3 + split * stride_alpha_d4 + end * stride_alpha_d5)\n        tl.atomic_add(c_bwd_ptr, new_grad)\n        tl.atomic_add(d_bwd_ptr, new_grad)\n        c_ptr = (alpha_c_ptr + split * stride_alpha_c2 + end *\n            stride_alpha_c3 + 3 * r4)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + split *\n            stride_alpha_d5)\n        child_c = tl.load(c_ptr, mask=mask, other=0)\n        child_d = tl.load(d_ptr, mask=mask, other=0)\n        new_grad = tl.exp(child_c + child_d - parent_score) * do\n        c_bwd_ptr = (alpha_c_ptr + end * stride_alpha_c2 + split *\n            stride_alpha_c3 + 3 * r4)\n        d_bwd_ptr = (alpha_d_ptr + gap_start * stride_alpha_d2 + start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + split *\n            stride_alpha_d5)\n        tl.atomic_add(c_bwd_ptr, new_grad)\n        tl.atomic_add(d_bwd_ptr, new_grad)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "74a4c2b5-f4f3-4a2d-882b-ee7845b3fedb"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_1o = tl.zeros([BV, BK], dtype=tl.float32)\n    b_h_2o = tl.zeros([BV, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i * BT\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i * BT < T)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h_1o, allow_tf32=False)\n        if i_v == 0:\n            b_dq += b_dz[:, None] * k_1o\n        b_dq_2o = tl.dot(b_do, b_h_2o, allow_tf32=False) * 0.5\n        if i_v == 0:\n            b_dq_2o += b_dz[:, None] * k_2o * 0.5\n        b_dq_2o = tl.reshape(b_dq_2o, [BT, BK, BK])\n        b_dq += tl.sum(b_dq_2o * b_q[:, :, None], axis=1)\n        b_dq += tl.sum(b_dq_2o * b_q[:, None, :], axis=2)\n        b_dq *= scale\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_h_2o = b_h_2o + tl.dot(b_v, b_k_2o, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_v, b_k, allow_tf32=False)\n        if i_v == 0:\n            k_1o += tl.sum(b_k, axis=0)[None, :]\n            k_2o += tl.sum(b_k_2o, axis=0)[None, :]\n    tl.debug_barrier()\n    b_h_1o = None\n    b_h_2o = None\n    b_dh_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_dh_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    b_dh_0o = tl.zeros([BV], dtype=tl.float32)\n    m_s = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    dq_1o = tl.zeros([1, BK], dtype=tl.float32)\n    dq_2o = tl.zeros([BK * BK, 1], dtype=tl.float32)\n    for i in range(tl.cdiv(T, BT) * BT - BT, -BT, -BT):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i < T)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds *= 1 + b_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s2, b_do, allow_tf32=False)\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_dv += tl.dot(b_k, b_dh_1o, allow_tf32=False)\n        b_dv += tl.dot(b_k_2o, b_dh_2o, allow_tf32=False)\n        b_dv += b_dh_0o\n        b_dk += tl.dot(b_v, tl.trans(b_dh_1o), allow_tf32=False)\n        if i_v == 0:\n            b_dk += dq_1o\n        b_dk_2o = tl.dot(b_dh_2o, tl.trans(b_v), allow_tf32=False)\n        if i_v == 0:\n            b_dk_2o += dq_2o\n        b_dk_2o = tl.reshape(b_dk_2o, [BK, BK, BT])\n        b_k_fp32 = tl.trans(b_k)\n        b_dk2 = tl.sum(b_dk_2o * b_k_fp32[:, None, :], axis=0)\n        b_dk2 += tl.sum(b_dk_2o * b_k_fp32[None, :, :], axis=1)\n        b_dk += tl.trans(b_dk2)\n        b_dh_0o += tl.sum(b_do, axis=0)\n        b_dh_1o = b_dh_1o + tl.dot(b_q, b_do, allow_tf32=False)\n        b_q_2o = b_q[None, :, :] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BK * BK, BT])\n        b_dh_2o = b_dh_2o + tl.dot(b_q_2o, b_do, allow_tf32=False) * 0.5\n        if i_v == 0:\n            dq_1o += tl.sum(b_dz[None, :] * b_q, axis=1)[None, :]\n            dq_2o += (tl.sum(b_dz[None, :] * b_q_2o, axis=1) * 0.5)[:, None]\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "d219b032-39f5-4c30-b645-ede5823325d9"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "d96b6932-54b3-4f95-bd51-ee90d497317e"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr', fp8_v: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        qk = tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr)\n        if fp8_v:\n            p = p\n        else:\n            p = p\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "0c99dc0d-ac44-4b6e-b42c-e7b4d13ad453"
  },
  {
    "input": "@triton.jit\ndef accum_linear(accum, input1, input2, fp16: 'tl.constexpr', tf32:\n    'tl.constexpr'):\n    \"\"\"\n    Accumulates matrix multiplications of input tensors for linear functions.\n\n    Args:\n        accum: Accumulator holding aggregation of matrix multiplications.\n            The accumulator must be of shape [BLOCK_SIZE1, BLOCK_SIZE3].\n        input1: First operand of matrix multiplication.\n            The operand must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        input2: Second operand of matrix multiplication.\n            The operand must be of shape [BLOCK_SIZE2, BLOCK_SIZE3].\n        fp16: Flag for converting operands to FP16.\n        tf32: Flag for performing matrix multiplication in TF32.\n\n    Returns:\n        Accumulator with the result of the new matrix multiplication added to it.\n    \"\"\"\n    if fp16:\n        input1 = input1\n        input2 = input2\n    return accum + tl.dot(input1, input2, allow_tf32=tf32)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b6acf0c1-c35a-4be4-9649-5bd6c1e2020e"
  },
  {
    "input": "@triton.jit\ndef rope_kernel_bw(input_ptr, in_seq_len_stride, in_batch_stride,\n    output_ptr, cos_ptr, sin_ptr, cos_stride, sin_stride, seq_len, head_dim,\n    BLOCK_SIZE: 'tl.constexpr', BATCH_NUM: 'tl.constexpr'):\n    pid_seq = tl.program_id(axis=0)\n    pid_head = tl.program_id(axis=1)\n    head_dim_offset = tl.arange(0, BLOCK_SIZE)\n    head_dim_mid = head_dim // 2\n    mask = head_dim_offset < head_dim_mid\n    cos_offset = pid_seq % seq_len * cos_stride + head_dim_offset\n    sin_offset = pid_seq % seq_len * sin_stride + head_dim_offset\n    cos = tl.load(cos_ptr + cos_offset, mask=mask, other=0.0)\n    sin = tl.load(sin_ptr + sin_offset, mask=mask, other=0.0)\n    for batch_idx in tl.static_range(0, BATCH_NUM):\n        x1_offset = (pid_seq * in_seq_len_stride + batch_idx *\n            in_batch_stride + pid_head * head_dim + head_dim_offset)\n        x2_offset = (pid_seq * in_seq_len_stride + batch_idx *\n            in_batch_stride + pid_head * head_dim + head_dim_mid +\n            head_dim_offset)\n        x1 = tl.load(input_ptr + x1_offset, mask=mask, other=0.0)\n        x2 = tl.load(input_ptr + x2_offset, mask=mask, other=0.0)\n        y1 = x1 * cos - x2 * -sin\n        y2 = x1 * -sin + x2 * cos\n        tl.store(output_ptr + x1_offset, y1, mask=mask)\n        tl.store(output_ptr + x2_offset, y2, mask=mask)\n    return\n",
    "category": "Helper Functions",
    "subcategory": "shape manipulation",
    "uuid": "10266e0f-b754-4dbd-9a1b-c06197c49e1b"
  },
  {
    "input": "@eval(\n    \"\"\"triton.heuristics({\n    'ROW_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['C'] // kwargs['groups']),\n    'BLOCK_SIZE':\n    lambda kwargs: max(\n        1, min(triton.next_power_of_2(kwargs['HxW']),\n               4096 // (triton.next_power_of_2(kwargs['C'] // kwargs['groups']))\n               )),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['ROW_SIZE'] * kwargs['BLOCK_SIZE'] // 128)),\n    'C_G': lambda kwargs: kwargs['C'] // kwargs['groups'],\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel(input_ptr, N,\n    C, HxW, groups, eps, mean_ptr, rstd_ptr, C_G, ROW_SIZE: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    group = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    offset = pid_batch * C * HxW + group * C_G\n    X = input_ptr + offset\n    _mean = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _m2 = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _weight = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    row = tl.arange(0, ROW_SIZE)\n    for off in range(0, HxW, BLOCK_SIZE):\n        r = off + tl.arange(0, BLOCK_SIZE)\n        m2_ = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n        mask = (r < HxW)[:, None] & (row[None, :] < C_G)\n        weight_ = mask\n        x = tl.load(X + (r * C)[:, None] + row[None, :], mask=mask)\n        _mean, _m2, _weight = welford_combine(_mean, _m2, _weight, x, m2_,\n            weight_)\n    _mean = tl.view(_mean, (BLOCK_SIZE * ROW_SIZE,))\n    _m2 = tl.view(_m2, (BLOCK_SIZE * ROW_SIZE,))\n    _weight = tl.view(_weight, (BLOCK_SIZE * ROW_SIZE,))\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    var = m2 / weight\n    rstd = 1.0 / tl.sqrt(var + eps)\n    offset = pid_batch * groups + group\n    tl.store(mean_ptr + offset, mean)\n    tl.store(rstd_ptr + offset, rstd)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "61d46a95-c484-4150-a8fe-2bb62ca10d44"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_hh, s_ht, H, T, TDK, scale,\n    DK, DV, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b\n        )\n    d_q = d_q * scale\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    for i_k in range(0, tl.cdiv(DK, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i_t * BT, 0), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (DV, TDK), (1, s_ht), (0, \n            i_t * DK + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i_t * BT, 0), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_hh, (TDK, DV), (s_ht, 1), (\n            i_t * DK + i_k * BK, 0), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i_t * BT, 0), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, DK), (s_qk_t,\n            s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, DK), (s_qk_t,\n            s_qk_d), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        for _ in range(tl.cdiv(DV, BV)):\n            b_v = tl.load(p_v, boundary_check=(0, 1))\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_dh = tl.load(p_dh, boundary_check=(0, 1))\n            b_ds = tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n            b_ds = b_ds * d_s\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False) * d_q[:, None\n                ] + tl.dot(b_ds, b_k, allow_tf32=False)\n            b_ds = tl.trans(b_ds)\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n            b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(\n                b_s, b_do, allow_tf32=False)\n            b_dv += tl.load(p_dv, boundary_check=(0, 1))\n            tl.store(p_dv, b_dv, boundary_check=(0, 1))\n            p_v = tl.advance(p_v, (0, BV))\n            p_h = tl.advance(p_h, (BV, 0))\n            p_do = tl.advance(p_do, (0, BV))\n            p_dh = tl.advance(p_dh, (0, BV))\n            p_dv = tl.advance(p_dv, (0, BV))\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "13b34ef1-a934-4fa4-ab20-03720d6f2a90"
  },
  {
    "input": "@triton.jit\ndef update_ema(prev_ema, new_val, momentum):\n    \"\"\"\n    Updates exponential moving average.\n\n    Args:\n        prev_ema: Previous exponential moving average.\n        new_val: Value used to update the exponential moving average.\n        momentum: Momentum.\n\n    Returns:\n        Updated running statistic.\n    \"\"\"\n    return (1 - momentum) * prev_ema + momentum * new_val\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "94a8041f-406b-46fc-a55e-ddd292b8eb5c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128,\n    'PACKED_BLOCK_SIZE': 16}), triton.Config({'BLOCK_SIZE': 256,\n    'PACKED_BLOCK_SIZE': 32}), triton.Config({'BLOCK_SIZE': 512,\n    'PACKED_BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 1024,\n    'PACKED_BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 2048,\n    'PACKED_BLOCK_SIZE': 256})], key=['n_elements'])\n@triton.jit\ndef unpack_sign_kernel(packed_ptr, unpacked_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr', PACKED_BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    packed_start = pid * PACKED_BLOCK_SIZE\n    packed_offsets = packed_start + tl.arange(0, PACKED_BLOCK_SIZE)\n    packed_mask = packed_offsets < (n_elements + 7) // 8\n    packed = tl.load(packed_ptr + packed_offsets, mask=packed_mask, other=0)\n    bit_offsets = tl.arange(0, 8)\n    packed = packed[:, None]\n    bits = packed >> 7 - bit_offsets & 1\n    signs = bits * 2 - 1\n    element_offsets = block_start + (tl.arange(0, PACKED_BLOCK_SIZE)[:,\n        None] * 8 + bit_offsets)\n    element_offsets = tl.reshape(element_offsets, [BLOCK_SIZE])\n    signs = tl.reshape(signs, [BLOCK_SIZE])\n    mask = element_offsets < n_elements\n    tl.store(unpacked_ptr + element_offsets, signs, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "cd4b0538-efd4-4691-91a7-04166f92e66a"
  },
  {
    "input": "@triton.jit\ndef causal_product_bwd_kernel(q_ptr, k_ptr, v_ptr, grad_out, grad_Q_ptr,\n    grad_K_ptr, grad_V_ptr, batch, length, dim, vdim, **meta):\n    BLOCK_SIZE = meta['BLOCK_SIZE']\n    pid = tl.program_id(axis=0)\n    state = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n    cur_qk_pos = pid * matrix_size * dim\n    cur_v_pos = pid * matrix_size * vdim\n    dim_ptrs = tl.arange(0, BLOCK_SIZE)\n    qkmask = dim_ptrs < dim\n    vmask = dim_ptrs < vdim\n    for _ in range(0, length, 1):\n        qk_row_offsets = cur_qk_pos + dim_ptrs\n        v_row_offsets = cur_v_pos + dim_ptrs\n        k = tl.load(k_ptr + qk_row_offsets, mask=qkmask, other=0)\n        v = tl.load(v_ptr + v_row_offsets, mask=vmask, other=0)\n        context = tl.dot(k[:, None], v[None, :])\n        state += context\n        g = tl.load(grad_out + v_row_offsets, mask=vmask, other=0)\n        grad_q = tl.dot(state, g[:, None])\n        tl.store(grad_Q_ptr + qk_row_offsets[:, None], grad_q, mask=qkmask[\n            :, None])\n        cur_qk_pos += dim\n        cur_v_pos += vdim\n    \"\"\"\n    state *= 0\n\n    for _ in range(0, length, 1):\n        # Move back one row\n        cur_pos -= dim\n\n        # Offset for a single row in Q, K, V\n        row_offsets = cur_pos + dim_ptrs\n\n        # Load the current row of Q, K, V vectors. All are vectors of shape [dim]\n        q = tl.load(q_ptr + row_offsets, mask=mask, other=0)\n        k = tl.load(k_ptr + row_offsets, mask=mask, other=0)\n        v = tl.load(v_ptr + row_offsets, mask=vmask, other=0)\n        # Load gradient\n        g = tl.load(grad_out + row_offsets, mask=vmask, other=0)\n        # Compute context [D, M] matrix from [D, 1] x [1, M]\n        context = tl.dot(q[:, None], g[None, :])\n        # state += context\n\n        # Compute gradients [1, D] x [D, M] => [1, M]\n        grad_v = tl.dot(k[None, :], context)\n        grad_v = tl.reshape(grad_v, (meta['BLOCK_SIZE'],))\n        # grad_v = tl.dot(k[None, :], state)\n\n        # Enabling the follownig leads to a hang\n\n        # grad_k = tl.dot(state, v[:, None])\n        # print(grad_v.shape)\n        # print(grad_k.shape)\n        # Store the result of this row\n        # tl.store(grad_V_ptr + row_offsets[None,\n        #          :], grad_v, mask=vmask[None, :])\n        tl.store(grad_V_ptr + row_offsets, grad_v, mask=vmask)\n        # tl.store(grad_K_ptr + row_offsets[:, None], grad_k, mask=mask[:, None])\n    \"\"\"\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "c4a40af5-d426-4418-b8d2-98da6a367072"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n    BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', start_m, start_n, num_steps, MASK: 'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    offs_n = start_n + tl.arange(0, BLOCK_N2)\n    offs_k = tl.arange(0, HEAD_DIM)\n    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    Di = tl.load(D + offs_m)\n    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n    curr_n = start_n\n    step_n = BLOCK_N2\n    for blk_idx in range(num_steps):\n        kT = tl.load(kT_ptrs)\n        vT = tl.load(vT_ptrs)\n        qk = tl.dot(q, kT, allow_tf32=False)\n        p = tl.math.exp2(qk - m)\n        if MASK:\n            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(mask, p, 0.0)\n        dp = tl.dot(do, vT, allow_tf32=False)\n        ds = p * (dp - Di[:, None])\n        ds = ds\n        dq += tl.dot(ds, tl.trans(kT), allow_tf32=False)\n        curr_n += step_n\n        kT_ptrs += step_n * stride_tok\n        vT_ptrs += step_n * stride_tok\n    return dq\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "37fc6dd7-e116-433c-86b6-170490f43fdb"
  },
  {
    "input": "@triton.jit\ndef relu6(input):\n    \"\"\"\n    Applies ReLU6 to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by ReLU6.\n    \"\"\"\n    return tl.minimum(relu(input), 6)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "57f72501-3e94-4213-818b-660af107dd88"
  },
  {
    "input": "@triton.jit\ndef _attention_score_backward_compute(GRAD_VALUES, stride_grad_values_z, Q,\n    stride_q_n, stride_q_tdst, stride_q_hid, K, stride_k_n, stride_k_tsrc,\n    stride_k_hid, INDICES, stride_indices_d, stride_indices_z, COS,\n    stride_cos_t, stride_cos_hid, SIN, stride_sin_t, stride_sin_hid, GRAD_Q,\n    stride_grad_q_n, stride_grad_q_tdst, stride_grad_q_hid, GRAD_K,\n    stride_grad_k_n, stride_grad_k_tsrc, stride_grad_k_hid, N, TDST, TSRC,\n    HID, NNZ, NUM_SINK, WINDOW_SIZE, BLOCK_HID: 'tl.constexpr'):\n    idx_z = tl.program_id(0)\n    idx_n = tl.load(INDICES + 0 * stride_indices_d + idx_z * stride_indices_z)\n    idx_tdst = tl.load(INDICES + 1 * stride_indices_d + idx_z *\n        stride_indices_z)\n    idx_tsrc = tl.load(INDICES + 2 * stride_indices_d + idx_z *\n        stride_indices_z)\n    tdst = idx_tdst + TSRC - TDST\n    idx_k = idx_z % (NUM_SINK + WINDOW_SIZE)\n    key, key_origin, key_rot, cos_k, sin_k = load_rotary_embedded_vector(K,\n        stride_k_n, stride_k_tsrc, stride_k_hid, COS, stride_cos_t,\n        stride_cos_hid, SIN, stride_sin_t, stride_sin_hid, idx_n, idx_tsrc,\n        idx_k, HID, BLOCK_HID)\n    query, query_origin, query_rot, cos_q, sin_q = load_rotary_embedded_vector(\n        Q, stride_q_n, stride_q_tdst, stride_q_hid, COS, stride_cos_t,\n        stride_cos_hid, SIN, stride_sin_t, stride_sin_hid, idx_n, idx_tdst,\n        tl.minimum(tdst, WINDOW_SIZE + NUM_SINK - 1), HID, BLOCK_HID)\n    grad_score = tl.load(GRAD_VALUES + idx_z * stride_grad_values_z)\n    grad_score = tl.where(idx_tsrc <= tdst, grad_score, 0)\n    grad_score = grad_score * (1 / tl.sqrt(HID))\n    grad_key = grad_score * query\n    grad_query = grad_score * key\n    grad_key_origin, idx_key_origin_hid, grad_key_rot, idx_key_rot_hid = (\n        grad_rotary_embedded_vector(grad_key, key_origin, key_rot, cos_k,\n        sin_k, HID, BLOCK_HID))\n    (grad_query_origin, idx_query_origin_hid, grad_query_rot, idx_query_rot_hid\n        ) = (grad_rotary_embedded_vector(grad_query, query_origin,\n        query_rot, cos_q, sin_q, HID, BLOCK_HID))\n    mask_hid = tl.arange(0, BLOCK_HID) < HID\n    tl.atomic_add(GRAD_K + idx_n * stride_grad_k_n + idx_tsrc *\n        stride_grad_k_tsrc + idx_key_origin_hid * stride_grad_k_hid, mask=\n        mask_hid, val=grad_key_origin)\n    tl.atomic_add(GRAD_K + idx_n * stride_grad_k_n + idx_tsrc *\n        stride_grad_k_tsrc + idx_key_rot_hid * stride_grad_k_hid, mask=\n        mask_hid, val=grad_key_rot)\n    tl.atomic_add(GRAD_Q + idx_n * stride_grad_q_n + idx_tdst *\n        stride_grad_q_tdst + idx_query_origin_hid * stride_grad_q_hid, mask\n        =mask_hid, val=grad_query_origin)\n    tl.atomic_add(GRAD_Q + idx_n * stride_grad_q_n + idx_tdst *\n        stride_grad_q_tdst + idx_query_rot_hid * stride_grad_q_hid, mask=\n        mask_hid, val=grad_query_rot)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "d5c1f32a-19e0-4447-9c45-96a189ec88f8"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256,\n    'N_BLOCK_SIZE': 128}, num_warps=8)], key=['V', 'N'], restore_value=[\n    'z_nv_ptr'])\n@triton.jit\ndef linear_xent_mini_bwd_prologue_kernel(z_nv_ptr, y_ptr, sumexp_ptr,\n    stride_z_N, stride_z_V, idx_N_group, N_group: 'tl.constexpr', V:\n    'tl.constexpr', N: 'tl.constexpr', V_BLOCK_SIZE: 'tl.constexpr',\n    N_BLOCK_SIZE: 'tl.constexpr'):\n    idx_N = tl.program_id(axis=0)\n    idx_V = tl.program_id(axis=1)\n    z_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, \n        idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),\n        order=(1, 0))\n    N_range = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range)\n    lse = tl.log(tl.load(sumexp_ptr + N_range))\n    z_j_to_k = tl.load(z_block_ptr)\n    mask = y[:, None] == v_range[None, :]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\n    tl.store(z_block_ptr, z_grad)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "b4aad8e2-c518-4a26-b2fb-afaebe3d8c99"
  },
  {
    "input": "@triton.jit\ndef atomic_kernel(x_ptr, increment):\n    tl.atomic_add(x_ptr, increment)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "03695e71-2604-4eac-ba6b-22ec565de6e7"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp0 + i_w * BW * DH + tl.arange(\n        0, BW)[None, :] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NW - i_w - 1\n        ) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]) * DH + (NH - i_h - 1\n        ) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (DH - NH * BH) + (DW -\n        NW * BW) * DH\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y1 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y2 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y3 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y4 + _idx, _x, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "da2ad05a-fa9b-440e-b4da-fd793289ddc3"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_no_prompt_cache(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen,\n    Out, stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, head_dim, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n        :, None] * stride_kd\n    off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[\n        None, :] * stride_vd\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim), other=0.0)\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_kbs, mask=(start_n + offs_n[None, :] < cur_batch_seq_len\n            ) & (offs_d[:, None] < head_dim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) *\n            stride_vbs, mask=(start_n + offs_n[:, None] < cur_batch_seq_len\n            ) & (offs_d[None, :] < head_dim), other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "c6d1fa7b-afb5-4ebc-9c27-91c8eb5a19cd"
  },
  {
    "input": "@triton.jit\ndef kernel_log_and_diagonal_copy(out, normalizer, alpha_c, stride_alpha_c1,\n    stride_alpha_c2, stride_alpha_c3, stride_out0, stride_out1,\n    stride_normalizer0, stride_normalizer1, batch, r, BLOCK_R1:\n    'tl.constexpr', w):\n    b_idx = tl.program_id(0)\n    if b_idx >= batch:\n        return\n    start = tl.program_id(1)\n    mask = tl.arange(0, BLOCK_R1) < r\n    x = tl.load(out + b_idx * stride_out0 + start * stride_out1 + tl.arange\n        (0, BLOCK_R1), mask=mask, other=1)\n    x_normalizer = tl.load(normalizer + b_idx * stride_normalizer0 + start)\n    out_log = tl.log(x + 1e-09)\n    out_log = out_log + x_normalizer\n    tl.store(alpha_c + b_idx * stride_alpha_c1 + start * stride_alpha_c2 + \n        (start + w) * stride_alpha_c3 + tl.arange(0, BLOCK_R1), out_log,\n        mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "88081eaf-ce2e-495f-a9f1-da2880eb1c1d"
  },
  {
    "input": "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] ==\n    args['V_BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_sampled_col_kernel(Q, K, V, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_dob, stride_doh, stride_dom,\n    stride_dqb, stride_dqh, stride_dqm, stride_dkb, stride_dkh, stride_dkn,\n    stride_dvb, stride_dvh, stride_dvn, nheads, seqlen_q, headdim,\n    v_headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BLOCK_HEADDIM:\n    'tl.constexpr', V_BLOCK_HEADDIM: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr', EVEN_V_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    D += off_hb * seqlen_q\n    LSE += off_hb * seqlen_q\n    start_n = tl.program_id(0)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_vd[None, :])\n    dv = tl.zeros([BLOCK_N, V_BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_HEADDIM:\n        k = tl.load(k_ptrs)\n    else:\n        k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    if EVEN_V_HEADDIM:\n        v = tl.load(v_ptrs)\n    else:\n        v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n    for start_m in range(0, seqlen_q, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        q_ptrs = Q + (offs_m_curr[:, None] * stride_qm + offs_d[None, :])\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n        qk = tl.dot(q, tl.trans(k))\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        do_ptrs = DO + (offs_m_curr[:, None] * stride_dom + offs_vd[None, :])\n        if EVEN_V_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n        dv += tl.dot(tl.trans(p), do)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, tl.trans(v))\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(tl.trans(ds), q)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        dq_ptrs = DQ + (offs_m_curr[:, None] * stride_dqm + offs_d[None, :])\n        dq = tl.dot(ds, k)\n        if EVEN_HEADDIM:\n            tl.atomic_add(dq_ptrs, dq)\n        else:\n            tl.atomic_add(dq_ptrs, dq, mask=offs_d[None, :] < headdim)\n    dv_ptrs = DV + off_b * stride_dvb + off_h * stride_dvh + (offs_n[:,\n        None] * stride_dvn + offs_vd[None, :])\n    dk_ptrs = DK + off_b * stride_dkb + off_h * stride_dkh + (offs_n[:,\n        None] * stride_dkn + offs_d[None, :])\n    dk += tl.load(dk_ptrs)\n    dv += tl.load(dv_ptrs)\n    _bwd_store_dx(dk_ptrs, dk, offs_d, headdim, even_headdim=EVEN_HEADDIM)\n    _bwd_store_dx(dv_ptrs, dv, offs_vd, v_headdim, even_headdim=EVEN_V_HEADDIM)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "bd172f34-809c-41fb-97ac-a9deb33e467b"
  },
  {
    "input": "@triton.jit\ndef triton_batch_lora_B(output, x, w, a_start, a_len, a_loc, batch_req_bins,\n    a_scaling, qkvo_offset: 'tl.constexpr', NUM_TOKENS: 'tl.constexpr',\n    HIDDEN: 'tl.constexpr', MAX_LORA_RANK: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    return\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "cddb88d5-628d-4669-98f8-5841fdfa5171"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS,\n    NUM_STAGES)], key=['M'])\n@triton.jit\ndef triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum(\n    input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_b), tl.load(\n        input_ptr_offsets + (pid_b + 1))\n    buffer_max_all = tl.full((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float\n        ('-inf'), dtype=tl.float32)\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED\n        ):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf')\n            )\n        buffer_max_all = tl.maximum(buffer_max_all, input)\n    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED\n        ):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf')\n            )\n        buffer += tl.exp(input - buffer_max)\n    buffer_exp_sum = tl.sum(buffer, axis=0)\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED\n        ):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf')\n            )\n        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)\n        tl.store(output_ptr + idxs, output, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "db1055af-fadd-4ea0-a1da-6f49d942ba25"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.\n    Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}),\n    triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dz_kernel(dout_ptr, out_ptr, z_ptr, x_ptr, D_ptr,\n    outz_ptr, dz_ptr, dout_x_ptr, dD_ptr, ddA_cumsum_ptr, chunk_size, hdim,\n    batch, seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head,\n    stride_out_hdim, stride_z_batch, stride_z_seqlen, stride_z_head,\n    stride_z_hdim, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_D_head, stride_outz_batch, stride_outz_seqlen,\n    stride_outz_head, stride_outz_hdim, stride_dz_batch, stride_dz_seqlen,\n    stride_dz_head, stride_dz_hdim, stride_doutx_batch, stride_doutx_seqlen,\n    stride_doutx_head, stride_doutx_hdim, stride_dD_batch, stride_dD_chunk,\n    stride_dD_head, stride_dD_csize, stride_dD_hdim, stride_ddA_cs_batch,\n    stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_D:\n    'tl.constexpr', D_HAS_HDIM: 'tl.constexpr', HAS_DDACS: 'tl.constexpr',\n    RECOMPUTE_OUTPUT: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dout_x_ptr += (pid_b * stride_doutx_batch + pid_c * chunk_size *\n        stride_doutx_seqlen + pid_h * stride_doutx_head)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    z_ptr += (pid_b * stride_z_batch + pid_c * chunk_size * stride_z_seqlen +\n        pid_h * stride_z_head)\n    dz_ptr += (pid_b * stride_dz_batch + pid_c * chunk_size *\n        stride_dz_seqlen + pid_h * stride_dz_head)\n    if RECOMPUTE_OUTPUT:\n        outz_ptr += (pid_b * stride_outz_batch + pid_c * chunk_size *\n            stride_outz_seqlen + pid_h * stride_outz_head)\n    if HAS_DDACS:\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    if HAS_D:\n        x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size *\n            stride_x_seqlen + pid_h * stride_x_head)\n        dD_ptr += (pid_b * stride_dD_batch + pid_c * stride_dD_chunk + \n            pid_h * stride_dD_head + pid_m * stride_dD_csize)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    dout_x_ptrs = dout_x_ptr + (offs_m[:, None] * stride_doutx_seqlen + \n        offs_n[None, :] * stride_doutx_hdim)\n    out_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None,\n        :] * stride_out_hdim)\n    z_ptrs = z_ptr + (offs_m[:, None] * stride_z_seqlen + offs_n[None, :] *\n        stride_z_hdim)\n    dz_ptrs = dz_ptr + (offs_m[:, None] * stride_dz_seqlen + offs_n[None, :\n        ] * stride_dz_hdim)\n    if RECOMPUTE_OUTPUT:\n        outz_ptrs = outz_ptr + (offs_m[:, None] * stride_outz_seqlen + \n            offs_n[None, :] * stride_outz_hdim)\n    if HAS_D:\n        x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None,\n            :] * stride_x_hdim)\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    out = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    z = tl.load(z_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    z_sigmoid = tl.sigmoid(z)\n    if RECOMPUTE_OUTPUT:\n        outz = out * z * z_sigmoid\n        tl.store(outz_ptrs, outz, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_n[None, :] < hdim))\n    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))\n    tl.store(dz_ptrs, dz, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n    dout *= z * z_sigmoid\n    tl.store(dout_x_ptrs, dout, mask=(offs_m[:, None] < chunk_size_limit) &\n        (offs_n[None, :] < hdim))\n    if HAS_D:\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        out -= x * D\n    if HAS_DDACS:\n        ddA_cs = tl.sum(dout * out, axis=1)\n        tl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs,\n            mask=offs_m < chunk_size)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "99584859-0e4e-4341-9bfa-8c28e7928539"
  },
  {
    "input": "@triton.jit\ndef index_select_cat_bwd_kernel(grad_source_ptr, index_ptr, grad_output_ptr,\n    num_rows, num_indices, num_cols, stride0, stride1, BLOCK_SIZE_INDEX:\n    'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    grad_output_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0,\n        BLOCK_SIZE_INDEX)\n    grad_output_offsets = grad_output_ptr + grad_output_indices[:, None\n        ] * stride0 + cols[None, :] * stride1\n    grad_output_mask = (grad_output_indices[:, None] < num_indices) & (cols\n        [None, :] < num_cols)\n    grad_output = tl.load(grad_output_offsets, mask=grad_output_mask)\n    grad_source_indices = tl.load(index_ptr + grad_output_indices, mask=\n        grad_output_indices < num_indices)\n    grad_source_offsets = grad_source_ptr + grad_source_indices[:, None\n        ] * stride0 + cols[None, :] * stride1\n    tl.store(grad_source_offsets, grad_output, mask=grad_output_mask)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "83fce7c6-b670-4463-a297-0237abf0e800"
  },
  {
    "input": "@triton.jit\ndef demo2(x_ptr):\n    i_range = tl.arange(0, 8)[:, None]\n    j_range = tl.arange(0, 4)[None, :]\n    range = i_range * 4 + j_range\n    None\n    x = tl.load(x_ptr + range, (i_range < 4) & (j_range < 3), 0)\n    None\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "93b4469f-2b2c-47bc-8e58-fc9cd2314dee"
  },
  {
    "input": "@triton.jit\ndef embedding_kernel(weight, input_ids, out, vob_start_id, vob_end_id,\n    stride_weight_seq, stride_out_seq, n_ctx, hiden_size: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_NN:\n    'tl.constexpr'):\n    start_n = tl.program_id(0) * BLOCK_N\n    offs_nn = start_n + tl.arange(0, BLOCK_NN)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    for start_nn in range(0, BLOCK_N, BLOCK_NN):\n        start_nn = tl.multiple_of(start_nn, BLOCK_NN)\n        offs_seq = start_nn + offs_nn\n        n_ctx_mask = offs_seq < n_ctx\n        token_ids = tl.load(input_ids + offs_seq, mask=n_ctx_mask, other=\n            vob_end_id)\n        id_mask = (token_ids >= vob_start_id) & (token_ids < vob_end_id)\n        token_ids = token_ids - vob_start_id\n        dim_mask = offs_d < hiden_size\n        load_mask = id_mask[:, None] & dim_mask[None, :]\n        store_mask = n_ctx_mask[:, None] & dim_mask[None, :]\n        vecs = tl.load(weight + token_ids[:, None] * stride_weight_seq +\n            offs_d[None, :], mask=load_mask, other=0.0)\n        tl.store(out + offs_seq[:, None] * stride_out_seq + offs_d[None, :],\n            vecs, mask=store_mask)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "eb057fc8-95c3-4ae1-8441-c9c5fbcf94fa"
  },
  {
    "input": "@triton.jit\ndef triton_cross_merge_bidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr',\n    BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _y1 = tl.load(p_y1 + _idx, mask=_mask_hw)\n        _y2 = tl.load(p_y2 + _idx, mask=_mask_hw)\n        _y3 = tl.load(p_y3 + _idx, mask=_mask_hw)\n        _y4 = tl.load(p_y4 + _idx, mask=_mask_hw)\n        tl.store(p_x + _idx, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "7440656c-d131-4ac4-ac15-4cf74554d318"
  },
  {
    "input": "@triton.heuristics({'BACKWARD_PASS': lambda args: bool(args['BACKWARD_PASS'])})\n@triton.jit\ndef _rope_embedding(Q, Q_row_stride, cos, cos_row_stride, sin,\n    sin_row_stride, seqlen, head_dim: 'tl.constexpr', n_heads:\n    'tl.constexpr', BACKWARD_PASS: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    ROPE_GROUP_SIZE = 4\n    row_position = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n    sin1 = tl.load(sin + row_position % seqlen * sin_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    cos1 = tl.load(cos + row_position % seqlen * cos_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    if BACKWARD_PASS:\n        sin1 = -sin1\n    pass\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min(head_start + ROPE_GROUP_SIZE, n_heads)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = (row_position * Q_row_stride + k * head_dim + col_offsets +\n            half_head_dim)\n        Q1 = tl.load(Q + offs_q1, mask=mask, other=0)\n        Q2 = tl.load(Q + offs_q2, mask=mask, other=0)\n        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)\n    pass\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "de5be70e-eb9b-4b8b-bdc7-adf41962a700"
  },
  {
    "input": "@triton.autotune(configs=configs, key=['CACHE_KEY_M', 'CACHE_KEY_N',\n    'BATCHSIZE', 'SPARSITY_BIN'])\n@triton.jit\ndef splitk_sparse_gemv_kernel(Y, A, X, threshold, N, M, CACHE_KEY_N,\n    CACHE_KEY_M, BATCHSIZE: 'tl.constexpr', SPARSITY_BIN: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    start_m = tl.program_id(1)\n    rn = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rm = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    A_ptr = A + (rm[:, None] * N + rn[None, :])\n    X_ptr = X + rm\n    Y_ptr = Y + rn\n    if BATCHSIZE == 1:\n        x0 = tl.load(X_ptr, mask=rm < M, other=0.0, eviction_policy=\n            'evict_last')\n        idx = tl.abs(x0) > threshold\n        a = tl.load(A_ptr, mask=idx[:, None], other=0.0, eviction_policy=\n            'evict_first')\n        acc0 = tl.sum(a * x0[:, None], 0)\n    rn = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    tl.atomic_add(Y_ptr, acc0, mask=rn < N)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4eddf085-d098-497a-b686-f266aa479c0a"
  },
  {
    "input": "@triton.jit\ndef gemm_split_k_kernel(a_ptr, b_ptr, c_ptr, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, scale_a, scale_b, m, n, k,\n    block_m: 'tl.constexpr', block_n: 'tl.constexpr', block_k:\n    'tl.constexpr', split_k: 'tl.constexpr', group_m: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    grid_k = tl.cdiv(k, block_k * split_k)\n    pid_m, pid_n = grouped_launch(pid, m, n, block_m, block_n, group_m)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_k = pid_k * block_k + tl.arange(0, block_k)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, block_m), block_m)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, block_n), block_n)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k_ in range(0, grid_k):\n        k_remaining = k - k_ * (block_k * split_k)\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < k_remaining, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < k_remaining, other=0.0)\n        acc = tl.dot(a, b, acc, out_dtype=tl.float32)\n        a_ptrs += block_k * split_k * stride_ak\n        b_ptrs += block_k * split_k * stride_bk\n    acc = scale_a * scale_b * acc\n    acc\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    mask = (offs_m < m)[:, None] & (offs_n < n)[None, :]\n    tl.atomic_add(c_ptrs, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "24cae558-18af-44d9-b46b-5f8c482fcf1b"
  },
  {
    "input": "@triton.jit\ndef parallel_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "1fbcecaa-e46e-4717-9462-5d9364b9a9c9"
  },
  {
    "input": "@triton.jit\ndef _splat_3d(to_splat, grad_image, w, batch_index, ix, iy, iz, ID:\n    'tl.constexpr', IH: 'tl.constexpr', IW: 'tl.constexpr', C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    w = tl.view(w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW) * (iz <\n        ID) * (iz >= 0))[:, None] * channel_bcast, (BLOCK_SIZE, C))\n    offs = tl.view((batch_index * ID * IW * IH * C + iz_ * IW * IH * C + \n        iy_ * IW * C + ix_ * C)[:, None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "c927cc34-f745-4b26-844c-1d7d73655feb"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "769f99b1-69ca-4c6c-9790-1b3c48a4ddb9"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dx_kernel(DX, DY, DW, DB, X, W, Mean, Rstd, Lock,\n    stride, N, GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    c2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    partial_db = dy\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0cd4e930-f522-4e9d-ba78-115d3e2dbbeb"
  },
  {
    "input": "@triton.jit\ndef var_len_copy_kernel_triton(old_a_start, old_a_len, old_a_location,\n    new_a_start, new_a_location, BLOCK_SIZE: 'tl.constexpr'):\n    a_id = tl.program_id(0)\n    length = tl.load(old_a_len + a_id)\n    old_start = tl.load(old_a_start + a_id)\n    new_start = tl.load(new_a_start + a_id)\n    old_offset = tl.arange(0, BLOCK_SIZE)\n    new_offset = tl.arange(0, BLOCK_SIZE)\n    for i in range(0, length, BLOCK_SIZE):\n        v = tl.load(old_a_location + old_start + i + old_offset, mask=\n            old_offset < length)\n        tl.store(new_a_location + new_start + i + new_offset, v, mask=\n            new_offset < length)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "5faea25b-09fa-46f8-af46-731d256d3f41"
  },
  {
    "input": "@triton.jit\ndef chunk_gla_fwd_kernel(k, v, g, h, initial_state, final_state, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_hh, s_ht, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_db = g + i_bh * s_qk_h + (BT - 1) * s_qk_t + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        d_b = tl.load(p_db)\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, ((i + 1) * DK, DV), (s_ht,\n            1), (i * DK + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_h *= tl.math.exp2(d_b)[:, None]\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_db += BT * DK\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "ded20b01-ed01-4817-98cc-93867496a306"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_K(v, z, o, A, s_v_h, s_v_t, s_v_d, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, tl.exp(b_v - b_zn[None, :]), allow_tf32=False)\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_o *= tl.exp(b_zn[None, :] - b_z)\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n        b_v = tl.load(p_v, boundary_check=(0,))\n        m_i = o_i[:, None] >= j\n        b_o += tl.where(m_i, b_A[:, None] * tl.exp(b_v[None, :] - b_z), 0)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "e4dca307-fa0e-44f1-8ae8-0b0fe9739c98"
  },
  {
    "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_z_row, M, N, eps, BLOCK_N: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr', NORM_BEFORE_GATE:\n    'tl.constexpr', IS_RMS_NORM: 'tl.constexpr'):\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask)\n        y *= z * tl.sigmoid(z)\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "fcbc7efb-4011-4b1c-a304-25c97cd2ddbc"
  },
  {
    "input": "@triton.jit\ndef _geglu_tanh_forward_kernel(a, b, c, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "a846c50a-9049-465a-aef7-9b0b92d5a065"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef bwd_prepare_wy_repr_kernel(k, v, beta, A, dw, du, dk, dv, dbeta, T:\n    'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, \n            i_t * BT), (BT, BT), (0, 1))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (BT, T), (\n            1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dbeta = tl.zeros([BT], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_du = tl.make_block_ptr(du + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_du = tl.make_block_ptr(du + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v_beta = b_v * b_beta[:, None]\n        b_du = tl.load(p_du, boundary_check=(0, 1))\n        b_dA += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)\n        b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)\n        b_dv = b_dv_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dv_beta * b_v, 1)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (\n                i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dw = tl.make_block_ptr(dw + i_bh * T * K, (T, K), (K, 1), (\n                i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dw = tl.make_block_ptr(dw + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_beta = b_k * b_beta[:, None]\n        b_dw = tl.load(p_dw, boundary_check=(0, 1))\n        b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)\n        b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)\n        b_dk = b_dk_beta * b_beta[:, None]\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :],\n        b_dA, 0)\n    b_dA = tl.dot(b_dA, b_A)\n    b_dA = tl.dot(b_A, b_dA)\n    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], \n        -b_dA, 0)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (\n                i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dk = tl.load(p_dk, boundary_check=(0, 1))\n        b_k_beta = b_k * b_beta[:, None]\n        b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)\n        b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n        b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)\n        b_dk += b_dk_beta * b_beta[:, None]\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    if HEAD_FIRST:\n        p_dbeta = tl.make_block_ptr(dbeta + i_bh * T, (T,), (1,), (i_t * BT\n            ,), (BT,), (0,))\n    else:\n        p_dbeta = tl.make_block_ptr(dbeta + i_b * T * H + i_h, (T,), (H,),\n            (i_t * BT,), (BT,), (0,))\n    tl.store(p_dbeta, b_dbeta, boundary_check=(0,))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "eb7a6ca8-0091-426f-adad-a580d60e1e07"
  },
  {
    "input": "@triton.jit\ndef parallel_retention_bwd_kernel(q, k, v, do, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "8ff2e8d2-7d84-46c9-ab97-775eb759e27a"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BS': 32}, num_warps=2), triton.\n    Config({'BS': 32}, num_warps=4), triton.Config({'BS': 32}, num_warps=8),\n    triton.Config({'BS': 64}, num_warps=2), triton.Config({'BS': 64},\n    num_warps=4), triton.Config({'BS': 64}, num_warps=8), triton.Config({\n    'BS': 128}, num_warps=2), triton.Config({'BS': 128}, num_warps=4),\n    triton.Config({'BS': 128}, num_warps=8)], key=['S'])\n@triton.jit\ndef recurrent_cumsum_fwd_kernel(s, z, s_s_h, s_s_t, T: 'tl.constexpr', S:\n    'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_s = i_s * BS + tl.arange(0, BS)\n    mask = o_s < S\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(0, T):\n        b_s = tl.load(s + i_bh * s_s_h + i_t * s_s_t + o_s, mask=mask, other=0)\n        b_z = b_z + b_s\n        tl.store(z + i_bh * s_s_h + i_t * s_s_t + o_s, b_z, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "2086cf5e-18a0-444c-bb79-91eecbc8e22c"
  },
  {
    "input": "@triton.jit\ndef _plane_grid_splat_one(gi, to_splat, grad_feature_grid,\n    feature_grid_size, batch_index, ix_in, iy_in, IH, IW, C: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    ix, iy, ix0, iy0, grid_numel = _get_plane_grid_sample_info(gi, ix_in,\n        iy_in, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    V00x, V00y, V10x, V10y, V01x, V01y, V11x, V11y, x, y = (\n        _get_plane_grid_sample_locs_weights(ix, iy, ix0, iy0))\n    to_splat_now = to_splat\n    _splat_2d(to_splat_now, grad_feature_grid, (1 - x) * (1 - y),\n        batch_index, V00x, V00y, IH, IW, C, BLOCK_SIZE)\n    _splat_2d(to_splat_now, grad_feature_grid, (1 - x) * y, batch_index,\n        V10x, V10y, IH, IW, C, BLOCK_SIZE)\n    _splat_2d(to_splat_now, grad_feature_grid, x * (1 - y), batch_index,\n        V01x, V01y, IH, IW, C, BLOCK_SIZE)\n    _splat_2d(to_splat_now, grad_feature_grid, x * y, batch_index, V11x,\n        V11y, IH, IW, C, BLOCK_SIZE)\n    return grid_numel\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "990ef8d2-57a9-44cd-947c-a1ddc04f9ca5"
  },
  {
    "input": "@triton.jit\ndef parallel_simple_gla_bwd_kernel_dq(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n    dq, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_s in range(0, i_t * BT, BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (1, s_v_t), (i_v *\n            BV, i_s), (BV, BS), (0, 1))\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_gn = tl.load(g + i_bh * T + min(i_s + BS, T) - 1)\n        b_gp = tl.load(g + i_bh * T + i_s - 1) if i_s % BT > 0 else 0.0\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * tl.exp(b_gn - b_g)[None, :\n            ]\n        if i_s > 0:\n            b_dq *= tl.exp(b_gn - b_gp)\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    p_gq = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    b_gq = tl.load(p_gq, boundary_check=(0,))\n    b_dq *= tl.exp(b_gq)[:, None] * scale\n    o_q = i_t * BT + tl.arange(0, BT)\n    o_k = i_t * BT + tl.arange(0, BS)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_s,\n            i_k * BK), (BS, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (1, s_v_t), (i_v *\n            BV, i_s), (BV, BS), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.where(m_s, tl.dot(b_do, b_v, allow_tf32=False) * tl.exp(\n            b_gq[:, None] - b_gk[None, :]), 0) * scale\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        o_k += BS\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq + (i_v * B * H + i_bh) * s_k_h, (T, K), (\n        s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (i_v * B * H + i_bh) * T, (T,), (1,), (\n        i_t * BT,), (BT,), (0,))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg = tl.sum(b_dq * b_q, 1)\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0,))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f2f36a27-8bbc-4e17-bf7a-850a44b6f1b3"
  },
  {
    "input": "@triton.jit\ndef gated_matmul_bwd_input(w1, w2, y1_grad, y2_grad, din, M, N, K,\n    stride_dom, stride_im, stride_wn, dtype: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', IS_EVEN_MNK: 'tl.constexpr'):\n    \"\"\"\n    Kernel for backward gated MLP\n    We group along the N axis\n\n    Ref :\n    x_grad = torch.matmul(y2_grad, w2.t()) + torch.matmul(y1_grad, w1.t())\n    \"\"\"\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_k = tl.cdiv(K, BLOCK_K)\n    num_pid_in_group = GROUP_M * num_pid_k\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    GROUP_M = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % GROUP_M\n    pid_k = pid % num_pid_in_group // GROUP_M\n    y1_grad_block_ptr = tl.make_block_ptr(base=y1_grad, shape=(M, N),\n        strides=(stride_dom, 1), offsets=(pid_m * BLOCK_M, 0), block_shape=\n        (BLOCK_M, BLOCK_N), order=(1, 0))\n    y2_grad_block_ptr = tl.make_block_ptr(base=y2_grad, shape=(M, N),\n        strides=(stride_dom, 1), offsets=(pid_m * BLOCK_M, 0), block_shape=\n        (BLOCK_M, BLOCK_N), order=(1, 0))\n    w1_block_ptr = tl.make_block_ptr(base=w1, shape=(N, K), strides=(\n        stride_wn, 1), offsets=(0, pid_k * BLOCK_K), block_shape=(BLOCK_N,\n        BLOCK_K), order=(1, 0))\n    w2_block_ptr = tl.make_block_ptr(base=w2, shape=(N, K), strides=(\n        stride_wn, 1), offsets=(0, pid_k * BLOCK_K), block_shape=(BLOCK_N,\n        BLOCK_K), order=(1, 0))\n    acc_dx = tl.zeros((BLOCK_M, BLOCK_K), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        if IS_EVEN_MNK:\n            w1_blk = tl.load(w1_block_ptr)\n            w2_blk = tl.load(w2_block_ptr)\n            y1_grad_blk = tl.load(y1_grad_block_ptr)\n            y2_grad_blk = tl.load(y2_grad_block_ptr)\n        else:\n            w1_blk = tl.load(w1_block_ptr, boundary_check=(0, 1))\n            w2_blk = tl.load(w2_block_ptr, boundary_check=(0, 1))\n            y1_grad_blk = tl.load(y1_grad_block_ptr, boundary_check=(0, 1))\n            y2_grad_blk = tl.load(y2_grad_block_ptr, boundary_check=(0, 1))\n        acc_dx += tl.dot(y2_grad_blk, w2_blk)\n        acc_dx += tl.dot(y1_grad_blk, w1_blk)\n        w1_block_ptr = tl.advance(w1_block_ptr, (BLOCK_N, 0))\n        w2_block_ptr = tl.advance(w2_block_ptr, (BLOCK_N, 0))\n        y1_grad_block_ptr = tl.advance(y1_grad_block_ptr, (0, BLOCK_N))\n        y2_grad_block_ptr = tl.advance(y2_grad_block_ptr, (0, BLOCK_N))\n    dx_ptrs = tl.make_block_ptr(base=din, shape=(M, K), strides=(stride_im,\n        1), offsets=(pid_m * BLOCK_M, pid_k * BLOCK_K), block_shape=(\n        BLOCK_M, BLOCK_K), order=(1, 0))\n    if IS_EVEN_MNK:\n        tl.store(dx_ptrs, acc_dx)\n    else:\n        tl.store(dx_ptrs, acc_dx, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8e97b682-7377-4cd4-9c82-0dbf84b77242"
  },
  {
    "input": "@triton.jit\ndef add_kernel(x_ptr, y_ptr, z_ptr, size, block_size: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    offsets = tl.arange(0, block_size) + pid * block_size\n    mask = offsets < size\n    x = tl.load(x_ptr + offsets, mask)\n    y = tl.load(y_ptr + offsets, mask)\n    z = x + y\n    tl.store(z_ptr + offsets, z, mask)\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "7897175d-d708-42c5-856d-7246c7bd3770"
  },
  {
    "input": "@triton.jit\ndef fourth_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST000 = 1.125\n    CONST001 = 2.25\n    CONST002 = 3.0\n    CONST005 = 2.21852991866236\n    CONST007 = 9.48683298050514\n    CONST010 = 20.1246117974981\n    CONST011 = -18.8248505970167\n    CONST012 = -13.3111795119741\n    CONST013 = -10.0623058987491\n    CONST014 = -9.0\n    CONST015 = -8.87411967464942\n    CONST016 = -7.11512473537885\n    CONST017 = -6.27495019900557\n    CONST018 = -3.35410196624968\n    CONST019 = -1.67705098312484\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    Y00 = CONST015 * VAR07 * z - CONST015 * VAR25 * x\n    Y01 = y * (-CONST011 * VAR26 * x + CONST017 * VAR07)\n    Y02 = CONST018 * VAR07 * z + x * (CONST010 * VAR17 * z + CONST018 * VAR25)\n    Y03 = CONST016 * VAR07 * y + x * (CONST007 * VAR16 + CONST016 * VAR26 * y)\n    Y04 = (CONST000 * VAR06 + CONST000 * VAR24 + CONST002 * VAR15 + \n        CONST014 * VAR17 * VAR26 + VAR08 * (CONST001 * VAR26 + CONST014 *\n        VAR17))\n    Y05 = CONST016 * VAR25 * y + z * (CONST007 * VAR16 + CONST016 * VAR08 * y)\n    Y06 = -CONST019 * VAR06 + CONST019 * VAR24 + VAR17 * (CONST013 * VAR08 -\n        CONST013 * VAR26)\n    Y07 = y * (CONST011 * VAR08 * z - CONST017 * VAR25)\n    Y08 = CONST005 * VAR06 + CONST005 * VAR24 + CONST012 * VAR08 * VAR26\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y07, mask=\n        output_row_offset + 7 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y08, mask=\n        output_row_offset + 8 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "2a4d7f94-d58a-45eb-aafa-025366f339b1"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dx(dk_ptrs, dk, offs_n, offs_d, seqlen_k, headdim,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n        _bwd_store_dx(dv_ptrs, dv, offs_n, offs_d, seqlen_k, headdim,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dx(dk_ptrs, dk, offs_n, offs_d, seqlen_k, headdim, EVEN_M=\n        EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n    _bwd_store_dx(dv_ptrs, dv, offs_n, offs_d, seqlen_k, headdim, EVEN_M=\n        EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "90128da7-b287-4cc0-888b-e728ad9eec7d"
  },
  {
    "input": "@triton.jit\ndef print_value(value):\n    if tl.program_id(0) == 0 and tl.program_id(1) == 0:\n        tl.device_print(str(value))\n",
    "category": "Helper Functions",
    "subcategory": "",
    "uuid": "3f26e92c-a61d-4cb6-b213-50664de49787"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "4f803f4f-9cf1-4b1b-b815-225a350ed898"
  },
  {
    "input": "@triton.jit\ndef _exact_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    f = 1/2 * e * (1 + erf(1/sqrt(2) * e))\n    h = f * up\n\n    df/de (with help of Wolfram :)\n    df/de = 1/2 * (1 + erf(1/sqrt(2) * e)) + 1/sqrt(2*pi) * e * exp(-1/2 * e^2)\n\n    Reuse via\n    f =        1/2 * (1 + erf(1/sqrt(2) * e)) * e\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_partial_row = 0.5 * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_partial_row * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    t = 0.3989422804014327\n    df_de = f_partial_row + t * e_row * tl.exp(-0.5 * e_row * e_row)\n    de_row = dg_row * df_de\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "70c10e97-0a5d-45ce-a07c-2be998762b92"
  },
  {
    "input": "@triton.jit\ndef relu_kernel(X, Y, N):\n    idx = tl.program_id(0)\n    if idx < N:\n        x = tl.load(X + idx)\n        y = tl.max(x, 0)\n        tl.store(Y + idx, y)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "b64a3e26-8b68-40e8-9558-f27d103fd9e8"
  },
  {
    "input": "@triton.jit\ndef attn_fwd(Q, K, V, B, sm_scale, L, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_bz, stride_bh, stride_bm,\n    stride_bn, stride_oz, stride_oh, stride_om, stride_on, num_head_q:\n    \"'i32'\", num_head_k: \"'i32'\", cu_seqlens_q, cu_seqlens_k, num_seqlens:\n    \"'i32'\", max_seqlen_q: \"'i32'\", max_seqlen_k: \"'i32'\", head_dim:\n    \"'i32'\", dropout_p, philox_seed_ptr, philox_offset1: \"'*u32'\",\n    philox_offset2: \"'i32'\", philox_seed_output: \"'*u64'\",\n    philox_offset_output: \"'*u64'\", encoded_softmax, CAUSAL: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', pre_load_v: 'tl.constexpr', ENABLE_DROPOUT:\n    'tl.constexpr', RETURN_ENCODED_SOFTMAX: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    PRE_LOAD_V: 'tl.constexpr' = pre_load_v\n    USE_ALIBI: 'tl.constexpr' = False\n    start_m = tl.program_id(0)\n    off_h_q = tl.program_id(1)\n    off_z = tl.program_id(2)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    philox_seed = 0\n    philox_offset_base = philox_offset2\n    if ENABLE_DROPOUT:\n        philox_seed = tl.load(philox_seed_ptr)\n        philox_offset_base += tl.load(philox_offset1)\n        if (tl.program_id(0) == 0 and tl.program_id(1) == 0) and tl.program_id(\n            2) == 0:\n            if philox_seed_output.cast(dtype=tl.uint64, bitcast=True) != 0:\n                tl.store(philox_seed_output, philox_seed)\n            if philox_offset_output.cast(dtype=tl.uint64, bitcast=True) != 0:\n                tl.store(philox_offset_output, philox_offset_base)\n    if num_seqlens > 0:\n        cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n        cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n        seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n        if start_m * BLOCK_M >= seqlen_q:\n            return\n        cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)\n        cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)\n        seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start\n        batch_index = 0\n    elif num_seqlens == 0:\n        cu_seqlens_q_start = 0\n        cu_seqlens_k_start = 0\n        seqlen_q = max_seqlen_q\n        seqlen_k = max_seqlen_k\n        batch_index = off_z\n    else:\n        cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n        cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n        seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n        if start_m * BLOCK_M >= seqlen_q:\n            return\n        cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)\n        cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)\n        seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start\n        cu_seqlens_q_start = 0\n        cu_seqlens_k_start = 0\n        batch_index = off_z\n    n_blocks = cdiv_fn(seqlen_k, BLOCK_N)\n    if CAUSAL:\n        n_blocks_seqlen = cdiv_fn((start_m + 1) * BLOCK_M, BLOCK_N)\n        n_blocks = min(n_blocks, n_blocks_seqlen)\n        if n_blocks <= 0:\n            o_offset = (Out + batch_index * stride_oz + off_h_q * stride_oh +\n                cu_seqlens_q_start * stride_om)\n            o_ptrs = o_offset + offs_m[:, None] * stride_om + offs_d[None, :\n                ] * stride_on\n            acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=Out.type.element_ty)\n            o_ptrs_mask = offs_m[:, None] < seqlen_q\n            tl.store(o_ptrs, acc, mask=o_ptrs_mask)\n            L_ptr_base = L + (off_z * num_head_q + off_h_q) * max_seqlen_q\n            l_ptrs = L_ptr_base + offs_m\n            l = tl.full([BLOCK_M], value=float('inf'), dtype=tl.float32)\n            l_ptrs_mask = offs_m < max_seqlen_q\n            tl.store(l_ptrs, l, mask=l_ptrs_mask)\n            return\n    off_h_k = off_h_q // (num_head_q // num_head_k)\n    n_extra_tokens = 0\n    if seqlen_k < BLOCK_N:\n        n_extra_tokens = BLOCK_N - seqlen_k\n    elif seqlen_k % BLOCK_N:\n        n_extra_tokens = seqlen_k % BLOCK_N\n    q_offset = (Q + batch_index * stride_qz + off_h_q * stride_qh + \n        cu_seqlens_q_start * stride_qm)\n    q_ptrs = q_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_offset = (batch_index * stride_kz + off_h_k * stride_kh + \n        cu_seqlens_k_start * stride_kn)\n    k_ptrs = K + k_offset + offs_d[:, None] * stride_kk + offs_n[None, :\n        ] * stride_kn\n    v_offset = (V + batch_index * stride_vz + off_h_k * stride_vh + \n        cu_seqlens_k_start * stride_vk)\n    v_ptrs = v_offset + offs_n[:, None] * stride_vk + offs_d[None, :\n        ] * stride_vn\n    if BIAS_TYPE == 0:\n        bias_ptrs = None\n    elif BIAS_TYPE == 1:\n        bias_offset = off_h_q * stride_bh\n        bias_ptrs = B + bias_offset + offs_m[:, None] * stride_bm + offs_n[\n            None, :] * stride_bn\n    else:\n        tl.static_assert(False, f'Unsupported BIAS_TYPE {BIAS_TYPE}')\n    if USE_ALIBI:\n        a_offset = batch_index * stride_az + off_h_q * stride_ah\n        alibi_slope = tl.load(alibi_slopes + a_offset)\n    else:\n        alibi_slope = None\n    off_zh = batch_index * num_head_q + off_h_q\n    if ENABLE_DROPOUT:\n        batch_philox_offset = (philox_offset_base + off_zh * max_seqlen_q *\n            max_seqlen_k)\n    else:\n        batch_philox_offset = 0\n    if RETURN_ENCODED_SOFTMAX:\n        encoded_sm_base = (encoded_softmax + off_zh * max_seqlen_q *\n            max_seqlen_k)\n    else:\n        encoded_sm_base = None\n    m_i = tl.full([BLOCK_M], float('-inf'), dtype=tl.float32)\n    l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504089\n    bias_scale = 1.0 / sm_scale\n    q_ptrs_mask = offs_m[:, None] < seqlen_q\n    if PADDED_HEAD:\n        q_ptrs_mask = q_ptrs_mask & (offs_d[None, :] < head_dim)\n    q = tl.load(q_ptrs, mask=q_ptrs_mask, other=0.0)\n    padded_block_k = n_extra_tokens != 0\n    is_modulo_mn = not padded_block_k and seqlen_q % BLOCK_M == 0\n    if CAUSAL:\n        masked_blocks = BLOCK_M // BLOCK_N + (not is_modulo_mn)\n    else:\n        masked_blocks = padded_block_k\n    masked_blocks = min(masked_blocks, n_blocks)\n    n_full_blocks = n_blocks - masked_blocks\n    block_min = 0\n    block_max = n_blocks * BLOCK_N\n    if n_full_blocks > 0:\n        block_max = (n_blocks - masked_blocks) * BLOCK_N\n        acc, l_i, m_i = attn_fwd_inner(acc, l_i, m_i, qk_scale, bias_scale,\n            q, k_ptrs, v_ptrs, bias_ptrs, stride_kn, stride_vk, stride_bn,\n            seqlen_q, seqlen_k, head_dim, start_m, block_min, block_max,\n            dropout_p, philox_seed, batch_philox_offset, max_seqlen_k,\n            encoded_sm_base, 0, 0, 0, alibi_slope, False, BLOCK_M,\n            BLOCK_DMODEL, BLOCK_N, PRE_LOAD_V, False, ENABLE_DROPOUT,\n            RETURN_ENCODED_SOFTMAX, PADDED_HEAD)\n        block_min = block_max\n        block_max = n_blocks * BLOCK_N\n    tl.debug_barrier()\n    if masked_blocks > 0:\n        if CAUSAL:\n            offs_n_causal = offs_n\n        else:\n            offs_n_causal = 0\n        k_ptrs += n_full_blocks * BLOCK_N * stride_kn\n        v_ptrs += n_full_blocks * BLOCK_N * stride_vk\n        if BIAS_TYPE == 0:\n            pass\n        elif BIAS_TYPE == 1:\n            bias_ptrs += n_full_blocks * BLOCK_N * stride_bn\n        else:\n            tl.static_assert(False, f'Unsupported BIAS_TYPE {BIAS_TYPE}')\n        acc, l_i, m_i = attn_fwd_inner(acc, l_i, m_i, qk_scale, bias_scale,\n            q, k_ptrs, v_ptrs, bias_ptrs, stride_kn, stride_vk, stride_bn,\n            seqlen_q, seqlen_k, head_dim, start_m, block_min, block_max,\n            dropout_p, philox_seed, batch_philox_offset, max_seqlen_k,\n            encoded_sm_base, offs_n_causal, masked_blocks, n_extra_tokens,\n            alibi_slope, CAUSAL, BLOCK_M, BLOCK_DMODEL, BLOCK_N, PRE_LOAD_V,\n            True, ENABLE_DROPOUT, RETURN_ENCODED_SOFTMAX, PADDED_HEAD)\n    acc = acc / l_i[:, None]\n    if ENABLE_DROPOUT:\n        acc = acc / (1 - dropout_p)\n    end_m_idx = (start_m + 1) * BLOCK_M\n    start_m_idx = start_m * BLOCK_M\n    causal_start_idx = 0\n    acc = acc\n    if CAUSAL:\n        tl.assume(start_m_idx >= 0)\n        tl.assume(end_m_idx >= 0)\n        if causal_start_idx > start_m_idx and causal_start_idx < end_m_idx:\n            out_mask_boundary = tl.full((BLOCK_DMODEL,), causal_start_idx,\n                dtype=tl.int32)\n            mask_m_offsets = start_m_idx + tl.arange(0, BLOCK_M)\n            out_ptrs_mask = mask_m_offsets[:, None] >= out_mask_boundary[\n                None, :]\n            z = 0.0\n            acc = tl.where(out_ptrs_mask, acc, z)\n    L_ptr_base = L + (off_z * num_head_q + off_h_q) * max_seqlen_q\n    l_ptrs = L_ptr_base + offs_m\n    overflow_size = end_m_idx - seqlen_q\n    if overflow_size > 0:\n        boundary = tl.full((BLOCK_M,), BLOCK_M - overflow_size, dtype=tl.int32)\n        l_ptrs_mask = tl.arange(0, BLOCK_M) < boundary\n        tl.store(l_ptrs, m_i + tl.math.log2(l_i), mask=l_ptrs_mask)\n    else:\n        tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n    o_base = (Out + batch_index * stride_oz + off_h_q * stride_oh + \n        cu_seqlens_q_start * stride_om)\n    mstore2d(acc, BLOCK_M, BLOCK_DMODEL, o_base=o_base, o_start_row=start_m *\n        BLOCK_M, o_start_col=0, o_rows=seqlen_q, o_cols=head_dim,\n        stride_row=stride_om, stride_col=stride_on)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "38a15ebb-4eee-4216-8949-7bc4a6ea379a"
  },
  {
    "input": "@triton.jit\ndef gelu_grad(x):\n    cdf = 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n    pdf = tl.exp(-0.5 * x * x) * _gaussian_pdf_normalization\n    return cdf + x * pdf\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "c4544ded-c199-4658-ac0b-205cc762240d"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ca01de63-0501-4dd0-ba6f-a1b3d20284c3"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_fwd_kernel_h(k, v, h, h0, ht, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if i_t == NT - 1 and T % BT != 0:\n            d_b = tl.math.exp2(T % BT * b_b)\n            d_i = tl.math.exp2((T % BT - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, b_v * d_i[:, None], allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Memory Management",
    "subcategory": "caching",
    "uuid": "abce001c-0e08-443d-a225-27a7b4dc52cd"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_attention_kernel(Q, K, V, B, Do, L, D, softmax_scale:\n    'tl.constexpr', stride_qb, stride_qm, stride_qh, stride_qg, stride_kb,\n    stride_kn, stride_kh, stride_vb, stride_vn, stride_vh, stride_bb,\n    stride_bh, stride_bg, stride_bm, stride_dob, stride_dom, stride_doh,\n    stride_dog, stride_dqb, stride_dqm, stride_dqh, stride_dqg, stride_dkb,\n    stride_dkn, stride_dkh, stride_dvb, stride_dvn, stride_dvh, stride_lb,\n    stride_lh, stride_lg, seqlen_q, seqlen_k, headdim, num_kv_heads,\n    num_groups, Dq, Dk, Dv, HAVE_BIAS: 'tl.constexpr', BIAS_SINGLE_HEAD:\n    'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M: 'tl.constexpr',\n    EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_n, off_bh, off_gp = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    softmax_scale = softmax_scale\n    off_h = off_bh % num_kv_heads\n    off_b = off_bh // num_kv_heads\n    offs_qm = tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    l_ptrs = L + (off_b * stride_lb + off_h * stride_lh + offs_qm + off_gp *\n        stride_lg)\n    d_ptrs = D + (off_b * stride_lb + off_h * stride_lh + offs_qm + off_gp *\n        stride_lg)\n    q_ptrs = Q + (off_b * stride_qb + off_h * stride_qh + off_gp * stride_qg\n        ) + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (off_b * stride_kb + off_h * stride_kh) + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + (off_b * stride_vb + off_h * stride_vh) + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    do_ptrs = Do + (off_b * stride_dob + off_h * stride_doh + off_gp *\n        stride_dog) + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = Dq + (off_b * stride_dqb + off_h * stride_dqh + off_gp *\n        stride_dqg) + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = (0 if BIAS_SINGLE_HEAD else off_h *\n            stride_bh + off_gp * stride_bg)\n        b_ptrs = B + (off_b * stride_bb + bias_h_pos) + (offs_qm[:, None] *\n            stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(0, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k.T) * softmax_scale\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if HAVE_BIAS:\n            bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk + bias\n        lse_i = tl.load(l_ptrs + start_m, mask=offs_m_curr < seqlen_q,\n            other=0.0)\n        p = tl.exp(qk - lse_i[:, None])\n        do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p.T, do)\n        dp = tl.dot(do, v.T)\n        Di = tl.load(d_ptrs + start_m, mask=offs_m_curr < seqlen_q, other=0.0)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds.T, q)\n        dq = tl.dot(ds, k)\n        tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q) &\n            (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if HAVE_BIAS:\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = Dv + (off_b * stride_dvb + off_h * stride_dvh) + (offs_n[:,\n        None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = Dk + (off_b * stride_dkb + off_h * stride_dkh) + (offs_n[:,\n        None] * stride_dkn + offs_d[None, :])\n    tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n    tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "1182d8e3-e2f6-4929-9a3f-3841e45d6ff4"
  },
  {
    "input": "@triton.jit\ndef triton_cross_merge(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp0 + i_w * BW * DH + tl.arange(\n        0, BW)[None, :] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NW - i_w - 1\n        ) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]) * DH + (NH - i_h - 1\n        ) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (DH - NH * BH) + (DW -\n        NW * BW) * DH\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _y1 = tl.load(p_y1 + _idx, mask=_mask_hw)\n        _y2 = tl.load(p_y2 + _idx, mask=_mask_hw)\n        _y3 = tl.load(p_y3 + _idx, mask=_mask_hw)\n        _y4 = tl.load(p_y4 + _idx, mask=_mask_hw)\n        tl.store(p_x + _idx, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "7bea5170-84d7-48c7-b54d-8d939e7ecf25"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, Out, b: 'tl.constexpr', h: 'tl.constexpr', n:\n    'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', BLOCK_MODEL: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_bh % h\n    off_e = tl.program_id(1)\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    e_offset = off_e * BLOCK_MODEL\n    Q_block_ptr = Q + qk_offset + tl.arange(0, d)[None, :]\n    K_trans_block_ptr = K + qk_offset + tl.arange(0, d)[:, None]\n    V_block_ptr = V + v_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :]\n    O_block_ptr = Out + o_offset + e_offset + tl.arange(0, BLOCK_MODEL)[None, :\n        ]\n    off_block = tl.arange(0, BLOCK)\n    index = off_block[:, None] - off_block[None, :]\n    kv = tl.zeros([d, BLOCK_MODEL], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        q = tl.load(Q_block_ptr + off_block[:, None] * d, mask=off_block[:,\n            None] < n, other=0.0)\n        k_trans = tl.load(K_trans_block_ptr + off_block[None, :] * d, mask=\n            off_block[None, :] < n, other=0.0)\n        v = tl.load(V_block_ptr + off_block[:, None] * e, mask=off_block[:,\n            None] < n, other=0.0)\n        qk = tl.dot(q, k_trans)\n        qk = tl.where(index >= 0, qk, 0)\n        o_intra = tl.dot(qk, v)\n        o_inter = tl.dot(q, kv)\n        o = o_intra + o_inter\n        tl.store(O_block_ptr + off_block[:, None] * e, o, mask=off_block[:,\n            None] < n)\n        kv += tl.dot(k_trans, v)\n        off_block += BLOCK\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "d652dcf1-26ec-4ebc-814e-b92c324c84b7"
  },
  {
    "input": "@triton.jit\ndef _abs_max(val1, val2):\n    val1_abs = tl.abs(val1)\n    val2_abs = tl.abs(val2)\n    if val1_abs >= val2_abs:\n        return val1_abs\n    else:\n        return val2_abs\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "dc22a640-9980-4907-8237-7c42f89c9d82"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8),\n    triton.Config({}, num_stages=2, num_warps=4), triton.Config({},\n    num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1\n    )], key=['K'])\n@triton.jit\ndef quantize_int8_perrow_kernel(fpa_ptr, a_ptr, as_ptr, M, K, stride_fpam,\n    stride_fpak, stride_am, stride_ak, stride_asm, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        inta = fpa / a_scale[:, None]\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "0944088b-5ad3-4d1f-8b12-9811367e242f"
  },
  {
    "input": "@triton.jit\ndef leaky_relu_grad(input, negative_slope):\n    \"\"\"\n    Calculates the gradient of leaky ReLU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        negative_slope: Slope of the negative component.\n\n    Returns:\n        Gradient of leaky ReLU.\n    \"\"\"\n    return tl.where(input <= 0, negative_slope, 1)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "b10671bf-31af-4231-b750-13c826d570ce"
  },
  {
    "input": "@triton.jit\ndef chunk_rwkv6_fwd_kernel_h(k, v, g, h, h0, ht, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i_t in range(NT):\n        o_t = min(i_t * BT + BT, T)\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            o_t - 1) * K + i_k * BK,), (BK,), (0,))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        if i_t < NT - 1:\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n        else:\n            b_gn = tl.min(b_g, axis=1)\n        b_h *= tl.exp(b_gn)[:, None]\n        b_k = b_k * tl.exp(b_gn[:, None] - b_g)\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "6e25d3c6-04df-4eac-9f4e-0cda37d40b10"
  },
  {
    "input": "@triton.jit\ndef identity(x):\n    return x\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "2dd4ef26-aadc-4130-8448-7ae5751ba767"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, L, NewDO, Delta, BLOCK_M: 'tl.constexpr',\n    D_HEAD: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    denom = tl.load(L + off_m)\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n    tl.store(Delta + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5a2236d3-5360-4aaa-9be6-ca0232417543"
  },
  {
    "input": "@triton.jit\ndef add_kernel(x_ptr, y_ptr, length, output_ptr, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < length\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "7b82e1e5-8571-4f85-8bfd-1be8b26267f6"
  },
  {
    "input": "@triton.jit\ndef _triton_block_sparse_attn_fwd_kernel(Q, K, V, seqlens, sm_scale,\n    block_index, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vn,\n    stride_vk, stride_oz, stride_oh, stride_om, stride_ok, Z, H, N_CTX,\n    NUM_ROWS, MAX_BLOCKS_PRE_ROW, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', dtype: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    blocks_ptr = block_index + (off_hz * NUM_ROWS + start_m\n        ) * MAX_BLOCKS_PRE_ROW\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = q * qk_scale\n    m_mask = offs_m[:, None] < seqlen\n    block_count = tl.minimum((start_m + 1) * BLOCK_M // BLOCK_N,\n        MAX_BLOCKS_PRE_ROW)\n    for sparse_block_idx in range(block_count):\n        real_block_idx = tl.load(blocks_ptr + sparse_block_idx)\n        start_n = real_block_idx * BLOCK_N\n        cols = start_n + offs_n\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc, mask=m_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "cf9e9715-19b8-4936-8a67-c33bc2fd1265"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_INP': 128,\n    'BLOCK_SIZE_OUT': 256, 'BLOCK_SIZE_HIDDEN': 64, 'GROUP_SIZE_INP': 8},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 64,\n    'BLOCK_SIZE_OUT': 256, 'BLOCK_SIZE_HIDDEN': 32, 'GROUP_SIZE_INP': 8},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_INP': 128,\n    'BLOCK_SIZE_OUT': 128, 'BLOCK_SIZE_HIDDEN': 32, 'GROUP_SIZE_INP': 8},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_INP': 128,\n    'BLOCK_SIZE_OUT': 64, 'BLOCK_SIZE_HIDDEN': 32, 'GROUP_SIZE_INP': 8},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_INP': 64,\n    'BLOCK_SIZE_OUT': 128, 'BLOCK_SIZE_HIDDEN': 32, 'GROUP_SIZE_INP': 8},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_INP': 128,\n    'BLOCK_SIZE_OUT': 32, 'BLOCK_SIZE_HIDDEN': 32, 'GROUP_SIZE_INP': 8},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_INP': 64,\n    'BLOCK_SIZE_OUT': 32, 'BLOCK_SIZE_HIDDEN': 32, 'GROUP_SIZE_INP': 8},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_INP': 32,\n    'BLOCK_SIZE_OUT': 64, 'BLOCK_SIZE_HIDDEN': 32, 'GROUP_SIZE_INP': 8},\n    num_stages=5, num_warps=2)], key=['inp_size', 'hidden_size', 'out_size'])\n@triton.jit\ndef matmul_kernel(inp_ptr, weights_ptr, out_ptr, ir_vector_ptr,\n    assumed_wmax_ptr, reduced_assumed_wmax_ptr, input_range_ptr,\n    upper_end_of_slices_ptr, inp_size, hidden_size, out_size, num_slices,\n    stride_inp_inp_size, stride_inp_hidden_size, stride_weights_hidden_size,\n    stride_weights_out_size, stride_out_inp_size, stride_out_out_size,\n    stride_assumed_wmax_num_slices, stride_assumed_wmax_out_size,\n    out_noise_seed, inp_res: 'tl.constexpr', is_fp: 'tl.constexpr', out_res:\n    'tl.constexpr', out_quant: 'tl.constexpr', out_bound: 'tl.constexpr',\n    bound_per_channel: 'tl.constexpr', out_noise: 'tl.constexpr',\n    out_noise_std: 'tl.constexpr', out_noise_per_channel: 'tl.constexpr',\n    ir_vector_is_none: 'tl.constexpr', dtype: 'tl.constexpr',\n    BLOCK_SIZE_INP: 'tl.constexpr', BLOCK_SIZE_HIDDEN: 'tl.constexpr',\n    BLOCK_SIZE_OUT: 'tl.constexpr', GROUP_SIZE_INP: 'tl.constexpr'):\n    \"\"\"\n    Computes the block-wise matmul.\n    Applies input range to the input and quantizes it. Converts\n    back to the original range before accumulating the dot products.\n    Can handle different input ranges per slice in the input dimension.\n    Stores the MVM result inp_ptr @ weights_ptr in out_ptr.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(inp_size, BLOCK_SIZE_INP)\n    num_pid_n = tl.cdiv(out_size, BLOCK_SIZE_OUT)\n    num_pid_in_group = GROUP_SIZE_INP * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_INP\n    GROUP_SIZE_INP = min(num_pid_m - first_pid_m, GROUP_SIZE_INP)\n    pid_m = first_pid_m + pid % num_pid_in_group % GROUP_SIZE_INP\n    pid_n = pid % num_pid_in_group // GROUP_SIZE_INP\n    accumulator = tl.zeros((BLOCK_SIZE_INP, BLOCK_SIZE_OUT), dtype=tl.float32)\n    per_slice_accumulator = tl.zeros((BLOCK_SIZE_INP, BLOCK_SIZE_OUT),\n        dtype=tl.float32)\n    increase_out_offsets_by = BLOCK_SIZE_INP * BLOCK_SIZE_OUT\n    output_random_offsets = tl.arange(0, BLOCK_SIZE_INP * BLOCK_SIZE_OUT\n        ).reshape((BLOCK_SIZE_INP, BLOCK_SIZE_OUT), can_reorder=True)\n    offs_am = pid_m * BLOCK_SIZE_INP + tl.arange(0, BLOCK_SIZE_INP)\n    offs_bn = pid_n * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_INP),\n        BLOCK_SIZE_INP)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_OUT),\n        BLOCK_SIZE_OUT)\n    offs_assumed_wmax = pid_n * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)\n    ir_range_lower = 0\n    for slice_idx in range(0, num_slices):\n        if slice_idx > 0:\n            per_slice_accumulator = tl.zeros((BLOCK_SIZE_INP,\n                BLOCK_SIZE_OUT), dtype=tl.float32)\n        abs_max_slice_ptrs = (assumed_wmax_ptr + slice_idx *\n            stride_assumed_wmax_num_slices + offs_bn *\n            stride_assumed_wmax_out_size)\n        if out_noise and out_noise_per_channel:\n            assumed_wmax_per_slice = tl.load(abs_max_slice_ptrs, mask=\n                offs_assumed_wmax < out_size, other=float('-inf'))\n            assumed_wmax_per_slice = assumed_wmax_per_slice[None, :]\n        else:\n            assumed_wmax_per_slice = tl.load(reduced_assumed_wmax_ptr +\n                slice_idx)\n        if bound_per_channel and not (out_noise and out_noise_per_channel):\n            bound_scale = tl.load(abs_max_slice_ptrs, mask=\n                offs_assumed_wmax < out_size, other=float('-inf'))\n            bound_scale = bound_scale[None, :]\n        else:\n            bound_scale = assumed_wmax_per_slice\n        if num_slices == 1:\n            ir_range_upper = hidden_size\n        else:\n            ir_range_upper = tl.load(upper_end_of_slices_ptr + slice_idx)\n        current_lower = ir_range_lower\n        input_range = tl.load(input_range_ptr + slice_idx)\n        offs_k = current_lower + tl.arange(0, BLOCK_SIZE_HIDDEN)\n        a_ptrs = inp_ptr + (offs_am[:, None] * stride_inp_inp_size + offs_k\n            [None, :] * stride_inp_hidden_size)\n        b_ptrs = weights_ptr + (offs_k[:, None] *\n            stride_weights_hidden_size + offs_bn[None, :] *\n            stride_weights_out_size)\n        num_k = tl.cdiv(ir_range_upper - ir_range_lower, BLOCK_SIZE_HIDDEN)\n        for k in range(0, num_k):\n            current_upper = min(ir_range_upper, ir_range_lower + (k + 1) *\n                BLOCK_SIZE_HIDDEN, hidden_size)\n            inp_block = tl.load(a_ptrs, mask=(offs_am[:, None] < inp_size) &\n                (offs_k[None, :] < current_upper), other=0.0)\n            weight_block = tl.load(b_ptrs, mask=(offs_k[:, None] <\n                current_upper) & (offs_bn[None, :] < out_size), other=0.0)\n            input_range = input_range\n            if not ir_vector_is_none:\n                tl.store(ir_vector_ptr + offs_k, input_range, mask=offs_k <\n                    current_upper)\n            over_ir_mask = inp_block > input_range\n            under_ir_mask = inp_block < -input_range\n            inp_block = tl.where(over_ir_mask, input_range, inp_block)\n            inp_block = tl.where(under_ir_mask, -input_range, inp_block)\n            inp_block = inp_block / input_range\n            if not is_fp:\n                inp_block = inp_block / inp_res\n                inp_block = tl.extra.cuda.libdevice.rint(inp_block)\n                inp_block = inp_block * inp_res\n            inp_block = inp_block\n            dot_prod = tl.dot(inp_block, weight_block)\n            dot_prod = input_range * dot_prod\n            per_slice_accumulator = per_slice_accumulator + dot_prod\n            offs_k += current_upper - current_lower\n            a_ptrs += (current_upper - current_lower) * stride_inp_hidden_size\n            b_ptrs += (current_upper - current_lower\n                ) * stride_weights_hidden_size\n            current_lower = current_upper\n        if out_noise:\n            randn_block = tl.randn(out_noise_seed + pid, output_random_offsets)\n            randn_block = assumed_wmax_per_slice * out_noise_std / tl.sqrt(\n                num_slices * num_k) * randn_block\n            per_slice_accumulator += randn_block\n            output_random_offsets += increase_out_offsets_by\n        if out_quant or out_bound > 0:\n            bound = bound_scale * out_bound * input_range\n            if out_quant:\n                alpha = bound * out_res\n                per_slice_accumulator = per_slice_accumulator / tl.where(\n                    alpha == 0.0, FLOAT32_TINY, alpha)\n                per_slice_accumulator = tl.extra.cuda.libdevice.rint(\n                    per_slice_accumulator)\n                per_slice_accumulator = per_slice_accumulator * alpha\n            if out_bound > 0:\n                over_out_bound_mask = per_slice_accumulator > bound\n                under_out_bound_mask = per_slice_accumulator < -bound\n                per_slice_accumulator = tl.where(over_out_bound_mask, bound,\n                    per_slice_accumulator)\n                per_slice_accumulator = tl.where(under_out_bound_mask, -\n                    bound, per_slice_accumulator)\n        accumulator = accumulator + per_slice_accumulator\n        ir_range_lower = ir_range_upper\n    out_block = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_INP + tl.arange(0, BLOCK_SIZE_INP)\n    offs_cn = pid_n * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)\n    c_ptrs = out_ptr + stride_out_inp_size * offs_cm[:, None\n        ] + stride_out_out_size * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < inp_size) & (offs_cn[None, :] < out_size)\n    tl.store(c_ptrs, out_block, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d7efcd71-b52d-4793-adaf-07f471308aff"
  },
  {
    "input": "@triton.jit\ndef fwbw_splatter_init(directions, origins, grid_idx, near, far,\n    splatting_feature, mask, num_samples: 'tl.constexpr', num_samples_inf:\n    'tl.constexpr', num_rays: 'tl.constexpr', grid_channel: 'tl.constexpr',\n    feature_channel: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    tot_num_samples = num_samples + num_samples_inf\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = offs < num_rays\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = (pid * BLOCK_SIZE * feature_channel + feature_channel *\n        tl.arange(0, BLOCK_SIZE)[:, None] + tl.arange(0, feature_channel)[\n        None, :])\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    center_x = tl.load(origins + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(origins + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(origins + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(directions + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(directions + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(directions + offs_z, mask=offs_z < num_rays * 3)\n    near_buffer = tl.load(near + offs, mask=offs_mask)\n    far_buffer = tl.load(far + offs, mask=offs_mask)\n    grid_idx_buffer = tl.load(grid_idx + offs, mask=offs_mask)\n    sample_index_buffer = tl.arange(0, BLOCK_SIZE\n        ) * tot_num_samples + pid * BLOCK_SIZE * tot_num_samples + 1\n    feature = tl.load(splatting_feature + offs_features, mask=\n        offs_features_mask)\n    mask = tl.load(mask + pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:,\n        None], mask=offs_features_mask)\n    mask = tl.view(mask, (BLOCK_SIZE, 1))\n    return (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature, mask)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "5bbce5e8-d110-4098-9921-bab564c8dba2"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "4858dc38-c216-4b14-8716-35d6dcbb7841"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef cross_entropy_loss_backward_kernel(output_grad_pointer, target_pointer,\n    input_pointer, weight_pointer, sum_weights_pointer, input_grad_pointer,\n    batch_dim, feat_dim, input_batch_stride, input_feat_stride,\n    input_grad_batch_stride, input_grad_feat_stride, weighted:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT:\n    'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of cross entropy loss.\n\n    Args:\n        output_grad_pointer: Pointer to the loss's output gradients.\n            The output gradient must be a scalar.\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to the sum of the class weights if the classes were weighed.\n            The sum of weights must be a scalar.\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :], other=-float('inf'))\n    input -= tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    softmax = numerator / tl.sum(numerator, axis=1)[:, None]\n    output_grad = tl.load(output_grad_pointer)\n    target = tl.load(target_pointer + batch_offset, mask=batch_mask)\n    broadcasted_feat_offset = tl.broadcast_to(feat_offset[None, :], (\n        BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT))\n    broadcasted_target = tl.broadcast_to(target[:, None], (BLOCK_SIZE_BATCH,\n        BLOCK_SIZE_FEAT))\n    input_grad = output_grad * (softmax - (broadcasted_feat_offset ==\n        broadcasted_target))\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask)\n        sum_weights = tl.load(sum_weights_pointer)\n        input_grad *= weight[:, None] / sum_weights\n    else:\n        input_grad /= batch_dim\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5ac78dab-ec60-46cb-ac02-9b33957a302b"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1 + tanh(_kAlpha * (x + 0.044715 * x * x * x)))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "50469055-fa6b-4ff6-b930-701cc405d1a5"
  },
  {
    "input": "@triton.jit\ndef eighth_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_7 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_8 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    g_9 = tl.load(sph_grad_ptr + output_row_offset + 9, mask=\n        output_row_offset + 9 < output_numel)\n    g_10 = tl.load(sph_grad_ptr + output_row_offset + 10, mask=\n        output_row_offset + 10 < output_numel)\n    g_11 = tl.load(sph_grad_ptr + output_row_offset + 11, mask=\n        output_row_offset + 11 < output_numel)\n    g_12 = tl.load(sph_grad_ptr + output_row_offset + 12, mask=\n        output_row_offset + 12 < output_numel)\n    g_13 = tl.load(sph_grad_ptr + output_row_offset + 13, mask=\n        output_row_offset + 13 < output_numel)\n    g_14 = tl.load(sph_grad_ptr + output_row_offset + 14, mask=\n        output_row_offset + 14 < output_numel)\n    g_15 = tl.load(sph_grad_ptr + output_row_offset + 15, mask=\n        output_row_offset + 15 < output_numel)\n    g_16 = tl.load(sph_grad_ptr + output_row_offset + 16, mask=\n        output_row_offset + 16 < output_numel)\n    CONST000 = 2.0\n    CONST001 = 3.0\n    CONST002 = 4.50964677801932\n    CONST004 = 5.0\n    CONST005 = 6.78376969317208\n    CONST006 = 4.0\n    CONST007 = 9.01929355603863\n    CONST008 = 6.76447016702898\n    CONST009 = 6.0\n    CONST011 = 13.5675393863442\n    CONST012 = 15.0965641786467\n    CONST013 = 13.136713523081\n    CONST015 = 13.136713523081\n    CONST017 = 19.4042118494929\n    CONST019 = -489.184589393411\n    CONST020 = 24.738633753706\n    CONST023 = 26.2734270461621\n    CONST024 = 27.0578806681159\n    CONST025 = 24.738633753706\n    CONST026 = 32.9848450049413\n    CONST027 = 33.9188484658604\n    CONST028 = 550.332663067587\n    CONST030 = -978.369178786822\n    CONST031 = 48.5105296237322\n    CONST033 = 51.744564931981\n    CONST035 = 48.9184589393411\n    CONST041 = 65.6835676154051\n    CONST043 = -1467.55376818023\n    CONST045 = -12.2296147348353\n    CONST047 = 582.126355484786\n    CONST048 = -437.890450769368\n    CONST049 = -434.108258927137\n    CONST050 = -434.108258927137\n    CONST052 = -432.926090689854\n    CONST054 = -1447.02752975712\n    CONST055 = 91.9569946615672\n    CONST056 = -420.374832738593\n    CONST057 = 6.46807061649763\n    CONST058 = 97.0210592474644\n    CONST061 = 103.489129863962\n    CONST062 = -407.026181590325\n    CONST063 = 108.231522672464\n    CONST065 = 110.066532613517\n    CONST066 = 110.066532613517\n    CONST067 = 620.934779183772\n    CONST068 = -396.284809689477\n    CONST070 = 132.094936563159\n    CONST071 = 434.108258927137\n    CONST073 = 649.389136034781\n    CONST076 = -366.888442045058\n    CONST077 = -366.888442045058\n    CONST078 = -361.756882439281\n    CONST080 = -6.78376969317208\n    CONST082 = -350.312360615494\n    CONST083 = -346.340872551883\n    CONST084 = -346.340872551883\n    CONST085 = 173.170436275942\n    CONST086 = 173.170436275942\n    CONST088 = 183.444221022529\n    CONST089 = 183.444221022529\n    CONST090 = -325.62094527226\n    CONST091 = -13.5289403340579\n    CONST092 = -13.5675393863442\n    CONST093 = 194.042118494929\n    CONST095 = 197.050702846215\n    CONST096 = -11.3224231339851\n    CONST097 = 203.513090795162\n    CONST098 = -814.05236318065\n    CONST102 = -814.05236318065\n    CONST104 = 217.054129463568\n    CONST105 = 216.463045344927\n    CONST106 = 220.133065227035\n    CONST107 = -291.063177742393\n    CONST108 = 220.133065227035\n    CONST109 = -792.569619378954\n    CONST111 = -271.350787726883\n    CONST112 = 244.592294696705\n    CONST113 = 244.592294696706\n    CONST114 = 244.592294696706\n    CONST115 = -776.168473979715\n    CONST116 = -262.734270461621\n    CONST117 = -259.755654413913\n    CONST118 = -258.722824659905\n    CONST120 = 262.734270461621\n    CONST121 = -244.215708954195\n    CONST122 = 271.350787726883\n    CONST124 = -236.460843415458\n    CONST127 = -217.054129463568\n    CONST128 = -216.463045344927\n    CONST129 = -216.463045344927\n    CONST130 = -216.463045344927\n    CONST131 = -723.513764878561\n    CONST133 = -210.187416369296\n    CONST134 = -210.187416369296\n    CONST135 = 814.05236318065\n    CONST136 = -197.050702846215\n    CONST137 = 317.027847751582\n    CONST138 = -194.042118494929\n    CONST139 = -13.136713523081\n    CONST140 = 324.694568017391\n    CONST142 = 324.694568017391\n    CONST143 = -175.156180307747\n    CONST146 = -162.81047263613\n    CONST147 = -162.347284008695\n    CONST148 = 865.852181379709\n    CONST149 = -158.513923875791\n    CONST151 = -144.702752975712\n    CONST152 = -649.389136034782\n    CONST153 = -129.877827206956\n    CONST154 = -129.361412329953\n    CONST155 = 388.084236989858\n    CONST157 = -115.446957517294\n    CONST158 = -108.231522672464\n    CONST159 = -108.231522672464\n    CONST160 = 407.026181590325\n    CONST161 = -103.489129863962\n    CONST162 = -97.0210592474644\n    CONST163 = -94.7025823384056\n    CONST165 = -91.9569946615672\n    CONST167 = -87.5780901538735\n    CONST168 = -85.6073031438469\n    CONST169 = -85.6073031438469\n    CONST170 = -81.1736420043477\n    CONST171 = 432.926090689854\n    CONST172 = -79.2569619378954\n    CONST173 = -81.1736420043477\n    CONST177 = -79.2569619378954\n    CONST178 = -72.3513764878561\n    CONST179 = -72.1543484483091\n    CONST180 = -70.0624721230988\n    CONST181 = -72.1543484483091\n    CONST182 = -67.8376969317208\n    CONST183 = -65.6835676154052\n    CONST184 = -61.1480736741764\n    CONST185 = -1085.27064731784\n    CONST186 = -61.1480736741764\n    CONST187 = -1085.40315090753\n    CONST188 = -57.7234787586472\n    CONST189 = -12.9361412329953\n    CONST190 = -1085.27064731784\n    CONST191 = -52.8379746252636\n    CONST192 = -51.744564931981\n    CONST193 = -1585.13923875791\n    CONST194 = -48.5105296237322\n    CONST195 = -47.4863878522046\n    CONST197 = 978.369178786822\n    CONST198 = -517.44564931981\n    CONST199 = -40.7026181590325\n    CONST200 = -40.5868210021738\n    CONST201 = -39.4101405692431\n    CONST202 = -40.7026181590325\n    CONST203 = -36.0771742241545\n    CONST204 = -1056.75949250527\n    CONST205 = -29.1063177742393\n    CONST206 = 485.105296237322\n    CONST207 = -26.2734270461621\n    CONST208 = -26.4189873126318\n    CONST209 = -1050.93708184648\n    CONST210 = -22.6382471577417\n    CONST211 = -20.6718218536732\n    CONST212 = -19.4042118494929\n    CONST213 = -20.3513090795162\n    CONST214 = -528.379746252636\n    CONST215 = -15.0965641786467\n    CONST216 = -13.5675393863442\n    CONST217 = -525.468540923241\n    CONST218 = -11.3224231339851\n    CONST219 = -13.5289403340579\n    CONST220 = -9.70210592474644\n    CONST221 = -10.3359109268366\n    CONST222 = -6.46807061649763\n    CONST223 = -13.136713523081\n    CONST224 = -12.2296147348353\n    CONST225 = -3.23403530824881\n    CONST226 = -1034.89129863962\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR03 = VAR06 * VAR07\n    VAR04 = VAR07 * VAR07\n    VAR05 = VAR07 * VAR08\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR12 = VAR15 * VAR16\n    VAR13 = VAR16 * VAR16\n    VAR14 = VAR16 * VAR17\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR21 = VAR24 * VAR25\n    VAR22 = VAR25 * VAR25\n    VAR23 = VAR25 * VAR26\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += g_0 * (CONST049 * VAR08 * VAR23 - CONST131 * VAR06 * VAR25 + \n        CONST151 * VAR04 * z - CONST211 * VAR21) + g_1 * y * (CONST178 *\n        VAR04 - CONST178 * VAR22 + CONST185 * VAR08 * VAR24 - CONST190 *\n        VAR06 * VAR26) + g_10 * (CONST017 * VAR05 * VAR26 + CONST161 *\n        VAR13 * x - CONST189 * VAR03 - CONST198 * VAR07 * VAR15 + CONST222 *\n        VAR22 * x + VAR17 * (CONST058 * VAR24 * x + CONST107 * VAR05 + \n        CONST138 * VAR07 * VAR26)) + g_11 * (CONST056 * VAR14 * x * z + \n        VAR16 * (-CONST082 * VAR25 * x - CONST209 * VAR07 * z) + y * (\n        CONST116 * VAR07 * VAR25 + CONST124 * VAR05 * z + CONST207 * VAR23 * x)\n        ) + g_12 * (CONST011 * VAR03 + CONST182 * VAR07 * VAR24 + CONST199 *\n        VAR05 * VAR26 + CONST216 * VAR22 * x + VAR15 * (CONST098 * VAR26 *\n        x + CONST122 * VAR07) + VAR17 * (-CONST102 * VAR07 * VAR26 + \n        CONST121 * VAR05 + CONST160 * VAR24 * x)) + g_13 * (VAR16 * (-\n        CONST030 * VAR07 * z + CONST030 * VAR25 * x) + y * (CONST076 *\n        VAR05 * z + CONST106 * VAR23 * x + CONST112 * VAR07 * VAR25)\n        ) + g_14 * (CONST012 * VAR03 + CONST149 * VAR05 * VAR26 - CONST191 *\n        VAR22 * x + VAR17 * (CONST109 * VAR24 * x + CONST149 * VAR05 - \n        CONST193 * VAR07 * VAR26)) + g_15 * y * (CONST050 * VAR05 * z + \n        CONST050 * VAR23 * x - CONST054 * VAR07 * VAR25) + g_16 * (CONST050 *\n        VAR05 * VAR26 - CONST131 * VAR07 * VAR24 + CONST151 * VAR22 * x - \n        CONST211 * VAR03) + g_2 * (CONST001 * VAR08 * (-CONST208 * VAR23 + \n        CONST214 * VAR17 * VAR25) + CONST004 * VAR06 * (-CONST149 * VAR17 *\n        z - CONST208 * VAR25) - CONST149 * VAR17 * VAR23 + CONST172 * VAR04 *\n        z + CONST218 * VAR21) + g_3 * (VAR16 * (CONST043 * VAR08 * VAR26 + \n        CONST113 * VAR06 + CONST114 * VAR24) + y * (CONST028 * VAR06 *\n        VAR26 + CONST088 * VAR08 * VAR24 + CONST168 * VAR04 + CONST184 * VAR22)\n        ) + g_4 * (CONST001 * VAR08 * (CONST005 * VAR23 + CONST111 * VAR15 *\n        z) + CONST004 * VAR06 * (CONST080 * VAR25 - CONST146 * VAR17 * z) +\n        CONST005 * VAR21 - CONST111 * VAR15 * VAR25 + CONST146 * VAR17 *\n        VAR23 + CONST195 * VAR04 * z) + g_5 * (VAR14 * (CONST133 * VAR08 - \n        CONST134 * VAR26) + VAR16 * (-CONST048 * VAR06 + CONST116 * VAR24 +\n        CONST217 * VAR08 * VAR26) + y * (CONST041 * VAR06 * VAR26 + \n        CONST095 * VAR08 * VAR24 + CONST165 * VAR04 - CONST201 * VAR22)\n        ) + g_6 * (CONST001 * VAR08 * (CONST093 * VAR17 * VAR25 + CONST118 *\n        VAR15 * z + CONST220 * VAR23) + CONST004 * VAR06 * (-CONST162 *\n        VAR17 * z + CONST220 * VAR25) + CONST118 * VAR15 * VAR25 - CONST161 *\n        VAR13 * z - CONST162 * VAR17 * VAR23 + CONST210 * VAR04 * z + \n        CONST225 * VAR21) + g_7 * (CONST001 * VAR08 * (-CONST128 * VAR16 *\n        VAR26 + CONST153 * VAR14 + CONST200 * VAR24 * y) + CONST004 * VAR06 *\n        (CONST063 * VAR16 + CONST200 * VAR26 * y) + CONST020 * VAR12 + \n        CONST153 * VAR14 * VAR26 - CONST158 * VAR16 * VAR24 + CONST163 *\n        VAR04 * y + CONST219 * VAR22 * y) + g_8 * (CONST000 * x * (CONST002 *\n        VAR22 - CONST128 * VAR15 * VAR26 + CONST158 * VAR17 * VAR24 + \n        CONST188 * VAR13) + CONST006 * VAR07 * (CONST008 * VAR24 - CONST158 *\n        VAR15 + CONST159 * VAR17 * VAR26) + CONST007 * VAR03 + CONST009 *\n        VAR05 * (CONST002 * VAR26 + CONST203 * VAR17)) + g_9 * (CONST173 *\n        VAR23 * x * y + VAR25 * (CONST147 * VAR07 * y + CONST171 * VAR16 *\n        x) + z * (CONST117 * VAR14 * x + CONST170 * VAR05 * y + CONST171 *\n        VAR07 * VAR16))\n    g_y += CONST000 * g_14 * y * (-CONST068 * VAR06 * VAR26 + CONST068 *\n        VAR08 * VAR24 + CONST208 * VAR04 - CONST208 * VAR22) + g_1 * (\n        CONST078 * VAR07 * VAR24 + CONST104 * VAR05 * VAR26 - CONST178 *\n        VAR22 * x + CONST221 * VAR03) + g_10 * (CONST000 * y * (CONST031 *\n        VAR08 * VAR24 + CONST031 * VAR22 + CONST194 * VAR04 + CONST194 *\n        VAR06 * VAR26) + CONST006 * VAR16 * (-CONST154 * VAR06 + CONST154 *\n        VAR24) + CONST009 * VAR14 * (CONST033 * VAR26 + CONST192 * VAR08)\n        ) + g_11 * (CONST001 * VAR17 * (-CONST116 * VAR06 * z - CONST143 *\n        VAR08 * VAR25 + CONST167 * VAR23) + CONST004 * VAR15 * (CONST134 *\n        VAR08 * z - CONST180 * VAR25) + CONST013 * VAR21 + CONST183 * VAR06 *\n        VAR25 + CONST201 * VAR04 * z + CONST223 * VAR08 * VAR23) + g_12 * (\n        CONST000 * y * (CONST097 * VAR06 * VAR26 + CONST097 * VAR08 * VAR24 +\n        CONST199 * VAR04 + CONST199 * VAR22) + CONST006 * VAR16 * (CONST062 *\n        VAR08 * VAR26 - CONST182 * VAR06 - CONST182 * VAR24)) + g_13 * (\n        CONST001 * VAR17 * (CONST019 * VAR08 * VAR25 + CONST035 * VAR23 + \n        CONST113 * VAR06 * z) + CONST065 * VAR08 * VAR23 - CONST184 * VAR06 *\n        VAR25 + CONST186 * VAR04 * z + CONST224 * VAR21) + g_15 * (-\n        CONST078 * VAR06 * VAR25 + CONST127 * VAR08 * VAR23 + CONST178 *\n        VAR04 * z - CONST221 * VAR21) + g_2 * (CONST137 * VAR05 * y * z + \n        CONST137 * VAR23 * x * y + CONST204 * VAR07 * VAR25 * y) + g_3 * (\n        CONST001 * VAR17 * (CONST019 * VAR07 * VAR26 + CONST035 * VAR05 + \n        CONST114 * VAR24 * x) + CONST045 * VAR03 + CONST066 * VAR05 * VAR26 +\n        CONST184 * VAR22 * x - CONST186 * VAR07 * VAR24) + g_4 * (-CONST090 *\n        VAR05 * y * z + CONST187 * VAR07 * VAR16 * z + x * (CONST090 *\n        VAR23 * y - CONST187 * VAR16 * VAR25)) + g_5 * (CONST001 * VAR17 *\n        (CONST116 * VAR24 * x + CONST143 * VAR07 * VAR26 - CONST167 * VAR05\n        ) + CONST004 * VAR15 * (-CONST134 * VAR26 * x + CONST180 * VAR07) +\n        CONST015 * VAR05 * VAR26 + CONST041 * VAR07 * VAR24 + CONST139 *\n        VAR03 - CONST201 * VAR22 * x) + g_6 * (-CONST138 * VAR05 * y * z + \n        VAR07 * (CONST155 * VAR25 * y + CONST226 * VAR16 * z) + x * (\n        CONST067 * VAR14 * z - CONST138 * VAR23 * y + CONST226 * VAR16 * VAR25)\n        ) + g_7 * (CONST219 * VAR03 + VAR05 * (CONST142 * VAR17 + CONST200 *\n        VAR26) + VAR07 * (CONST152 * VAR15 - CONST152 * VAR17 * VAR26 + \n        CONST200 * VAR24) + x * (CONST085 * VAR13 + CONST140 * VAR17 *\n        VAR24 + CONST152 * VAR15 * VAR26 + CONST219 * VAR22)) + g_8 * (\n        CONST026 * VAR12 - CONST052 * VAR16 * VAR24 + CONST084 * VAR14 *\n        VAR26 + CONST179 * VAR04 * y + CONST181 * VAR22 * y + VAR06 * (-\n        CONST052 * VAR16 + CONST129 * VAR26 * y) + VAR08 * (CONST083 *\n        VAR14 + CONST128 * VAR24 * y + CONST148 * VAR16 * VAR26)) + g_9 * (\n        CONST219 * VAR21 + VAR23 * (CONST142 * VAR17 + CONST200 * VAR08) + \n        VAR25 * (CONST073 * VAR08 * VAR17 + CONST152 * VAR15 + CONST200 *\n        VAR06) + z * (CONST086 * VAR13 + CONST091 * VAR04 + CONST142 *\n        VAR06 * VAR17 + CONST152 * VAR08 * VAR15))\n    g_z += g_0 * (-CONST049 * VAR05 * VAR26 + CONST131 * VAR07 * VAR24 - \n        CONST151 * VAR22 * x + CONST211 * VAR03) + g_1 * y * (-CONST050 *\n        VAR23 * x + CONST054 * VAR07 * VAR25 + CONST071 * VAR05 * z) + g_10 * (\n        CONST057 * VAR04 * z + CONST061 * VAR13 * z + CONST189 * VAR21 + \n        CONST198 * VAR15 * VAR25 + CONST212 * VAR08 * VAR23 + VAR17 * (\n        CONST093 * VAR08 * VAR25 - CONST107 * VAR23 + CONST162 * VAR06 * z)\n        ) + g_11 * (VAR14 * (-CONST133 * VAR26 + CONST134 * VAR08) + VAR16 *\n        (CONST048 * VAR24 - CONST116 * VAR06 - CONST217 * VAR08 * VAR26) + \n        y * (CONST055 * VAR22 + CONST136 * VAR06 * VAR26 + CONST183 * VAR08 *\n        VAR24 + CONST201 * VAR04)) + g_12 * (CONST011 * VAR21 + CONST092 *\n        VAR04 * z + CONST182 * VAR06 * VAR25 + CONST202 * VAR08 * VAR23 + \n        VAR15 * (CONST098 * VAR08 * z + CONST122 * VAR25) + VAR17 * (-\n        CONST102 * VAR08 * VAR25 + CONST121 * VAR23 + CONST160 * VAR06 * z)\n        ) + g_13 * (VAR16 * (CONST043 * VAR08 * VAR26 + CONST113 * VAR06 + \n        CONST113 * VAR24) + y * (CONST028 * VAR08 * VAR24 + CONST089 *\n        VAR06 * VAR26 + CONST169 * VAR22 + CONST186 * VAR04)) + g_14 * (-\n        CONST149 * VAR08 * VAR23 + CONST191 * VAR04 * z + CONST215 * VAR21 +\n        VAR17 * (-CONST109 * VAR06 * z - CONST149 * VAR23 + CONST193 *\n        VAR08 * VAR25)) + g_15 * y * (CONST178 * VAR04 - CONST178 * VAR22 -\n        CONST185 * VAR06 * VAR26 + CONST190 * VAR08 * VAR24) + g_16 * (\n        CONST050 * VAR08 * VAR23 - CONST131 * VAR06 * VAR25 + CONST151 *\n        VAR04 * z - CONST211 * VAR21) + g_2 * (CONST096 * VAR03 + VAR05 * (\n        -CONST149 * VAR17 - CONST177 * VAR26) + VAR07 * (CONST070 * VAR24 +\n        CONST193 * VAR17 * VAR26) + x * (-CONST109 * VAR17 * VAR24 + \n        CONST177 * VAR22)) + g_3 * (VAR16 * (CONST030 * VAR07 * z + \n        CONST197 * VAR25 * x) + y * (CONST077 * VAR23 * x + CONST108 *\n        VAR05 * z + CONST114 * VAR07 * VAR25)) + g_4 * (CONST080 * VAR03 + \n        VAR05 * (-CONST146 * VAR17 + CONST213 * VAR26) + VAR07 * (CONST027 *\n        VAR24 + CONST111 * VAR15) + x * (CONST102 * VAR17 * VAR24 + \n        CONST135 * VAR15 * VAR26 - CONST195 * VAR22)) + g_5 * (-CONST056 *\n        VAR14 * x * z + VAR16 * (CONST082 * VAR07 * z + CONST209 * VAR25 *\n        x) + y * (CONST023 * VAR05 * z + CONST120 * VAR07 * VAR25 - \n        CONST124 * VAR23 * x)) + g_6 * (CONST225 * VAR03 + VAR05 * (-\n        CONST162 * VAR17 + CONST205 * VAR26) + VAR07 * (CONST047 * VAR17 *\n        VAR26 + CONST118 * VAR15 + CONST194 * VAR24) + x * (CONST115 *\n        VAR15 * VAR26 - CONST161 * VAR13 + CONST206 * VAR17 * VAR24 + \n        CONST210 * VAR22)) + g_7 * (CONST173 * VAR05 * y * z + VAR07 * (-\n        CONST052 * VAR16 * z + CONST147 * VAR25 * y) + x * (-CONST052 *\n        VAR16 * VAR25 + CONST117 * VAR14 * z + CONST173 * VAR23 * y)) + g_8 * (\n        CONST007 * VAR04 * z + CONST007 * VAR21 - CONST052 * VAR15 * VAR25 +\n        CONST130 * VAR17 * VAR23 + CONST157 * VAR13 * z + VAR06 * (CONST024 *\n        VAR25 + CONST129 * VAR17 * z) + VAR08 * (CONST024 * VAR23 - \n        CONST052 * VAR15 * z + CONST052 * VAR17 * VAR25)) + g_9 * (CONST001 *\n        VAR26 * (CONST105 * VAR08 * VAR16 + CONST153 * VAR14 + CONST200 *\n        VAR06 * y) + CONST004 * VAR24 * (CONST063 * VAR16 + CONST200 *\n        VAR08 * y) + CONST025 * VAR12 + CONST063 * VAR06 * VAR16 + CONST091 *\n        VAR04 * y + CONST153 * VAR08 * VAR14 + CONST163 * VAR22 * y)\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0e220202-0d13-4170-97e6-1814517a2edb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK'])\n@triton.jit\ndef fwd_recompute_w_kernel(k, beta, w, A, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_w = tl.dot(b_A, b_kb, allow_tf32=False)\n        p_w = tl.make_block_ptr(w + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_w, b_w, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "69e23bdb-26af-4f4b-9749-f5abbe7d96f0"
  },
  {
    "input": "@triton.jit\ndef relu_grad(input):\n    \"\"\"\n    Calculates the gradient of ReLU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of ReLU.\n    \"\"\"\n    return tl.where(input <= 0, 0, 1)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "e2e294f0-49cd-43ef-9165-adc636bc4f18"
  },
  {
    "input": "@triton.heuristics({'BACKWARD_PASS': lambda args: bool(args['BACKWARD_PASS'])})\n@triton.jit\ndef _rope_embedding(Q, Q_row_stride, cos, cos_row_stride, sin,\n    sin_row_stride, seqlen, head_dim: 'tl.constexpr', n_heads:\n    'tl.constexpr', BACKWARD_PASS: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    ROPE_GROUP_SIZE = 4\n    row_position = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n    sin1 = tl.load(sin + row_position % seqlen * sin_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    cos1 = tl.load(cos + row_position % seqlen * cos_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    if BACKWARD_PASS:\n        sin1 = -sin1\n    pass\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min(head_start + ROPE_GROUP_SIZE, n_heads)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = (row_position * Q_row_stride + k * head_dim + col_offsets +\n            half_head_dim)\n        Q1 = tl.load(Q + offs_q1, mask=mask, other=0)\n        Q2 = tl.load(Q + offs_q2, mask=mask, other=0)\n        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)\n    pass\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "2e551778-f324-4336-81b6-dab860d193b6"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_asm, stride_bk, stride_bn, stride_bsn, stride_cm,\n    stride_cn, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', SPLIT_K:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    c = accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "5fc09b7d-236e-4294-8c43-e7ce9458e19f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef fwd_recompute_w_u_kernel(k, v, beta, w, u, A, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_u = tl.make_block_ptr(u + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_u = tl.make_block_ptr(u + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_vb = b_v * b_beta[:, None]\n        b_u = tl.dot(b_A, b_vb, allow_tf32=False)\n        tl.store(p_u, b_u, boundary_check=(0, 1))\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_w = tl.make_block_ptr(w + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_w = tl.make_block_ptr(w + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_w = tl.dot(b_A, b_kb, allow_tf32=False)\n        tl.store(p_w, b_w, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "01188e90-2f9b-4c62-95c7-f44c471cb163"
  },
  {
    "input": "@triton.jit\ndef _softmax_kernel_fwd(output_ptr, output_row_stride, input_ptr,\n    input_row_stride, n_cols, block_size: 'tl.constexpr'):\n    row_index = tl.program_id(0)\n    input_row_ptr = input_ptr + row_index * input_row_stride\n    col_offsets = tl.arange(0, block_size)\n    input_ptrs = input_row_ptr + col_offsets\n    rw_mask = col_offsets < n_cols\n    row = tl.load(input_ptrs, mask=rw_mask, other=float('-inf'))\n    safe_row = row - tl.max(row, axis=0)\n    numerator = tl.exp(safe_row)\n    denom = tl.sum(numerator, axis=0)\n    sm_out = numerator / denom\n    out_row_ptr = output_ptr + row_index * output_row_stride\n    out_row_ptrs = out_row_ptr + col_offsets\n    tl.store(out_row_ptrs, sm_out, mask=rw_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "0de701fb-325a-46d4-8883-d1f5a789fb13"
  },
  {
    "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate,\n    stride_state_batch, stride_state_dim, stride_state_dstate,\n    stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim,\n    stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch,\n    stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim,\n    stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim,\n    DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS:\n    'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    state_ptr += pid_b * stride_state_batch\n    x_ptr += pid_b * stride_x_batch\n    dt_ptr += pid_b * stride_dt_batch\n    B_ptr += pid_b * stride_B_batch\n    C_ptr += pid_b * stride_C_batch\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch\n    out_ptr += pid_b * stride_out_batch\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n    if DT_SOFTPLUS:\n        dt = tl.log(1.0 + tl.exp(dt))\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n        dstate), other=0.0)\n    dA = tl.exp(A * dt[:, None])\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    dB = B[None, :] * dt[:, None]\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "7b7ff898-a2cf-4878-b506-c05b6fa87da5"
  },
  {
    "input": "@triton.jit\ndef _floor(x):\n    return x - x % 1\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "89e547e9-3c17-43d2-84e3-c32ce3083a81"
  },
  {
    "input": "@triton.jit\ndef ub2(X, Y):\n    r = tl.arange(0, 16)\n    r2 = tl.arange(0, 32)\n    x = tl.load(X + 16 * r2[:, None] + r)\n    y = triton_unbroadcast(x, tl.arange(0, 32)[:, None].shape)\n    tl.store(Y + r2[:, None], y)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "dcffc9c7-b6f9-4e1d-bb4f-762f06d01563"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, B, Mean, Rstd, stride, N, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "340e220c-852b-4525-b8d3-8a5687b07005"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_attention_kernel(Q, K, V, B, Do, L, D, softmax_scale:\n    'tl.constexpr', stride_qb, stride_qm, stride_qh, stride_qg, stride_kb,\n    stride_kn, stride_kh, stride_vb, stride_vn, stride_vh, stride_bb,\n    stride_bh, stride_bg, stride_bm, stride_dob, stride_dom, stride_doh,\n    stride_dog, stride_dqb, stride_dqm, stride_dqh, stride_dqg, stride_dkb,\n    stride_dkn, stride_dkh, stride_dvb, stride_dvn, stride_dvh, stride_lb,\n    stride_lh, stride_lg, seqlen_q, seqlen_k, headdim, num_kv_heads,\n    num_groups, Dq, Dk, Dv, HAVE_BIAS: 'tl.constexpr', BIAS_SINGLE_HEAD:\n    'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M: 'tl.constexpr',\n    EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_n, off_bh, off_gp = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    softmax_scale = softmax_scale\n    off_h = off_bh % num_kv_heads\n    off_b = off_bh // num_kv_heads\n    offs_qm = tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    l_ptrs = L + (off_b * stride_lb + off_h * stride_lh + offs_qm + off_gp *\n        stride_lg)\n    d_ptrs = D + (off_b * stride_lb + off_h * stride_lh + offs_qm + off_gp *\n        stride_lg)\n    q_ptrs = Q + (off_b * stride_qb + off_h * stride_qh + off_gp * stride_qg\n        ) + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (off_b * stride_kb + off_h * stride_kh) + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + (off_b * stride_vb + off_h * stride_vh) + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    do_ptrs = Do + (off_b * stride_dob + off_h * stride_doh + off_gp *\n        stride_dog) + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = Dq + (off_b * stride_dqb + off_h * stride_dqh + off_gp *\n        stride_dqg) + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = (0 if BIAS_SINGLE_HEAD else off_h *\n            stride_bh + off_gp * stride_bg)\n        b_ptrs = B + (off_b * stride_bb + bias_h_pos) + (offs_qm[:, None] *\n            stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(0, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k.T) * softmax_scale\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if HAVE_BIAS:\n            bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk + bias\n        lse_i = tl.load(l_ptrs + start_m, mask=offs_m_curr < seqlen_q,\n            other=0.0)\n        p = tl.exp(qk - lse_i[:, None])\n        do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p.T, do)\n        dp = tl.dot(do, v.T)\n        Di = tl.load(d_ptrs + start_m, mask=offs_m_curr < seqlen_q, other=0.0)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds.T, q)\n        dq = tl.dot(ds, k)\n        tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q) &\n            (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if HAVE_BIAS:\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = Dv + (off_b * stride_dvb + off_h * stride_dvh) + (offs_n[:,\n        None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = Dk + (off_b * stride_dkb + off_h * stride_dkh) + (offs_n[:,\n        None] * stride_dkn + offs_d[None, :])\n    tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n    tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "64f95efc-5d4a-4b7a-891f-ac791e64efcd"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_flash_decode_stage1(Q, K, V, sm_scale, Req_to_tokens,\n    B_req_idx, B_Seqlen, Mid_O, Mid_O_LogExpSum, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_mid_ob,\n    stride_mid_oh, stride_mid_os, stride_mid_od, stride_mid_o_eb,\n    stride_mid_o_eh, stride_mid_o_es, gqa_group_size, head_dim, BLOCK_SEQ:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    seq_start_block = tl.program_id(2)\n    cur_kv_head = cur_head // gqa_group_size\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = seq_start_block * BLOCK_SEQ\n    cur_batch_end_index = tl.minimum(cur_batch_seq_len, \n        cur_batch_start_index + BLOCK_SEQ)\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d\n    block_n_size = tl.where(cur_batch_end_index - cur_batch_start_index <= \n        0, 0, cur_batch_end_index - cur_batch_start_index + BLOCK_N - 1\n        ) // BLOCK_N\n    offs_n = cur_batch_start_index + tl.arange(0, BLOCK_N)\n    q = tl.load(Q + off_q, mask=offs_d < head_dim, other=0.0)\n    sum_exp = 0.0\n    max_logic = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, block_n_size, 1):\n        offs_n_new = start_n * BLOCK_N + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + offs_n_new, mask=offs_n_new <\n            cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :]\n        k = tl.load(K + off_k, mask=(offs_n_new[:, None] <\n            cur_batch_end_index) & (offs_d[None, :] < head_dim), other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        att_value = tl.where(offs_n_new < cur_batch_end_index, att_value,\n            float('-inf'))\n        v = tl.load(V + off_k, mask=(offs_n_new[:, None] <\n            cur_batch_end_index) & (offs_d[None, :] < head_dim), other=0.0)\n        cur_max_logic = tl.max(att_value, axis=0)\n        new_max_logic = tl.maximum(cur_max_logic, max_logic)\n        exp_logic = tl.exp(att_value - new_max_logic)\n        logic_scale = tl.exp(max_logic - new_max_logic)\n        acc *= logic_scale\n        acc += tl.sum(exp_logic[:, None] * v, axis=0)\n        sum_exp = sum_exp * logic_scale + tl.sum(exp_logic, axis=0)\n        max_logic = new_max_logic\n    need_store = tl.where(block_n_size == 0, 0, 1)\n    for _ in range(0, need_store, 1):\n        off_mid_o = (cur_batch * stride_mid_ob + cur_head * stride_mid_oh +\n            seq_start_block * stride_mid_os + offs_d)\n        off_mid_o_logexpsum = (cur_batch * stride_mid_o_eb + cur_head *\n            stride_mid_o_eh + seq_start_block)\n        tl.store(Mid_O + off_mid_o, acc / sum_exp, mask=offs_d < head_dim)\n        tl.store(Mid_O_LogExpSum + off_mid_o_logexpsum, max_logic + tl.log(\n            sum_exp))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "05daa905-0974-4da9-ae7d-ab4733a3b755"
  },
  {
    "input": "@triton.jit\ndef triton_add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "24cb9939-9e4a-4c0c-86b7-4f0dee1f3c6b"
  },
  {
    "input": "@triton.autotune(config_gen(), key=['N_BLK', 'BLK1_IN', 'BLK2_OUT'])\n@triton.jit\ndef monarch_backward(dout_ptr, out1_ptr, x_ptr, w1_bfly_ptr, w2_bfly_ptr,\n    dx_ptr, dw1_bfly_ptr, dw2_bfly_ptr, SEQ_DIM, N_BLK, BLK1_IN, BLK1_OUT,\n    BLK2_OUT, stride_dout_l, stride_dout_m, stride_dout_n, stride_out1_r,\n    stride_out1_m, stride_out1_l, stride_xl, stride_xm, stride_xk,\n    stride_w1l, stride_w1r, stride_w1k, stride_w2l, stride_w2n, stride_w2r,\n    BLOCK_SIZE_SEQ: 'tl.constexpr'=64, BLOCK_SIZE_N: 'tl.constexpr'=64,\n    BLOCK_SIZE_K: 'tl.constexpr'=32, GROUP_SIZE_M: 'tl.constexpr'=8):\n    BLK2_IN = BLK1_OUT\n    pid_batch = tl.program_id(0)\n    pid = tl.program_id(1)\n    num_pid_m = tl.cdiv(SEQ_DIM, BLOCK_SIZE_SEQ)\n    num_pid_n = tl.cdiv(N_BLK * BLK1_IN, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_m = pid_m * BLOCK_SIZE_SEQ\n    offs_n = pid_n * BLOCK_SIZE_N\n    offs_k = 0\n    x_ptrs = tl.make_block_ptr(x_ptr + pid_batch * stride_xl, shape=(\n        SEQ_DIM, BLK1_IN), strides=(stride_xm, stride_xk), offsets=(offs_m,\n        offs_k), block_shape=(BLOCK_SIZE_SEQ, BLOCK_SIZE_K), order=(0, 1))\n    dx_ptrs = tl.make_block_ptr(dx_ptr + pid_batch * stride_xl, shape=(\n        SEQ_DIM, BLK1_IN), strides=(stride_xm, stride_xk), offsets=(offs_m,\n        offs_k), block_shape=(BLOCK_SIZE_SEQ, BLOCK_SIZE_K), order=(1, 0))\n    out1_ptrs = tl.make_block_ptr(out1_ptr + pid_batch * stride_out1_l,\n        shape=(SEQ_DIM, BLK1_OUT), strides=(stride_out1_m, stride_out1_r),\n        offsets=(offs_m, 0), block_shape=(BLOCK_SIZE_SEQ, BLK1_OUT), order=\n        (1, 0))\n    dout_ptrs = tl.make_block_ptr(dout_ptr + pid_batch * stride_dout_l,\n        shape=(SEQ_DIM, BLK2_OUT), strides=(stride_dout_m, stride_dout_n),\n        offsets=(offs_m, offs_n), block_shape=(BLOCK_SIZE_SEQ, BLOCK_SIZE_N\n        ), order=(1, 0))\n    w1_ptrs = tl.make_block_ptr(w1_bfly_ptr + pid_batch * stride_w1l, shape\n        =(BLK1_OUT, BLK1_IN), strides=(stride_w1r, stride_w1k), offsets=(0,\n        offs_k), block_shape=(BLK1_OUT, BLOCK_SIZE_K), order=(1, 0))\n    dw1_ptrs = tl.make_block_ptr(dw1_bfly_ptr + pid_batch * stride_w1l,\n        shape=(BLK1_OUT, BLK1_IN), strides=(stride_w1r, stride_w1k),\n        offsets=(0, offs_k), block_shape=(BLK1_OUT, BLOCK_SIZE_K), order=(1, 0)\n        )\n    w2_ptrs = tl.make_block_ptr(w2_bfly_ptr + pid_batch * stride_w2l, shape\n        =(BLK2_OUT, BLK2_IN), strides=(stride_w2n, stride_w2r), offsets=(\n        offs_n, 0), block_shape=(BLOCK_SIZE_N, BLK2_IN), order=(1, 0))\n    dw2_ptrs = tl.make_block_ptr(dw2_bfly_ptr + pid_batch * stride_w2l,\n        shape=(BLK2_OUT, BLK2_IN), strides=(stride_w2n, stride_w2r),\n        offsets=(offs_n, 0), block_shape=(BLOCK_SIZE_N, BLK2_IN), order=(1, 0))\n    dout = tl.load(dout_ptrs, boundary_check=(0, 1), eviction_policy=\n        'evict_first')\n    out1 = tl.load(out1_ptrs, boundary_check=(1,), eviction_policy=\n        'evict_first')\n    w2_bfly = tl.load(w2_ptrs, boundary_check=(0,))\n    dw2_bfly = tl.dot(tl.trans(out1), dout)\n    tl.store(dw2_ptrs, dw2_bfly, boundary_check=(0,))\n    x = tl.load(x_ptrs, boundary_check=(0, 1))\n    dout1 = tl.dot(dout, w2_bfly)\n    dx = tl.zeros((BLOCK_SIZE_SEQ, BLOCK_SIZE_K), dtype=tl.float32)\n    for k in range(BLOCK_SIZE_K, BLK1_IN, BLOCK_SIZE_K):\n        w1_bfly = tl.load(w1_ptrs, boundary_check=(1,))\n        dx += tl.dot(dout1, w1_bfly)\n        tl.advance(w1_ptrs, (0, BLOCK_SIZE_K))\n    tl.store(dx_ptrs, dx, boundary_check=(0, 1))\n    dw1_bfly = tl.dot(tl.trans(dout1), x)\n    tl.store(dw1_ptrs, dw1_bfly, boundary_check=(1,))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "6645af97-e4ef-4166-976f-e7f79ff5ad18"
  },
  {
    "input": "@triton.jit\ndef _jsd_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr, loss_stride,\n    dX_ptr, dX_stride, label_ptr, beta: 'tl.constexpr', n_non_ignore: 'int',\n    ignore_index: 'tl.constexpr', n_cols, BLOCK_SIZE: 'tl.constexpr',\n    HAS_LABEL: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    X_ptr += pid * X_stride\n    dX_ptr += pid * dX_stride\n    Y_ptr += pid * Y_stride\n    loss_ptr += pid * loss_stride\n    label_ptr += pid\n    if HAS_LABEL:\n        label = tl.load(label_ptr)\n        if label == ignore_index:\n            for i in range(0, n_cols, BLOCK_SIZE):\n                offsets = i + tl.arange(0, BLOCK_SIZE)\n                tl.store(dX_ptr + offsets, 0.0, mask=offsets < n_cols)\n            return\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        X = tl.load(X_ptr + offsets, mask=mask, other=float('-inf'))\n        Y = tl.load(Y_ptr + offsets, mask=mask, other=float('-inf'))\n        if beta == 0.0:\n            Y_prob = tl.exp(Y)\n            loss = Y_prob * (Y - X)\n            dX = -Y_prob\n        elif beta == 1.0:\n            X_prob = tl.exp(X)\n            loss = X_prob * (X - Y)\n            dX = loss + X_prob\n        else:\n            Q = tl.exp(X)\n            P = tl.exp(Y)\n            M = beta * P + (1 - beta) * Q\n            log_M = tl.log(M)\n            loss = beta * P * Y + (1 - beta) * Q * X - M * log_M\n            dX = (1 - beta) * Q * (X - log_M)\n        loss = loss / n_non_ignore\n        dX = dX / n_non_ignore\n        tl.store(loss_ptr + offsets, loss, mask=mask)\n        tl.store(dX_ptr + offsets, dX, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "5c14f70d-b745-496e-832e-1c18109316f1"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_fused_single_pass(output_ptr, a_ptr, weight_ptr,\n    bias_ptr, mean_ptr, rstd_ptr, output_row_stride, output_col_stride,\n    a_row_stride, a_col_stride, N_SIZE, eps, HAS_BIAS: 'tl.constexpr',\n    IS_RMSNORM: 'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Layernorm based on Welford's variance computation algorithm.\n    https://changyaochen.github.io/welford/\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\n    :param output_ptr: output tensor\n    :param a_ptr: input tensor\n    :param weight_ptr: weights applied to the normalized input\n    :param bias_ptr: bias added to the normalized input\n    :param mean_ptr: save mean tensor for backward\n    :param rstd_ptr: save standard deviation tensor for backward\n    :param a_row_stride: stride of the input tensor\n    :param N_SIZE: number of elements per row in the input tensor\n    :param eps: epsilon value to avoid division by zero\n    :param HAS_BIAS: whether the bias is provided\n    :param IS_RMSNORM: whether the normalization is rmsnorm (False == layernorm)\n    :param BLOCK_N_SIZE: number of threads per block\n    :return: None\n    \"\"\"\n    row_idx = tl.program_id(0)\n    a_row_off = row_idx * a_row_stride\n    block_range_offs = tl.arange(0, BLOCK_N_SIZE)\n    mean = 0.0\n    var = 0.0\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        n_end_off = min(block_n_start_idx + BLOCK_N_SIZE, N_SIZE)\n        block_cols_count = n_end_off - block_n_start_idx\n        col_offs = block_n_start_idx + block_range_offs\n        a_ptr_mask = col_offs < N_SIZE\n        a = tl.load(a_ptr + a_row_off + col_offs * a_col_stride, mask=\n            a_ptr_mask, other=0.0, eviction_policy='evict_last')\n        if IS_RMSNORM:\n            var += tl.sum(a * a, axis=0)\n        else:\n            block_mean = tl.sum(a, axis=0) / block_cols_count\n            delta_mean = block_mean - mean\n            delta_mean_sqr = delta_mean * delta_mean\n            block_delta = tl.sum((a - block_mean) * a, axis=0)\n            mean += tl.sum((a - mean) * a_ptr_mask, axis=0) / n_end_off\n            var += block_delta + delta_mean_sqr * (block_n_start_idx *\n                block_cols_count) / n_end_off\n    var /= N_SIZE\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(mean_ptr + row_idx, mean)\n    tl.store(rstd_ptr + row_idx, rstd)\n    for block_n_start_idx in range(0, N_SIZE, BLOCK_N_SIZE):\n        col_offs = block_n_start_idx + block_range_offs\n        a_ptr_mask = col_offs < N_SIZE\n        weight = tl.load(weight_ptr + col_offs, mask=a_ptr_mask)\n        a = tl.load(a_ptr + a_row_off + col_offs * a_col_stride, mask=\n            a_ptr_mask, other=0.0, eviction_policy='evict_first')\n        a_hat = (a - mean) * rstd\n        out = a_hat * weight\n        if HAS_BIAS:\n            bias = tl.load(bias_ptr + col_offs, mask=a_ptr_mask)\n            out = out + bias\n        tl.store(output_ptr + row_idx * output_row_stride + col_offs *\n            output_col_stride, out, mask=a_ptr_mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "d3cab186-d365-49ce-bc20-e0d75a2a14f5"
  },
  {
    "input": "@triton.jit\ndef maximum_kernel(x_ptr, y_ptr, output_ptr, N, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = tl.maximum(x, y)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "a1f96350-7d58-488f-b556-da1cea180cc5"
  },
  {
    "input": "@triton.jit\ndef triton_unbroadcast(array, other):\n    l: 'tl.constexpr' = tl.constexpr(shape_l(array.shape))\n    ol: 'tl.constexpr' = tl.constexpr(shape_l(other.value))\n    for i in tl.static_range(0, l):\n        if i >= ol:\n            array = tl.sum(array, l - (1 + i))\n            array = tl.expand_dims(array, l - (1 + i))\n        elif array.shape[l - (1 + i)] > other.value[ol - (1 + i)]:\n            array = tl.sum(array, l - (1 + i))\n            array = tl.expand_dims(array, l - (1 + i))\n        tl.static_assert(tl.constexpr(shape_l(array.shape)) == l)\n    return tl.view(array, other.value)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "48a72e95-8c5d-494f-b7e8-94703ddd6412"
  },
  {
    "input": "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] ==\n    args['V_BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_hyper_kernel(Q, K, V, q_sort_idx, k_sort_idx, Out, Lse,\n    softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh,\n    stride_kn, stride_vb, stride_vh, stride_vn, stride_q_sort_idxb,\n    stride_q_sort_idxh, stride_q_sort_idxm, stride_k_sort_idxb,\n    stride_k_sort_idxh, stride_k_sort_idxn, stride_ob, stride_oh, stride_om,\n    nheads, block_size, sample_size, seqlen_k, seqlen_q, headdim, v_headdim,\n    smooth_block, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BLOCK_HEADDIM:\n    'tl.constexpr', V_BLOCK_HEADDIM: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr', EVEN_V_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)\n    q_idx_ptrs = (q_sort_idx + off_b * stride_q_sort_idxb + off_h *\n        stride_q_sort_idxh + offs_m * stride_q_sort_idxm)\n    q_idx = tl.load(q_idx_ptrs)\n    k_sort_idx += off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, V_BLOCK_HEADDIM], dtype=tl.float32)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (q_idx[:, None] *\n        stride_qm + offs_d[None, :])\n    if EVEN_HEADDIM:\n        q = tl.load(q_ptrs)\n    else:\n        q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    block_id = start_m // block_size\n    block_offs = seqlen_k + start_m % block_size * BLOCK_N - (block_size - 1\n        ) * BLOCK_N // 2\n    end_n = tl.minimum((block_id + 1) * BLOCK_N * block_size, seqlen_k)\n    for start_n in range(block_id * BLOCK_N * block_size, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if smooth_block:\n            k_idx_ptrs = (start_n + block_offs + offs_n\n                ) * stride_k_sort_idxn % seqlen_k\n        else:\n            k_idx_ptrs = (start_n + offs_n) * stride_k_sort_idxn\n        k_idx = tl.load(k_sort_idx + k_idx_ptrs)\n        k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (k_idx[:, None\n            ] * stride_kn + offs_d[None, :])\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n        p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (k_idx[:, None\n            ] * stride_vn + offs_vd[None, :])\n        if EVEN_V_HEADDIM:\n            v = tl.load(v_ptrs)\n        else:\n            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    for col_block in range(0, sample_size):\n        curr_offs_n = col_block * BLOCK_N * stride_kn + offs_n\n        k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (curr_offs_n[:,\n            None] * stride_kn + offs_d[None, :])\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n        p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (curr_offs_n[:,\n            None] * stride_vn + offs_vd[None, :])\n        if EVEN_V_HEADDIM:\n            v = tl.load(v_ptrs)\n        else:\n            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    lse_ptrs = Lse + off_hb * seqlen_q + q_idx\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (q_idx[:, None\n        ] * stride_om + offs_vd[None, :])\n    tl.store(lse_ptrs, lse_i)\n    if EVEN_V_HEADDIM:\n        tl.store(out_ptrs, acc_o)\n    else:\n        tl.store(out_ptrs, acc_o, mask=offs_vd[None, :] < v_headdim)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "c803e1f5-d6d0-407d-877b-2a0a20fb3cba"
  },
  {
    "input": "@triton.jit\ndef _bwd_do_attn_kernel(O, Do, De, stride_ob, stride_om, stride_oh,\n    stride_dob, stride_dom, stride_doh, stride_deb, stride_deh, nheads,\n    headdim, seqlen_q, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr'):\n    off_q = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = off_q * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o_ptrs = O + off_b * stride_ob + off_h * stride_oh + offs_m[:, None\n        ] * stride_om + offs_d[None, :]\n    do_ptrs = Do + off_b * stride_dob + off_h * stride_doh + offs_m[:, None\n        ] * stride_dom + offs_d[None, :]\n    o = tl.load(o_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    do = tl.load(do_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None,\n        :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(De + (off_b * stride_deb + off_h * stride_deh + offs_m), delta,\n        mask=offs_m < seqlen_q)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "73e485b2-99fd-4c4f-97f3-444bb163d604"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "9625b484-5125-4a46-9a67-b15d839a312b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "1965d70a-dd02-4c7a-b951-f3645d73c9d0"
  },
  {
    "input": "@triton.jit\ndef srms_norm_bwd_dx_fused(DX, DY, X, V, stride, N, BLOCK_SIZE_N:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    x_ptrs = X + row * stride + cols\n    dy_ptrs = DY + row * stride + cols\n    x = tl.load(x_ptrs, mask=mask, other=0)\n    dy = tl.load(dy_ptrs, mask=mask, other=0)\n    rstd = tl.load(V + row)\n    xhat = x * rstd\n    wdy = dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    mean1 = tl.sum(xhat * wdy, axis=0) / N\n    dx = (wdy - xhat * mean1) * rstd\n    mask = cols < N\n    dx_ptrs = DX + row * stride + cols\n    tl.store(dx_ptrs, dx, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "6903f9f7-b431-4ed0-9d65-0781249095a5"
  },
  {
    "input": "@triton.jit\ndef triton_local_reverse(x, y, K: 'tl.constexpr', flip: 'tl.constexpr', BC:\n    'tl.constexpr', BH: 'tl.constexpr', BW: 'tl.constexpr', DC:\n    'tl.constexpr', DH: 'tl.constexpr', DW: 'tl.constexpr', NH:\n    'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    _i = (tl.arange(0, BH) + BH * i_h)[:, None]\n    _j = (tl.arange(0, BW) + BW * i_w)[None, :]\n    _o = _i * DW + _j\n    _i = _o // (K * K) // (DW // K) * K + _o % (K * K) // K\n    _j = _o // (K * K) % (DW // K) * K + _o % (K * K) % K\n    _c_offset = _i * DW + _j\n    if flip:\n        _c_offset = DH * DW - _c_offset - 1\n    p_y = y + i_b * _tmp1 + _tmp0 + _c_offset\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y + _idx, _x, mask=_mask_hw)\n    tl.debug_barrier()\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "5ba5fa84-81d6-49b3-9ec4-e3d27e45ecee"
  },
  {
    "input": "@triton.jit\ndef _general_matmul(pid_n, start_off, end_off, input, other, output, K, N,\n    stride_input_m, stride_input_k, stride_other_k, stride_other_n,\n    stride_output_m, stride_output_n, out_dtype: 'tl.constexpr', MASK_M:\n    'tl.constexpr', TILE_M: 'tl.constexpr', TILE_N: 'tl.constexpr', TILE_K:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_K: 'tl.constexpr'):\n    offs_m = start_off + tl.arange(0, TILE_M)\n    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_k = tl.arange(0, TILE_K)\n    rn = tl.max_contiguous(tl.multiple_of(offs_n % N, TILE_N), TILE_N)\n    input_ptrs = input + (offs_m[:, None] * stride_input_m + offs_k[None, :\n        ] * stride_input_k)\n    other_ptrs = other + (offs_k[:, None] * stride_other_k + rn[None, :] *\n        stride_other_n)\n    acc = tl.zeros((TILE_M, TILE_N), dtype=out_dtype)\n    mask_m = offs_m[:, None] < end_off if MASK_M else True\n    k_iter = K // TILE_K if EVEN_K else tl.cdiv(K, TILE_K)\n    for k in range(0, k_iter):\n        if EVEN_K:\n            if MASK_M:\n                a = tl.load(input_ptrs, mask=mask_m, other=0.0)\n                b = tl.load(other_ptrs)\n            else:\n                a = tl.load(input_ptrs)\n                b = tl.load(other_ptrs)\n        elif MASK_M:\n            a = tl.load(input_ptrs, mask=mask_m & (offs_k[None, :] + k *\n                TILE_K < K), other=0.0)\n            b = tl.load(other_ptrs, mask=offs_k[:, None] + k * TILE_K < K,\n                other=0.0)\n        else:\n            a = tl.load(input_ptrs, mask=offs_k[None, :] + k * TILE_K < K,\n                other=0.0)\n            b = tl.load(other_ptrs, mask=offs_k[:, None] + k * TILE_K < K,\n                other=0.0)\n        acc += tl.dot(a, b, out_dtype=out_dtype)\n        input_ptrs += TILE_K * stride_input_k\n        other_ptrs += TILE_K * stride_other_k\n    acc = acc\n    c_ptrs = output + stride_output_m * offs_m[:, None\n        ] + stride_output_n * offs_n[None, :]\n    if EVEN_N:\n        if MASK_M:\n            tl.store(c_ptrs, acc, mask=mask_m)\n        else:\n            tl.store(c_ptrs, acc)\n    else:\n        mask_n = offs_n[None, :] < N\n        if MASK_M:\n            tl.store(c_ptrs, acc, mask=mask_m & mask_n)\n        else:\n            tl.store(c_ptrs, acc, mask_n)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "ce83029d-b4b4-4ee1-ae68-f5ecc422cae5"
  },
  {
    "input": "@triton.jit\ndef tanh_kernel(x_ptr, length, output_ptr, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < length\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = libdevice.tanh(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "806589e7-c950-4b22-8666-d3ea513a3803"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_linear_attn_fwd_kernel(q, k, v, o, h0, ht, s_k_h, s_v_h,\n    scale, B, H, T, K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += K\n        p_k += K\n        p_o += V\n        p_v += V\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "261a3565-9f08-4c08-a571-38b7ae790d13"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BM': 128, 'BK': 64, 'BN': 256,\n    'G': 4}, num_stages=3, num_warps=8), triton.Config({'BM': 64, 'BK': 32,\n    'BN': 256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': \n    128, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.\n    Config({'BM': 128, 'BK': 32, 'BN': 64, 'G': 4}, num_stages=4, num_warps\n    =4), triton.Config({'BM': 64, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=\n    4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 32, 'G': 4},\n    num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 32, 'BN': 32,\n    'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 32, 'BK': 32,\n    'BN': 64, 'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 128,\n    'BK': 128, 'BN': 256, 'G': 4}, num_stages=3, num_warps=8), triton.\n    Config({'BM': 256, 'BK': 128, 'BN': 128, 'G': 4}, num_stages=3,\n    num_warps=8), triton.Config({'BM': 256, 'BK': 128, 'BN': 64, 'G': 4},\n    num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 128, 'BN': \n    256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128,\n    'BK': 128, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.\n    Config({'BM': 128, 'BK': 64, 'BN': 64, 'G': 4}, num_stages=4, num_warps\n    =4), triton.Config({'BM': 64, 'BK': 64, 'BN': 128, 'G': 4}, num_stages=\n    4, num_warps=4), triton.Config({'BM': 128, 'BK': 64, 'BN': 32, 'G': 4},\n    num_stages=4, num_warps=4)], key=['M', 'N', 'K'])\n@triton.heuristics({'HAS_INPUT': lambda args: args['input'] is not None,\n    'HAS_ALPHA': lambda args: args['alpha'] is not None, 'HAS_BETA': lambda\n    args: args['beta'] is not None})\n@triton.jit\ndef matmul_kernel(a, b, c, input, alpha, beta, M, N, K, s_am, s_ak, s_bk,\n    s_bn, s_cm, s_cn, BM: 'tl.constexpr', BK: 'tl.constexpr', BN:\n    'tl.constexpr', G: 'tl.constexpr', ACTIVATION: 'tl.constexpr',\n    HAS_INPUT: 'tl.constexpr', HAS_ALPHA: 'tl.constexpr', HAS_BETA:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    NM, NN = tl.num_programs(0), tl.num_programs(1)\n    i_m, i_n = tl.program_id(0), tl.program_id(1)\n    i_m, i_n = tl.swizzle2d(i_m, i_n, NM, NN, G)\n    o_am = (i_m * BM + tl.arange(0, BM)) % M\n    o_bn = (i_n * BN + tl.arange(0, BN)) % N\n    o_k = tl.arange(0, BK)\n    p_a = a + (o_am[:, None] * s_am + o_k[None, :] * s_ak)\n    p_b = b + (o_k[:, None] * s_bk + o_bn[None, :] * s_bn)\n    b_acc = tl.zeros((BM, BN), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BK)):\n        b_a = tl.load(p_a, mask=o_k[None, :] < K - k * BK, other=0.0)\n        b_b = tl.load(p_b, mask=o_k[:, None] < K - k * BK, other=0.0)\n        b_acc += tl.dot(b_a, b_b, allow_tf32=False)\n        p_a += BK * s_ak\n        p_b += BK * s_bk\n    o_cm = i_m * BM + tl.arange(0, BM)\n    o_cn = i_n * BN + tl.arange(0, BN)\n    mask = (o_cm[:, None] < M) & (o_cn[None, :] < N)\n    b_c = b_acc\n    if ACTIVATION == 'leaky_relu':\n        b_c = leaky_relu(b_c)\n    if HAS_ALPHA:\n        b_c *= tl.load(alpha)\n    if HAS_INPUT:\n        p_i = input + s_cm * o_cm[:, None] + s_cn * o_cn[None, :]\n        b_i = tl.load(p_i, mask=mask, other=0.0)\n        if HAS_BETA:\n            b_i *= tl.load(beta)\n        b_c += b_i\n    p_c = c + s_cm * o_cm[:, None] + s_cn * o_cn[None, :]\n    tl.store(p_c, b_c, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "5bcfbc1a-87c2-4f53-9935-9ddeec51f9c3"
  },
  {
    "input": "@triton.jit\ndef squared_relu(x):\n    \"\"\"\n    Squared ReLU activation, as proposed in the Primer_ paper.\n\n    .. _Primer: https://arxiv.org/abs/2109.08668\n    \"\"\"\n    x_ = relu(x)\n    return x_ * x_\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "af4080fa-f0ad-4096-aa4c-5d29082b3b3e"
  },
  {
    "input": "@triton.jit\ndef joint_second_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr'):\n    \"\"\"\n    This Triton implementation includes l=0, 1, 2 within the\n    same kernel, as it would be a common operation.\n    \"\"\"\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST_00 = 3.87298334620742\n    CONST_01 = 2.23606797749979\n    CONST_02 = -1.11803398874989\n    CONST_03 = 1.93649167310371\n    CONST_04 = tl.sqrt(3.0)\n    Y10 = CONST_04 * x\n    Y11 = CONST_04 * y\n    Y12 = CONST_04 * z\n    Y20 = CONST_00 * x * z\n    Y21 = CONST_00 * x * y\n    Y23 = CONST_00 * y * z\n    Y22 = CONST_02 * x * x + CONST_01 * y * y + CONST_02 * z * z\n    Y24 = -CONST_03 * x * x + CONST_03 * z * z\n    output_stride = 9\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = output_striding + block_size * output_stride * block_id\n    tl.store(output_ptr + output_row_offset, 1.0, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y10, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y11, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y12, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y20, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y21, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y22, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y23, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y24, mask=\n        output_row_offset + 7 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "4fc79bf9-56fd-4c4f-bae4-a902a4c8f312"
  },
  {
    "input": "@triton.jit\ndef parallel_rebased_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "2d5ffcf9-f330-46c1-ba11-d1cc6b288fcf"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef glu_forward_kernel(input1_pointer, input2_pointer, output_pointer, size,\n    param, act_func: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Applies the gated linear unit with an arbitrary activation function\n    to the input.\n\n    Args:\n        input1_pointer: Pointer to the first half of the input to gate.\n            The first half must be contiguous and contain size elements.\n        input2_pointer: Pointer to the second half of the input to gate.\n            The second half must be contiguous and contain size elements.\n        output_pointer: Pointer to a container the result is written to.\n            The container must be contiguous and contain size elements.\n        size: Number of elements in each half of the input.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input1 = tl.load(input1_pointer + offset, mask=mask)\n    input2 = tl.load(input2_pointer + offset, mask=mask)\n    output = input1 * apply_act_func(input2, None, None, None, param,\n        act_func, False)\n    tl.store(output_pointer + offset, output, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "62d29194-6c4c-4883-8561-432f6699402c"
  },
  {
    "input": "@triton.jit\ndef awq_dequantize_kernel(qweight_ptr, scales_ptr, zeros_ptr, group_size,\n    result_ptr, num_cols, num_rows, BLOCK_SIZE_X: 'tl.constexpr',\n    BLOCK_SIZE_Y: 'tl.constexpr'):\n    pid_x = tl.program_id(axis=0)\n    pid_y = tl.program_id(axis=1)\n    offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n    offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    offsets = num_cols * offsets_y[:, None] + offsets_x[None, :]\n    masks_y = offsets_y < num_rows\n    masks_x = offsets_x < num_cols\n    masks = masks_y[:, None] & masks_x[None, :]\n    result_offsets_y = pid_y * BLOCK_SIZE_Y + tl.arange(0, BLOCK_SIZE_Y)\n    result_offsets_x = pid_x * BLOCK_SIZE_X * 8 + tl.arange(0, BLOCK_SIZE_X * 8\n        )\n    result_offsets = 8 * num_cols * result_offsets_y[:, None\n        ] + result_offsets_x[None, :]\n    result_masks_y = result_offsets_y < num_rows\n    result_masks_x = result_offsets_x < num_cols * 8\n    result_masks = result_masks_y[:, None] & result_masks_x[None, :]\n    iweights = tl.load(qweight_ptr + offsets, masks)\n    iweights = tl.interleave(iweights, iweights)\n    iweights = tl.interleave(iweights, iweights)\n    iweights = tl.interleave(iweights, iweights)\n    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] + tl.arange(\n        0, 4)[:, None]).reshape(8)\n    shifts = reverse_awq_order_tensor * 4\n    shifts = tl.broadcast_to(shifts[None, :], (BLOCK_SIZE_Y * BLOCK_SIZE_X, 8))\n    shifts = tl.reshape(shifts, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    iweights = iweights >> shifts & 15\n    zero_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)\n    zero_offsets_x = pid_x * BLOCK_SIZE_X + tl.arange(0, BLOCK_SIZE_X)\n    zero_offsets = num_cols * zero_offsets_y[:, None] + zero_offsets_x[None, :]\n    zero_masks_y = zero_offsets_y < num_rows // group_size\n    zero_masks_x = zero_offsets_x < num_cols\n    zero_masks = zero_masks_y[:, None] & zero_masks_x[None, :]\n    zeros = tl.load(zeros_ptr + zero_offsets, zero_masks)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.interleave(zeros, zeros)\n    zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    zeros = zeros >> shifts & 15\n    scale_offsets_y = pid_y * BLOCK_SIZE_Y // group_size + tl.arange(0, 1)\n    scale_offsets_x = pid_x * BLOCK_SIZE_X * 8 + tl.arange(0, BLOCK_SIZE_X * 8)\n    scale_offsets = num_cols * 8 * scale_offsets_y[:, None] + scale_offsets_x[\n        None, :]\n    scale_masks_y = scale_offsets_y < num_rows // group_size\n    scale_masks_x = scale_offsets_x < num_cols * 8\n    scale_masks = scale_masks_y[:, None] & scale_masks_x[None, :]\n    scales = tl.load(scales_ptr + scale_offsets, scale_masks)\n    scales = tl.broadcast_to(scales, (BLOCK_SIZE_Y, BLOCK_SIZE_X * 8))\n    iweights = (iweights - zeros) * scales\n    iweights = iweights\n    tl.store(result_ptr + result_offsets, iweights, result_masks)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "5d08a7e3-28a9-4f88-a9fa-efd7d084720a"
  },
  {
    "input": "@triton.jit\ndef add_v4_bf16(a, b):\n    return tl.inline_asm_elementwise(\n        \"\"\"\n        {\n            .reg .v4 .b32 %acc, %tmp;\n            mov.v4.b32  %acc, 0;\n            mov.b64     {%acc.x, %acc.y}, $1;\n            mov.b64     {%tmp.x, %tmp.y}, $2;\n            add.bf16x2  %acc.x, %acc.x, %tmp.x;\n            add.bf16x2  %acc.y, %acc.y, %tmp.y;\n            mov.b64     $0, {%acc.x, %acc.y};\n        }\n        \"\"\"\n        , '=l,l,l', args=[a, b], dtype=tl.uint64, is_pure=True, pack=1)\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "8cfb35e5-1c91-4710-b3e9-c9c5c92180cf"
  },
  {
    "input": "@triton.jit\ndef chunk_gla_bwd_kernel(q, g, do, dh, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, s_hh, s_ht, B, H, T, TDK, scale, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'\n    ):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    mask = (i_k * BK + tl.arange(0, BK)[:, None] < DK) & (i_v * BV + tl.\n        arange(0, BV)[None, :] < DV)\n    p_dh = dh + i_bh * s_hh + (TDK - DK + i_k * BK + tl.arange(0, BK)[:, None]\n        ) * DV + i_v * BV + tl.arange(0, BV)[None, :]\n    for i in range((tl.cdiv(T, BT) - 1) * BT, -BT, -BT):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_db = g + i_bh * s_qk_h + (i + BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        d_b = tl.math.exp2(tl.load(p_db))\n        tl.store(p_dh, b_dh, mask=mask)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b[:, None] * b_dh + tl.dot(b_q, b_do, allow_tf32=False)\n        p_dh -= DK * DV\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f2c170ca-647f-4cd8-83db-281edf7d287d"
  },
  {
    "input": "@triton.jit\ndef dropout_offsets(philox_seed, philox_offset, m, n, stride):\n    ms = tl.arange(0, m)\n    ns = tl.arange(0, n)\n    return philox_offset + ms[:, None] * stride + ns[None, :]\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "aa87acd8-e318-4e55-96e7-360597bed231"
  },
  {
    "input": "@triton.jit\ndef hardswish_grad(input):\n    \"\"\"\n    Calculates the gradient of hard Swish.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of hard Swish.\n    \"\"\"\n    return (relu6(input + 3) + input * relu6_grad(input + 3)) / 6\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "6225349d-7373-466d-aacd-2fc421c19d46"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4, pre_hook=init_to_zero([\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128},\n    num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size',\n    'dstate', 'hdim'])\n@triton.jit\ndef _chunk_state_bwd_db_kernel(x_ptr, dstates_ptr, b_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, db_ptr, ddA_cumsum_ptr, chunk_size, dstate,\n    hdim, batch, seqlen, nheads, nheads_per_program, ngroups,\n    stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim,\n    stride_dstates_batch, stride_dstates_chunk, stride_states_head,\n    stride_states_hdim, stride_states_dstate, stride_b_batch,\n    stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dt_batch,\n    stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch,\n    stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize,\n    stride_seq_idx_batch, stride_seq_idx_seqlen, stride_db_batch,\n    stride_db_seqlen, stride_db_split, stride_db_group, stride_db_dstate,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize, HAS_DDA_CS: 'tl.constexpr', HAS_SEQ_IDX:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (\n        pid_g * (nheads // ngroups) + pid_s * nheads_per_program\n        ) * stride_x_head\n    db_ptr += (pid_b * stride_db_batch + pid_c * chunk_size *\n        stride_db_seqlen + pid_g * stride_db_group + pid_s * stride_db_split)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_states_head)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g *\n        (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dA_cs_head)\n    if HAS_DDA_CS:\n        b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size *\n            stride_b_seqlen + pid_g * stride_b_head)\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n            nheads_per_program) * stride_ddA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_k[None, :] *\n        stride_x_hdim)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_dstate + \n        offs_k[:, None] * stride_states_hdim)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    if HAS_DDA_CS:\n        b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_n[None,\n            :] * stride_b_dstate)\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < dstate), other=0.0)\n    if HAS_SEQ_IDX:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n    nheads_iter = min(nheads_per_program, nheads // ngroups - pid_s *\n        nheads_per_program)\n    for h in range(nheads_iter):\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < hdim), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < hdim) & (\n            offs_n[None, :] < dstate), other=0.0)\n        dstates = dstates\n        db = tl.dot(x, dstates)\n        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) *\n            stride_dA_cs_csize)\n        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0)\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0)\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_last - dA_cs_m)\n        else:\n            scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last -\n                dA_cs_m), 0.0)\n        db *= (scale * dt_m)[:, None]\n        if HAS_DDA_CS:\n            ddA_cs = tl.sum(db * b, axis=1)\n            tl.atomic_add(ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs,\n                mask=offs_m < chunk_size - 1)\n        acc += db\n        x_ptrs += stride_x_head\n        dstates_ptrs += stride_states_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_ptr += stride_dA_cs_head\n        dA_cumsum_ptrs += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    db_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :\n        ] * stride_db_dstate)\n    tl.store(db_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < dstate))\n",
    "category": "Kernel Operations",
    "subcategory": "kernel configuration",
    "uuid": "fec349d5-b087-4b50-9cf5-73d24845ddf3"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['hdim', 'dstate', 'chunk_size'])\n@triton.jit\ndef _chunk_scan_bwd_dstates_kernel(dout_ptr, c_ptr, dprev_states_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, hdim, dstate, chunk_size, batch, seqlen,\n    nchunks, nheads_ngroups_ratio, stride_dout_batch, stride_dout_seqlen,\n    stride_dout_head, stride_dout_hdim, stride_c_batch, stride_c_seqlen,\n    stride_c_head, stride_c_dstate, stride_dprev_states_batch,\n    stride_dprev_states_chunk, stride_dprev_states_head,\n    stride_dprev_states_hdim, stride_dprev_states_dstate,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    c_ptr += (pid_b * stride_c_batch + pid_c * chunk_size * stride_c_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_c_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_hdim + offs_k[\n        None, :] * stride_dout_seqlen)\n    c_ptrs = c_ptr + (offs_n[None, :] * stride_c_dstate + offs_k[:, None] *\n        stride_c_seqlen)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[\n            None, :] < chunk_size_limit - k), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k,\n            other=0.0)\n        if not HAS_SEQ_IDX:\n            scale_k = tl.exp(dA_cs_k)\n        else:\n            seq_idx_k = tl.load(seq_idx_ptrs, mask=offs_k < \n                chunk_size_limit - k, other=-1)\n            scale_k = tl.where(seq_idx_k == seq_idx_prev, tl.exp(dA_cs_k), 0.0)\n        dout = dout * scale_k\n        c = tl.load(c_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) &\n            (offs_n[None, :] < dstate), other=0.0)\n        acc += tl.dot(dout, c)\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n        if HAS_SEQ_IDX:\n            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\n    out = acc\n    dprev_states_ptr += (pid_b * stride_dprev_states_batch + pid_c *\n        stride_dprev_states_chunk + pid_h * stride_dprev_states_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dprev_states_ptrs = dprev_states_ptr + (offs_m[:, None] *\n        stride_dprev_states_hdim + offs_n[None, :] * stride_dprev_states_dstate\n        )\n    tl.store(dprev_states_ptrs, out, mask=(offs_m[:, None] < hdim) & (\n        offs_n[None, :] < dstate))\n",
    "category": "Kernel Operations",
    "subcategory": "kernel configuration",
    "uuid": "9e35eec6-3535-49d6-914f-7492aebf751b"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_K(q, k, v, z, h, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT,\n        1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A = tl.dot(b_q * scale, tl.trans(b_k), allow_tf32=False)\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_zp = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_p *\n            V + i_v * BV,), (BV,), (0,))\n        p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_zp = tl.load(p_zp, boundary_check=(0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = tl.exp(b_v - b_zc[None, :])\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_z = tl.exp(b_zp[None, :] - b_z)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * b_z * scale\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = b_v * tl.dot(b_k, b_dh, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_k, allow_tf32=False)\n    b_dk += tl.dot(tl.trans(b_dA), b_q, allow_tf32=False)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c6a8d10b-0b11-4b44-b57a-ff6924097991"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "bc6087c2-dbad-4a2d-8e3b-6c910d2744b1"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    return 0.5 * x * (1.0 + tl.tanh(0.7978845608028654 * (x + 0.044715 * x *\n        x * x)))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "b0f94819-564a-4ce1-a62a-c6c639ab650c"
  },
  {
    "input": "@triton.jit\ndef gelu_new(x):\n    pi = tl.constexpr(tl.float32(math.pi))\n    a = tl.math.sqrt(2.0 / pi)\n    b = x + 0.044715 * x * x * x\n    return 0.5 * x * (1.0 + tanh(a * b))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "b9874655-f037-413a-86f0-c285382f424d"
  },
  {
    "input": "@triton.jit\ndef _get_voxel_grid_sample_locs_weights(ix, iy, iz, ix0, iy0, iz0):\n    return (ix0, iy0, iz0, ix0, iy0, iz0 + 1, ix0, iy0 + 1, iz0, ix0 + 1,\n        iy0, iz0, ix0 + 1, iy0, iz0 + 1, ix0 + 1, iy0 + 1, iz0, ix0, iy0 + \n        1, iz0 + 1, ix0 + 1, iy0 + 1, iz0 + 1, ix - ix0, iy - iy0, iz - iz0)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "250beef0-2784-4eaa-a00e-576b6cdf5e7e"
  },
  {
    "input": "@triton.jit\ndef debug_fill_dropout_rng(R, stride_rz, stride_rh, stride_rm, stride_rn,\n    seqlen_q, seqlen_k, philox_seed, philox_offset_base, BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    d_offset = off_h * stride_rh + off_z * stride_rz\n    num_h = tl.num_programs(1)\n    off_zh = off_z * num_h + off_h * 1\n    batch_philox_offset = philox_offset_base + off_zh * seqlen_q * seqlen_k\n    R_block_ptr = tl.make_block_ptr(base=R + d_offset, shape=(seqlen_q,\n        seqlen_k), strides=(stride_rm, stride_rn), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    for start_n in range(0, seqlen_k, BLOCK_N):\n        philox_offset = (batch_philox_offset + start_m * BLOCK_M * seqlen_k +\n            start_n)\n        rng = dropout_rng(philox_seed, philox_offset, BLOCK_M, BLOCK_N,\n            seqlen_k)\n        tl.store(R_block_ptr, rng, boundary_check=(0, 1))\n        R_block_ptr = tl.advance(R_block_ptr, (0, BLOCK_N))\n",
    "category": "Memory Management",
    "subcategory": "memory padding",
    "uuid": "9880a00d-c651-4728-a780-5a4797f3267a"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel(output_ptr, input_ptr, input_row_stride,\n    output_row_stride, n_cols, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    input_row_ptr = input_ptr + row_idx * input_row_stride + col_offsets\n    output_row_ptr = output_ptr + row_idx * output_row_stride + col_offsets\n    logits = tl.load(input_row_ptr, mask=mask, other=float('-inf'))\n    max_logits = tl.max(logits, axis=0)\n    logits = logits - max_logits\n    exp_logits = tl.exp(logits)\n    sum_exp_logits = tl.sum(exp_logits, axis=0) + 1e-06\n    softmax_output = exp_logits / sum_exp_logits\n    tl.store(output_row_ptr, softmax_output, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "f33757b4-3081-4468-a8ad-9965fac86d0e"
  },
  {
    "input": "@triton.heuristics({'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _fwd_attention_kernel(Q, K, V, B, softmax_scale: 'tl.constexpr',\n    stride_qb, stride_qh, stride_qg, stride_qm, stride_kb, stride_kh,\n    stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh,\n    stride_bg, stride_bm, stride_bn, stride_ob, stride_oh, stride_og,\n    stride_om, stride_lb, stride_lh, stride_lg, headdim: 'tl.constexpr',\n    num_kv_heads: 'tl.constexpr', num_groups: 'tl.constexpr', CQL:\n    'tl.constexpr', CKL: 'tl.constexpr', seqlen_q, seqlen_k, O, L,\n    HAVE_BIAS: 'tl.constexpr', BIAS_SINGLE_HEAD: 'tl.constexpr',\n    BLOCK_HEADDIM: 'tl.constexpr', EVEN_N: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m, off_bh, off_gp = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    off_h = off_bh % num_kv_heads\n    off_b = off_bh // num_kv_heads\n    if not EVEN_N:\n        offs_n = tl.arange(0, BLOCK_N)\n    Q_Block_ptr = tl.make_block_ptr(base=Q + (off_b * stride_qb + off_h *\n        stride_qh + off_gp * stride_qg), shape=(seqlen_q, headdim),\n        block_shape=(BLOCK_M, BLOCK_HEADDIM), strides=(stride_qm, 1),\n        offsets=(start_m * BLOCK_M, 0), order=(0, 1))\n    O_Block_ptr = tl.make_block_ptr(base=O + (off_b * stride_ob + off_h *\n        stride_oh + off_gp * stride_og), shape=(seqlen_q, headdim),\n        block_shape=(BLOCK_M, BLOCK_HEADDIM), strides=(stride_om, 1),\n        offsets=(start_m * BLOCK_M, 0), order=(0, 1))\n    L_Block_ptr = tl.make_block_ptr(base=L + (off_b * stride_lb + off_h *\n        stride_lh + off_gp * stride_lg), shape=(seqlen_q,), strides=(1,),\n        offsets=(start_m * BLOCK_M,), block_shape=(BLOCK_M,), order=(0,))\n    kv_stride = off_b * stride_kb + off_h * stride_kh\n    K_Block_ptr = tl.make_block_ptr(base=K + kv_stride, shape=(headdim,\n        seqlen_k), block_shape=(BLOCK_HEADDIM, BLOCK_N), strides=(1,\n        stride_kn), offsets=(0, 0), order=(1, 0))\n    V_Block_ptr = tl.make_block_ptr(base=V + kv_stride, shape=(seqlen_k,\n        headdim), block_shape=(BLOCK_N, BLOCK_HEADDIM), strides=(stride_vn,\n        1), offsets=(0, 0), order=(0, 1))\n    q = tl.load(Q_Block_ptr, boundary_check=(0, 1))\n    softmax_scale = softmax_scale\n    if HAVE_BIAS:\n        bias_h_pos: 'tl.constexpr' = (0 if BIAS_SINGLE_HEAD else off_h *\n            stride_bh + off_gp * stride_bg)\n        B_Block_ptr = tl.make_block_ptr(base=B + (off_b * stride_bb +\n            bias_h_pos), shape=(seqlen_q, seqlen_k), block_shape=(BLOCK_M,\n            BLOCK_N), strides=(stride_bm, stride_bn), offsets=(start_m *\n            BLOCK_M, 0), order=(0, 1))\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    max_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    for j in range(0, seqlen_k, BLOCK_N):\n        j = tl.multiple_of(j, BLOCK_N)\n        k = tl.load(K_Block_ptr, boundary_check=(0, 1))\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k) * softmax_scale\n        if not EVEN_N:\n            qk += tl.where((j + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n        if HAVE_BIAS:\n            b = tl.load(B_Block_ptr, boundary_check=(0, 1))\n            B_Block_ptr = tl.advance(B_Block_ptr, (0, BLOCK_N))\n            qk = qk + b\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        else:\n            max_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - max_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(max_i - max_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        v = tl.load(V_Block_ptr, boundary_check=(0, 1))\n        acc_o += tl.dot(p, v)\n        max_i = max_ij\n        lse_i = max_ij + tl.log(tl.exp(lse_i - max_ij) + l_ij)\n        K_Block_ptr = tl.advance(K_Block_ptr, (0, BLOCK_N))\n        V_Block_ptr = tl.advance(V_Block_ptr, (BLOCK_N, 0))\n    o_scale = tl.exp(max_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    tl.store(L_Block_ptr, lse_i, boundary_check=(0,))\n    tl.store(O_Block_ptr, acc_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "9c29ce0f-9bfa-421f-910e-5f6da2019780"
  },
  {
    "input": "@triton.jit\ndef _triton_fourth_order_bwd(x_ptr: 'tl.tensor', y_ptr: 'tl.tensor', z_ptr:\n    'tl.tensor', g_x_ptr: 'tl.tensor', g_y_ptr: 'tl.tensor', g_z_ptr:\n    'tl.tensor', g_1_0_ptr: 'tl.tensor', g_1_1_ptr: 'tl.tensor', g_1_2_ptr:\n    'tl.tensor', g_2_0_ptr: 'tl.tensor', g_2_1_ptr: 'tl.tensor', g_2_2_ptr:\n    'tl.tensor', g_2_3_ptr: 'tl.tensor', g_2_4_ptr: 'tl.tensor', g_3_0_ptr:\n    'tl.tensor', g_3_1_ptr: 'tl.tensor', g_3_2_ptr: 'tl.tensor', g_3_3_ptr:\n    'tl.tensor', g_3_4_ptr: 'tl.tensor', g_3_5_ptr: 'tl.tensor', g_3_6_ptr:\n    'tl.tensor', g_4_0_ptr: 'tl.tensor', g_4_1_ptr: 'tl.tensor', g_4_2_ptr:\n    'tl.tensor', g_4_3_ptr: 'tl.tensor', g_4_4_ptr: 'tl.tensor', g_4_5_ptr:\n    'tl.tensor', g_4_6_ptr: 'tl.tensor', g_4_7_ptr: 'tl.tensor', g_4_8_ptr:\n    'tl.tensor', BLOCK_SIZE: 'tl.constexpr', vector_length: 'tl.constexpr'):\n    sqrt_3 = 3 ** 0.5\n    sqrt_5 = 5 ** 0.5\n    sqrt_15 = 15 ** 0.5\n    block_id = tl.program_id(0)\n    offset = tl.arange(0, BLOCK_SIZE) + BLOCK_SIZE * block_id\n    x_row_start = x_ptr + offset\n    y_row_start = y_ptr + offset\n    z_row_start = z_ptr + offset\n    x = tl.load(x_row_start, mask=offset < vector_length)\n    y = tl.load(y_row_start, mask=offset < vector_length)\n    z = tl.load(z_row_start, mask=offset < vector_length)\n    g_1_0 = tl.load(g_1_0_ptr + offset, mask=offset < vector_length)\n    g_1_1 = tl.load(g_1_1_ptr + offset, mask=offset < vector_length)\n    g_1_2 = tl.load(g_1_2_ptr + offset, mask=offset < vector_length)\n    g_x = sqrt_3 * g_1_0\n    g_y = sqrt_3 * g_1_1\n    g_z = sqrt_3 * g_1_2\n    g_2_0 = tl.load(g_2_0_ptr + offset, mask=offset < vector_length)\n    g_2_1 = tl.load(g_2_1_ptr + offset, mask=offset < vector_length)\n    g_2_2 = tl.load(g_2_2_ptr + offset, mask=offset < vector_length)\n    g_2_3 = tl.load(g_2_3_ptr + offset, mask=offset < vector_length)\n    g_2_4 = tl.load(g_2_4_ptr + offset, mask=offset < vector_length)\n    g_x += sqrt_15 * z * g_2_0\n    g_z += sqrt_15 * x * g_2_0\n    g_x += sqrt_15 * y * g_2_1\n    g_y += sqrt_15 * x * g_2_1\n    g_y += sqrt_15 * z * g_2_2\n    g_z += sqrt_15 * y * g_2_2\n    g_x += -1.0 * sqrt_5 * x * g_2_3\n    g_y += 2.0 * sqrt_5 * y * g_2_3\n    g_z += -1.0 * sqrt_5 * z * g_2_3\n    g_x += -1.0 * sqrt_15 * x * g_2_4\n    g_z += sqrt_15 * z * g_2_4\n    g_3_0 = tl.load(g_3_0_ptr + offset, mask=offset < vector_length)\n    g_3_1 = tl.load(g_3_1_ptr + offset, mask=offset < vector_length)\n    g_3_2 = tl.load(g_3_2_ptr + offset, mask=offset < vector_length)\n    g_3_3 = tl.load(g_3_3_ptr + offset, mask=offset < vector_length)\n    g_3_4 = tl.load(g_3_4_ptr + offset, mask=offset < vector_length)\n    g_3_5 = tl.load(g_3_5_ptr + offset, mask=offset < vector_length)\n    g_3_6 = tl.load(g_3_6_ptr + offset, mask=offset < vector_length)\n    sq_x = x * x\n    sq_y = y * y\n    sq_z = z * z\n    cu_z = sq_z * z\n    cu_x = sq_x * x\n    cu_y = sq_y * y\n    g_x += sqrt_15 * g_3_0 * (-1.62018517460196 * sq_x + 1.08012344973464 *\n        sq_z + 0.540061724867322 * sq_z)\n    g_x += 2.64575131106459 * sqrt_15 * g_3_1 * y * z\n    g_x -= g_3_2 * (4.8605555238059 * sq_x - 6.48074069840786 * sq_y + \n        1.62018517460197 * sq_z)\n    g_x -= 7.93725393319377 * g_3_3 * x * y\n    g_x -= 3.24037034920393 * g_3_4 * x * z\n    g_x -= 2.64575131106459 * sqrt_15 * g_3_5 * x * y\n    g_x -= sqrt_15 * g_3_6 * z * (1.08012344973464 * x + 2.16024689946929 * x)\n    g_y += 2.64575131106459 * sqrt_15 * g_3_1 * x * z\n    g_y += 12.9614813968157 * g_3_2 * x * y\n    g_y -= g_3_3 * (3.96862696659689 * sq_x - 7.93725393319377 * sq_y + \n        3.96862696659689 * sq_z)\n    g_y += 12.9614813968157 * g_3_4 * y * z\n    g_y -= 1.3228756555323 * sqrt_15 * g_3_5 * (sq_x - sq_z)\n    g_z += sqrt_15 * g_3_0 * x * (1.08012344973464 * z + 2.16024689946929 * z)\n    g_z += 2.64575131106459 * sqrt_15 * g_3_1 * x * y\n    g_z -= 3.24037034920393 * g_3_2 * x * z\n    g_z -= 7.93725393319377 * g_3_3 * y * z\n    g_z -= g_3_4 * (1.62018517460197 * sq_x - 6.48074069840786 * sq_y + \n        4.8605555238059 * sq_z)\n    g_z += 2.64575131106459 * sqrt_15 * g_3_5 * y * z\n    g_z -= sqrt_15 * g_3_6 * (1.08012344973464 * sq_x + 0.540061724867322 *\n        sq_x - 1.62018517460196 * sq_z)\n    g_4_0 = tl.load(g_4_0_ptr + offset, mask=offset < vector_length)\n    g_4_1 = tl.load(g_4_1_ptr + offset, mask=offset < vector_length)\n    g_4_2 = tl.load(g_4_2_ptr + offset, mask=offset < vector_length)\n    g_4_3 = tl.load(g_4_3_ptr + offset, mask=offset < vector_length)\n    g_4_4 = tl.load(g_4_4_ptr + offset, mask=offset < vector_length)\n    g_4_5 = tl.load(g_4_5_ptr + offset, mask=offset < vector_length)\n    g_4_6 = tl.load(g_4_6_ptr + offset, mask=offset < vector_length)\n    g_4_7 = tl.load(g_4_7_ptr + offset, mask=offset < vector_length)\n    g_4_8 = tl.load(g_4_8_ptr + offset, mask=offset < vector_length)\n    g_x -= sqrt_15 * g_4_0 * (3.43693177121688 * sq_x * z + \n        3.43693177121688 * sq_x * z - 1.14564392373896 * cu_z - \n        1.14564392373896 * cu_z)\n    g_x += sqrt_15 * g_4_1 * y * (-4.8605555238059 * sq_x + \n        3.24037034920393 * sq_z + 1.62018517460197 * sq_z)\n    g_x -= g_4_2 * (0.649519052838329 * sqrt_15 * sq_x * z + \n        7.54672942406179 * sq_x * z - 2.59807621135332 * sqrt_15 * sq_y * z -\n        10.0623058987491 * sq_y * z + 0.21650635094611 * sqrt_15 * cu_z + \n        2.51557647468726 * cu_z)\n    g_x -= g_4_3 * y * (0.918558653543692 * sqrt_15 * sq_x + \n        16.0090306546024 * sq_x - 9.48683298050514 * sq_y + \n        0.918558653543692 * sqrt_15 * sq_z + 5.33634355153414 * sq_z + \n        0.459279326771846 * sqrt_15 * (sq_x - sq_z))\n    g_x += g_4_4 * (-9.0 * x * sq_y + 2.25 * x * sq_z - 9.0 * x * sq_y + \n        2.25 * x * sq_z + 4.5 * cu_x)\n    g_x -= g_4_5 * y * z * (-0.918558653543692 * sqrt_15 * x + \n        10.6726871030683 * x + 1.83711730708738 * sqrt_15 * x)\n    g_x -= g_4_6 * (2.59807621135332 * sqrt_15 * x * sq_y - \n        0.21650635094611 * sqrt_15 * x * sq_z + 2.51557647468726 * x * sq_z +\n        10.0623058987491 * x * sq_y - 2.51557647468726 * x * sq_z + \n        0.21650635094611 * sqrt_15 * x * sq_z - 5.03115294937453 * cu_x - \n        0.433012701892219 * sqrt_15 * cu_x)\n    g_x -= sqrt_15 * g_4_7 * y * z * (3.24037034920393 * x + \n        6.48074069840786 * x)\n    g_x -= sqrt_15 * g_4_8 * (1.14564392373896 * x * sq_z + \n        4.58257569495584 * x * sq_z + 1.14564392373896 * x * sq_z - \n        2.29128784747792 * cu_x)\n    g_y += sqrt_15 * g_4_1 * x * (-1.62018517460197 * sq_x + \n        3.24037034920393 * sq_z + 1.62018517460197 * sq_z)\n    g_y += g_4_2 * x * z * (5.19615242270663 * sqrt_15 * y + \n        20.1246117974981 * y)\n    g_y -= g_4_3 * x * (5.33634355153414 * sq_x - 28.4604989415154 * sq_y +\n        0.918558653543692 * sqrt_15 * sq_z + 5.33634355153414 * sq_z + \n        0.459279326771846 * sqrt_15 * (sq_x - sq_z))\n    g_y -= g_4_4 * (9.0 * sq_x * y + 9.0 * sq_x * y + 9.0 * y * sq_z + 9.0 *\n        y * sq_z - 12.0 * cu_y)\n    g_y -= g_4_5 * z * (0.918558653543692 * sqrt_15 * sq_x + \n        5.33634355153414 * sq_x - 28.4604989415154 * sq_y + \n        5.33634355153414 * sq_z - 0.459279326771846 * sqrt_15 * (sq_x - sq_z))\n    g_y -= g_4_6 * (10.0623058987491 * sq_x * y + 2.59807621135332 *\n        sqrt_15 * y * (sq_x - sq_z) - 10.0623058987491 * y * sq_z)\n    g_y -= sqrt_15 * g_4_7 * z * (3.24037034920393 * sq_x + \n        1.62018517460197 * sq_x - 1.62018517460197 * sq_z)\n    g_z -= sqrt_15 * g_4_0 * (1.14564392373896 * cu_x - 3.43693177121688 *\n        x * sq_z - 3.43693177121688 * x * sq_z + 1.14564392373896 * cu_x)\n    g_z += sqrt_15 * g_4_1 * x * y * (3.24037034920393 * z + \n        6.48074069840786 * z)\n    g_z -= g_4_2 * (0.21650635094611 * sqrt_15 * cu_x - 2.59807621135332 *\n        sqrt_15 * x * sq_y - 10.0623058987491 * x * sq_y + \n        0.649519052838329 * sqrt_15 * x * sq_z + 7.54672942406179 * x *\n        sq_z + 2.51557647468726 * cu_x)\n    g_z -= g_4_3 * x * y * (-0.918558653543692 * sqrt_15 * z + \n        10.6726871030683 * z + 1.83711730708738 * sqrt_15 * z)\n    g_z += g_4_4 * (2.25 * sq_x * z + 2.25 * sq_x * z - 9.0 * sq_y * z - \n        9.0 * sq_y * z + 4.5 * cu_z)\n    g_z -= g_4_5 * y * (0.918558653543692 * sqrt_15 * sq_x + \n        5.33634355153414 * sq_x - 9.48683298050514 * sq_y + \n        0.918558653543692 * sqrt_15 * sq_z + 16.0090306546024 * sq_z - \n        0.459279326771846 * sqrt_15 * (sq_x - sq_z))\n    g_z += g_4_6 * (-0.21650635094611 * sqrt_15 * sq_x * z + \n        2.51557647468726 * sq_x * z - 2.51557647468726 * sq_x * z + \n        0.21650635094611 * sqrt_15 * sq_x * z + 2.59807621135332 * sqrt_15 *\n        sq_y * z + 10.0623058987491 * sq_y * z - 5.03115294937453 * cu_z - \n        0.433012701892219 * sqrt_15 * cu_z)\n    g_z -= sqrt_15 * g_4_7 * y * (3.24037034920393 * sq_x + \n        1.62018517460197 * sq_x - 4.8605555238059 * sq_z)\n    g_z -= sqrt_15 * g_4_8 * (1.14564392373896 * sq_x * z + \n        4.58257569495584 * sq_x * z + 1.14564392373896 * sq_x * z - \n        2.29128784747792 * cu_z)\n    tl.store(g_x_ptr + offset, g_x, mask=offset < vector_length)\n    tl.store(g_y_ptr + offset, g_y, mask=offset < vector_length)\n    tl.store(g_z_ptr + offset, g_z, mask=offset < vector_length)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "a1757961-f16b-447e-82d3-4143a529d69d"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dq(k, rk, ck, dq, ds, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BK: 'tl.constexpr', BM:\n    'tl.constexpr', DK: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_ck = tl.make_block_ptr(ck + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t),\n        (i_m * BM, 0), (BM, BT), (0, 1))\n    p_dq = tl.make_block_ptr(dq + (i_m * n_bh + i_bh) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (0, i_k * BK), (BT, BK), (1, 0))\n    p_ds = tl.make_block_ptr(ds + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hk = tl.zeros([BM, BK], dtype=tl.float32)\n    for _ in range(NT):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_rk = tl.load(p_rk, boundary_check=(0,))\n        b_ck = tl.load(p_ck, boundary_check=(0, 1))\n        b_ds = tl.load(p_ds, boundary_check=(0, 1))\n        b_inter = tl.dot(b_ds * b_rk[None, :], b_hk, allow_tf32=False)\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_ds, b_ck, allow_tf32=False),\n            0), b_k, allow_tf32=False)\n        b_dq = b_inter + b_intra\n        b_hk = b_hk * b_rk[:, None] + tl.dot(b_ck, b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        p_k = tl.advance(p_k, (BT, 0))\n        p_rk = tl.advance(p_rk, (DM,))\n        p_ck = tl.advance(p_ck, (0, BT))\n        p_dq = tl.advance(p_dq, (BT, 0))\n        p_ds = tl.advance(p_ds, (BT, 0))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "61024ee2-897d-45c6-9948-a3f0c77df430"
  },
  {
    "input": "@triton.jit\ndef backward_scan(gates, tokens, outputs, SEQUENCE_LENGTH: 'tl.constexpr'):\n    sequence_id = tl.num_programs(axis=1) * tl.program_id(axis=0\n        ) + tl.program_id(axis=1)\n    forward_strides = tl.arange(0, SEQUENCE_LENGTH\n        ) + sequence_id * SEQUENCE_LENGTH\n    reverse_strides = tl.num_programs(axis=0) * tl.num_programs(axis=1\n        ) * SEQUENCE_LENGTH - 1 - forward_strides\n    tokens_ = tl.load(tokens + reverse_strides)\n    gates_ = tl.load(gates + reverse_strides)\n    tuples = pack64(tokens_, gates_)\n    output_tuples_ = tl.associative_scan(tuples, axis=0, combine_fn=\n        first_order_op)\n    output_tokens_, output_gates_ = unpack64(output_tuples_)\n    tl.store(outputs + reverse_strides, output_tokens_)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "7cd08a7f-7019-4e64-be79-b260223830e8"
  },
  {
    "input": "@triton.jit\ndef bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g,\n    dg, s_k_h, BT: 'tl.constexpr', BK: 'tl.constexpr', K: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_g = g + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_dg = dg + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * K\n    p_dq_inner = dq_inner + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dk_inner = dk_inner + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dq_inter = dq_inter + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    p_dk_inter = dk_inter + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * K\n    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < K\n    last_g = tl.zeros([BK], dtype=tl.float32)\n    for j in range(BT - 1, -1, -1):\n        _g = tl.load(p_g, mask=mask, other=0)\n        if j == BT - 1:\n            last_g = _g\n        b_dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n        b_dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n        b_dq2 *= tl.exp(_g)\n        b_dq = b_dq1 + b_dq2\n        tl.store(p_dq_inter, b_dq, mask=mask)\n        b_dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n        b_dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n        b_dk2 *= tl.exp(last_g - _g)\n        b_dk = b_dk1 + b_dk2\n        tl.store(p_dk_inter, b_dk, mask=mask)\n        b_q = tl.load(p_q, mask=mask, other=0)\n        b_k = tl.load(p_k, mask=mask, other=0)\n        b_dg = b_dq * b_q - b_dk * b_k\n        cum_grad_dg += b_dg\n        tl.store(p_dg, cum_grad_dg, mask=mask)\n        p_g -= K\n        p_k -= K\n        p_q -= K\n        p_dq_inner -= K\n        p_dk_inner -= K\n        p_dq_inter -= K\n        p_dk_inter -= K\n        p_dg -= K\n",
    "category": "Gradient Operations",
    "subcategory": "gradient accumulation",
    "uuid": "1ca18889-8b33-4bf4-bb6e-3ae75ba11029"
  },
  {
    "input": "@triton.jit\ndef depth_lin(near, far, n, step):\n    frac_step = step / (n - 1)\n    return (far - near) * frac_step + near\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "733c5b90-a880-4c44-944d-e6e72db6d15a"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_SPATIAL': lambda args: next_power_of_2(args['spatial_dim'])})\n@triton.jit\ndef nll_loss_forward_kernel(input_pointer, target_pointer, weight_pointer,\n    sum_weights_pointer, output_pointer, batch_dim, spatial_dim,\n    input_batch_stride, input_feat_stride, input_spatial_stride,\n    target_batch_stride, target_spatial_stride, output_batch_stride,\n    output_spatial_stride, reduction: 'tl.constexpr', weighted:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_SPATIAL:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the negative log likelihood loss between the input and target,\n    with optional reweighing of each class.\n\n    Args:\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim, spatial_dim].\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim, spatial_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to a container the sum of the class weights is written to.\n            The container must be of shape [batch_dim/BLOCK_SIZE_BATCH].\n        output_pointer: Pointer to a container the loss is written to.\n            The container must be of shape [batch_dim, spatial_dim] if reduction is 'none',\n            and otherwise of shape [batch_dim/BLOCK_SIZE].\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        target_batch_stride: Stride necessary to jump one element along the\n            target's batch dimension.\n        target_spatial_stride: Stride necessary to jump one element along the\n            target's spatial dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_spatial_stride: Stride necessary to jump one element along the\n            output container's spatial dimension.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the loss\n            across all entries, and 'sum' for summing the loss across all entries.\n            If a reduction method is specified, the reduced result of each\n            program is written to a separate index in the summed weights and\n            output container, which should later be summed.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)\n    batch_mask = batch_offset < batch_dim\n    spatial_mask = spatial_offset < spatial_dim\n    target_pointer += target_batch_stride * batch_offset[:, None\n        ] + target_spatial_stride * spatial_offset[None, :]\n    target = tl.load(target_pointer, mask=batch_mask[:, None] &\n        spatial_mask[None, :])\n    input_pointer += (input_feat_stride * target + input_batch_stride *\n        batch_offset[:, None] + input_spatial_stride * spatial_offset[None, :])\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & spatial_mask[\n        None, :])\n    output = -input\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        output *= weight\n    if reduction == 'none':\n        output_pointer += output_batch_stride * batch_offset[:, None\n            ] + output_spatial_stride * spatial_offset[None, :]\n        tl.store(output_pointer, output, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n    elif reduction == 'mean':\n        if weighted:\n            tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))\n            tl.store(output_pointer + batch_pid, tl.sum(output))\n        else:\n            tl.store(output_pointer + batch_pid, tl.sum(output) / (\n                batch_dim * spatial_dim))\n    elif reduction == 'sum':\n        tl.store(output_pointer + batch_pid, tl.sum(output))\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "d24fca6d-db27-474c-97f6-ff34ccced589"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "fbe00aa4-3a01-4617-9ddf-e2bf4668ff11"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, DO, Delta, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_hz = tl.program_id(1)\n    off_n = tl.arange(0, HEAD_DIM)\n    o = tl.load(O + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    do = tl.load(DO + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "cb127a8e-04c2-4e42-bf56-2463ce2c8609"
  },
  {
    "input": "@triton.jit\ndef tanh_grad(input):\n    \"\"\"\n    Calculates the gradient of tanh.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of tanh.\n    \"\"\"\n    output_tanh = tanh(input)\n    return 1 - output_tanh * output_tanh\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "30bf625c-3551-4125-8afe-cdd4c7ae3688"
  },
  {
    "input": "@triton.jit\ndef rdiv_scalar_kernel(x_ptr, scalar, output_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = scalar / x\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "62979ad8-b0c7-45a3-adbd-0104f059823b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({\n    'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({\n    'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({\n    'BLOCK_N': 1024})], key=['ncols'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['OUT'] is not None})\n@triton.jit\ndef _swiglu_bwd_kernel(X, Y, DOUT, OUT, DX, DY, stride_x_row, stride_y_row,\n    stride_dout_row, stride_out_row, stride_dx_row, stride_dy_row, ncols,\n    BLOCK_N: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    DOUT += row * stride_dout_row\n    if RECOMPUTE_OUTPUT:\n        OUT += row * stride_out_row\n    DX += row * stride_dx_row\n    DY += row * stride_dy_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0)\n    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0)\n    x_sigmoid = tl.sigmoid(x)\n    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\n    dy = x * x_sigmoid * dout\n    tl.store(DX + cols, dx, mask=cols < ncols)\n    tl.store(DY + cols, dy, mask=cols < ncols)\n    if RECOMPUTE_OUTPUT:\n        out = x * x_sigmoid * y\n        tl.store(OUT + cols, out, mask=cols < ncols)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c519f772-4768-415c-bf0e-fe66ccfd2ac7"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "9448c76b-b2b5-40c8-b60f-c3b921883b96"
  },
  {
    "input": "@triton.jit\ndef layernorm_forward(Y, Y_row_stride, X, X_row_stride, W, b, r, mu, n_cols,\n    eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    mean_X = tl.sum(X_row, axis=0) / n_cols\n    XX = X_row - mean_X\n    row_var = tl.sum(XX * XX, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    tl.store(mu, mean_X)\n    output = XX * inv_var * W_row + b_row\n    tl.store(Y + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "caa35fb3-1a59-49ff-9e6e-dcfd2ed8b008"
  },
  {
    "input": "@triton.jit\ndef parallel_retention_bwd_kernel_dq(i_bh, i_t, i_k, i_v, i_h, k, v, do, dq,\n    scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK),\n        (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, 0),\n        (BV, BS), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BS)\n    d_h = tl.math.exp2((BS - tl.arange(0, BS)) * b_b)\n    for i in range(0, i_t * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        if i != 0:\n            b_dq *= d_b\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BS, 0))\n        p_v = tl.advance(p_v, (0, BS))\n    b_dq *= tl.math.exp2(tl.arange(0, BT) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BT)\n    o_k = tl.arange(0, BS)\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, \n        i_t * BT), (BV, BS), (0, 1))\n    for _ in range(i_t * BT, (i_t + 1) * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BS, 0))\n        p_v = tl.advance(p_v, (0, BS))\n        o_k += BS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * T * K, (T, K), (K,\n        1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "2f825364-ef15-4186-ba88-56b9e32cd43c"
  },
  {
    "input": "@triton.jit\ndef chunk_rwkv6_fwd_kernel_inter(q, v, gs, h, o, A, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_gs = tl.make_block_ptr(gs + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_gs = tl.load(p_gs, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_gs)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h, allow_tf32=False)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "c35bf85e-96b6-477d-a249-2d73f012d130"
  },
  {
    "input": "@triton.jit\ndef _attn_bias_bwd(Q, K, V, seq_offsets, TS, TW, PW, num_targets, DOut, DTW,\n    DPW, stride_qm, stride_qh, stride_kn, stride_kh, stride_vn, stride_vh,\n    stride_ts, stride_dom, stride_doh, alpha, Z, H, MAX_SEQ_LEN, DimQ, DimV,\n    num_buckets, max_pos_ind, time_bucket_incr, time_bucket_div, time_delta,\n    MAX_ATTN_LEN: 'tl.constexpr', INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', BUCKET_FN: 'tl.constexpr', USE_TIME_BIAS:\n    'tl.constexpr', USE_POS_BIAS: 'tl.constexpr', HAS_MAX_POS_IND:\n    'tl.constexpr', HAS_MULTIPLE_TARGETS: 'tl.constexpr',\n    CONTEXTUAL_SEQ_LEN: 'tl.constexpr', ALLOW_TF32: 'tl.constexpr',\n    BLOCK_D_Q: 'tl.constexpr', BLOCK_D_V: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', NUM_N_BLOCKS: 'tl.constexpr', NUM_OUT_GROUPS:\n    'tl.constexpr'):\n    off_mn = tl.program_id(0)\n    off_m = off_mn // NUM_N_BLOCKS\n    off_n = off_mn % NUM_N_BLOCKS\n    widx = off_m * (off_m + 1) // 2 + off_n\n    widx = widx % NUM_OUT_GROUPS\n    start_m = off_m * BLOCK_N\n    start_n = off_n * BLOCK_N\n    offs_m = start_m + tl.arange(0, BLOCK_N)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    offs_qk_d = tl.arange(0, BLOCK_D_Q)\n    offs_v_d = tl.arange(0, BLOCK_D_V)\n    dbias_pos = None\n    offs_pos_w = None\n    if USE_POS_BIAS:\n        if not HAS_MULTIPLE_TARGETS:\n            dbias_pos = tl.zeros((BLOCK_N, BLOCK_N), dtype=tl.float32)\n            if HAS_MAX_POS_IND:\n                offs_pos_w = offs_n[None, :] - offs_m[:, None\n                    ] + max_pos_ind - 1\n                offs_pos_w = tl.where(offs_pos_w > 0, offs_pos_w, 0)\n                offs_pos_w = tl.where(offs_pos_w < 2 * max_pos_ind - 2,\n                    offs_pos_w, 2 * max_pos_ind - 2)\n            else:\n                offs_pos_w = offs_n[None, :] - offs_m[:, None\n                    ] + MAX_SEQ_LEN - 1\n    if HAS_MULTIPLE_TARGETS:\n        invalid_mask = offs_m[:, None] == offs_n[None, :]\n    elif MAX_ATTN_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            invalid_mask = offs_m[:, None] >= offs_n[None, :] and offs_m[:,\n                None] - offs_n[None, :] <= MAX_ATTN_LEN\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            invalid_mask = offs_m[:, None] <= offs_n[None, :] and offs_n[\n                None, :] - offs_m[:, None] <= MAX_ATTN_LEN\n    elif INVALID_MASK_TYPE == 'lower_triangular':\n        invalid_mask = offs_m[:, None] >= offs_n[None, :]\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        invalid_mask = offs_m[:, None] <= offs_n[None, :]\n    for off_z in range(Z):\n        seq_start = tl.load(seq_offsets + off_z)\n        seq_end = tl.load(seq_offsets + off_z + 1)\n        seq_len = seq_end - seq_start\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            if HAS_MULTIPLE_TARGETS:\n                low = start_n\n                if MAX_ATTN_LEN > 0:\n                    n_targets = tl.load(num_targets + off_z)\n                    high = start_n + MAX_ATTN_LEN + BLOCK_N\n                    high = high if high + n_targets < seq_len else seq_len\n                else:\n                    high = seq_len\n            else:\n                low = start_n\n                if MAX_ATTN_LEN > 0:\n                    high = start_n + MAX_ATTN_LEN + BLOCK_N\n                    high = high if high < seq_len else seq_len\n                else:\n                    high = seq_len\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            low = 0\n            high = start_n + BLOCK_N\n        if start_n < seq_len and (start_m >= low and start_m < high):\n            q_ptrs = Q + seq_start * stride_qm + offs_m[:, None\n                ] * stride_qm + offs_qk_d[None, :]\n            k_ptrs = K + seq_start * stride_kn + offs_n[:, None\n                ] * stride_kn + offs_qk_d[None, :]\n            v_ptrs = V + seq_start * stride_vn + offs_n[:, None\n                ] * stride_vn + offs_v_d[None, :]\n            do_ptrs = DOut + seq_start * stride_dom + offs_m[:, None\n                ] * stride_dom + offs_v_d[None, :]\n            mask_m = offs_m < seq_len\n            mask_n = offs_n < seq_len\n            if HAS_MULTIPLE_TARGETS:\n                if (INVALID_MASK_TYPE != 'lower_triangular' or MAX_ATTN_LEN ==\n                    0):\n                    n_targets = tl.load(num_targets + off_z)\n                if INVALID_MASK_TYPE == 'lower_triangular':\n                    pos_offs_m = tl.where(offs_m < seq_len - n_targets,\n                        offs_m, seq_len - n_targets)\n                    pos_offs_n = tl.where(offs_n < seq_len - n_targets,\n                        offs_n, seq_len - n_targets)\n                elif INVALID_MASK_TYPE == 'upper_triangular':\n                    pos_offs_m = tl.where(offs_m > n_targets - 1, offs_m, \n                        n_targets - 1)\n                    pos_offs_n = tl.where(offs_n > n_targets - 1, offs_n, \n                        n_targets - 1)\n            else:\n                pos_offs_n = offs_n\n                pos_offs_m = offs_m\n            mt_offs_pos_w = None\n            if USE_POS_BIAS:\n                if HAS_MULTIPLE_TARGETS:\n                    if HAS_MAX_POS_IND:\n                        mt_offs_pos_w = pos_offs_n[None, :] - pos_offs_m[:,\n                            None] + max_pos_ind - 1\n                        mt_offs_pos_w = tl.where(mt_offs_pos_w > 0,\n                            mt_offs_pos_w, 0)\n                        mt_offs_pos_w = tl.where(mt_offs_pos_w < 2 *\n                            max_pos_ind - 2, mt_offs_pos_w, 2 * max_pos_ind - 2\n                            )\n                    else:\n                        mt_offs_pos_w = pos_offs_n[None, :] - pos_offs_m[:,\n                            None] + MAX_SEQ_LEN - 1\n                else:\n                    mt_offs_pos_w = offs_pos_w\n            if HAS_MULTIPLE_TARGETS:\n                if MAX_ATTN_LEN > 0:\n                    if INVALID_MASK_TYPE == 'lower_triangular':\n                        mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                            ] > pos_offs_n[None, :] and pos_offs_n[None, :\n                            ] - pos_offs_m[:, None] >= -MAX_ATTN_LEN\n                    elif INVALID_MASK_TYPE == 'upper_triangular':\n                        mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                            ] < pos_offs_n[None, :] and pos_offs_n[None, :\n                            ] - pos_offs_m[:, None] <= MAX_ATTN_LEN\n                elif INVALID_MASK_TYPE == 'lower_triangular':\n                    mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                        ] > pos_offs_n[None, :]\n                elif INVALID_MASK_TYPE == 'upper_triangular':\n                    mt_invalid_mask = invalid_mask or pos_offs_m[:, None\n                        ] < pos_offs_n[None, :]\n            else:\n                mt_invalid_mask = invalid_mask\n            if CONTEXTUAL_SEQ_LEN > 0:\n                if INVALID_MASK_TYPE == 'lower_triangular':\n                    row_filter = offs_m < CONTEXTUAL_SEQ_LEN\n                    if HAS_MULTIPLE_TARGETS:\n                        col_filter = offs_n < seq_len - n_targets\n                    else:\n                        col_filter = offs_n < seq_len\n                    invalid_mask = invalid_mask or row_filter[:, None\n                        ] and col_filter[None, :]\n            ts = None\n            if USE_TIME_BIAS:\n                ts_ptrs = TS + off_z * stride_ts\n                ts_0_ptrs = ts_ptrs + offs_m\n                ts_1_ptrs = ts_ptrs + offs_n\n                if CAUSAL:\n                    ts_0 = tl.load(ts_0_ptrs + 1, mask=mask_m)\n                    ts_1 = tl.load(ts_1_ptrs, mask=mask_n)\n                else:\n                    ts_0 = tl.load(ts_0_ptrs, mask=mask_m)\n                    ts_1 = tl.load(ts_1_ptrs + 1, mask=mask_n)\n                ts = ts_0[:, None] - ts_1[None, :]\n                ts = ts + time_delta\n                ts = tl.where(ts > 1e-06, ts, 1e-06)\n                ts = ts * (1.0 / time_bucket_incr)\n                if BUCKET_FN == 'log':\n                    ts = tl.log(ts)\n                elif BUCKET_FN == 'sqrt':\n                    ts = tl.sqrt(ts)\n                ts = ts * (1.0 / time_bucket_div)\n                ts = ts\n                ts = tl.where(ts > 0, ts, 0)\n                ts = tl.where(ts < num_buckets, ts, num_buckets)\n            attn_bias = tl.zeros([BLOCK_N, BLOCK_N], dtype=tl.float32)\n            if USE_TIME_BIAS:\n                ts_w = tl.load(TW + ts, mask=mask_m[:, None] & mask_n[None,\n                    :] & mt_invalid_mask)\n                attn_bias = attn_bias + ts_w\n            if USE_POS_BIAS:\n                pos_w = tl.load(PW + mt_offs_pos_w, mask=mask_m[:, None] &\n                    mask_n[None, :] & mt_invalid_mask)\n                attn_bias = attn_bias + pos_w\n            dbias = tl.zeros((BLOCK_N, BLOCK_N), dtype=tl.float32)\n            for off_h in range(H):\n                q = tl.load(q_ptrs + off_h * stride_qh, mask=mask_m[:, None\n                    ], other=0.0)\n                k = tl.load(k_ptrs + off_h * stride_kh, mask=mask_n[:, None\n                    ], other=0.0)\n                qk = tl.dot(q, tl.trans(k), allow_tf32=ALLOW_TF32) * alpha\n                qk = qk + attn_bias\n                sig = fast_dividef(1.0, 1.0 + tl.exp(-qk))\n                do = tl.load(do_ptrs + off_h * stride_doh, mask=mask_m[:,\n                    None], other=0.0)\n                v = tl.load(v_ptrs + off_h * stride_vh, mask=mask_n[:, None\n                    ], other=0.0)\n                dqk = tl.dot(do, tl.trans(v), allow_tf32=ALLOW_TF32)\n                dqk = dqk * sig * (1 + qk * (1 - sig)) * (1.0 / MAX_SEQ_LEN)\n                dbias = dbias + dqk\n            if USE_TIME_BIAS:\n                dtw_ptrs = DTW + widx * (num_buckets + 1)\n                tl.atomic_add(dtw_ptrs + ts, dbias, mask=mask_m[:, None] &\n                    mask_n[None, :] & mt_invalid_mask, sem='relaxed')\n            if USE_POS_BIAS:\n                if HAS_MULTIPLE_TARGETS:\n                    if HAS_MAX_POS_IND:\n                        dpw_ptrs = DPW + widx * (2 * max_pos_ind - 1)\n                    else:\n                        dpw_ptrs = DPW + widx * (2 * MAX_SEQ_LEN - 1)\n                    tl.atomic_add(dpw_ptrs + mt_offs_pos_w, dbias, mask=\n                        mask_m[:, None] & mask_n[None, :] & mt_invalid_mask,\n                        sem='relaxed')\n                else:\n                    dbias_pos += dbias\n    if USE_POS_BIAS and not HAS_MULTIPLE_TARGETS:\n        if HAS_MAX_POS_IND:\n            dpw_ptrs = DPW + widx * (2 * max_pos_ind - 1)\n        else:\n            dpw_ptrs = DPW + widx * (2 * MAX_SEQ_LEN - 1)\n        tl.atomic_add(dpw_ptrs + offs_pos_w, dbias_pos, mask=invalid_mask,\n            sem='relaxed')\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "3b0a0e36-3975-461e-a2f0-89ae6a80bd12"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT'])\n@triton.jit\ndef compute_final_dg(dg, o, T: 'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_o = tl.make_block_ptr(dg + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,)\n        )\n    b_o = tl.load(p_o, boundary_check=(0,))\n    b_o = b_o - tl.cumsum(b_o, axis=0) + tl.sum(b_o, axis=0)\n    p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_o, b_o, boundary_check=(0,))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f37b2838-c40c-4226-ade9-9e4cd00face8"
  },
  {
    "input": "@triton.autotune(list(filter(keep, configs)), key=['N_CTX', 'HEAD_DIM'])\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, HEAD_DIM: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', STAGE: 'tl.constexpr'):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    v_order: 'tl.constexpr' = (0, 1\n        ) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, HEAD_DIM), order=v_order)\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(HEAD_DIM,\n        N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape\n        =(HEAD_DIM, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    q = tl.load(Q_block_ptr)\n    if STAGE & 1:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 4 -\n            STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5)\n    if STAGE & 2:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 2,\n            offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5)\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "1b2afcc1-8ad6-4293-9a46-29d2164def0e"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128},\n    num_warps=8, num_stages=1)], key=['CACHE_KEY_SEQLEN_Q',\n    'CACHE_KEY_SEQLEN_K', 'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    else:\n        raise ValueError(\"BIAS_TYPE must be one of {'vector', 'matrix'}\")\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            else:\n                raise ValueError(\n                    \"BIAS_TYPE must be one of {'vector', 'matrix'}\")\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_n = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_n[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "43acfa08-b38f-47a5-b9e3-abfbc754e8c7"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef p_loss_forward_kernel(input_pointer, target_pointer, output_pointer,\n    size, p_loss: 'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the L1 or squared L2 norm of the difference between the input\n    and target (i.e., mean absolute error or mean squared error).\n\n    Args:\n        input_pointer: Pointer to the input.\n            The input must be of shape [size].\n        target_pointer: Pointer to the target.\n            The target must be of shape [size].\n        output_pointer: Pointer to a container the error is written to.\n            The container must be of shape [size] if reduction is 'none',\n            and otherwise of shape [size/BLOCK_SIZE].\n        size: Number of elements in the input and target.\n        p_loss: p-norm used to compute the error.\n            Options are 1 for MAE and 2 for MSE.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the error\n            across all entries, and 'sum' for summing the error across all entries.\n            If a reduction method is specified, the reduced result of each\n            program is written to a separate index in the output container,\n            which should later be summed.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input = tl.load(input_pointer + offset, mask=mask)\n    target = tl.load(target_pointer + offset, mask=mask)\n    diff = input - target\n    if p_loss == 1:\n        error = tl.abs(diff)\n    elif p_loss == 2:\n        error = diff * diff\n    if reduction == 'none':\n        tl.store(output_pointer + offset, error, mask=mask)\n    elif reduction == 'mean':\n        tl.store(output_pointer + pid, tl.sum(error) / size)\n    elif reduction == 'sum':\n        tl.store(output_pointer + pid, tl.sum(error))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "d40547b0-a55f-4d1e-8cf2-a92f7160938a"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "6a5a1709-77fd-4fcf-a8cc-88cde77be473"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_dquant_kernel(X, Y, W, scale, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    _max_x = 0.0\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        w = tl.load(W + cols, mask=mask)\n        norm = x * rstd * w\n        _max_x = tl.maximum(_max_x, tl.max(tl.abs(norm), axis=0))\n    scale_x = _max_x / 127.0\n    tl.store(scale + row, scale_x)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        w = tl.load(W + cols, mask=mask)\n        norm = x * rstd * w\n        norm = norm / scale_x\n        norm = tl.where(norm > 0, norm + 0.5, norm - 0.5)\n        tl.store(Y + cols, norm, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "0a187eec-6f01-4f8b-8e04-8eb49949036c"
  },
  {
    "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x(x, w, bias, y, stride_xn, stride_xc, stride_xh,\n    stride_xw, stride_wn, stride_wc, stride_wh, stride_ww, stride_yn,\n    stride_yc, stride_yh, stride_yw, delta_x_ptr, BATCH, IN_C, IN_H, IN_W,\n    KERNEL_N, KERNEL_H, KERNEL_W, OUT_H, OUT_W, stride_h, stride_w,\n    padding_h, padding_w, dilation_h, dilation_w, output_padding_h,\n    output_padding_w, groups, ACC_TYPE: 'tl.constexpr', CONV1X1_NHWC:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', GROUP_H: 'tl.constexpr', WITH_BIAS: 'tl.constexpr'\n    ):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_x_ptrs = delta_x_ptr + off_x_crs\n        off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n    mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n        off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[None, :]\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w, out_dtype=ACC_TYPE)\n        w_ptrs += BLOCK_K\n        if not CONV1X1_NHWC:\n            delta_x_ptrs += BLOCK_K\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS,\n                other=0)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            x_ptrs += BLOCK_K\n        mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n            off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[\n            None, :]\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    if WITH_BIAS:\n        acc += tl.load(bias + off_y_k)[None, :]\n    acc = acc\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "02ba8e35-bdb2-4f80-b4de-bf2871b58be6"
  },
  {
    "input": "@triton.jit\ndef bwd_prepare_wy_repr(A, cumsum, cumdecay, d_cumsum, d_cumdecay, dA, NT,\n    DK, BT: 'tl.constexpr', BK: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_dcumsum = d_cumsum + i_bh * NT * BT * DK + (i_t * BT + tl.arange(0,\n        BT)[:, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    p_dcumdecay = d_cumdecay + i_bh * NT * BT * DK + (i_t * BT + tl.arange(\n        0, BT)[:, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    p_cumsum = cumsum + i_bh * NT * BT * DK + (i_t * BT + tl.arange(0, BT)[\n        :, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    p_cumdecay = cumdecay + i_bh * NT * BT * DK + (i_t * BT + tl.arange(0,\n        BT)[:, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    o = tl.load(p_cumsum)\n    o2 = tl.load(p_cumdecay)\n    do = tl.load(p_dcumsum)\n    do2 = tl.load(p_dcumdecay)\n    p_A = A + i_bh * NT * BT * BT + i_t * BT * BT + tl.arange(0, BT) + (BT - 1\n        ) * BT\n    p_dA = dA + i_bh * NT * BT * BT + i_t * BT * BT + tl.arange(0, BT) + (BT -\n        1) * BT\n    for i in range(BT - 1, -1, -1):\n        attn = tl.load(p_A)\n        mask = tl.arange(0, BT) < i\n        attn = tl.where(mask, attn, 0)\n        mask2 = tl.arange(0, BT) == i\n        do_ = tl.sum(tl.where(mask2[:, None], do, 0), axis=0)\n        do2_ = tl.sum(tl.where(mask2[:, None], do2, 0), axis=0)\n        dA_ = tl.where(mask[:, None], o, 0) * do_[None, :] + tl.where(mask[\n            :, None], o2, 0) * do2_[None, :]\n        dA_ = tl.sum(dA_, axis=1)\n        tl.store(p_dA, -dA_)\n        do = do - attn[:, None] * do_[None, :]\n        do2 = do2 - attn[:, None] * do2_[None, :]\n        p_A -= BT\n        p_dA -= BT\n    p_dcumsum = d_cumsum + i_bh * NT * BT * DK + (i_t * BT + tl.arange(0,\n        BT)[:, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    p_dcumdecay = d_cumdecay + i_bh * NT * BT * DK + (i_t * BT + tl.arange(\n        0, BT)[:, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    tl.store(p_dcumsum, do)\n    tl.store(p_dcumdecay, do2)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "04a8c591-87c2-4f9d-8fb1-ff996070f38a"
  },
  {
    "input": "@triton.jit\ndef awq_gemm_kernel(a_ptr, b_ptr, c_ptr, zeros_ptr, scales_ptr, M, N, K,\n    group_size, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_z = tl.program_id(1)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_n\n    pid_n = pid % num_pid_n\n    accumulator_dtype = c_ptr.type.element_ty\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=\n        accumulator_dtype)\n    reverse_awq_order_tensor = ((tl.arange(0, 2) * 4)[None, :] + tl.arange(\n        0, 4)[:, None]).reshape(8)\n    shifts = reverse_awq_order_tensor * 4\n    shifts = tl.broadcast_to(shifts[None, :], (BLOCK_SIZE_K * (BLOCK_SIZE_N //\n        8), 8))\n    shifts = tl.reshape(shifts, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n    offsets_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    masks_am = offsets_am < M\n    offsets_bn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)\n    masks_bn = offsets_bn < N // 8\n    offsets_zn = pid_n * (BLOCK_SIZE_N // 8) + tl.arange(0, BLOCK_SIZE_N // 8)\n    masks_zn = offsets_zn < N // 8\n    offsets_sn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    masks_sn = offsets_sn < N\n    offsets_k = pid_z * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    offsets_a = K * offsets_am[:, None] + offsets_k[None, :]\n    offsets_b = N // 8 * offsets_k[:, None] + offsets_bn[None, :]\n    a_ptrs = a_ptr + offsets_a\n    b_ptrs = b_ptr + offsets_b\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        masks_k = offsets_k < K\n        masks_a = masks_am[:, None] & masks_k[None, :]\n        a = tl.load(a_ptrs, mask=masks_a)\n        masks_b = masks_k[:, None] & masks_bn[None, :]\n        b = tl.load(b_ptrs, mask=masks_b)\n        b = tl.interleave(b, b)\n        b = tl.interleave(b, b)\n        b = tl.interleave(b, b)\n        offsets_szk = (BLOCK_SIZE_K * SPLIT_K * k + pid_z * BLOCK_SIZE_K\n            ) // group_size + tl.arange(0, 1)\n        offsets_z = N // 8 * offsets_szk[:, None] + offsets_zn[None, :]\n        masks_zk = offsets_szk < K // group_size\n        masks_z = masks_zk[:, None] & masks_zn[None, :]\n        zeros_ptrs = zeros_ptr + offsets_z\n        zeros = tl.load(zeros_ptrs, mask=masks_z)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.interleave(zeros, zeros)\n        zeros = tl.broadcast_to(zeros, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n        offsets_s = N * offsets_szk[:, None] + offsets_sn[None, :]\n        masks_sk = offsets_szk < K // group_size\n        masks_s = masks_sk[:, None] & masks_sn[None, :]\n        scales_ptrs = scales_ptr + offsets_s\n        scales = tl.load(scales_ptrs, mask=masks_s)\n        scales = tl.broadcast_to(scales, (BLOCK_SIZE_K, BLOCK_SIZE_N))\n        b = b >> shifts & 15\n        zeros = zeros >> shifts & 15\n        b = (b - zeros) * scales\n        b = b\n        accumulator = tl.dot(a, b, accumulator, out_dtype=accumulator_dtype)\n        offsets_k += BLOCK_SIZE_K * SPLIT_K\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * (N // 8)\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + N * offs_cm[:, None] + offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "31937de9-8b9e-4f84-b671-202e3a0db3cd"
  },
  {
    "input": "@triton.jit\ndef _bwd_do_attention_kernel(O, Do, De, stride_ob, stride_om, stride_oh,\n    stride_og, stride_dob, stride_dom, stride_doh, stride_dog, stride_deb,\n    stride_deh, stride_deg, num_kv_heads, num_groups, headdim, seqlen_q,\n    BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr'):\n    off_q = tl.program_id(0)\n    off_bh = tl.program_id(1)\n    off_gp = tl.program_id(2)\n    off_b = off_bh // num_kv_heads\n    off_h = off_bh % num_kv_heads\n    offs_m = off_q * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o_ptrs = (O + off_b * stride_ob + off_h * stride_oh + off_gp *\n        stride_og + offs_m[:, None] * stride_om + offs_d[None, :])\n    do_ptrs = (Do + off_b * stride_dob + off_h * stride_doh + off_gp *\n        stride_dog + offs_m[:, None] * stride_dom + offs_d[None, :])\n    o = tl.load(o_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    do = tl.load(do_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None,\n        :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(De + off_b * stride_deb + off_h * stride_deh + off_gp *\n        stride_deg + offs_m, delta, mask=offs_m < seqlen_q)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a909bbce-e10d-4975-b168-93dc72ef0ad0"
  },
  {
    "input": "@triton.jit\ndef _bwd_log_and_diagonal_copy(out, out_grad, alpha_c, stride_alpha_c1,\n    stride_alpha_c2, stride_alpha_c3, stride_out0, stride_out1, batch, r,\n    BLOCK_R1: 'tl.constexpr', w):\n    b_idx = tl.program_id(0)\n    if b_idx >= batch:\n        return\n    mask = tl.arange(0, BLOCK_R1) < r\n    start = tl.program_id(1)\n    x = tl.load(out + b_idx * stride_out0 + start * stride_out1 + tl.arange\n        (0, BLOCK_R1), mask=mask, other=1)\n    out_log = 1 / (x + 1e-09)\n    do = tl.load(alpha_c + b_idx * stride_alpha_c1 + (start + w) *\n        stride_alpha_c2 + start * stride_alpha_c3 + tl.arange(0, BLOCK_R1),\n        mask=mask, other=0)\n    do *= out_log\n    tl.store(out_grad + b_idx * stride_out0 + start * stride_out1 + tl.\n        arange(0, BLOCK_R1), do, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "53b6418f-abd6-4b5a-a80c-73f6c51c0d76"
  },
  {
    "input": "@triton.jit\ndef bw_kernel_wMLP(grad_feature_grid, grad_feature_grid_sizes, feature_grid,\n    feature_grid_sizes, input_feature_grid, input_feature_grid_sizes,\n    directions, origins, grid_idx, near, far, splatting_feature, mask,\n    mlp_params, DIM_HIDDEN: 'tl.constexpr', DIM_IN: 'tl.constexpr', DIM_OUT:\n    'tl.constexpr', num_samples: 'tl.constexpr', num_samples_inf:\n    'tl.constexpr', num_rays: 'tl.constexpr', grid_channel: 'tl.constexpr',\n    NUM_GRIDS: 'tl.constexpr', feature_channel: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr',\n    contract_coords: 'tl.constexpr', disparity_at_inf: 'tl.constexpr',\n    grad_splatting_feature, grad_mlp_params, grad_input_feature_grid):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    depth = near_buffer\n    grad_splatting_feature_buffer = tl.zeros((BLOCK_SIZE, feature_channel),\n        dtype=tl.float32)\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        prev_vec = sample_grid_rep(input_feature_grid,\n            input_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, feature_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_vec = sample_grid_rep(grad_feature_grid,\n            grad_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, grid_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_vec = grad_vec * mask_buffer\n        fused_feature = feature_buffer + prev_vec\n        splat_grid_rep(grad_splatting, grad_input_feature_grid,\n            input_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, feature_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        grad_splatting_feature_buffer += grad_splatting\n    tl.store(grad_splatting_feature + offs_features,\n        grad_splatting_feature_buffer, mask=offs_features_mask)\n    update_mlp_params(grad_mlp_params, DIM_IN, DIM_HIDDEN)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d2587341-2db0-43fc-8878-595fa98b4987"
  },
  {
    "input": "@triton.jit\ndef gemm_kernel_tma(a_desc_ptr, b_desc_ptr, c_desc_ptr, prob_m, prob_n,\n    prob_k, block_m: 'tl.constexpr', block_n: 'tl.constexpr', block_k:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(prob_m, block_m)\n    num_pid_k = tl.cdiv(prob_k, block_k)\n    pid_m = pid % num_pid_m\n    pid_n = pid // num_pid_m\n    offs_am = pid_m * block_m\n    offs_bn = pid_n * block_n\n    offs_k = 0\n    accumulator = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for kk in range(0, num_pid_k):\n        a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k],\n            [block_m, block_k], tl.float8e4nv)\n        b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k],\n            [block_n, block_k], tl.float8e4nv)\n        accumulator = tl.dot(a, b.T, acc=accumulator, out_dtype=tl.float32)\n        offs_k += block_k\n    accumulator = accumulator\n    tl._experimental_descriptor_store(c_desc_ptr, accumulator, [offs_am,\n        offs_bn])\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "1307a34c-d905-4ee1-9404-e1be89927bab"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_X': 64}, num_warps=2),\n    triton.Config({'BLOCK_X': 128}, num_warps=2), triton.Config({'BLOCK_X':\n    256}, num_warps=2), triton.Config({'BLOCK_X': 128}, num_warps=4),\n    triton.Config({'BLOCK_X': 256}, num_warps=4)], key=['NUM_COLUMNS'])\n@triton.jit\ndef _padded_copy_wgrad(x, grad, wgrad, indices, bin_ids, bins, padded_bins,\n    NUM_COLUMNS: 'tl.constexpr', TOP_K: 'tl.constexpr', BLOCK_X: 'tl.constexpr'\n    ):\n    index_out = tl.load(indices + tl.program_id(0))\n    bin_idx = tl.load(bin_ids + tl.program_id(0))\n    offset_in_bin = tl.program_id(0)\n    if bin_idx > 0:\n        offset_in_bin -= tl.load(bins + bin_idx - 1)\n    index_x = offset_in_bin\n    if bin_idx > 0:\n        index_x += tl.load(padded_bins + bin_idx - 1)\n    wgrad += index_out\n    grad += tl.multiple_of(index_out // TOP_K * NUM_COLUMNS, NUM_COLUMNS)\n    x += tl.multiple_of(index_x * NUM_COLUMNS, NUM_COLUMNS)\n    offsets = tl.max_contiguous(tl.arange(0, BLOCK_X), BLOCK_X)\n    acc = tl.zeros((BLOCK_X,), dtype=tl.float32)\n    iterations = tl.cdiv(NUM_COLUMNS, BLOCK_X)\n    for i in range(iterations):\n        mask = offsets < NUM_COLUMNS\n        data = tl.load(x + offsets, mask=mask)\n        scale = tl.load(grad + offsets, mask=mask)\n        acc += data * scale\n        offsets += BLOCK_X\n    out = tl.sum(acc)\n    tl.store(wgrad, out)\n",
    "category": "Gradient Operations",
    "subcategory": "gradient accumulation",
    "uuid": "43894632-44bc-47fb-8d2f-2b43a4af3c73"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_OUT': 256,\n    'BLOCK_SIZE_HIDDEN': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_OUT': 256, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 128, 'BLOCK_SIZE_HIDDEN':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 64,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_OUT': 128, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 32, 'BLOCK_SIZE_HIDDEN':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 32,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_OUT': 64, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=5, num_warps\n    =2)], key=['hidden_size', 'out_size'], restore_value=['weights_ptr'])\n@triton.jit\ndef modifier_kernel(weights_ptr, assumed_wmax_ptr, reduced_assumed_wmax_ptr,\n    upper_end_of_slices_ptr, hidden_size, out_size, num_slices,\n    stride_weights_hidden_size, stride_weights_out_size,\n    stride_assumed_wmax_num_slices, stride_assumed_wmax_out_size,\n    modifier_type: 'tl.constexpr', modifier_weight_res: 'tl.constexpr',\n    modifier_seed, modifier_std: 'tl.constexpr', BLOCK_SIZE_HIDDEN:\n    'tl.constexpr', BLOCK_SIZE_OUT: 'tl.constexpr'):\n    \"\"\"\n    Modifier kernel for the weights.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offs_bn = (pid * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)) % out_size\n    offs_assumed_wmax = pid * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)\n    increase_weight_offsets_by = BLOCK_SIZE_HIDDEN * BLOCK_SIZE_OUT\n    weight_random_offsets = tl.arange(0, BLOCK_SIZE_HIDDEN * BLOCK_SIZE_OUT\n        ).reshape((BLOCK_SIZE_HIDDEN, BLOCK_SIZE_OUT), can_reorder=True)\n    ir_range_lower = 0\n    for slice_idx in range(0, num_slices):\n        abs_max_slice_ptrs = (assumed_wmax_ptr + slice_idx *\n            stride_assumed_wmax_num_slices + offs_bn *\n            stride_assumed_wmax_out_size)\n        if modifier_type == 'AddNormal' or (modifier_type == 'Discretize' or\n            modifier_type == 'DiscretizeAddNormal'):\n            assumed_wmax_per_slice = tl.load(reduced_assumed_wmax_ptr +\n                slice_idx)\n        else:\n            assumed_wmax_per_slice = tl.load(abs_max_slice_ptrs, mask=\n                offs_assumed_wmax < out_size, other=float('-inf'))\n            assumed_wmax_per_slice = assumed_wmax_per_slice[None, :]\n        ir_range_upper = tl.load(upper_end_of_slices_ptr + slice_idx)\n        current_lower = ir_range_lower\n        num_k = tl.cdiv(ir_range_upper - ir_range_lower, BLOCK_SIZE_HIDDEN)\n        for k in range(0, num_k):\n            current_upper = min(ir_range_upper, ir_range_lower + (k + 1) *\n                BLOCK_SIZE_HIDDEN, hidden_size)\n            offs_k = current_lower + tl.arange(0, BLOCK_SIZE_HIDDEN)\n            b_ptrs = weights_ptr + (offs_k[:, None] *\n                stride_weights_hidden_size + offs_bn[None, :] *\n                stride_weights_out_size)\n            weight_block = tl.load(b_ptrs, mask=offs_k[:, None] <\n                current_upper, other=0.0)\n            if (modifier_type == 'Discretize' or modifier_type ==\n                'DiscretizeAddNormal') or (modifier_type ==\n                'DiscretizePerChannel' or modifier_type ==\n                'DiscretizeAddNormalPerChannel'):\n                if modifier_weight_res > 0:\n                    n_states = max(modifier_weight_res, 1 / modifier_weight_res\n                        )\n                    res = 2 * assumed_wmax_per_slice / n_states\n                    weight_block = weight_block / res\n                    weight_block = tl.extra.cuda.libdevice.rint(weight_block)\n                    weight_block = weight_block * res\n            if (modifier_type == 'AddNormal' or modifier_type ==\n                'AddNormalPerChannel') or (modifier_type ==\n                'DiscretizeAddNormal' or modifier_type ==\n                'DiscretizeAddNormalPerChannel'):\n                randn_block = tl.randn(modifier_seed + pid,\n                    weight_random_offsets)\n                weight_random_offsets += increase_weight_offsets_by\n                randn_block = (assumed_wmax_per_slice * modifier_std *\n                    randn_block)\n                weight_block += randn_block\n            tl.store(b_ptrs, weight_block, mask=(offs_k[:, None] <\n                current_upper) & (offs_assumed_wmax[None, :] < out_size))\n            current_lower = current_upper\n        ir_range_lower = ir_range_upper\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "e585d11d-eea1-45b9-9bea-b34f2f63f3ce"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel16(q, k, v, w, u, o, h0, ht, s_k_h,\n    s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr',\n    K: 'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr'):\n    TargetDType = tl.bfloat16\n    TargetDType2 = tl.bfloat16\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_h = tl.zeros([BV, BK], dtype=TargetDType)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w.to(tl.float32))\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "d6a2bbdf-6c94-4623-b37c-7a07cdee7b00"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "6ab5580d-085c-453b-a2d6-1f46897507b3"
  },
  {
    "input": "@triton.jit\ndef _noncontiguous_block(input_tiles, next_id, next_next_id, pid_n, input,\n    other, output, K, N, stride_input_m, stride_input_k, stride_other_b,\n    stride_other_k, stride_other_n, stride_output_m, stride_output_n,\n    out_dtype: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', NUM_TILES:\n    'tl.constexpr', TILE_M: 'tl.constexpr', TILE_N: 'tl.constexpr', TILE_K:\n    'tl.constexpr', EVEN_K: 'tl.constexpr', EVEN_N: 'tl.constexpr'):\n    for _ in range(0, BLOCK_SIZE):\n        if next_id < NUM_TILES and next_id != -1:\n            start_off = tl.load(input_tiles + 5 * next_id + 2)\n            end_off = tl.load(input_tiles + 5 * next_id + 3)\n            length = end_off - start_off\n            if length > 0:\n                type_id = tl.load(input_tiles + 5 * next_id + 1)\n                for i in range(0, tl.cdiv(length, TILE_M)):\n                    _dispatch(pid_n, start_off + i * TILE_M, end_off, input,\n                        other + type_id * stride_other_b, output, K, N,\n                        stride_input_m, stride_input_k, stride_other_k,\n                        stride_other_n, stride_output_m, stride_output_n,\n                        out_dtype=out_dtype, MASK_M=True, EVEN_K=EVEN_K,\n                        EVEN_N=EVEN_N, TILE_M=TILE_M, TILE_N=TILE_N, TILE_K\n                        =TILE_K, DYNAMIC_TILING=True)\n            next_id = next_next_id\n            next_next_id += 1\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "dff4b13d-8de9-4556-a973-12b22787d226"
  },
  {
    "input": "@triton.autotune(config_gen(), key=['N_BLK', 'BLK1_IN', 'BLK2_OUT'], rep=80,\n    warmup=15)\n@triton.jit\ndef monarch_forward(x_ptr, o_ptr1, o_ptr2, w1_bfly_ptr, w2_bfly_ptr,\n    SEQ_DIM, N_BLK, BLK1_IN, BLK1_OUT: 'tl.constexpr', BLK2_OUT:\n    'tl.constexpr', stride_xl, stride_xm, stride_xk, stride_w1l, stride_w1r,\n    stride_w1k, stride_w2l, stride_w2n, stride_w2r, stride_o1l, stride_o1m,\n    stride_o1k, stride_o2l, stride_o2m, stride_o2n, BLOCK_SIZE_SEQ:\n    'tl.constexpr'=64, BLOCK_SIZE_N: 'tl.constexpr'=64, BLOCK_SIZE_K:\n    'tl.constexpr'=32, GROUP_SIZE_M: 'tl.constexpr'=8):\n    \"\"\"\n    Implements fused monarch forward as in `BlockdiagButterflyMultiply`.\n    \"\"\"\n    BLK2_IN: 'tl.constexpr' = BLK1_OUT\n    pid_batch = tl.program_id(0)\n    pid = tl.program_id(1)\n    num_pid_m = tl.cdiv(SEQ_DIM, BLOCK_SIZE_SEQ)\n    num_pid_n = tl.cdiv(N_BLK * BLK1_IN, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_SEQ\n    offs_bn = pid_n * BLOCK_SIZE_N\n    offs_k = 0\n    x_ptrs = tl.make_block_ptr(x_ptr + pid_batch * stride_xl, shape=(\n        SEQ_DIM, BLK1_IN), strides=(stride_xm, stride_xk), offsets=(offs_am,\n        offs_k), block_shape=(BLOCK_SIZE_SEQ, BLOCK_SIZE_K), order=(1, 0))\n    w1_ptrs = tl.make_block_ptr(w1_bfly_ptr + pid_batch * stride_w1l, shape\n        =(BLK1_OUT, BLK1_IN), strides=(stride_w1r, stride_w1k), offsets=(0,\n        offs_k), block_shape=(BLK1_OUT, BLOCK_SIZE_K), order=(1, 0))\n    w2_ptrs = tl.make_block_ptr(w2_bfly_ptr + pid_batch * stride_w2l, shape\n        =(BLK2_OUT, BLK2_IN), strides=(stride_w2n, stride_w2r), offsets=(\n        offs_bn, 0), block_shape=(BLOCK_SIZE_N, BLK2_IN), order=(1, 0))\n    out1_ptrs = tl.make_block_ptr(o_ptr1 + pid_batch * stride_o1l, shape=(\n        SEQ_DIM, BLK1_OUT), strides=(stride_o1m, stride_o1k), offsets=(\n        offs_am, offs_k), block_shape=(BLOCK_SIZE_SEQ, BLK1_OUT), order=(1, 0))\n    out2_ptrs = tl.make_block_ptr(o_ptr2, shape=(SEQ_DIM, N_BLK, BLK2_OUT),\n        strides=(stride_o2l, stride_o2m, stride_o2n), offsets=(offs_am,\n        pid_batch, offs_bn), block_shape=(BLOCK_SIZE_SEQ, 1, BLOCK_SIZE_N),\n        order=(2, 1, 0))\n    offs_am = pid_m * BLOCK_SIZE_SEQ + tl.arange(0, BLOCK_SIZE_SEQ)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x = tl.load(x_ptrs, boundary_check=(0, 1), eviction_policy=\n        'evict_first', padding_option='zero')\n    dtype = x.dtype\n    out1 = tl.zeros((BLOCK_SIZE_SEQ, BLK1_OUT), dtype=tl.float16 if dtype ==\n        tl.float16 else tl.float32)\n    for k in range(0, BLK1_IN, BLOCK_SIZE_K):\n        w1_bfly = tl.load(w1_ptrs, boundary_check=(0, 1), eviction_policy=\n            'evict_first', padding_option='zero')\n        w1_bfly = tl.trans(w1_bfly)\n        out1 += tl.dot(x, w1_bfly, out_dtype=tl.float16 if dtype == tl.\n            float16 else tl.float32)\n        x_ptrs = tl.advance(x_ptrs, (0, BLOCK_SIZE_K))\n        w1_ptrs = tl.advance(w1_ptrs, (0, BLOCK_SIZE_K))\n        x = tl.load(x_ptrs, boundary_check=(0, 1), eviction_policy=\n            'evict_first', padding_option='zero')\n    out1 = out1\n    tl.store(out1_ptrs, out1, boundary_check=(0,))\n    w2_bfly = tl.load(w2_ptrs, boundary_check=(0,), padding_option='zero')\n    w2_bfly = tl.trans(w2_bfly)\n    out2 = tl.dot(out1, w2_bfly)\n    tl.store(out2_ptrs, out2[:, None, :], boundary_check=(0, 2))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "ebe9e7e1-85fa-4f41-91cc-10c2d4097c50"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_V(q, k, z, A, s_k_h, s_k_t, s_k_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_q = b_q * tl.exp(b_zn[None, :] - b_z) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k = tl.exp(b_k - b_zn[:, None])\n        b_A = tl.dot(b_q, b_k, allow_tf32=False)\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_k * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_k = tl.load(p_k, boundary_check=(0,))\n            b_A = tl.sum(b_q * tl.exp(b_k[None, :] - b_z) * scale, 1)\n            b_A = tl.where(o_i >= j, b_A, 0.0)\n            tl.store(A + o_A + j, b_A, mask=m_A)\n            p_k = tl.advance(p_k, (K,))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a81b59c5-7642-4820-b007-21cdca89a4e1"
  },
  {
    "input": "@triton.jit\ndef apply_act_func_grad(output_grad, input, drop_p, seed, offset, param,\n    act_func: 'tl.constexpr', dropout: 'tl.constexpr'):\n    \"\"\"\n    Calculates the gradient of an activation function.\n\n    Args:\n        output_grad: Output gradients. The output gradients must be\n            loaded and cannot be a pointer.\n        input: Input. The input must be loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        offset: Offset to generate the dropout mask for if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function whose gradient is calculated.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n\n    Returns:\n        Gradient of the desired activation function.\n    \"\"\"\n    if act_func == 'sigmoid':\n        input = input\n        output = sigmoid_grad(input)\n    elif act_func == 'tanh':\n        input = input\n        output = tanh_grad(input)\n    elif act_func == 'relu':\n        output = relu_grad(input)\n    elif act_func == 'gelu':\n        input = input\n        output = gelu_grad(input)\n    elif act_func == 'silu':\n        input = input\n        output = silu_grad(input)\n    elif act_func == 'relu6':\n        output = relu6_grad(input)\n    elif act_func == 'hardsigmoid':\n        output = hardsigmoid_grad(input)\n    elif act_func == 'hardswish':\n        output = hardswish_grad(input)\n    elif act_func == 'selu':\n        input = input\n        output = selu_grad(input)\n    elif act_func == 'mish':\n        input = input\n        output = mish_grad(input)\n    elif act_func == 'leaky_relu':\n        output = leaky_relu_grad(input, param)\n    if dropout:\n        output_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)\n    return output_grad * output\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "b3fb38af-45c3-42f7-8d20-bbb59c87f14e"
  },
  {
    "input": "@triton.jit\ndef get_flat_tid():\n    return tl.inline_asm_elementwise(\n        \"\"\"\n        {\n            .reg .u32   %tmp32_<2>;\n\n            mov.u32     %tmp32_0, %tid.z;\n            mov.u32     %tmp32_1, %ntid.y;\n            mul.lo.u32  %tmp32_0, %tmp32_0, %tmp32_1; // tid.z * ntid.y\n            mov.u32     %tmp32_1, %ntid.x;\n            mul.lo.u32  $0, %tmp32_0, %tmp32_1;       // $0 = tid.z * ntid.y * ntid.x\n            mov.u32     %tmp32_0, %tid.y;\n            mov.u32     %tmp32_1, %ntid.x;\n            mul.lo.u32  %tmp32_0, %tmp32_0, %tmp32_1; // tid.y * ntid.x\n            add.u32     $0, $0, %tmp32_0;             // $0 += tid.y * ntid.x\n            mov.u32     %tmp32_0, %tid.x;\n            add.u32     $0, $0, %tmp32_0;             // $0 += tid.x\n        }\n        \"\"\"\n        , '=r', [], dtype=tl.int32, is_pure=True, pack=1)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "d73e6d44-8f07-4402-9d88-cc4e0c4ac2e6"
  },
  {
    "input": "@triton.jit\ndef elementwise_mul_kernel(x, g, N: 'tl.constexpr', B: 'tl.constexpr'):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by x with the value pointed by g.\n    The multiplication is performed in-place on the tensor pointed by x.\n\n    Parameters:\n    x:\n        Pointer to the input tensor.\n    g:\n        Pointer to the gradient output value.\n    N (int):\n        The number of columns in the input tensor.\n    B (int):\n        The block size for Triton operations.\n    \"\"\"\n    i_x = tl.program_id(0)\n    o_x = i_x * B + tl.arange(0, B)\n    b_g = tl.load(g)\n    b_x = tl.load(x + o_x, mask=o_x < N)\n    tl.store(x + o_x, b_x * b_g, mask=o_x < N)\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "fe95884a-aa31-49d6-a909-5953f1cb6851"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_backward_kernel(X_ptr, W_ptr, Mean_ptr, RSTD_ptr, DX_ptr,\n    DW_ptr, DB_ptr, DY_ptr, stride_x, stride_dx, stride_dw, stride_db,\n    stride_dy, n_rows, n_cols, rows_per_program: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', dtype: 'tl.constexpr'):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/ops/triton/layer_norm.py\n    \"\"\"\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    row_end = min((row_block_id + 1) * rows_per_program, n_rows)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < n_cols\n    dw_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    db_row = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    X_ptr += row_start * stride_x\n    Mean_ptr += row_start\n    RSTD_ptr += row_start\n    DX_ptr += row_start * stride_dx\n    DY_ptr += row_start * stride_dy\n    for _ in range(row_start, row_end):\n        x = tl.load(X_ptr + cols, mask=mask, other=0.0)\n        w = tl.load(W_ptr + cols, mask=mask, other=0.0)\n        dy = tl.load(DY_ptr + cols, mask=mask, other=0.0)\n        mean = tl.load(Mean_ptr)\n        rstd = tl.load(RSTD_ptr)\n        x_hat = (x - mean) * rstd\n        wdy = w * dy\n        c1 = tl.sum(x_hat * wdy, axis=0) / n_cols\n        c2 = tl.sum(wdy, axis=0) / n_cols\n        dx = (wdy - (x_hat * c1 + c2)) * rstd\n        tl.store(DX_ptr + cols, dx, mask=mask)\n        dw_row += dy * x_hat\n        db_row += dy\n        X_ptr += stride_x\n        Mean_ptr += 1\n        RSTD_ptr += 1\n        DX_ptr += stride_dx\n        DY_ptr += stride_dy\n    tl.store(DW_ptr + row_block_id * stride_dw + cols, dw_row, mask=mask)\n    tl.store(DB_ptr + row_block_id * stride_db + cols, db_row, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "fbd89126-7919-47c5-b84c-528c0ca3cda0"
  },
  {
    "input": "@triton.jit\ndef vector_add(a_ptr, b_ptr, c_ptr, num_elements, block_size: 'constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * block_size\n    thread_offsets = block_start + tl.arange(0, block_size)\n    mask = offsets < num_elements\n    a = tl.load(a_ptr + offsets, mask=mask)\n    b = tl.load(b_ptr + offsets, mask=mask)\n    output = a + b\n    tl.store(c_ptr, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "e3f1bf13-518d-4a91-a49a-a8b93e395b0b"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride, BLOCK_SIZE:\n    'tl.constexpr', HAS_SMOOTHING: 'tl.constexpr', SPLIT: 'tl.constexpr',\n    PRECOMPUTED_LSE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    sum_logits = 0.0\n    if not PRECOMPUTED_LSE:\n        m_i = -float('inf')\n        l_i = 0.0\n        for col_offset in range(0, n_cols, BLOCK_SIZE):\n            cols = col_offset + tl.arange(0, BLOCK_SIZE)\n            logits = tl.load(logits_ptr + cols, mask=cols < n_cols, other=-\n                float('inf')) * logit_scale\n            if HAS_SMOOTHING:\n                sum_logits += tl.sum(tl.where(cols < n_cols, logits, 0.0))\n            m_i_new = tl.maximum(m_i, tl.max(logits))\n            l_i = tl.exp(m_i - m_i_new) * l_i + tl.sum(tl.exp(logits - m_i_new)\n                )\n            m_i = m_i_new\n        lse = tl.log(l_i) + m_i\n        tl.store(lse_ptr + row_idx, lse)\n    else:\n        lse = tl.load(lse_ptr + row_idx)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx == ignore_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= 0 and label_idx < n_cols:\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - smoothing * sum_logits / total_classes - (1 - smoothing\n                    ) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits /\n                total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + row_idx, z_loss)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "8fb2b515-165e-4a39-96d8-4fcebf6b8432"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_kernel(x_ptr, h1_ptr, w_ptr, eps, stride, N_COLS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', INCLUDE_WEIGHT: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    x_ptr += row * stride\n    h1_ptr += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(x_ptr + cols, mask=cols < N_COLS, other=0.0,\n            eviction_policy='evict_last')\n        _mean += a * a\n    rstd = rsqrt(tl.sum(_mean, axis=0) / N_COLS + eps)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        a = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first')\n        if INCLUDE_WEIGHT:\n            w = tl.load(w_ptr + cols, mask=mask)\n            tl.store(h1_ptr + cols, a * rstd * w, mask=mask)\n        else:\n            tl.store(h1_ptr + cols, a * rstd, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "6b21dedb-6287-452c-80e4-943b4bcffd58"
  },
  {
    "input": "@triton.jit\ndef _chunked_cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE: 'tl.constexpr', N_CHUNKS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        256K vocab divided in 4 chunks\n\n        |-65536-| |-65536-| |-65536-| |-65536-|\n        |-------| |-------| |-------| |-------|\n        |-------| |-------| |-------| |-------|\n\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        Notice we can do logsumexp for each chunk and then\n        logsumexp[chunk_sum(logsumexp)] == logsumexp\n\n        chunk_sum = log[chunk_sum(logsumexp)]\n                  = log[exp(logsumexp(a)) + ... + exp(logsumexp(z))]\n                  = log[exp(log[sum(exp(a))]) + ... + exp(log[sum(exp(z))])]\n                  = log[sum(exp(a)) + ... + sum(exp(z))]\n                  = logsumexp(x)\n\n        This means we can perform a logsumexp for each chunk, then do a\n        final logsumexp reduction!\n\n        Ie do: logsumexp(chunked_logsumexp) - x\n    \"\"\"\n    row_idx = tl.program_id(0)\n    chunk_idx = tl.program_id(1)\n    logits_ptr += row_idx * logits_row_stride\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx\n    labels_ptr += row_idx\n    col_offsets = chunk_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if chunk_idx == 0:\n        if label_idx != -100:\n            x = tl.load(logits_ptr + label_idx)\n            loss = -1.0 * x\n        else:\n            loss = 0.0\n        tl.store(loss_ptr, loss)\n    pass\n    tl.store(logsumexp_ptr, logsumexp)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "7abb6962-16e0-4ca9-8f02-16ec328be619"
  },
  {
    "input": "@triton.jit\ndef _bwd_dkv_kernel(K, V, dK, dV, dS, stride_qk_bh, stride_qk_l,\n    stride_qk_d, stride_s_bh, stride_s_dk, stride_s_dv, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', L: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', BL: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    start_kv, start_m, off_bs_head = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = start_kv // NV\n    i_v = start_kv % NV\n    qkv_base_offset = off_bs_head * stride_qk_bh\n    K_block_ptr = tl.make_block_ptr(base=K + qkv_base_offset, shape=(L, DK),\n        strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, 0),\n        block_shape=(BL, BK), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + qkv_base_offset, shape=(L, DV),\n        strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, 0),\n        block_shape=(BL, BV), order=(1, 0))\n    dS_block_ptr = tl.make_block_ptr(base=dS + off_bs_head * stride_s_bh,\n        shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(0, 0),\n        block_shape=(BK, BV), order=(1, 0))\n    dk = tl.zeros([BL, BK], dtype=tl.float32)\n    dv = tl.zeros([BL, BV], dtype=tl.float32)\n    ds = tl.load(dS_block_ptr, boundary_check=(0, 1))\n    k = tl.load(K_block_ptr, boundary_check=(0, 1))\n    v = tl.load(V_block_ptr, boundary_check=(0, 1))\n    v = v * scale\n    dk += tl.dot(v, tl.trans(ds), allow_tf32=False)\n    dv += tl.dot(k, ds, allow_tf32=False) * scale\n    dK_block_ptr = tl.make_block_ptr(base=dK + qkv_base_offset, shape=(L,\n        DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, 0),\n        block_shape=(BL, BK), order=(1, 0))\n    dV_block_ptr = tl.make_block_ptr(base=dV + qkv_base_offset, shape=(L,\n        DV), strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, 0),\n        block_shape=(BL, BV), order=(1, 0))\n    tl.store(dK_block_ptr, dk, boundary_check=(0, 1))\n    tl.store(dV_block_ptr, dv, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "21847748-f175-49dd-ba11-3ea835c90ed4"
  },
  {
    "input": "@triton.jit\ndef jagged_recover_first_or_last_1D_kernel(JaggedIn_no_first,\n    JaggedIn_no_last, JaggedOut, OffsetsOut, BLOCK_N: 'tl.constexpr'):\n    off_z = tl.program_id(0)\n    group_n = tl.program_id(1)\n    in_no_last_seq_start = tl.load(OffsetsOut + off_z) - off_z\n    in_no_last_seq_end = tl.load(OffsetsOut + off_z + 1) - off_z - 1\n    in_seq_len = in_no_last_seq_end - in_no_last_seq_start\n    out_seq_start = tl.load(OffsetsOut + off_z)\n    off_n = group_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    x_first = tl.load(JaggedIn_no_last + in_no_last_seq_start + off_n, mask\n        =off_n == 0)\n    tl.store(JaggedOut + out_seq_start + off_n, x_first, mask=off_n == 0)\n    x = tl.load(JaggedIn_no_last + in_no_last_seq_start + off_n, mask=(\n        off_n < in_seq_len) & (off_n > 0)) + tl.load(JaggedIn_no_first +\n        in_no_last_seq_start + off_n - 1, mask=(off_n < in_seq_len) & (\n        off_n > 0))\n    tl.store(JaggedOut + out_seq_start + off_n, x, mask=(off_n < in_seq_len\n        ) & (off_n > 0))\n    x_last = tl.load(JaggedIn_no_first + in_no_last_seq_start + off_n, mask\n        =off_n == in_seq_len - 1)\n    tl.store(JaggedOut + out_seq_start + off_n + 1, x_last, mask=off_n == \n        in_seq_len - 1)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "2e7441b9-3d23-4f92-a6b4-426b6f8b1c15"
  },
  {
    "input": "@triton.jit\ndef rms_layer_norm_fwd_fused(X, Y, W, RMS, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    row = tl.program_id(axis=0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    mean_ = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for i in range(0, N, BLOCK_SIZE):\n        offset = i + tl.arange(0, BLOCK_SIZE)\n        mask = offset < N\n        x = tl.load(X + offset, mask=mask, other=0.0)\n        mean_ += x * x\n    mean = tl.sum(mean_, axis=0) / N\n    rms = 1 / tl.sqrt(mean + eps)\n    tl.store(RMS + row, rms)\n    for i in range(0, N, BLOCK_SIZE):\n        offset = i + tl.arange(0, BLOCK_SIZE)\n        mask = offset < N\n        x = tl.load(X + offset, mask=mask, other=0.0)\n        x_hat = x * rms\n        w = tl.load(W + offset, mask=mask)\n        y = x_hat * w\n        tl.store(Y + offset, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "fda2eb8d-4cbe-47fe-9608-9e25647947de"
  },
  {
    "input": "@triton.jit\ndef debug_fill_dropout_rng_tensor(R, stride_rz, stride_rh, stride_rm,\n    stride_rn, seqlen_q, seqlen_k, philox_seed_ptr, philox_offset_base_ptr,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    philox_seed = tl.load(philox_seed_ptr)\n    philox_offset_base = tl.load(philox_offset_base_ptr)\n    debug_fill_dropout_rng(R, stride_rz, stride_rh, stride_rm, stride_rn,\n        seqlen_q, seqlen_k, philox_seed, philox_offset_base, BLOCK_M, BLOCK_N)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "63bbefa1-dd64-42e4-af27-cfe3c13fa97a"
  },
  {
    "input": "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})\n@triton.jit\ndef fused_recurrent_retention_fwd_kernel(q, k, v, o, h0, ht, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr',\n    HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_o = o + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_o = o + (i_k * B + i_b) * T * H * V + i_h * V + i_v * BV + tl.arange(\n            0, BV)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_h, other=0)\n    for _ in range(0, T):\n        b_q = tl.load(p_q, mask=mask_k, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_h = b_b * b_h + b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o, mask=mask_v)\n        p_q += K if HEAD_FIRST else H * K\n        p_k += K if HEAD_FIRST else H * K\n        p_v += V if HEAD_FIRST else H * V\n        p_o += V if HEAD_FIRST else H * V\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_h)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "726a2274-cb90-40e5-8678-c2d09ca6a05f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({\n    'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({\n    'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({\n    'BLOCK_N': 1024})], key=['ncols'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['OUT'] is not None})\n@triton.jit\ndef _swiglu_bwd_kernel(X, Y, DOUT, OUT, DX, DY, stride_x_row, stride_y_row,\n    stride_dout_row, stride_out_row, stride_dx_row, stride_dy_row, ncols,\n    BLOCK_N: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    DOUT += row * stride_dout_row\n    if RECOMPUTE_OUTPUT:\n        OUT += row * stride_out_row\n    DX += row * stride_dx_row\n    DY += row * stride_dy_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0)\n    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0)\n    x_sigmoid = tl.sigmoid(x)\n    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\n    dy = x * x_sigmoid * dout\n    tl.store(DX + cols, dx, mask=cols < ncols)\n    tl.store(DY + cols, dy, mask=cols < ncols)\n    if RECOMPUTE_OUTPUT:\n        out = x * x_sigmoid * y\n        tl.store(OUT + cols, out, mask=cols < ncols)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "985d28a5-e4f9-49ae-a74b-33d8f52a52cb"
  },
  {
    "input": "@triton.jit\ndef _seeded_dropout(x_ptr, output_ptr, n_elements, p, seed, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE * 4\n    offset = block_start + tl.arange(0, BLOCK_SIZE)\n    r0, r1, r2, r3 = tl.random.rand4x(seed, offset)\n    scale = 1.0 / (1.0 - p)\n    for i in tl.static_range(4):\n        curr_offset = offset + BLOCK_SIZE * i\n        mask = curr_offset < n_elements\n        x = tl.load(x_ptr + curr_offset, mask=mask)\n        r = tl.where(i == 0, r0, tl.where(i == 1, r1, tl.where(i == 2, r2, r3))\n            )\n        keep = r > p\n        output = tl.where(keep, x * scale, 0.0)\n        tl.store(output_ptr + curr_offset, output, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "6443a3eb-d910-435e-97c0-f81e7134010a"
  },
  {
    "input": "@triton.jit\ndef _rotary_embedding_kernel(q_rot_ptr, k_rot_ptr, q_ptr, k_ptr, cos_ptr,\n    sin_ptr, seq_len, batch_size, num_heads, num_kv, hidden_size, q_strides,\n    q_strideb, q_strideh, q_strided, k_strides, k_strideb, k_stridekv,\n    k_strided, seq_offset, BLOCK_SIZE_SEQ: 'tl.constexpr', BLOCK_SIZE_BH:\n    'tl.constexpr', BLOCK_SIZE_D: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_bh_blocks = tl.cdiv(batch_size * num_heads, BLOCK_SIZE_BH)\n    num_d_blocks = tl.cdiv(hidden_size // 2, BLOCK_SIZE_D)\n    bh_id = pid % num_bh_blocks\n    d_id = pid // num_bh_blocks % num_d_blocks\n    seq_block_id = pid // num_bh_blocks // num_d_blocks\n    seq_offs = seq_offset + seq_block_id * BLOCK_SIZE_SEQ + tl.arange(0,\n        BLOCK_SIZE_SEQ)\n    bh_offs = bh_id * BLOCK_SIZE_BH + tl.arange(0, BLOCK_SIZE_BH)\n    q_common_offs = seq_offs[:, None, None] * q_strides + bh_offs[None, :, None\n        ] * q_strideh\n    k_common_offs = seq_offs[:, None, None] * k_strides + bh_offs[None, :, None\n        ] // (num_heads // num_kv) * k_stridekv\n    q_base_offs, qo_base_offs = (q_ptr + q_common_offs, q_rot_ptr +\n        q_common_offs)\n    k_base_offs, ko_base_offs = (k_ptr + k_common_offs, k_rot_ptr +\n        k_common_offs)\n    c_base_offs = cos_ptr + seq_offs[:, None] * hidden_size\n    s_base_offs = sin_ptr + seq_offs[:, None] * hidden_size\n    hidden_block_range = tl.arange(0, BLOCK_SIZE_D)\n    hidden_offs_l = d_id * BLOCK_SIZE_D + hidden_block_range\n    hidden_offs_r = hidden_size // 2 + hidden_offs_l\n    mask_l, mask_r = (hidden_offs_l < hidden_size // 2, hidden_offs_r <\n        hidden_size)\n    mask_bh = bh_offs < batch_size * num_heads\n    mask_seq = seq_offs < seq_len\n    mask_bh_seq = mask_bh[None, :, None] & mask_seq[:, None, None]\n    q_l, k_l = tl.load(q_base_offs + hidden_offs_l[None, None, :] *\n        q_strided, mask=mask_l[None, None, :] & mask_bh_seq, other=0), tl.load(\n        k_base_offs + hidden_offs_l[None, None, :] * k_strided, mask=mask_l\n        [None, None, :] & mask_bh_seq, other=0)\n    q_r, k_r = tl.load(q_base_offs + hidden_offs_r[None, None, :] *\n        q_strided, mask=mask_r[None, None, :] & mask_bh_seq, other=0), tl.load(\n        k_base_offs + hidden_offs_r[None, None, :] * k_strided, mask=mask_r\n        [None, None, :] & mask_bh_seq, other=0)\n    cos_l, cos_r = tl.load(c_base_offs + hidden_offs_l[None, :], mask=\n        mask_l[None, :], other=0)[:, None, :], tl.load(c_base_offs +\n        hidden_offs_r[None, :], mask=mask_r[None, :], other=0)[:, None, :]\n    sin_l, sin_r = tl.load(s_base_offs + hidden_offs_l[None, :], mask=\n        mask_l[None, :], other=0)[:, None, :], tl.load(s_base_offs +\n        hidden_offs_r[None, :], mask=mask_r[None, :], other=0)[:, None, :]\n    qo_l = q_l * cos_l - q_r * sin_l\n    tl.store(qo_base_offs + hidden_offs_l, qo_l, mask=mask_l[None, None, :] &\n        mask_bh_seq)\n    qo_r = q_r * cos_r + q_l * sin_r\n    tl.store(qo_base_offs + hidden_offs_r, qo_r, mask=mask_r[None, None, :] &\n        mask_bh_seq)\n    ko_l = k_l * cos_l - k_r * sin_l\n    tl.store(ko_base_offs + hidden_offs_l, ko_l, mask=mask_l[None, None, :] &\n        mask_bh_seq)\n    ko_r = k_r * cos_r + k_l * sin_r\n    tl.store(ko_base_offs + hidden_offs_r, ko_r, mask=mask_r[None, None, :] &\n        mask_bh_seq)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "6e1a785f-0574-49a9-afca-b8fa3e7474d1"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_unidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr',\n    BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp2\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp2\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y1 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y2 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y3 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y4 + _idx, _x, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "0a521083-f37b-43b2-ba32-31a0f3a3610e"
  },
  {
    "input": "@triton.jit\ndef parallel_retention_bwd_kernel(q, k, v, do, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5815ca1f-a087-4c7e-950d-64662c79a428"
  },
  {
    "input": "@triton.jit\ndef gelu_grad(x):\n    tanh_out = tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n    return 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + \n        0.1070322243 * x * x)) + 0.5 * (1 + tanh_out)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "c9b03785-f36b-4c59-a18e-49e344466ecb"
  },
  {
    "input": "@triton.jit\ndef _fused_linear_kernel_fwd(x_ptr, w_ptr, z_ptr, M, N, K, b_ptr=None,\n    r_ptr=None, apply_silu=False, seed=1337, BLOCK_SIZE_M: 'tl.constexpr'=\n    128, BLOCK_SIZE_N: 'tl.constexpr'=128, BLOCK_SIZE_K: 'tl.constexpr'=64):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)[:, None]\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)[None, :]\n    z = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        x_k = tl.arange(0, BLOCK_SIZE_K)[None, :] + k\n        x = tl.load(x_ptr + offs_m * K + x_k, mask=(offs_m < M) & (x_k < K),\n            other=0.0)\n        x = x\n        w_k = tl.arange(0, BLOCK_SIZE_K)[:, None] + k\n        w = tl.load(w_ptr + w_k * N + offs_n, mask=(w_k < K) & (offs_n < N),\n            other=0.0)\n        w = w\n        z = tl.dot(x, w, acc=z)\n    if b_ptr is not None:\n        b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n        z += b\n    z_offset = offs_m * N + offs_n\n    z_mask = (offs_m < M) & (offs_n < N)\n    if apply_silu:\n        z = silu(z)\n    if r_ptr is not None:\n        r = tl.load(r_ptr + z_offset, mask=z_mask)\n        z += r\n    tl.store(z_ptr + z_offset, z, mask=z_mask)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "57dc3d5f-4042-4b38-8c34-c56faf7e5805"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel_forward(out_ptr, inp_ptr, inp_stride, out_stride,\n    seq_len, is_causal, BLOCK_SIZE: 'tl.constexpr', num_warps: 'tl.constexpr'):\n    batch_idx = tl.program_id(0)\n    batch_start_ptr = inp_ptr + batch_idx * inp_stride\n    pos_offsets = tl.arange(0, BLOCK_SIZE)\n    batch_ptrs = batch_start_ptr + pos_offsets\n    valid_mask = pos_offsets < seq_len\n    logits = tl.load(batch_ptrs, mask=valid_mask, other=-float('inf'))\n    if is_causal:\n        attn_mask = pos_offsets > batch_idx % seq_len\n        logits = logits + tl.where(attn_mask, -float('inf'), 0.0)\n    shifted_logits = logits - tl.max(logits, axis=0)\n    exp_logits = tl.exp(shifted_logits)\n    sum_exp = tl.sum(exp_logits, axis=0)\n    probs = exp_logits / sum_exp\n    out_batch_ptr = out_ptr + batch_idx * out_stride\n    out_ptrs = out_batch_ptr + pos_offsets\n    tl.store(out_ptrs, probs, mask=valid_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "6ef71501-5844-4ae6-93d7-09006a795786"
  },
  {
    "input": "@triton.jit\ndef load_fn(ptrs, offset_first, offset_second, _in_boundary_first,\n    _in_boundary_second):\n    boundary_first = _in_boundary_first\n    boundary_second = _in_boundary_second\n    \"\"\"\n    # Debugging GPU segfault\n    boundary_first = 0\n    boundary_second = 0\n    mask = (offset_first[:, None] < boundary_first) &            (offset_second[None, :] < boundary_second)\n    return tl.load(ptrs, mask=mask, other=0.0)\n    \"\"\"\n    if offset_first is not None and offset_second is not None:\n        mask = (offset_first[:, None] < boundary_first) & (offset_second[\n            None, :] < boundary_second)\n        tensor = tl.load(ptrs, mask=mask, other=0.0)\n    elif offset_first is not None:\n        mask = offset_first[:, None] < boundary_first\n        tensor = tl.load(ptrs, mask=mask, other=0.0)\n    elif offset_second is not None:\n        mask = offset_second[None, :] < boundary_second\n        tensor = tl.load(ptrs, mask=mask, other=0.0)\n    else:\n        tensor = tl.load(ptrs)\n    return tensor\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "8a6d9d8a-318a-4be5-ad45-4951a6838b2a"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_V(k, v, h, g, A, do, dh, dq, dk, dv, dA, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NG:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    n_bh = tl.num_programs(2)\n    o_t = min(i_t * BT + BT, T)\n    o_k = i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bg * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * K + (o_t - 1) *\n        K + o_k, BK), BK)\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    m_k = o_k < K\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gn = tl.exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)\n    b_k = b_k * b_gn\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bg * NT * K * V + i_t * V * K, (V, K),\n            (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * T * V, (T, V),\n            (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dh = b_dh\n        b_dv = tl.dot(b_k, b_dh)\n        if i_k == 0:\n            b_dv += tl.dot(b_A, b_do)\n        b_do = b_do * scale\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, tl.trans(b_dh))\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * b_gn\n    p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_dA = tl.where(m_s, b_dA, 0.0)\n    if i_k == 0:\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "bf0b86b1-e83b-4b0b-a14b-2989cfc3d555"
  },
  {
    "input": "@triton.jit\ndef _softmax_kernel_fwd(output_ptr, output_row_stride, input_ptr,\n    input_row_stride, n_cols, block_size: 'tl.constexpr'):\n    row_index = tl.program_id(0)\n    input_row_ptr = input_ptr + row_index * input_row_stride\n    col_offsets = tl.arange(0, block_size)\n    input_ptrs = input_row_ptr + col_offsets\n    rw_mask = col_offsets < n_cols\n    row = tl.load(input_ptrs, mask=rw_mask, other=float('-inf'))\n    safe_row = row - tl.max(row, axis=0)\n    numerator = tl.exp(safe_row)\n    denom = tl.sum(numerator, axis=0)\n    sm_out = numerator / denom\n    out_row_ptr = output_ptr + row_index * output_row_stride\n    out_row_ptrs = out_row_ptr + col_offsets\n    tl.store(out_row_ptrs, sm_out, mask=rw_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "9bda97fe-654f-4ddc-912a-cc4199a59eb5"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8)], key=['M', 'N', 'K'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "663abda0-bc99-4e56-a283-ce647ae26404"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_inner(start_loop, end_loop, q, span_q, k_block_ptr,\n    v_block_ptr, bias_block_ptr, mask_block_ptr, k_start, k_end, seq_len_k,\n    acc, m_i, l_i, bias_advance: 'tl.constexpr', mask_advance:\n    'tl.constexpr', is_causal: 'tl.constexpr', use_attention_mask:\n    'tl.constexpr', use_k_start: 'tl.constexpr', use_k_end: 'tl.constexpr',\n    use_bias: 'tl.constexpr', block_k: 'tl.constexpr', use_mask_k:\n    'tl.constexpr', k_boundary_check: 'tl.constexpr', v_boundary_check:\n    'tl.constexpr', dot_fn_qk: 'tl.constexpr', dot_fn_kv: 'tl.constexpr'):\n    \"\"\"Triton MHA forward kernel's inner loop.\"\"\"\n    for start_k in range(start_loop, end_loop, block_k):\n        start_k = tl.multiple_of(start_k, block_k)\n        span_k = start_k + tl.arange(0, block_k)\n        k = tl.load(k_block_ptr, boundary_check=k_boundary_check,\n            padding_option='zero' if len(k_boundary_check.value) else '')\n        v = tl.load(v_block_ptr, boundary_check=v_boundary_check,\n            padding_option='zero' if len(v_boundary_check.value) else '')\n        if use_bias:\n            bias = tl.load(bias_block_ptr)\n        qk = dot_fn_qk(q, k)\n        if use_bias:\n            qk = qk & 4294967295\n            qk = qk\n            qk += bias\n        if use_attention_mask | use_k_start | use_k_end:\n            mask_value = float(jnp.finfo(jnp.float32).min)\n        if use_attention_mask:\n            mask = tl.load(mask_block_ptr)\n            qk = tl.where(mask, qk, mask_value)\n        if use_k_start:\n            if tl.sum(k_start) != 0:\n                qk = tl.where(k_start[:, None] <= span_k[None, :], qk,\n                    mask_value)\n        if is_causal:\n            qk = tl.where(span_q[:, None] >= span_k[None, :], qk, float('-inf')\n                )\n        elif use_k_end:\n            qk = tl.where(k_end[:, None] > span_k[None, :], qk, mask_value)\n        if use_mask_k:\n            qk = tl.where((span_k < seq_len_k)[None, :], qk, float('-inf'))\n        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))\n        p = tl.exp(qk - m_ij[:, None])\n        alpha = tl.exp(m_i - m_ij)\n        m_i = m_ij\n        acc *= alpha[:, None]\n        l_i *= alpha\n        l_i += tl.sum(p, axis=1)\n        acc += dot_fn_kv(p, v)\n        k_block_ptr = tl.advance(k_block_ptr, (0, block_k))\n        v_block_ptr = tl.advance(v_block_ptr, (block_k, 0))\n        bias_block_ptr = tl.advance(bias_block_ptr, bias_advance.value)\n        mask_block_ptr = tl.advance(mask_block_ptr, mask_advance.value)\n    return (k_block_ptr, v_block_ptr, bias_block_ptr, mask_block_ptr, acc,\n        m_i, l_i)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "bde82bd0-78a6-4f46-b1a2-6df9b52fd657"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}), triton.Config\n    ({'BLOCK_SIZE_H': 2}), triton.Config({'BLOCK_SIZE_H': 4}), triton.\n    Config({'BLOCK_SIZE_H': 8}), triton.Config({'BLOCK_SIZE_H': 16}),\n    triton.Config({'BLOCK_SIZE_H': 32}), triton.Config({'BLOCK_SIZE_H': 64}\n    )], key=['chunk_size', 'nheads'])\n@triton.jit\ndef _chunk_cumsum_fwd_kernel(dt_ptr, A_ptr, dt_bias_ptr, dt_out_ptr,\n    dA_cumsum_ptr, batch, seqlen, nheads, chunk_size, dt_min, dt_max,\n    stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head,\n    stride_dt_bias_head, stride_dt_out_batch, stride_dt_out_chunk,\n    stride_dt_out_head, stride_dt_out_csize, stride_dA_cs_batch,\n    stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, DT_SOFTPLUS:\n    'tl.constexpr', HAS_DT_BIAS: 'tl.constexpr', BLOCK_SIZE_H:\n    'tl.constexpr', BLOCK_SIZE_CHUNK: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\n    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    dt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] *\n        stride_dt_seqlen)\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    dt_out_ptrs = dt_out_ptr + (offs_h[:, None] * stride_dt_out_head + \n        offs_c[None, :] * stride_dt_out_csize)\n    dA_cs_ptrs = dA_cumsum_ptr + (offs_h[:, None] * stride_dA_cs_head + \n        offs_c[None, :] * stride_dA_cs_csize)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :\n        ] < chunk_size_limit), other=0.0)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=\n            offs_h < nheads, other=0.0)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt = tl.where(dt <= 20.0, softplus(dt), dt)\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] <\n        chunk_size_limit), dt, 0.0)\n    tl.store(dt_out_ptrs, dt, mask=(offs_h[:, None] < nheads) & (offs_c[\n        None, :] < chunk_size))\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0)\n    dA = dt * A[:, None]\n    dA_cs = tl.cumsum(dA, axis=1)\n    tl.store(dA_cs_ptrs, dA_cs, mask=(offs_h[:, None] < nheads) & (offs_c[\n        None, :] < chunk_size))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "687ae7ef-6ed3-40c5-85d4-d28ee0cf84b9"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n    stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr', start_n, start_m, num_steps, MASK: 'tl.constexpr'\n    ):\n    offs_m = start_m + tl.arange(0, BLOCK_M1)\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    offs_k = tl.arange(0, HEAD_DIM)\n    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n    curr_m = start_m\n    step_m = BLOCK_M1\n    for blk_idx in range(num_steps):\n        qT = tl.load(qT_ptrs)\n        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n        m = tl.load(M + offs_m)\n        qkT = tl.dot(k, qT)\n        pT = tl.math.exp2(qkT - m[None, :])\n        if MASK:\n            mask = offs_m[None, :] >= offs_n[:, None]\n            pT = tl.where(mask, pT, 0.0)\n        do = tl.load(do_ptrs)\n        ppT = pT\n        ppT = ppT\n        dv += tl.dot(ppT, do)\n        Di = tl.load(D + offs_m)\n        dpT = tl.dot(v, tl.trans(do))\n        dsT = pT * (dpT - Di[None, :])\n        dsT = dsT\n        dk += tl.dot(dsT, tl.trans(qT))\n        curr_m += step_m\n        qT_ptrs += step_m * stride_tok\n        do_ptrs += step_m * stride_tok\n    return dk, dv\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0eddccc4-941c-4b01-b46e-00e06cf171e6"
  },
  {
    "input": "@triton.jit\ndef _prefetch_matmul(pid_n, start_off, end_off, input, other, output, K, N,\n    stride_input_m, stride_input_k, stride_other_k, stride_other_n,\n    stride_output_m, stride_output_n, out_dtype: 'tl.constexpr', TILE_M:\n    'tl.constexpr', TILE_N: 'tl.constexpr', TILE_K: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_K: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    offs_m = start_off + tl.arange(0, TILE_M)\n    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_k = tl.arange(0, TILE_K)\n    rn = tl.max_contiguous(tl.multiple_of(offs_n % N, TILE_N), TILE_N)\n    input_ptrs = input + (offs_m[:, None] * stride_input_m + offs_k[None, :\n        ] * stride_input_k)\n    other_ptrs = other + (offs_k[:, None] * stride_other_k + rn[None, :] *\n        stride_other_n)\n    output_ptrs = output + stride_output_m * offs_m[:, None\n        ] + stride_output_n * offs_n[None, :]\n    original_input_ptrs = input_ptrs\n    original_other_ptrs = other_ptrs\n    acc = tl.zeros((TILE_M, TILE_N), dtype=out_dtype)\n    mask_n = offs_n[None, :] < N\n    k_iters = K // TILE_K if EVEN_K else tl.cdiv(K, TILE_K)\n    for k in range(0, k_iters * BLOCK_SIZE):\n        i = k % k_iters\n        if EVEN_K:\n            a = tl.load(input_ptrs)\n            b = tl.load(other_ptrs)\n        else:\n            a = tl.load(input_ptrs, mask=offs_k[None, :] + i * TILE_K < K,\n                other=0.0)\n            b = tl.load(other_ptrs, mask=offs_k[:, None] + i * TILE_K < K,\n                other=0.0)\n        acc += tl.dot(a, b, out_dtype=out_dtype)\n        if i == k_iters - 1:\n            if EVEN_N:\n                tl.store(output_ptrs, acc)\n            else:\n                tl.store(output_ptrs, acc, mask_n)\n            output_ptrs += TILE_M * stride_output_m\n        if i == k_iters - 1:\n            acc = tl.zeros((TILE_M, TILE_N), dtype=out_dtype)\n            original_input_ptrs += TILE_M * stride_input_m\n            input_ptrs = original_input_ptrs\n            other_ptrs = original_other_ptrs\n        else:\n            input_ptrs += TILE_K * stride_input_k\n            other_ptrs += TILE_K * stride_other_k\n",
    "category": "Memory Management",
    "subcategory": "prefetching",
    "uuid": "49fe2f96-f8ea-49d3-99cc-e70e12502a88"
  },
  {
    "input": "@triton.jit\ndef moe_align_block_size_stage2(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    last_cnt = 0\n    for i in range(1, num_experts + 1):\n        token_cnt = tl.load(tokens_cnts_ptr + i * num_experts + pid)\n        last_cnt = last_cnt + token_cnt\n        tl.store(tokens_cnts_ptr + i * num_experts + pid, last_cnt)\n",
    "category": "Helper Functions",
    "subcategory": "shape manipulation",
    "uuid": "751d608d-8e3e-4321-88de-80e697d8e016"
  },
  {
    "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate,\n    stride_state_batch, stride_state_dim, stride_state_dstate,\n    stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim,\n    stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch,\n    stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim,\n    stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim,\n    DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS:\n    'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    state_ptr += pid_b * stride_state_batch\n    x_ptr += pid_b * stride_x_batch\n    dt_ptr += pid_b * stride_dt_batch\n    B_ptr += pid_b * stride_B_batch\n    C_ptr += pid_b * stride_C_batch\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch\n    out_ptr += pid_b * stride_out_batch\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n    if DT_SOFTPLUS:\n        dt = tl.where(dt <= 20.0, tl.math.log1p(tl.exp(dt)), dt)\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n        dstate), other=0.0)\n    dA = tl.exp(A * dt[:, None])\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    dB = B[None, :] * dt[:, None]\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "a1fa0c40-8a4c-42a2-8958-c759313994fe"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "0c51e38e-b454-4dbe-ba1f-23352dcbab15"
  },
  {
    "input": "@triton.jit\ndef matmul_(x_ptr, w_ptr, out_ptr, M, N, K, stride_x_batch, stride_x_m,\n    stride_x_k, stride_w_k, stride_w_n, stride_out_batch, stride_out_m,\n    stride_out_n, USE_FP8: 'tl.constexpr', EPS: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_batch = tl.program_id(axis=0)\n    pid = tl.program_id(axis=1)\n    pid_m = pid // tl.cdiv(N, BLOCK_SIZE_N)\n    pid_n = pid % tl.cdiv(N, BLOCK_SIZE_N)\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (pid_batch * stride_x_batch + offs_m[:, None] *\n        stride_x_m + offs_k[None, :] * stride_x_k)\n    w_ptrs = w_ptr + (offs_k[:, None] * stride_w_k + offs_n[None, :] *\n        stride_w_n)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    x_sum = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n    for _ in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        x = tl.load(x_ptrs)\n        x_sum += tl.math.pow(x, 2)\n        w = tl.load(w_ptrs)\n        if USE_FP8:\n            w = w\n            w = w\n            w = w\n        accumulator += tl.dot(x, w)\n        x_ptrs += BLOCK_SIZE_K * stride_x_k\n        w_ptrs += BLOCK_SIZE_K * stride_w_k\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    out_ptrs = out_ptr + (pid_batch * stride_out_batch + offs_m[:, None] *\n        stride_out_m + offs_n[None, :] * stride_out_n)\n    out_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n    tl.store(out_ptrs, accumulator, mask=out_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "c24ae11c-cbca-47db-9d36-452db28c0b0f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128,\n    'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, fpb_ptr, K, N, stride_bk,\n    stride_bn, stride_fpbk, stride_fpbn, BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "a5c504a8-437b-4284-8a77-32977dbb302e"
  },
  {
    "input": "@triton.jit\ndef add_scalar_kernel(x_ptr, scalar, output_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = x + scalar\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "8abd0eb2-4161-427e-b9c2-05d3cb916593"
  },
  {
    "input": "@triton.heuristics({'GEMMA': lambda args: args['GEMMA']})\n@triton.jit\ndef _rms_layernorm_backward(dY, dY_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, dW, dW_row_stride, n_cols, eps, GEMMA:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Fast RMS Layernorm kernel for the backward pass\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    inv_var = tl.load(r)\n    normed = X_row * inv_var\n    if GEMMA:\n        dY_W = dY_row * (W_row + 1.0)\n    else:\n        dY_W = dY_row * W_row\n    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)\n    output = inv_var / n_cols * (n_cols * dY_W - normed * rowsum_dY_normed)\n    tl.store(dY + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "0f6aeb51-d69b-466d-8445-5aa5da8339a7"
  },
  {
    "input": "@triton.jit\ndef argmax_kernel(output_ptr, input_ptr, num_batches, size, block_size:\n    'tl.constexpr'):\n    batch = tl.program_id(0)\n    output_block_ptr = tl.make_block_ptr(output_ptr, shape=(num_batches,),\n        strides=(1,), offsets=(batch,), block_shape=(1,), order=(0,))\n    input_block_ptr = tl.make_block_ptr(input_ptr, shape=(num_batches, size\n        ), strides=(size, 1), offsets=(batch, 0), block_shape=(1,\n        block_size), order=(1, 0))\n    input = tl.load(input_block_ptr, boundary_check=(1,))\n    condition = tl.arange(0, block_size) < size\n    input = tl.where(condition, input, float('-inf'))\n    output = tl.argmax(input, 1)\n    tl.store(output_block_ptr, output)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "e5df293d-7888-41d2-b2c1-fcb427624729"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1 + tanh(_kAlpha * (x + 0.044715 * x * x * x)))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "79461b46-8cb6-441b-8a83-d574e9928078"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_cumsum_gv(V, GV, GV_cumsum, DGV_cumsum_exp, DV_reduce,\n    DGV_last_exp, DGV_cumsum, DV, DGV, NUM_CHUNK, L, D_MODEL_V:\n    'tl.constexpr', CHUNK_SIZE: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    V_ptr = (V + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_ptr = (GV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    GV_cumsum_ptr = (GV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DV_ptr = (DV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DV_reduce_ptr = (DV_reduce + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DGV_cumsum_ptr = (DGV_cumsum + offset_bh * L * D_MODEL_V + offset_c *\n        CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DGV_cumsum_exp_ptr = (DGV_cumsum_exp + offset_bh * L * D_MODEL_V + \n        offset_c * CHUNK_SIZE * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    DGV_ptr = (DGV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V))\n    D_GV_last_exp_ptr = (DGV_last_exp + offset_bh * NUM_CHUNK * D_MODEL_V +\n        offset_c * D_MODEL_V + tl.arange(0, D_MODEL_V))\n    cumsum_gradient = tl.zeros([D_MODEL_V], dtype=tl.float32)\n    grad_gv_last = tl.zeros([D_MODEL_V], dtype=tl.float32)\n    gv_last = tl.load(GV_cumsum_ptr + (CHUNK_SIZE - 1) * D_MODEL_V)\n    cumsum_gradient += tl.load(D_GV_last_exp_ptr) * tl.exp(gv_last)\n    GV_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    GV_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    V_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DV_reduce_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DGV_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DGV_cumsum_exp_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DV_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    DGV_ptr += (CHUNK_SIZE - 1) * D_MODEL_V\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        gv_cs = tl.load(GV_cumsum_ptr)\n        v = tl.load(V_ptr)\n        grad_v = tl.exp(gv_last - gv_cs) * tl.load(DV_reduce_ptr)\n        tl.store(DV_ptr, grad_v)\n        grad_v *= v\n        cumsum_gradient -= grad_v\n        grad_gv_last += grad_v\n        grad_v = tl.exp(gv_cs) * tl.load(DGV_cumsum_exp_ptr)\n        cumsum_gradient += grad_v\n        cumsum_gradient += tl.load(DGV_cumsum_ptr)\n        tl.store(DGV_ptr, cumsum_gradient)\n        V_ptr -= D_MODEL_V\n        DV_reduce_ptr -= D_MODEL_V\n        GV_cumsum_ptr -= D_MODEL_V\n        DGV_cumsum_ptr -= D_MODEL_V\n        DV_ptr -= D_MODEL_V\n        DGV_ptr -= D_MODEL_V\n        DGV_cumsum_exp_ptr -= D_MODEL_V\n    DGV_ptr = (DGV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V) + (CHUNK_SIZE - 1) * D_MODEL_V)\n    GV_ptr = (GV + offset_bh * L * D_MODEL_V + offset_c * CHUNK_SIZE *\n        D_MODEL_V + tl.arange(0, D_MODEL_V) + (CHUNK_SIZE - 1) * D_MODEL_V)\n    grad_gv_last = grad_gv_last + 0.0\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        dgv = tl.load(DGV_ptr)\n        dgv += grad_gv_last\n        tl.store(DGV_ptr, dgv)\n        DGV_ptr -= D_MODEL_V\n        GV_ptr -= D_MODEL_V\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "4b58b7e9-1718-46c9-add1-0751de6ce2ac"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_simple_gla_bwd_kernel_dqkg(q, k, v, h, g, do, dh, dq, dk, dg,\n    scale, B: 'tl.constexpr', T: 'tl.constexpr', H: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        b_g_last = tl.load(g + i_bh * T + min(i_t * BT + BT, T) - 1)\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H + i_h, (T,), (H,), (i_t *\n            BT,), (BT,), (0,))\n        b_g_last = tl.load(g + i_b * T * H + (min(i_t * BT + BT, T) - 1) * H)\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    b_dg = tl.zeros([BT], dtype=tl.float32)\n    b_dg_last = tl.zeros([1], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V, (V, NT * K), (1, V),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (V, NT * K), (1, V\n            ), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dg_last += tl.sum(b_h * b_dh)\n        b_ds += tl.dot(b_do, tl.trans(b_v))\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, b_dh)\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dg = tl.make_block_ptr(dg + (i_k * B * H + i_bh) * T, (T,), (1,),\n            (i_t * BT,), (BT,), (0,))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BK, BT), (0, 1))\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BK, BT), (0, 1))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BK, BT), (0, 1))\n        p_dg = tl.make_block_ptr(dg + (i_k * B + i_b) * T * H + i_h, (T,),\n            (H,), (i_t * BT,), (BT,), (0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg_last *= tl.exp(b_g_last)\n    b_dq = b_dq * tl.exp(b_g)[:, None] * scale\n    b_dk = b_dk * tl.exp(-b_g + b_g_last)[:, None]\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale * tl.exp(b_g\n        [:, None] - b_g[None, :]), 0)\n    b_ds = b_ds\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q)\n    b_dg += tl.sum(b_q * b_dq - b_k * b_dk, axis=1)\n    b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0,))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "bd32874b-72ba-43b3-aff2-9add6c14c967"
  },
  {
    "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, nheads, dim, dstate,\n    nheads_ngroups_ratio, stride_state_batch, stride_state_head,\n    stride_state_dim, stride_state_dstate, stride_x_batch, stride_x_head,\n    stride_x_dim, stride_dt_batch, stride_dt_head, stride_dt_dim,\n    stride_dt_bias_head, stride_dt_bias_dim, stride_A_head, stride_A_dim,\n    stride_A_dstate, stride_B_batch, stride_B_group, stride_B_dstate,\n    stride_C_batch, stride_C_group, stride_C_dstate, stride_D_head,\n    stride_D_dim, stride_z_batch, stride_z_head, stride_z_dim,\n    stride_out_batch, stride_out_head, stride_out_dim, DT_SOFTPLUS:\n    'tl.constexpr', TIE_HDIM: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    HAS_DT_BIAS: 'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z:\n    'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head\n    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head\n    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head\n    if HAS_DT_BIAS:\n        dt_bias_ptr += pid_h * stride_dt_bias_head\n    A_ptr += pid_h * stride_A_head\n    B_ptr += (pid_b * stride_B_batch + pid_h // nheads_ngroups_ratio *\n        stride_B_group)\n    C_ptr += (pid_b * stride_C_batch + pid_h // nheads_ngroups_ratio *\n        stride_C_group)\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    if HAS_D:\n        D_ptr += pid_h * stride_D_head\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    if not TIE_HDIM:\n        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n        if DT_SOFTPLUS:\n            dt = softplus(dt)\n        A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n            dstate), other=0.0)\n        dA = tl.exp(A * dt[:, None])\n    else:\n        dt = tl.load(dt_ptr)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptr)\n        if DT_SOFTPLUS:\n            dt = softplus(dt)\n        A = tl.load(A_ptr)\n        dA = tl.exp(A * dt)\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    if not TIE_HDIM:\n        dB = B[None, :] * dt[:, None]\n    else:\n        dB = B * dt\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8e439869-61ac-430b-abd8-f33141476e35"
  },
  {
    "input": "@triton.jit\ndef dcomp2dx(x, b_return):\n    _return2 = tl.expand_dims(x, 1)\n    bx = zeroslike(x)\n    b_return2 = zeroslike(_return2)\n    _b_return2 = triton_unbroadcast(b_return * x, _return2.shape)\n    return bx\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c0910b10-8bd6-4a8c-98b1-3c053a136312"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "308eed2c-a21a-4e75-b7d4-a96bf09051b4"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_V(q, v, g, h, o, A, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NG: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bg * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bg * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h)\n    p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_v)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "9a666e3c-b45b-40a5-bd0a-c1f6bef9e872"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BL': 128}, num_warps=8), triton.\n    Config({'BL': 128}, num_warps=4), triton.Config({'BL': 128}, num_warps=\n    2), triton.Config({'BL': 64}, num_warps=8), triton.Config({'BL': 64},\n    num_warps=4), triton.Config({'BL': 64}, num_warps=2)], key=['L'])\n@triton.jit\ndef _fwd_qs_kernel(Q, S, O, Z, stride_qk_bh, stride_qk_l, stride_qk_d,\n    stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk,\n    stride_s_dv, stride_z_bh, B: 'tl.constexpr', H: 'tl.constexpr', L:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr', BL:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    start_v, start_m, off_bs_head = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    qkv_base_offset = off_bs_head * stride_qk_bh\n    NV = tl.cdiv(DV, BV)\n    Q_block_ptr = tl.make_block_ptr(base=Q + off_bs_head * stride_qk_bh,\n        shape=(L, DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_m *\n        BL, 0), block_shape=(BL, BK), order=(1, 0))\n    S_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh,\n        shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(0, \n        start_v * BV), block_shape=(BK, BV), order=(1, 0))\n    Z_block_ptr = Z + off_bs_head * stride_z_bh + tl.arange(0, BK)\n    o = tl.zeros([BL, BV], dtype=tl.float32)\n    z_buffer = tl.zeros([BL], dtype=tl.float32)\n    for i_k in range(0, DK, BK):\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n        s = tl.load(S_block_ptr, boundary_check=(0, 1))\n        z = tl.load(Z_block_ptr, mask=i_k + tl.arange(0, BK) < DK)\n        z_buffer += tl.sum(q * z[None, :], axis=1, keep_dims=False)\n        o += tl.dot(q, s, allow_tf32=False)\n        Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))\n        S_block_ptr = tl.advance(S_block_ptr, (BK, 0))\n        Z_block_ptr = Z_block_ptr + tl.arange(0, BK)\n    o = o / (z_buffer[:, None] + 1e-06)\n    O_block_ptr = tl.make_block_ptr(base=O + off_bs_head * stride_vo_bh,\n        shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_m *\n        BL, start_v * BV), block_shape=(BL, BV), order=(1, 0))\n    tl.store(O_block_ptr, o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "da8e0cb8-fba4-4e44-9469-fb1a3c38aa6a"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, DO, Delta, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_hz = tl.program_id(1)\n    off_n = tl.arange(0, HEAD_DIM)\n    o = tl.load(O + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    do = tl.load(DO + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d36fa15e-5ae2-4f9d-8dd8-65459fb1f1d6"
  },
  {
    "input": "@triton.jit\ndef triton_dense_fwd_kernel(Q, K, V, seqlens, sm_scale, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_ok, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', dtype:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qo_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + kv_offset, shape=(BLOCK_DMODEL,\n        N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape\n        =(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + kv_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vn, stride_vk), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M\n    m_mask = offs_m[:, None] < seqlen\n    for start_n in range(lo, hi, BLOCK_N):\n        n_mask = start_n + offs_n[None, :] <= offs_m[:, None]\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = tl.where(m_mask, acc / l_i[:, None], 0.0)\n    O_block_ptr = tl.make_block_ptr(base=Out + qo_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_ok), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc, mask=m_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "25169c79-dbba-415a-9bef-0ac3e084543c"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, L, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, P_SEQ, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr',\n    SM_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    q_offset = off_hz * stride_qh\n    kv_offset = off_hz * stride_kh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + kv_offset, shape=(BLOCK_DMODEL,\n        N_CTX + P_SEQ), strides=(stride_kk, stride_kn), offsets=(0, 0),\n        block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + kv_offset, shape=(N_CTX +\n        P_SEQ, BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0\n        ), block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = P_SEQ + (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX + P_SEQ\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if IS_CAUSAL:\n            qk = tl.where(P_SEQ + offs_m[:, None] >= start_n + offs_n[None,\n                :], qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / (SM_N * tl.exp(-m_i[:, None]) + l_i[:, None])\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n    O_block_ptr = tl.make_block_ptr(base=Out + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "6e91255d-69e0-45bb-b8da-e1191c9076d5"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd(Q, K, V, sm_scale, DO, DQ, DK, DV, M, D, stride_z, stride_h,\n    stride_tok, stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1:\n    'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr',\n    BLK_SLICE_FACTOR: 'tl.constexpr', HEAD_DIM: 'tl.constexpr'):\n    LN2: 'tl.constexpr' = 0.6931471824645996\n    bhid = tl.program_id(2)\n    off_chz = bhid * N_CTX\n    adj = stride_h * (bhid % H) + stride_z * (bhid // H)\n    pid = tl.program_id(0)\n    Q += adj\n    K += adj\n    V += adj\n    DO += adj\n    DQ += adj\n    DK += adj\n    DV += adj\n    M += off_chz\n    D += off_chz\n    offs_k = tl.arange(0, HEAD_DIM)\n    start_n = pid * BLOCK_N1\n    start_m = start_n\n    MASK_BLOCK_M1: 'tl.constexpr' = BLOCK_M1 // BLK_SLICE_FACTOR\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, MASK_BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n,\n        start_m, num_steps, MASK=True)\n    start_m += num_steps * MASK_BLOCK_M1\n    num_steps = (N_CTX - start_m) // BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n, start_m,\n        num_steps, MASK=False)\n    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dv_ptrs, dv)\n    dk *= sm_scale\n    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dk_ptrs, dk)\n    start_m = pid * BLOCK_M2\n    end_n = start_m + BLOCK_M2\n    MASK_BLOCK_N2: 'tl.constexpr' = BLOCK_N2 // BLK_SLICE_FACTOR\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n        )\n    m = tl.load(M + offs_m)\n    m = m[:, None]\n    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, MASK_BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps *\n        MASK_BLOCK_N2, num_steps, MASK=True)\n    end_n -= num_steps * MASK_BLOCK_N2\n    num_steps = end_n // BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps * BLOCK_N2,\n        num_steps, MASK=False)\n    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    dq *= LN2\n    tl.store(dq_ptrs, dq)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c12826af-dc25-42ee-ab91-6c7f5c5d2458"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dx(DX, DY, X, Mean, Rstd, stride_dx, stride_dy,\n    stride_x, D, eps, BLOCK_D: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    X += row * stride_x\n    DY += row * stride_dy\n    DX += row * stride_dx\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    dy = tl.where(mask, dy, 0.0)\n    c1 = tl.sum(xhat * dy, axis=0) / D\n    c2 = tl.sum(dy, axis=0) / D\n    dx = (dy - (xhat * c1 + c2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "8bbeeb30-f27b-4066-9ca8-01ce037259b6"
  },
  {
    "input": "@triton.jit\ndef chunk_rwkv6_bwd_kernel_inter(k, v, h, g, gs, A, do, dh, dq, dk, dv, dA,\n    s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_t = min(i_t * BT + BT, T)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_gq = tl.make_block_ptr(gs + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((o_t - \n        1) * K + i_k * BK,), (BK,), (0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gq = tl.load(p_gq, boundary_check=(0, 1))\n    b_gn = tl.exp(tl.load(p_gn, boundary_check=(0,))[None, :] - b_gk)\n    b_k = b_k * b_gn\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n        if i_k == 0:\n            b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n        b_do = b_do * scale\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n    b_dq = b_dq * tl.exp(b_gq)\n    b_dk = b_dk * b_gn\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] > o_i[None, :]\n    b_dA = tl.where(m_s, b_dA, 0.0)\n    if i_k == 0:\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "6b3d21dd-aa3e-4e71-82e5-579d42fe3685"
  },
  {
    "input": "@triton.jit\ndef _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "edc151ac-75a9-4274-9e10-6f5ac2084b1e"
  },
  {
    "input": "@triton.jit\ndef split_2D_jagged_jagged_w_prefix(JaggedIn, OffsetsA, OffsetsB, OutA,\n    OutB, D, stride_id, stride_ad, stride_bd, n_prefix_to_B, BLOCK_D:\n    'tl.constexpr'):\n    split_2D_jagged_w_prefix(JaggedIn, 0, OffsetsA, OffsetsB, OutA, OutB, D,\n        stride_id, stride_ad, stride_bd, n_prefix_to_B, IS_DENSE_A=False,\n        IS_DENSE_B=False, BLOCK_D=BLOCK_D, IS_REPLACE=False)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "799e0183-e990-40c2-902a-1f22cf4ac0e9"
  },
  {
    "input": "@triton.jit\ndef relu_grad(x):\n    zero = 0.0\n    one = 1.0\n    return tl.where(x >= 0, one, zero)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "d425182e-03f5-415c-99a8-e367468cb27c"
  },
  {
    "input": "@triton.heuristics({'DO_SOFTCAPPING': lambda args: bool(args[\n    'DO_SOFTCAPPING']), 'DO_LOGIT_SCALING': lambda args: bool(args[\n    'DO_LOGIT_SCALING'])})\n@triton.jit\ndef _cross_entropy_forward(logits_ptr, logits_row_stride, loss_ptr,\n    logsumexp_ptr, labels_ptr, VOCAB_SIZE, BLOCK_SIZE: 'tl.constexpr',\n    DO_SOFTCAPPING, SOFTCAP, DO_LOGIT_SCALING, LOGIT_SCALE):\n    \"\"\"\n        Cross Entropy Loss = 1/n sum [ -yi log(Pi) ]\n        Pi = exp(xi) / sum(exp(xi))\n        CE_i = -y log(p) = -y log[ exp(x) / sum(exp(x)) ]\n             = -y [ x - log[sum(exp(x))] ]\n             = y * (log[sum(exp(x))] - x)\n        If y == 0: CE_i = 0\n        If y == 1: CE_i = logsumexp - x\n\n        logsumexp is also stable\n        Take    y =         log[sum(exp(x))]\n           exp(y) =             sum(exp(x))\n           exp(y) =             sum(exp(x - c)*exp(c)) Since e^(x-c)*e^c = e^x\n           exp(y) =      exp(c)*sum(exp(x - c))\n               y  = log(exp(c)*sum(exp(x - c)))\n               y  = c + log[sum(exp(x - c))]\n        This means we can set c = max(x) to make sure\n        exp(x - c) always is exp(x - max(x)).\n        This ensures exp(x - max(x))'s maximum is 1 as exp(0) = 1.\n    \"\"\"\n    row_idx = tl.program_id(0)\n    logits_ptr += row_idx * triton_cast(logits_row_stride, tl.int64)\n    loss_ptr += row_idx\n    logsumexp_ptr += row_idx\n    labels_ptr += row_idx\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr)\n    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    if DO_LOGIT_SCALING:\n        logits = LOGIT_SCALE * logits\n    if DO_SOFTCAPPING:\n        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)\n    c = tl.max(logits, 0)\n    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))\n    if label_idx != -100:\n        x = tl.load(logits_ptr + label_idx)\n        if DO_LOGIT_SCALING:\n            x = LOGIT_SCALE * x\n        if DO_SOFTCAPPING:\n            x = SOFTCAP * triton_tanh(x / SOFTCAP)\n        loss = logsumexp - x\n    else:\n        loss = 0.0\n    tl.store(logsumexp_ptr, logsumexp)\n    tl.store(loss_ptr, loss)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "5ec276d2-536c-4fbc-a0e6-aea7aba53a92"
  },
  {
    "input": "@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n\tCompute the matrix multiplication C = A x B.\n\tA is of shape (M, K) float16\n\tB is of shape (K//8, N) int32 **** quantized\n\tC is of shape (M, N) float16\n\tscales is of shape (G, N) float16\n\tzeros is of shape (G, N//8) int32 **** quantized\n\tgroupsize is an int specifying the size of groups for scales and zeros.\n\tG is K // groupsize.\n\tSet NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n\n\tWARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n\tWARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n\tWARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n\t\"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // 8 * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // 8 * stride_zeros_n\n    shifter = offs_k % 8 * 4\n    zeros_shifter = offs_bn % 8 * 4\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = (zeros + 1) * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = (zeros + 1) * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        b = b\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // 8 * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "ffa29bdd-35bb-45d0-b1b8-d62b530a800e"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "4058459f-7253-4acb-a905-4541104b4068"
  },
  {
    "input": "@triton.jit\ndef blora_bp_kernel(dr_ptr, dr_stride_bsk, dr_stride_m, dr_stride_hout,\n    dx_ptr, dx_stride_bsk, dx_stride_m, dx_stride_h, blA_ptr,\n    blA_stride_bsk, blA_stride_h, blA_stride_r, blB_ptr, blB_stride_bsk,\n    blB_stride_r, blB_stride_hout, h: 'tl.constexpr', hout: 'tl.constexpr',\n    m: 'tl.constexpr', r: 'tl.constexpr', block_size_h: 'tl.constexpr',\n    block_size_hout: 'tl.constexpr', block_size_m: 'tl.constexpr',\n    block_size_r: 'tl.constexpr'):\n    block_idx_bsk = tl.program_id(0)\n    block_idx_h = tl.program_id(1)\n    offsets_h = block_idx_h * block_size_h + tl.arange(0, block_size_h)\n    offsets_hout = tl.arange(0, block_size_hout)\n    offsets_m = tl.arange(0, block_size_m)\n    offsets_r = tl.arange(0, block_size_r)\n    block_mask_m_col = offsets_m[:, None] < m\n    block_mask_r_row = offsets_r[None, :] < r\n    block_mask_r_col = offsets_r[:, None] < r\n    block_mask_h_row = offsets_h[None, :] < h\n    block_mask_h_col = offsets_h[:, None] < h\n    dx_ptrs = dx_ptr + block_idx_bsk * dx_stride_bsk + (offsets_m[:, None] *\n        dx_stride_m + offsets_h[None, :] * dx_stride_h)\n    dr_ptrs = dr_ptr + block_idx_bsk * dr_stride_bsk + (offsets_m[:, None] *\n        dr_stride_m + offsets_hout[None, :] * dr_stride_hout)\n    blA_ptrs = blA_ptr + block_idx_bsk * blA_stride_bsk + (offsets_h[:,\n        None] * blA_stride_h + offsets_r[None, :] * blA_stride_r)\n    blB_ptrs = blB_ptr + block_idx_bsk * blB_stride_bsk + (offsets_r[:,\n        None] * blB_stride_r + offsets_hout[None, :] * blB_stride_hout)\n    olB = tl.zeros((block_size_r, block_size_m), dtype=tl.float32)\n    for block_idx_hout in range(0, tl.cdiv(hout, block_size_hout)):\n        block_offs_hout = block_idx_hout * block_size_hout + offsets_hout\n        block_mask_hout_row = block_offs_hout[None, :] < hout\n        block_mask_hout_col = block_offs_hout[:, None] < hout\n        dr = tl.load(dr_ptrs, mask=block_mask_m_col & block_mask_hout_row,\n            other=0.0)\n        blB = tl.load(blB_ptrs, mask=block_mask_r_col & block_mask_hout_row,\n            other=0.0)\n        olB += tl.dot(blB, dr.T)\n        dr_ptrs += block_size_hout * dr_stride_hout\n        blB_ptrs += block_size_hout * blB_stride_hout\n    blA = tl.load(blA_ptrs, mask=block_mask_h_col & block_mask_r_row, other=0.0\n        )\n    olA = tl.dot(blA, olB)\n    tl.store(dx_ptrs, olA.T, mask=block_mask_m_col & block_mask_h_row)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "9ea09cfe-fcb7-470a-acbe-714a43b08cda"
  },
  {
    "input": "@triton.jit\ndef triton_modulation_scale_shift(x_ptr, modulation_ptr, output_ptr,\n    batch_size, head_size, modulation_size, is_mod1, XBLOCK: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    xoffset = pid * XBLOCK + tl.arange(0, XBLOCK)[:]\n    batch_idx = xoffset // batch_size\n    head_dim_idx = xoffset % head_size\n    modulation_offset = head_dim_idx + modulation_size * batch_idx\n    x = tl.load(x_ptr + xoffset, None)\n    if is_mod1:\n        shift = tl.load(modulation_ptr + (modulation_offset + head_size * 0\n            ), None, eviction_policy='evict_last')\n        scale = tl.load(modulation_ptr + (modulation_offset + head_size * 1\n            ), None, eviction_policy='evict_last')\n    else:\n        shift = tl.load(modulation_ptr + (modulation_offset + head_size * 3\n            ), None, eviction_policy='evict_last')\n        scale = tl.load(modulation_ptr + (modulation_offset + head_size * 4\n            ), None, eviction_policy='evict_last')\n    output = (scale + 1.0) * x + shift\n    tl.store(output_ptr + xoffset, output, None)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "12b9a080-f00d-4f18-a10f-6f3927cb4263"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_inter(q, k, v, h, g, do, dh, dq, dk, dq2, dk2, dg,\n    scale, T: 'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    if HEAD_FIRST:\n        p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + min(T, \n            i_t * BT + BT) * K - K + o_k, BK), BK)\n    else:\n        p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (min(\n            T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k, BK), BK)\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros([BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * V * K, (V, K),\n            (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * V * K, (V,\n            K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, b_dh)\n    b_dgk *= tl.exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gn = tl.exp(b_gn[None, :] - b_gk)\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * b_gn\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    b_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :\n        ] + b_dgk[None, :]\n    if HEAD_FIRST:\n        p_dq = tl.make_block_ptr(dq2 + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk2 + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_dq = tl.make_block_ptr(dq2 + i_b * T * H * K + i_h * K, (T, K), (\n            H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk2 + i_b * T * H * K + i_h * K, (T, K), (\n            H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dg = tl.make_block_ptr(dg + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "c8eb8000-931a-4397-9da3-4e98ec34dc78"
  },
  {
    "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "ff4b6bc4-b41c-45ed-af47-5c8eca9fdffb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024},\n    num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048},\n    num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])\n@triton.jit\ndef triton_red_fused_native_layer_norm_no_welford(in_out_ptr0, in_out_ptr1,\n    in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK:\n    'tl.constexpr', RBLOCK: 'tl.constexpr'):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy\n            ='evict_last')\n        tmp1 = tmp0\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp4 = _tmp3 + tmp2\n        _tmp3 = tmp4\n    tmp3 = tl.sum(_tmp3, 1)[:, None]\n    tmp5 = rnumel\n    tmp6 = tmp3 / tmp5\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + x0, tmp6, None)\n    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp7 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy\n            ='evict_last')\n        tmp8 = tmp7\n        tmp9 = tmp8 - tmp6\n        tmp10 = tmp9 * tmp9\n        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])\n        tmp13 = _tmp12 + tmp11\n        _tmp12 = tmp13\n    tmp12 = tl.sum(_tmp12, 1)[:, None]\n    tmp14 = rnumel\n    tmp15 = tmp12 / tmp14\n    tmp16 = 1e-05\n    tmp17 = tmp15 + tmp16\n    tmp18 = libdevice.rsqrt(tmp17)\n    tl.debug_barrier()\n    tl.store(in_out_ptr1 + x0, tmp18, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp19 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask,\n            eviction_policy='evict_first')\n        tmp23 = tl.load(in_ptr1 + r1, rmask, eviction_policy='evict_last')\n        tmp26 = tl.load(in_ptr2 + r1, rmask, eviction_policy='evict_last')\n        tmp20 = tmp19\n        tmp21 = tmp20 - tmp6\n        tmp22 = tmp21 * tmp18\n        tmp24 = tmp23\n        tmp25 = tmp22 * tmp24\n        tmp27 = tmp26\n        tmp28 = tmp25 + tmp27\n        tmp29 = tmp28\n        tl.store(out_ptr0 + (r1 + rnumel * x0), tmp29, rmask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "b6fcca6e-d780-40c5-b68a-6924f0ff9a6a"
  },
  {
    "input": "@triton.jit\ndef voxel_grid_sample_one_nearest(gi, feature_grid, feature_grid_size,\n    batch_index, ix_in, iy_in, iz_in, C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    ID = tl.load(feature_grid_size + offs + 1)\n    IH = tl.load(feature_grid_size + offs + 2)\n    IW = tl.load(feature_grid_size + offs + 3)\n    ix11 = (ix_in + 1) / 2 * IW - 0.5\n    iy11 = (iy_in + 1) / 2 * IH - 0.5\n    iz11 = (iz_in + 1) / 2 * ID - 0.5\n    ix = ix11 * (ID > 1)\n    iy = iy11 * (IH > 1)\n    iz = iz11 * (IW > 1)\n    unit_weight = ix * 0.0 + 1.0\n    ix = _round(ix)\n    iy = _round(iy)\n    iz = _round(iz)\n    sampled = _sample_3d(feature_grid, unit_weight, batch_index, ix, iy, iz,\n        ID, IH, IW, C, BLOCK_SIZE)\n    if mask_out_of_bounds_samples:\n        in_bounds_mask = is_in_bounds(ix_in, iy_in, iz_in, C, BLOCK_SIZE)\n        sampled *= in_bounds_mask\n    return sampled\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "c01e3172-3041-4896-b4a0-79b3e995dd65"
  },
  {
    "input": "@triton.jit\ndef scaled_matmul_kernel_with_block_pointers(a_ptr, b_ptr, c_ptr, s1_ptr, M,\n    N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_s1m, stride_s1n, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', ACC_TYPE: 'tl.constexpr'=tl.int32):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = a_ptr + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = b_ptr + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    idx_m = rm[:, None]\n    idx_n = rn[None, :]\n    mask = (idx_m < M) & (idx_n < N)\n    xindex = idx_n + N * idx_m\n    tmp0 = tl.load(s1_ptr + tl.broadcast_to(idx_m, mask.shape), mask,\n        eviction_policy='evict_last')\n    tl.store(c_ptr + tl.broadcast_to(xindex, mask.shape), acc * tmp0, mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "c144f060-cf6a-4bb4-a8f6-450775cc1bda"
  },
  {
    "input": "@triton.jit\ndef fifth_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_7 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_8 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    g_9 = tl.load(sph_grad_ptr + output_row_offset + 9, mask=\n        output_row_offset + 9 < output_numel)\n    g_10 = tl.load(sph_grad_ptr + output_row_offset + 10, mask=\n        output_row_offset + 10 < output_numel)\n    CONST000 = 1.60565407233314\n    CONST001 = 3.0\n    CONST002 = 3.21130814466628\n    CONST003 = 1.60565407233314\n    CONST004 = 6.42261628933256\n    CONST005 = 6.42261628933256\n    CONST006 = 8.67152307844476\n    CONST007 = 8.02827036166571\n    CONST008 = 6.9372184627558\n    CONST009 = 11.6340690431164\n    CONST010 = 12.8452325786651\n    CONST011 = 6.21867148191637\n    CONST012 = 6.21867148191637\n    CONST014 = 12.4373429638327\n    CONST017 = 12.8452325786651\n    CONST018 = 13.8744369255116\n    CONST019 = 24.8746859276655\n    CONST020 = 24.8746859276655\n    CONST021 = 27.7488738510232\n    CONST024 = 29.4321253055229\n    CONST027 = 7.35803132638072\n    CONST029 = 46.5362761724657\n    CONST030 = 51.3809303146605\n    CONST031 = 51.3809303146605\n    CONST034 = 101.955872807799\n    CONST036 = -8.67152307844475\n    CONST037 = 3.4686092313779\n    CONST038 = -88.2963759165686\n    CONST039 = -83.2466215530696\n    CONST040 = -69.8044142586986\n    CONST041 = -50.9779364038993\n    CONST042 = -50.9779364038993\n    CONST043 = -46.5362761724657\n    CONST044 = -44.1481879582843\n    CONST045 = -41.6233107765348\n    CONST046 = -38.5356977359954\n    CONST047 = -38.5356977359954\n    CONST048 = -33.166247903554\n    CONST049 = -33.9852909359329\n    CONST050 = 6.42261628933257\n    CONST051 = -33.9852909359329\n    CONST052 = -29.4321253055229\n    CONST053 = -27.7488738510232\n    CONST054 = -20.8116553882674\n    CONST055 = -19.2678488679977\n    CONST056 = -19.2678488679977\n    CONST057 = -16.9926454679664\n    CONST058 = -16.9926454679664\n    CONST059 = -13.8744369255116\n    CONST060 = -16.583123951777\n    CONST061 = -8.49632273398321\n    CONST062 = -6.9372184627558\n    CONST063 = -5.20291384706685\n    CONST064 = -3.4686092313779\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += g_0 * (CONST009 * VAR06 + CONST009 * VAR24 + CONST040 * VAR08 *\n        VAR26) + g_1 * y * (CONST038 * VAR08 * z - CONST052 * VAR25) + g_10 * (\n        CONST029 * VAR07 * z + CONST043 * VAR25 * x) + g_2 * (CONST001 *\n        VAR08 * (CONST059 * VAR17 + CONST064 * VAR26) + CONST006 * VAR06 - \n        CONST045 * VAR17 * VAR26 + CONST063 * VAR24) + g_3 * (CONST041 *\n        VAR08 * y * z - CONST049 * VAR16 * z + CONST057 * VAR25 * y) + g_4 * (\n        CONST000 * VAR24 + CONST001 * VAR08 * (CONST002 * VAR26 + CONST055 *\n        VAR17) + CONST007 * VAR06 + CONST010 * VAR15 + CONST056 * VAR17 * VAR26\n        ) + g_5 * (CONST048 * VAR16 * x + y * (CONST019 * VAR07 + CONST019 *\n        VAR26 * x)) + g_6 * (CONST005 * VAR25 * x + z * (CONST004 * VAR07 +\n        CONST046 * VAR17 * x)) + g_7 * (CONST049 * VAR16 * x - CONST051 *\n        VAR07 * y) + g_8 * (CONST008 * VAR25 * x + z * (CONST039 * VAR17 *\n        x - CONST054 * VAR07)) + g_9 * y * (CONST024 * VAR07 + CONST038 *\n        VAR26 * x)\n    g_y += g_1 * (CONST052 * VAR07 * z - CONST052 * VAR25 * x) + g_2 * (-\n        CONST039 * VAR26 * x * y + CONST053 * VAR07 * y) + g_3 * (CONST058 *\n        VAR07 * z + x * (CONST034 * VAR17 * z + CONST057 * VAR25)) + g_4 * (\n        CONST047 * VAR07 * y + x * (CONST030 * VAR16 + CONST046 * VAR26 * y)\n        ) + g_5 * (CONST001 * VAR17 * (CONST060 * VAR08 + CONST060 * VAR26) +\n        CONST011 * VAR06 + CONST012 * VAR24 + CONST014 * VAR08 * VAR26 - \n        CONST060 * VAR15) + g_6 * (CONST046 * VAR25 * y + z * (CONST031 *\n        VAR16 + CONST046 * VAR08 * y)) + g_7 * (CONST001 * VAR17 * (\n        CONST057 * VAR08 - CONST057 * VAR26) - CONST061 * VAR06 + CONST061 *\n        VAR24) + g_8 * (CONST021 * VAR25 * y + CONST039 * VAR08 * y * z\n        ) + g_9 * (CONST027 * VAR06 + CONST027 * VAR24 + CONST044 * VAR08 *\n        VAR26)\n    g_z += g_0 * (CONST029 * VAR25 * x + CONST043 * VAR07 * z) + g_1 * y * (\n        -CONST038 * VAR26 * x + CONST052 * VAR07) + g_10 * (CONST009 *\n        VAR06 + CONST009 * VAR24 + CONST040 * VAR08 * VAR26) + g_2 * (\n        CONST062 * VAR07 * z + x * (-CONST039 * VAR17 * z + CONST054 * VAR25)\n        ) + g_3 * (CONST058 * VAR07 * y + x * (CONST042 * VAR26 * y - \n        CONST049 * VAR16)) + g_4 * (CONST005 * VAR07 * z + x * (CONST046 *\n        VAR17 * z + CONST050 * VAR25)) + g_5 * (CONST048 * VAR16 * z + y *\n        (CONST019 * VAR08 * z + CONST020 * VAR25)) + g_6 * (CONST001 *\n        VAR26 * (CONST002 * VAR08 + CONST056 * VAR17) + CONST003 * VAR06 + \n        CONST007 * VAR24 + CONST017 * VAR15 + CONST056 * VAR08 * VAR17\n        ) + g_7 * (-CONST049 * VAR16 * z + CONST051 * VAR25 * y) + g_8 * (\n        CONST001 * VAR26 * (CONST018 * VAR17 + CONST037 * VAR08) + CONST036 *\n        VAR24 + CONST045 * VAR08 * VAR17 - CONST063 * VAR06) + g_9 * y * (\n        CONST024 * VAR25 + CONST038 * VAR08 * z)\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "5a829b07-94f0-49aa-8ca3-0b39e10e3b80"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, desc_k,\n    desc_v, Q, qvk_offset, stride_kn, stride_vn, stride_vk, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr', fp8_v: 'tl.constexpr',\n    ENABLE_TMA: 'tl.constexpr', LOOP_SCHEDULE: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    if not ENABLE_TMA:\n        K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n        V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in tl.range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if ENABLE_TMA:\n            k = tl._experimental_descriptor_load(desc_k, [start_n + \n                qvk_offset // stride_kn, 0], [BLOCK_N, HEAD_DIM], Q.dtype.\n                element_ty)\n        else:\n            k = tl.load(K_block_ptr)\n        if ENABLE_TMA:\n            k = tl.trans(k)\n        qk = tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        if ENABLE_TMA:\n            if fp8_v:\n                v = tl._experimental_descriptor_load(desc_v, [qvk_offset //\n                    stride_vn, start_n], [HEAD_DIM, BLOCK_N], Q.dtype.\n                    element_ty)\n            else:\n                v = tl._experimental_descriptor_load(desc_v, [qvk_offset //\n                    stride_vk + start_n, 0], [BLOCK_N, HEAD_DIM], Q.dtype.\n                    element_ty)\n        else:\n            v = tl.load(V_block_ptr)\n        if fp8_v:\n            if ENABLE_TMA:\n                v = tl.trans(v)\n            p = p\n        else:\n            p = p\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n        if not ENABLE_TMA:\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "64b34ec3-65a8-4b9c-9435-abfddaaeaff4"
  },
  {
    "input": "@triton.jit\ndef relu(x):\n    \"\"\"\n    ReLU_ activation function\n\n    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n    \"\"\"\n    return tl.where(x >= 0, x, 0.0)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "1b7ef90c-524f-480c-9f4e-7491a1d8ad73"
  },
  {
    "input": "@triton.jit\ndef cross_entropy_kernel(logits, lse, target, loss, total, ignore_index,\n    label_smoothing: 'tl.constexpr', logit_scale: 'tl.constexpr', reduction:\n    'tl.constexpr', V: 'tl.constexpr', BV: 'tl.constexpr'):\n    \"\"\"\n    This kernel computes both cross entropy loss and the gradient of the input.\n    We only consider hard label + mean reduction for now.\n    Please refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for the math.\n\n    Args:\n        logits:\n            Pointer to logits tensor.\n        lse:\n            Pointer to logsumexp tensor.\n        target: Pointer to target tensor.\n        loss:\n            Pointer to tensor to store the loss.\n        V (int):\n            The number of columns in the input tensor.\n        total (int):\n            The number of non-ignored classes.\n        ignore_index (int):\n            The index to ignore in the target.\n        label_smoothing (float):\n            The amount of smoothing when computing the loss, where 0.0 means no smoothing.\n        reduction (str):\n            The string for the reduction to apply\n        BV (int):\n            The block size for vocab.\n    \"\"\"\n    i_n = tl.program_id(0)\n    NV = tl.cdiv(V, BV)\n    b_y = tl.load(target + i_n)\n    logits += i_n * V\n    if b_y == ignore_index:\n        for i in range(0, V, BV):\n            o_v = i + tl.arange(0, BV)\n            tl.store(logits + o_v, 0.0, mask=o_v < V)\n        return\n    b_l = tl.load(logits + b_y) * logit_scale\n    b_lse = tl.load(lse + i_n)\n    b_loss = b_lse - b_l\n    b_z = 0.0\n    eps = label_smoothing / V\n    tl.debug_barrier()\n    for iv in range(0, NV):\n        o_v = iv * BV + tl.arange(0, BV)\n        b_logits = tl.load(logits + o_v, mask=o_v < V, other=float('-inf')\n            ) * logit_scale\n        if label_smoothing > 0:\n            b_z += tl.sum(tl.where(o_v < V, -eps * b_logits, 0.0))\n        b_p = (tl.exp(b_logits - b_lse) - eps) * logit_scale\n        if reduction == 'mean':\n            b_p = b_p / total\n        tl.store(logits + o_v, b_p, mask=o_v < V)\n        tl.debug_barrier()\n    if label_smoothing > 0:\n        b_loss = b_loss * (1 - label_smoothing) + (b_z + label_smoothing *\n            b_lse)\n    b_l = tl.load(logits + b_y)\n    if reduction == 'mean':\n        b_loss = b_loss / total\n        b_l += (label_smoothing - 1) / total * logit_scale\n    else:\n        b_l += (label_smoothing - 1) * logit_scale\n    tl.store(loss + i_n, b_loss)\n    tl.store(logits + b_y, b_l)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "9c26bf4a-64e9-4189-a85d-e1db41d3b4ce"
  },
  {
    "input": "@triton.jit\ndef chunk_delta_rule_fwd_kernel_h(k, v, d, v_new, h, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h,\n    s_h_t, H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, \n            1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_v_new = tl.make_block_ptr(v_new + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v -= tl.dot(b_d, b_h, allow_tf32=False)\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_v_new, b_v, boundary_check=(0, 1))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "f6106ab1-18c6-4760-a290-f050c6b28f07"
  },
  {
    "input": "@triton.jit\ndef squared_relu_grad(x):\n    return tl.where(x >= 0, 2.0 * x, 0.0)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "17b2e21c-458d-4d40-9ac9-6bf1d7d471a6"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_reversed_cumsum_bwd_kernel(ds, dz, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_ds = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_ds = tl.make_block_ptr(ds + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_dz = tl.make_block_ptr(dz + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_dz = tl.load(p_dz, boundary_check=(0, 1))\n        b_c = b_ds[None, :] + tl.dot(m_s, b_dz, allow_tf32=False)\n        tl.store(p_ds, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_ds += tl.sum(b_dz, 0)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "7eeafe54-00f5-405a-abf0-99b7a8502829"
  },
  {
    "input": "@triton.jit\ndef cdiv_fn(x, y):\n    return (x + y - 1) // y\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "76d37287-89b4-4d3e-b450-06c4caf00b0d"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "def9038b-4257-4252-a4b4-07847994b82b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4)], key=['chunk_size',\n    'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dcb_kernel(x_ptr, dout_ptr, cb_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, dcb_ptr, ddA_cumsum_ptr, chunk_size, hdim,\n    batch, seqlen, nheads, nheads_per_program, ngroups, stride_x_batch,\n    stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch,\n    stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_cb_batch,\n    stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    stride_dcb_batch, stride_dcb_chunk, stride_dcb_split, stride_dcb_group,\n    stride_dcb_csize_m, stride_dcb_csize_n, stride_ddA_cs_batch,\n    stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n, HAS_DDA_CS: 'tl.constexpr', HAS_SEQ_IDX:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (\n        pid_g * (nheads // ngroups) + pid_s * nheads_per_program\n        ) * stride_x_head\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dout_head)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g *\n        (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dA_cs_head)\n    if HAS_DDA_CS:\n        cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + \n            pid_g * stride_cb_head)\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n            nheads_per_program) * stride_ddA_cs_head + pid_m *\n            stride_ddA_cs_csize_m)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    if HAS_DDA_CS:\n        cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[\n            None, :] * stride_cb_csize_n)\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n    if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n        dcb_ptr += (pid_b * stride_dcb_batch + pid_c * stride_dcb_chunk + \n            pid_g * stride_dcb_group + pid_s * stride_dcb_split)\n        dcb_ptrs = dcb_ptr + (offs_m[:, None] * stride_dcb_csize_m + offs_n\n            [None, :] * stride_dcb_csize_n)\n        tl.store(dcb_ptrs, tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=\n            dcb_ptr.dtype.element_ty), mask=(offs_m[:, None] < chunk_size) &\n            (offs_n[None, :] < chunk_size))\n        return\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n\n            [None, :] < chunk_size), other=0.0)\n    nheads_iter = min(nheads_per_program, nheads // ngroups - pid_s *\n        nheads_per_program)\n    for h in range(nheads_iter):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_k[None, :] < hdim), other=0.0)\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit_n), other=0.0)\n        dcb = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0)\n        dcb *= dt_n\n        dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask\n            =offs_m < chunk_size_limit, other=0.0)\n        dA_cs_n = tl.load(dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask\n            =offs_n < chunk_size_limit, other=0.0)\n        dcb *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        if HAS_DDA_CS:\n            tl.static_assert(not HAS_SEQ_IDX,\n                'HAS_SEQ_IDX not supported with HAS_DDA_CS yet')\n            ddA_cs = dcb * cb\n            mask = offs_m[:, None] >= offs_n[None, :] + 1\n            ddA_cs = tl.where(mask, ddA_cs, 0.0)\n            ddA_cs = tl.cumsum(ddA_cs, axis=1)\n            ddA_cs = tl.where(mask, ddA_cs, 0.0)\n            ddA_cs = tl.sum(ddA_cs, axis=0)\n            tl.store(ddA_cumsum_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=\n                offs_n < chunk_size - 1)\n            tl.store(ddA_cumsum_ptr, 0.0)\n        acc += dcb\n        dout_ptrs += stride_dout_head\n        x_ptrs += stride_x_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_ptr += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptr += stride_ddA_cs_head\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit, other=-2)\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    mask = offs_m[:, None] >= offs_n[None, :]\n    acc = tl.where(mask, acc, 0.0)\n    dcb_ptr += (pid_b * stride_dcb_batch + pid_c * stride_dcb_chunk + pid_g *\n        stride_dcb_group + pid_s * stride_dcb_split)\n    dcb_ptrs = dcb_ptr + (offs_m[:, None] * stride_dcb_csize_m + offs_n[\n        None, :] * stride_dcb_csize_n)\n    tl.store(dcb_ptrs, acc, mask=(offs_m[:, None] < chunk_size) & (offs_n[\n        None, :] < chunk_size))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "7cbd6d9d-ea84-426e-87a9-3b144b273b6f"
  },
  {
    "input": "@triton.jit\ndef cross_entropy_loss(input, pred):\n    \"\"\"\n    Measures the per-row cross entropy loss given\n    input and predicted logits corresponding to target class.\n\n    Args:\n        input: Input.\n            The input must be of shape [BLOCK_SIZE1, BLOCK_SIZE2].\n        pred: Predicted logits corresponding to target class.\n            The predictions must be of shape [BLOCK_SIZE1].\n\n    Returns:\n        Loss.\n    \"\"\"\n    input = input\n    pred = pred\n    mx = tl.max(input, axis=1)\n    input -= mx[:, None]\n    loss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx\n    return loss\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "7e13e1d1-dfd9-4254-8e8d-082a3644488d"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 2048},\n    num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 8},\n    num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 4},\n    num_stages=1, num_warps=8), triton.Config({'XBLOCK': 8, 'RBLOCK': 512},\n    num_stages=1, num_warps=8), triton.Config({'XBLOCK': 8, 'RBLOCK': 256},\n    num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 64},\n    num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])\n@triton.jit\ndef triton_red_fused_mv_0(in_ptr0, in_ptr1, in_ptr2, out_ptr1, xnumel,\n    rnumel, XBLOCK: 'tl.constexpr', RBLOCK: 'tl.constexpr'):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + x0 // rnumel, None, eviction_policy='evict_last')\n    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp7 = tl.load(in_ptr2 + r1, None, eviction_policy='evict_last')\n        tmp1 = tmp0 + 8\n        tmp2 = tmp0 < 0\n        tmp3 = tl.where(tmp2, tmp1, tmp0)\n        tmp4 = tl.load(in_ptr1 + (r1 + rnumel * (x0 % rnumel) + rnumel *\n            rnumel * tmp3), None, eviction_policy='evict_first')\n        tmp5 = tmp4\n        tmp6 = tmp5\n        tmp8 = tmp7\n        tmp9 = tmp6 * tmp8\n        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])\n        tmp12 = _tmp11 + tmp10\n        _tmp11 = tmp12\n    tmp11 = tl.sum(_tmp11, 1)[:, None]\n    tmp13 = tmp11\n    tl.store(out_ptr1 + x0, tmp13, None)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "c5b288dc-4f38-463b-ac23-12f05706c0cb"
  },
  {
    "input": "@triton_autotune(configs=_get_configs(), key=['Z', 'H', 'N'])\n@triton.heuristics({'EVEN_M': lambda args: args['N'] % args['BLOCK_M'] == 0,\n    'EVEN_N': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _jagged_bias_to_dense(Z, H, N, jg_offsets_ptr, jg2_offsets_ptr,\n    jagged_ptr, dense_bias_ptr, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr'):\n    start_m = tl.program_id(0) * BLOCK_M\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    seq_start = tl.load(jg_offsets_ptr + off_z)\n    seq_end = tl.load(jg_offsets_ptr + off_z + 1)\n    seq_len = seq_end - seq_start\n    if start_m >= seq_len:\n        return\n    bias_start = tl.load(jg2_offsets_ptr + off_z\n        ) * H + off_h * seq_len * seq_len\n    offs_m = start_m + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    mask_m = (offs_m < seq_len)[:, None]\n    off_jg_bias = bias_start + offs_m[:, None] * seq_len + offs_n[None, :]\n    jg_bias_ptrs = jagged_ptr + off_jg_bias\n    off_d_bias = off_hz * N * N + offs_m[:, None] * N + offs_n[None, :]\n    d_bias_ptrs = dense_bias_ptr + off_d_bias\n    for start_n in range(0, seq_len, BLOCK_N):\n        maxk_n = (offs_n < seq_len - start_n)[None, :]\n        jg_bias = tl.load(jg_bias_ptrs + start_n, mask=mask_m & maxk_n,\n            other=0.0)\n        tl.store(d_bias_ptrs + start_n, jg_bias, mask=mask_m & maxk_n)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "d5631cff-01ad-4743-81ce-c65b11dca6c9"
  },
  {
    "input": "@triton.jit\ndef fused_attention_kernel(Q, K, V, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, L, M, Out, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(q_ptrs)\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        k = tl.load(k_ptrs)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(qk - m_curr[:, None])\n        l_curr = tl.sum(p, 1) + l_prev\n        l_rcp = 1.0 / l_curr\n        p *= l_rcp\n        acc *= (l_prev * l_rcp)[:, None]\n        p = p\n        v = tl.load(v_ptrs)\n        acc += tl.dot(p, v)\n        l_prev = l_curr\n        m_prev = m_curr\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vk\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_prev)\n    tl.store(m_ptrs, m_prev)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "a03b65e0-2312-41c4-8981-b5704f3679c1"
  },
  {
    "input": "@triton.autotune(configs=_scatter2scatter_configs(), key=['M', 'N', 'K'])\n@triton.heuristics({'NO_K_MASK': lambda args: args['K'] % args['BLOCK_K'] ==\n    0, 'NO_N_MASK': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _scatter2scatter(X_ptr, stride_xm, stride_xk, W_ptr, stride_we,\n    stride_wk, stride_wn, Y_ptr, stride_ym, stride_yn, grouped_idx_ptr,\n    expert_idxs_ptr, block_start_idx_ptr, FAN_OUT: 'tl.constexpr', M, K:\n    'tl.constexpr', N: 'tl.constexpr', E: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr',\n    ACC_TYPE: 'tl.constexpr', OUT_M, allow_tf32: 'tl.constexpr', x_grouped:\n    'tl.constexpr', y_grouped: 'tl.constexpr', NO_K_MASK: 'tl.constexpr',\n    NO_N_MASK: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    N_BLOCK_COUNT = tl.cdiv(N, BLOCK_N)\n    M_block_id = pid // N_BLOCK_COUNT\n    N_block_id = pid % N_BLOCK_COUNT\n    M_range = tl.arange(0, BLOCK_M)\n    block_start_idx = tl.load(block_start_idx_ptr + M_block_id)\n    M_block = tl.max_contiguous(block_start_idx + M_range, BLOCK_M)\n    E_idxs = tl.load(expert_idxs_ptr + M_block, mask=M_block < FAN_OUT * M,\n        other=E)\n    E_idx = tl.min(E_idxs)\n    E_mask = E_idxs == E_idx\n    M_idx = tl.load(grouped_idx_ptr + M_block, mask=E_mask, other=0)\n    if x_grouped:\n        M_in_idx = M_block\n    else:\n        M_in_idx = M_idx // FAN_OUT\n    if y_grouped:\n        M_out_idx = M_block\n    else:\n        M_out_idx = M_idx\n    K_block = tl.arange(0, BLOCK_K)\n    N_block = N_block_id * BLOCK_N + tl.arange(0, BLOCK_N)\n    N_mask = N_block < N\n    X_blk_ptrs = X_ptr + M_in_idx[:, None] * stride_xm + K_block[None, :\n        ] * stride_xk\n    W_blk_ptrs = W_ptr + K_block[:, None] * stride_wk + N_block[None, :\n        ] * stride_wn + E_idx * stride_we\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    iters = tl.cdiv(K, BLOCK_K)\n    for K_block_id in range(0, iters):\n        if NO_K_MASK:\n            x = tl.load(X_blk_ptrs, mask=E_mask[:, None])\n            if NO_N_MASK or K_block_id < iters - 1:\n                w = tl.load(W_blk_ptrs)\n            else:\n                w = tl.load(W_blk_ptrs, mask=N_mask[None, :])\n        else:\n            K_mask = K_block_id * BLOCK_K + K_block < K\n            x = tl.load(X_blk_ptrs, mask=E_mask[:, None] & K_mask[None, :])\n            w = tl.load(W_blk_ptrs, mask=K_mask[:, None] & N_mask[None, :])\n        X_blk_ptrs += BLOCK_K * stride_xk\n        W_blk_ptrs += BLOCK_K * stride_wk\n        acc += tl.dot(x, w, allow_tf32=allow_tf32, out_dtype=ACC_TYPE)\n    Y_blk_ptrs = Y_ptr + (M_out_idx[:, None] * stride_ym + N_block[None, :] *\n        stride_yn)\n    tl.store(Y_blk_ptrs, acc, mask=E_mask[:, None] & N_mask[None, :])\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "e651f537-f5ef-4315-91ca-7268a1f6989f"
  },
  {
    "input": "@triton.jit\ndef silu_grad(input):\n    \"\"\"\n    Calculates the gradient of SiLU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of SiLU.\n    \"\"\"\n    output_sigmoid = sigmoid(input)\n    return output_sigmoid * (input * (1 - output_sigmoid) + 1)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "4415e2dd-9f88-4f8a-9f21-a9b7566c5e66"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_1b1(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp0 + i_w * BW * DH + tl.arange(\n        0, BW)[None, :] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NW - i_w - 1\n        ) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]) * DH + (NH - i_h - 1\n        ) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (DH - NH * BH) + (DW -\n        NW * BW) * DH\n    p_x1 = x + i_b * 4 * _tmp1 + _tmp2\n    p_x2 = p_x1 + _tmp1\n    p_x3 = p_x2 + _tmp1\n    p_x4 = p_x3 + _tmp1\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        tl.store(p_y1 + _idx, tl.load(p_x1 + _idx, mask=_mask_hw), mask=\n            _mask_hw)\n        tl.store(p_y2 + _idx, tl.load(p_x2 + _idx, mask=_mask_hw), mask=\n            _mask_hw)\n        tl.store(p_y3 + _idx, tl.load(p_x3 + _idx, mask=_mask_hw), mask=\n            _mask_hw)\n        tl.store(p_y4 + _idx, tl.load(p_x4 + _idx, mask=_mask_hw), mask=\n            _mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "4faa6e67-1283-42b0-a103-608651211502"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dx_fused(DX, DY, DW, X, W, M, V, Lock, stride, N,\n    GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    mean = tl.load(M + row)\n    rstd = tl.load(V + row)\n    xhat = (x - mean) * rstd\n    wdy = w * dy\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    mean1 = tl.sum(xhat * wdy, axis=0) / N\n    mean2 = tl.sum(wdy, axis=0) / N\n    dx = (wdy - (xhat * mean1 + mean2)) * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * xhat\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "7b13b672-df9b-4ccc-becd-a77d0a6d5345"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef softmax_backward_kernel(output_grad_pointer, output_pointer,\n    input_grad_pointer, batch_dim, feat_dim, output_grad_batch_stride,\n    output_grad_feat_stride, output_batch_stride, output_feat_stride,\n    input_grad_batch_stride, input_grad_feat_stride, log: 'tl.constexpr',\n    BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of softmax.\n\n    Args:\n        output_grad_pointer: Pointer to softmax's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim].\n        output_pointer: Pointer to softmax's output.\n            The output must be of shape [batch_dim, feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        log: Flag indicating if log of softmax was taken.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n        ] + output_grad_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    output = tl.load(output_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    if log:\n        input_grad = output_grad - tl.exp(output) * tl.sum(output_grad, axis=1\n            )[:, None]\n    else:\n        input_grad = output * (output_grad - tl.sum(output_grad * output,\n            axis=1)[:, None])\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "592686a3-b2d1-40ec-bfaf-1ed0e8c59f97"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, D_ptr, dx_ptr, ddt_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_cb_batch, stride_cb_chunk, stride_cb_head,\n    stride_cb_csize_m, stride_cb_csize_k, stride_dout_batch,\n    stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch,\n    stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch,\n    stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize,\n    stride_D_head, stride_dx_batch, stride_dx_seqlen, stride_dx_head,\n    stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head,\n    stride_ddt_csize, HAS_D: 'tl.constexpr', D_HAS_HDIM: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None,\n        :] * stride_cb_csize_k)\n    dout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size_limit, other=0.0)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    K_MAX = chunk_size_limit\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k\n            [None, :] < K_MAX - k), other=0.0)\n        dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0)\n        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None,\n            :] < K_MAX)\n        cb = tl.where(mask, cb, 0.0)\n        cb = cb\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (pid_b * stride_dx_batch + pid_c * chunk_size *\n        stride_dx_seqlen + pid_h * stride_dx_head)\n    dx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :\n        ] * stride_dx_hdim)\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + \n            offs_n[None, :] * stride_dout_hdim)\n        dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] <\n            chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        dx += dout_res * D\n    tl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "11af9543-caa9-40a9-a126-cbf5c895ba51"
  },
  {
    "input": "@triton.jit\ndef fwd_decay_cumsum(g, g_o, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', DK: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_g = g + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    p_go = g_o + i_bh * s_qk_h + i_c * BT * DK + i_k * BK + tl.arange(0, BK)\n    cum_decay = tl.zeros([BK], dtype=tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < DK\n    for i in range(BT):\n        _g = tl.load(p_g, mask=mask, other=0)\n        cum_decay += _g * inv_ln2\n        tl.store(p_go, cum_decay, mask=mask)\n        p_g += DK\n        p_go += DK\n",
    "category": "Math Utils",
    "subcategory": "exponential",
    "uuid": "dfca63c1-6a39-4d54-aee3-4dccb6471711"
  },
  {
    "input": "@triton.heuristics({'BACKWARD_PASS': lambda args: args['BACKWARD_PASS']})\n@triton.jit\ndef _rope_embedding(Q, Q_row_stride, cos, cos_row_stride, sin,\n    sin_row_stride, seqlen, head_dim: 'tl.constexpr', n_heads:\n    'tl.constexpr', BACKWARD_PASS: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Calculates the RoPE Embedding quickly\n        RoPE is Q * cos + rotate_half(Q) * sin\n        See our blog post for more info\n    \"\"\"\n    row_position = tl.program_id(0)\n    group_head_position = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    half_head_dim = head_dim // 2\n    mask = col_offsets < half_head_dim\n    sin1 = tl.load(sin + row_position % seqlen * sin_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    cos1 = tl.load(cos + row_position % seqlen * cos_row_stride + \n        half_head_dim * 0 + col_offsets, mask=mask, other=0)\n    if BACKWARD_PASS:\n        sin1 = -sin1\n    pass\n    head_start = group_head_position * ROPE_GROUP_SIZE\n    head_end = min(head_start + ROPE_GROUP_SIZE, n_heads)\n    for k in range(head_start, head_end):\n        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n        offs_q2 = (row_position * Q_row_stride + k * head_dim + col_offsets +\n            half_head_dim)\n        Q1 = tl.load(Q + offs_q1, mask=mask, other=0)\n        Q2 = tl.load(Q + offs_q2, mask=mask, other=0)\n        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)\n    pass\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "b72f9417-43d4-4fdf-b8e3-b23793436697"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef dropout_backward_kernel(output_grad_pointer, input_grad_pointer, size,\n    drop_p, seed, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of dropout.\n\n    Args:\n        output_grad_pointer: Pointer to dropout's output gradients.\n            The output gradients must be of shape [size].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element used in dropout.\n        seed: Seed for generating the dropout mask.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)\n    tl.store(input_grad_pointer + offset, input_grad, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "95468373-e115-4740-be51-555411f26484"
  },
  {
    "input": "@triton.jit\ndef gelu(input):\n    \"\"\"\n    Applies GELU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by GELU.\n    \"\"\"\n    cdf = 0.5 * (1 + tl.math.erf(0.707106781 * input))\n    return cdf * input\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "ae100f13-ea95-4cd3-a46c-82b707c06ae5"
  },
  {
    "input": "@triton.jit\ndef chunk_hgrn_fwd_kernel_o(gc, o, s_b, s_t, s_d, T: 'tl.constexpr', D:\n    'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    for i_t in range(1, tl.cdiv(T, BT)):\n        p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t *\n            BT, i_d * BD), (BT, BD), (1, 0))\n        b_h0 = tl.load(o + i_b * T * D + i_t * BT * D - D + o_d, mask=mask,\n            other=0)\n        b_gc = tl.load(p_gc, boundary_check=(0, 1))\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_o = b_o + tl.exp(b_gc) * b_h0[None, :]\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8d2c2d05-ea50-4c27-b83e-0bcfd25ae420"
  },
  {
    "input": "@triton.jit\ndef load_dequantize_k_v_group(K_block_ptr, V_block_ptr,\n    K_scale_shift_block_ptr, V_scale_shift_block_ptr, BOUNDS_CHECKS_N:\n    'tl.constexpr', PACKED_PER_VAL: 'tl.constexpr', PACKED_D_PER_GROUP:\n    'tl.constexpr', FP8_QUANTIZED: 'tl.constexpr', dtype: 'tl.constexpr',\n    group_id: 'tl.constexpr'):\n    \"\"\"Load K/V for a given block. In case of int4/fp8-quantized K/V, dequantize them after loading.\n    If quantization is group-wise, use group_id to advance the pointers to the current group.\n    \"\"\"\n    K_block_ptr = tl.advance(K_block_ptr, (PACKED_D_PER_GROUP * group_id, 0))\n    V_block_ptr = tl.advance(V_block_ptr, (0, PACKED_D_PER_GROUP * group_id))\n    k = tl.load(K_block_ptr, boundary_check=(1,) if BOUNDS_CHECKS_N else ())\n    v = tl.load(V_block_ptr, boundary_check=(0,) if BOUNDS_CHECKS_N else ())\n    if FP8_QUANTIZED:\n        v_scale_shift = tl.load(V_scale_shift_block_ptr, boundary_check=(0,\n            ) if BOUNDS_CHECKS_N else ())\n        v_scale, v_shift = cast_uint32_to_half2(v_scale_shift)\n        v = dequantize(v, v_scale, v_shift, PACKED_PER_VAL)\n        k_scale_shift = tl.load(K_scale_shift_block_ptr, boundary_check=(1,\n            ) if BOUNDS_CHECKS_N else ())\n        k_scale, k_shift = cast_uint32_to_half2(k_scale_shift)\n        k_t = dequantize(tl.trans(k), tl.trans(k_scale), tl.trans(k_shift),\n            PACKED_PER_VAL)\n        k = tl.trans(k_t)\n    elif PACKED_PER_VAL > 1:\n        K_scale_shift_block_ptr = tl.advance(K_scale_shift_block_ptr, (\n            group_id, 0))\n        V_scale_shift_block_ptr = tl.advance(V_scale_shift_block_ptr, (0,\n            group_id))\n        k_scale_shift = tl.load(K_scale_shift_block_ptr, boundary_check=(1,\n            ) if BOUNDS_CHECKS_N else ())\n        v_scale_shift = tl.load(V_scale_shift_block_ptr, boundary_check=(0,\n            ) if BOUNDS_CHECKS_N else ())\n        k_scale, k_shift = cast_uint32_to_half2(k_scale_shift)\n        v_scale, v_shift = cast_uint32_to_half2(v_scale_shift)\n        v = dequantize(v, v_scale, v_shift, PACKED_PER_VAL)\n        k_t = dequantize(tl.trans(k), tl.trans(k_scale), tl.trans(k_shift),\n            PACKED_PER_VAL)\n        k = tl.trans(k_t)\n    return k, v\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "2e2d8202-0293-4dc3-89dc-341c95d6df32"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(k, v, h, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, \n            1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "1d02c562-39c0-4f91-8fb3-da62e1c7a75d"
  },
  {
    "input": "@triton.jit\ndef print_if(*txt, conds):\n    \"\"\"Print txt, if condition on pids is fulfilled\"\"\"\n    if test_pid_conds(conds):\n        None\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "b3367efb-f5bf-4ab0-9fd4-2297a37e0d67"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "41de191c-a91d-4364-aab2-9693e7546cbe"
  },
  {
    "input": "@triton.jit\ndef leaky_relu(x):\n    x = x + 1\n    return tl.where(x >= 0, x, 0.01 * x)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "396912d8-9e09-41c7-bd6e-cb8286c1e827"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_dequantize_kv(mem_kv_buffer, mem_kv_scale,\n    req_to_token_indexs, b_seq_len, b_req_idx, Out, stride_kv_b,\n    stride_kv_h, stride_kv_g, stride_kv_d, stride_o_bh, stride_o_l,\n    stride_o_g, stride_o_d, stride_s_b, stride_s_h, stride_s_g,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, group_size, head_num:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', BLOCK_GROUP_NUM:\n    'tl.constexpr', BLOCK_GROUP_DIM: 'tl.constexpr'):\n    cur_group = tl.program_id(0)\n    start_m = tl.program_id(1)\n    cur_bh = tl.program_id(2)\n    cur_batch = cur_bh // head_num\n    cur_head = cur_bh % head_num\n    block_start_loc = BLOCK_SIZE * start_m\n    cur_batch_req_idx = tl.load(b_req_idx + cur_batch)\n    cur_seq_len = tl.load(b_seq_len + cur_batch)\n    offs_kv_loc = block_start_loc + tl.arange(0, BLOCK_SIZE)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM)\n    kv_loc = tl.load(req_to_token_indexs + cur_batch_req_idx *\n        stride_req_to_tokens_b + offs_kv_loc, mask=offs_kv_loc < cur_seq_len)\n    offs_kv = (kv_loc[:, None] * stride_kv_b + cur_head * stride_kv_h + \n        cur_group * stride_kv_g + offs_d[None, :])\n    src_data = tl.load(mem_kv_buffer + offs_kv, mask=offs_kv_loc[:, None] <\n        cur_seq_len, other=0.0)\n    s_ptrs = (mem_kv_scale + kv_loc * stride_s_b + cur_head * stride_s_h + \n        cur_group * stride_s_g)\n    data_scale = tl.load(s_ptrs, mask=offs_kv_loc < cur_seq_len)\n    out_data = src_data * data_scale[:, None]\n    o_ptrs = Out + cur_bh * stride_o_bh + offs_kv_loc[:, None\n        ] * stride_o_l + cur_group * stride_o_g + offs_d[None, :]\n    tl.store(o_ptrs, out_data, mask=offs_kv_loc[:, None] < cur_seq_len)\n    return\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "7b8da2a7-2825-468b-98c8-dd694dd07b20"
  },
  {
    "input": "@triton.jit\ndef parallel_rebased_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr', H: 'tl.constexpr',\n    T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B=B, H=H, T=T, K=K,\n        V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B=B, H=H, T=T,\n        K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a36e62b7-5485-4553-bf30-69b0ed28554a"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef layer_norm_forward_kernel(input_pointer, weight_pointer, bias_pointer,\n    mean_pointer, inv_std_pointer, output_pointer, batch_dim, feat_dim,\n    input_batch_stride, input_feat_stride, output_batch_stride,\n    output_feat_stride, eps, scale_by_weight: 'tl.constexpr', add_bias:\n    'tl.constexpr', save_stats: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Layer-normalizes the input.\n\n    Args:\n        input_pointer: Pointer to the input to layer-normalize.\n            The input must be of shape [batch_dim, feat_dim].\n        weight_pointer: Pointer to optional weights for affine transform.\n            The weights, if provided, must be of shape [feat_dim].\n        bias_pointer: Pointer to an optional bias vector for affine transform.\n            The bias vector, if provided, must be of shape [feat_dim].\n        mean_pointer: Pointer to an optional container the input's mean\n            is written to if save_stats is True.\n            The container, if provided, must be of shape [batch_dim].\n        inv_std_pointer: Pointer to an optional container the input's inverse\n            standard deviation is written to if save_stats is True.\n            The container, if provided, must be of shape [batch_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        add_bias: Flag for adding a bias vector to the normalized output\n            if scale_by_weight is True.\n        save_stats: Flag for saving the mean and standard deviation.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    mean = tl.sum(input, axis=1) / feat_dim\n    diff = tl.where(feat_mask[None, :], input - mean[:, None], 0)\n    inv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / feat_dim + eps)\n    if save_stats:\n        tl.store(mean_pointer + batch_offset, mean, mask=batch_mask)\n        tl.store(inv_std_pointer + batch_offset, inv_std, mask=batch_mask)\n    output = diff * inv_std[:, None]\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        output *= weight\n        if add_bias:\n            bias = tl.load(bias_pointer + feat_offset, mask=feat_mask)\n            output += bias\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "ca6d9b7b-d6ae-4231-93b0-228855b81d56"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_prepare_dv(q, k, do, dv, scale, T:\n    'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_A += tl.dot(b_k, b_q, allow_tf32=False)\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dv = tl.dot(b_A, b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e108bd56-134c-445e-90ee-7683f68e4d5f"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "eb38f8ac-5a2c-4df2-b3bb-1d604fbdb574"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_cumsum_gk(Q, K, GK, GK_cumsum, DQ_exp, DK_reduce,\n    DGK_last_exp, DGK_cumsum, DQ, DK, DGK, NUM_CHUNK, L, D_MODEL_K:\n    'tl.constexpr', D_BLOCK_K: 'tl.constexpr', CHUNK_SIZE: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_c = tl.program_id(1)\n    offset_nk = tl.program_id(2)\n    mask = D_BLOCK_K * offset_nk + tl.arange(0, D_BLOCK_K) < D_MODEL_K\n    Q_ptr = (Q + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    K_ptr = (K + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_ptr = (GK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    GK_cumsum_ptr = (GK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DQ_ptr = (DQ + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    DK_ptr = (DK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    DQ_exp_ptr = (DQ_exp + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DK_reduce_ptr = (DK_reduce + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DGK_cumsum_ptr = (DGK_cumsum + offset_bh * L * D_MODEL_K + offset_c *\n        CHUNK_SIZE * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K *\n        offset_nk)\n    DGK_ptr = (DGK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    D_GK_last_exp_ptr = (DGK_last_exp + offset_bh * NUM_CHUNK * D_MODEL_K +\n        offset_c * D_MODEL_K + tl.arange(0, D_BLOCK_K) + D_BLOCK_K * offset_nk)\n    cumsum_gradient = tl.zeros([D_BLOCK_K], dtype=tl.float32)\n    grad_gk_last = tl.zeros([D_BLOCK_K], dtype=tl.float32)\n    gk_last = tl.load(GK_cumsum_ptr + (CHUNK_SIZE - 1) * D_MODEL_K, mask=\n        mask, other=0)\n    cumsum_gradient += tl.load(D_GK_last_exp_ptr, mask=mask, other=0) * tl.exp(\n        gk_last)\n    GK_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    GK_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    Q_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    K_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DQ_exp_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DK_reduce_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DGK_cumsum_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DQ_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DK_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    DGK_ptr += (CHUNK_SIZE - 1) * D_MODEL_K\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        gk_cs = tl.load(GK_cumsum_ptr, mask=mask, other=0)\n        k = tl.load(K_ptr, mask=mask, other=0)\n        grad_k = tl.exp(gk_last - gk_cs) * tl.load(DK_reduce_ptr, mask=mask,\n            other=0)\n        tl.store(DK_ptr, grad_k, mask=mask)\n        grad_k *= k\n        cumsum_gradient -= grad_k\n        grad_gk_last += grad_k\n        q = tl.load(Q_ptr, mask=mask, other=0)\n        grad_q = tl.exp(gk_cs) * tl.load(DQ_exp_ptr, mask=mask, other=0)\n        tl.store(DQ_ptr, grad_q, mask=mask)\n        cumsum_gradient += grad_q * q\n        cumsum_gradient += tl.load(DGK_cumsum_ptr, mask=mask, other=0)\n        tl.store(DGK_ptr, cumsum_gradient, mask=mask)\n        Q_ptr -= D_MODEL_K\n        DQ_exp_ptr -= D_MODEL_K\n        K_ptr -= D_MODEL_K\n        DK_reduce_ptr -= D_MODEL_K\n        GK_cumsum_ptr -= D_MODEL_K\n        DGK_cumsum_ptr -= D_MODEL_K\n        DQ_ptr -= D_MODEL_K\n        DK_ptr -= D_MODEL_K\n        DGK_ptr -= D_MODEL_K\n    DGK_ptr = (DGK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + (CHUNK_SIZE - 1) * D_MODEL_K +\n        D_BLOCK_K * offset_nk)\n    GK_ptr = (GK + offset_bh * L * D_MODEL_K + offset_c * CHUNK_SIZE *\n        D_MODEL_K + tl.arange(0, D_BLOCK_K) + (CHUNK_SIZE - 1) * D_MODEL_K +\n        D_BLOCK_K * offset_nk)\n    grad_gk_last = grad_gk_last + 0.0\n    for idx in range(CHUNK_SIZE - 1, -1, -1):\n        dgk = tl.load(DGK_ptr, mask=mask, other=0)\n        dgk += grad_gk_last\n        tl.store(DGK_ptr, dgk, mask=mask)\n        DGK_ptr -= D_MODEL_K\n        GK_ptr -= D_MODEL_K\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0c7faea5-5e3b-4309-ba56-9e0b0d2741f0"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64,\n    'BLOCK_DMODEL': 64}, num_stages=3, num_warps=4)], key=['num_queries'])\n@triton.jit\ndef triton_tem_fused_no_exp2(arg_Q, arg_K, arg_V, out_ptr0, num_queries:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr'):\n    Q = arg_Q\n    K = arg_K\n    V = arg_V\n    stride_qz = 4194304\n    stride_qh = 262144\n    stride_qm = 64\n    stride_qk = 1\n    stride_kz = 4194304\n    stride_kh = 262144\n    stride_kn = 64\n    stride_kk = 1\n    stride_vz = 4194304\n    stride_vh = 262144\n    stride_vk = 64\n    stride_vn = 1\n    Z = 16\n    H = 16\n    N_CTX = 4096\n    qk_scale = 1.0\n    MATMUL_PRECISION = tl.float16\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    qkv_offset = off_hz * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qkv_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qkv_offset, shape=(\n        BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n        ), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qkv_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        tmp0 = tl.full([1], 1024, tl.int64)\n        tmp1 = offs_m[:, None] <= tmp0\n        tmp2 = start_n + offs_n[None, :] <= tmp0\n        tmp3 = tmp1 & tmp2\n        tmp4 = offs_m[:, None] >= start_n + offs_n[None, :]\n        tmp5 = tmp3 | tmp4\n        tmp6 = float('-inf')\n        tmp7 = tmp6\n        tmp8 = tl.where(tmp5, qk, tmp7)\n        qk = tmp8\n        row_max = tl.max(qk, 1)\n        m_i_new = tl.maximum(m_i, row_max)\n        masked_out_rows = m_i_new == float('-inf')\n        alpha = tl.math.exp(m_i - m_i_new)\n        alpha = tl.where(masked_out_rows, 0, alpha)\n        p = tl.math.exp(qk - m_i_new[:, None])\n        p = tl.where(masked_out_rows[:, None], 0, p)\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    idx_z = tl.program_id(1) // H\n    idx_h = tl.program_id(1) % H\n    idx_m = offs_m[:, None]\n    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]\n    mask = (idx_m != -1) & (idx_d != -1)\n    xindex = idx_d + 64 * idx_m + 262144 * idx_h + 4194304 * idx_z\n    tl.store(out_ptr0 + xindex, acc, None)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "9cf690d6-e6ba-4ec5-b5c9-439e27b1fd92"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_unidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr',\n    BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp2\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp2\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y1 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y2 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y3 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y4 + _idx, _x, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "bb106687-9279-4d4e-8f0a-9bddc9498094"
  },
  {
    "input": "@triton.jit\ndef _int_to_randn(x1, x2, seed):\n    x_hash_1 = _hash(x1)\n    x_hash_2 = _hash(x2)\n    x_hash_1 = _pair_hash(_pair_hash(2166136261, seed), x_hash_1)\n    x_hash_2 = _pair_hash(_pair_hash(2166136261, seed + 1), x_hash_2)\n    x_01_1 = (x_hash_1 + 10) / (4294967295.0 + 10)\n    x_01_2 = (x_hash_2 + 10) / (4294967295.0 + 10)\n    z = tl.sqrt(-2 * tl.log(x_01_1)) * tl.cos(6.28318530718 * x_01_2)\n    return z\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "3b15bad5-2d7d-4637-8589-1ed4b834ba95"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef chunk_hgrn_fwd_kernel_h(x, g, gc, o, h0, T: 'tl.constexpr', D:\n    'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr'):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_x = x + i_b * T * D + i_t * BT * D + o_d\n    p_g = g + i_b * T * D + i_t * BT * D + o_d\n    p_gc = gc + i_b * T * D + i_t * BT * D + o_d\n    p_o = o + i_b * T * D + i_t * BT * D + o_d\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    b_gc = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        if i_t == 0:\n            b_h += tl.load(h0 + i_b * D + o_d, mask=mask, other=0)\n    for i in range(0, BT):\n        mask_t = mask & (i_t * BT + i < T)\n        b_x = tl.load(p_x, mask=mask_t, other=0)\n        b_g = tl.load(p_g, mask=mask_t, other=0)\n        b_h = tl.exp(b_g) * b_h + b_x\n        b_gc = b_gc + b_g\n        tl.store(p_gc, b_gc, mask=mask_t)\n        tl.store(p_o, b_h, mask=mask_t)\n        p_x += D\n        p_g += D\n        p_gc += D\n        p_o += D\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "4c2f1ebe-6c8b-441f-a7c9-5141d4b080f0"
  },
  {
    "input": "@triton.jit\ndef triton_send(send_addrs):\n    flat_tid = get_flat_tid()\n    if flat_tid == 0:\n        tl.inline_asm_elementwise(\n            \"\"\"\n            {\n                .reg .u32   %tmp32_<1>;\n                .reg .pred  %p<1>;\n\n                send_signal:\n                    atom.global.release.sys.cas.b32 %tmp32_0, [$1], 0, 1;\n                    setp.eq.u32 %p0, %tmp32_0, 0;\n                    @!%p0 bra send_signal;\n\n                barrier_end:\n            }\n            \"\"\"\n            , '=r, l', [send_addrs], dtype=tl.int32, is_pure=False, pack=1)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "fda0e1ea-c17d-4a40-8ea0-d69651b466a2"
  },
  {
    "input": "@autotune(get_small_k_configs(), key=['M', 'N', 'K'], prune_configs_by={\n    'early_config_prune': small_k_early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': _AUTOTUNE_TOPK})\n@triton.jit\ndef _mm_small_k_kernel(A, B, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, acc_dtype: 'tl.constexpr', input_precision: 'tl.constexpr',\n    fp8_fast_accum: 'tl.constexpr', BLOCK_K: 'tl.constexpr', AB_DTYPE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'=256, BLOCK_N: 'tl.constexpr'=64,\n    C=None, stride_cm=None, stride_cn=None, Norm2=None, Source=None,\n    stride_sourcem=None, stride_sourcen=None, Magnitude=None, ADD_SOURCE:\n    'tl.constexpr'=False, EPILOGUE_NORM: 'tl.constexpr'=False,\n    EPILOGUE_MAGNITUDE: 'tl.constexpr'=False, STORE_ACC: 'tl.constexpr'=False):\n    pid_m = tl.program_id(0)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    a = tl.load(A)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    rn = tl.arange(0, BLOCK_N)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    if STORE_ACC:\n        C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    if ADD_SOURCE:\n        Source = Source + (rm[:, None] * stride_sourcem + rn[None, :] *\n            stride_sourcen)\n    if EPILOGUE_NORM:\n        norm_vec = tl.zeros((BLOCK_M,), dtype=acc_dtype)\n    if EPILOGUE_MAGNITUDE:\n        Magnitude = Magnitude + ram\n    mask_m = rm < M\n    for n in range(0, tl.cdiv(N, BLOCK_N)):\n        b = tl.load(B)\n        if AB_DTYPE is not None:\n            a = a\n            b = b\n        if fp8_fast_accum:\n            acc = tl.dot(a, b, acc, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        else:\n            acc = tl.dot(a, b, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        if ADD_SOURCE:\n            mask_n = (n * BLOCK_N + rn < N)[None, :]\n            source = tl.load(Source, mask=mask_m[:, None] & mask_n)\n            acc += source\n            Source += BLOCK_N * stride_sourcen\n        if EPILOGUE_NORM:\n            norm_vec += tl.sum(acc * acc, axis=1)\n        if STORE_ACC:\n            mask_n = (n * BLOCK_N + rn < N)[None, :]\n            tl.store(C, acc, mask=mask_m[:, None] & mask_n)\n            C += BLOCK_N * stride_cn\n        B += BLOCK_N * stride_bn\n    if EPILOGUE_NORM:\n        Norm2 = Norm2 + rm\n        norm_vec = tl.rsqrt(norm_vec)\n        if EPILOGUE_MAGNITUDE:\n            magnitude = tl.load(Magnitude, mask=mask_m)\n            norm_vec *= magnitude\n        tl.store(Norm2, norm_vec, mask=mask_m)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "ae0d2c9b-7e64-4199-9cbd-fad210218a7c"
  },
  {
    "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['cluster_num']),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['BLOCK_SIZE'] // 128)),\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel_stage_2(\n    cluster_mean_ptr, cluster_m2_ptr, cluster_weight_ptr, N, groups,\n    cluster_num, eps, mean_ptr, rstd_ptr, BLOCK_SIZE: 'tl.constexpr'):\n    group = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    block = tl.arange(0, BLOCK_SIZE)\n    mask = block < cluster_num\n    offset = pid_batch * groups * cluster_num + group * cluster_num + block\n    cluster_mean = tl.load(cluster_mean_ptr + offset, mask=mask)\n    cluster_m2 = tl.load(cluster_m2_ptr + offset, mask=mask)\n    cluster_weight = tl.load(cluster_weight_ptr + offset, mask=mask)\n    mean, m2, weight = tl.reduce((cluster_mean, cluster_m2, cluster_weight),\n        0, welford_combine)\n    var = m2 / weight\n    rstd = 1.0 / tl.sqrt(var + eps)\n    offset = pid_batch * groups + group\n    tl.store(mean_ptr + offset, mean)\n    tl.store(rstd_ptr + offset, rstd)\n",
    "category": "Normalization",
    "subcategory": "batch norm",
    "uuid": "83e05aa3-f176-4e19-9858-f3e18c384042"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32, 'BLOCK_SIZE_K': 16}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 16}, num_stages\n    =2, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': \n    64, 'BLOCK_SIZE_K': 16}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 16}, num_stages=4, num_warps=8)],\n    key=['N_ITER', 'M', 'N', 'K'])\n@triton.jit\ndef fused_reconstruct_and_forward_kernel(sign_ptr, u_ptr, vt_ptr,\n    output_ptr, x_ptr, N_ITER, M, N, K, BZ, L, stride_sign_iter,\n    stride_sign_m, stride_sign_n, stride_u_iter, stride_u_m, stride_u_k,\n    stride_vt_iter, stride_vt_k, stride_vt_n, stride_x_bz, stride_x_l,\n    stride_x_n, stride_output_bz, stride_output_l, stride_output_m,\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', num_warps: 'tl.constexpr', num_stages:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing (sign * u @ vt).sum(dim=0) followed by x @ w.T.\n    sign: (N_ITER, M, N)\n    u: (N_ITER, M, K)\n    vt: (N_ITER, K, N)\n    x: (BZ, L, N)\n    output: (BZ, L, M)\n    \"\"\"\n    pid_m = tl.program_id(axis=0)\n    pid_bzl = tl.program_id(axis=1)\n    bz = pid_bzl // L\n    l = pid_bzl % L\n    offsets_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_k = tl.arange(0, BLOCK_SIZE_K)\n    output_vals = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for n_off in range(0, N, BLOCK_SIZE_N):\n        offsets_n = n_off + tl.arange(0, BLOCK_SIZE_N)\n        n_mask = offsets_n < N\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float16)\n        for n_iter in range(N_ITER):\n            sign_ptrs = sign_ptr + (n_iter * stride_sign_iter + offsets_m[:,\n                None] * stride_sign_m + offsets_n[None, :] * stride_sign_n)\n            u_ptrs = u_ptr + (n_iter * stride_u_iter + offsets_m[:, None] *\n                stride_u_m)\n            vt_ptrs = vt_ptr + (n_iter * stride_vt_iter + offsets_n[None, :\n                ] * stride_vt_n)\n            iter_acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float16)\n            for k in range(0, K, BLOCK_SIZE_K):\n                u_block_ptrs = u_ptrs + (offsets_k[None, :] + k) * stride_u_k\n                vt_block_ptrs = vt_ptrs + (offsets_k[:, None] + k\n                    ) * stride_vt_k\n                k_mask = offsets_k + k < K\n                u = tl.load(u_block_ptrs, mask=k_mask[None, :], other=0.0)\n                vt = tl.load(vt_block_ptrs, mask=k_mask[:, None], other=0.0)\n                iter_acc += tl.dot(u, vt, out_dtype=tl.float16)\n            sign = tl.load(sign_ptrs, mask=(offsets_m[:, None] < M) &\n                n_mask[None, :], other=0.0)\n            acc += sign * iter_acc\n        x_ptrs = (x_ptr + bz * stride_x_bz + l * stride_x_l + offsets_n *\n            stride_x_n)\n        x_vals = tl.load(x_ptrs, mask=n_mask, other=0.0)\n        output_vals += tl.sum(acc * x_vals[None, :], axis=1)\n    output_ptrs = (output_ptr + bz * stride_output_bz + l * stride_output_l +\n        offsets_m * stride_output_m)\n    tl.store(output_ptrs, output_vals, mask=offsets_m < M)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "2cfe0e18-c2a4-4328-942b-5c2bdb41614a"
  },
  {
    "input": "@triton.jit\ndef _seeded_dropout(x_ptr, output_ptr, n_elements, p, seed, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    random = tl.rand(seed, offsets)\n    x_keep = random > p\n    output = tl.where(x_keep, x / (1 - p), 0.0)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "1283a995-915c-4479-9ff7-b6985084c9f7"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s = tl.where(o_i[:, None] <= o_i[None, :], b_s, 0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) + tl.dot(b_s, b_do,\n            allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale, 0)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "96c746ea-283e-465d-adb2-60fb043c9cbe"
  },
  {
    "input": "@triton.jit\ndef _kldiv_kernel_forward(y_ptr, y_stride, gt_ptr, gt_stride, loss_ptr,\n    loss_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr', log_target:\n    'tl.constexpr'=False, reduction: 'tl.constexpr'=_REDUCTION_MODE_BATCHMEAN):\n    pid = tl.program_id(0)\n    y_ptr += pid * y_stride\n    gt_ptr += pid * gt_stride\n    loss_ptr += pid * loss_stride\n    base_offsets = tl.arange(0, BLOCK_SIZE)\n    loss_sum = 0.0\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + base_offsets\n        mask = offsets < n_cols\n        y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n        y_true = tl.load(gt_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            loss = y_true * (tl.log(tl.maximum(y_true, eps)) - y)\n        else:\n            loss = tl.exp(y_true) * (y_true - y)\n        if reduction == _REDUCTION_MODE_NONE:\n            tl.store(loss_ptr + offsets, loss, mask=mask)\n        else:\n            loss_sum += tl.sum(loss, axis=0)\n    if reduction != _REDUCTION_MODE_NONE:\n        tl.store(loss_ptr, loss_sum)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "a3c02049-e55e-444d-9643-bd5ac621e08e"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignored_index,\n    total_classes, class_start_idx, n_cols, n_rows, logits_row_stride,\n    BLOCK_SIZE: 'tl.constexpr', HAS_SMOOTHING: 'tl.constexpr', SPLIT:\n    'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols,\n            (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - smoothing * sum_logits / total_classes - (1 - smoothing\n                    ) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits /\n                total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "97f32ca4-a9e3-4e40-9bf0-98f0aca6e49a"
  },
  {
    "input": "@triton.jit\ndef dtw_kernel(cost, trace, x, x_stride, cost_stride, trace_stride, N, M,\n    BLOCK_SIZE: 'tl.constexpr'):\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < M\n    for k in range(1, N + M + 1):\n        tl.debug_barrier()\n        p0 = cost + (k - 1) * cost_stride\n        p1 = cost + k * cost_stride\n        p2 = cost + k * cost_stride + 1\n        c0 = tl.load(p0 + offsets, mask=mask)\n        c1 = tl.load(p1 + offsets, mask=mask)\n        c2 = tl.load(p2 + offsets, mask=mask)\n        x_row = tl.load(x + (k - 1) * x_stride + offsets, mask=mask, other=0)\n        cost_row = x_row + tl.minimum(tl.minimum(c0, c1), c2)\n        cost_ptr = cost + (k + 1) * cost_stride + 1\n        tl.store(cost_ptr + offsets, cost_row, mask=mask)\n        trace_ptr = trace + (k + 1) * trace_stride + 1\n        tl.store(trace_ptr + offsets, 2, mask=mask & (c2 <= c0) & (c2 <= c1))\n        tl.store(trace_ptr + offsets, 1, mask=mask & (c1 <= c0) & (c1 <= c2))\n        tl.store(trace_ptr + offsets, 0, mask=mask & (c0 <= c1) & (c0 <= c2))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "f9dc6499-5423-4460-bf7b-ad641d46ec9f"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "3026b272-d37f-4909-bb9f-b4e18a3691ba"
  },
  {
    "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV'])})\n@triton.jit\ndef parallel_simple_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, dg, s_k_h,\n    s_k_t, s_v_h, s_v_t, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NV: 'tl.constexpr'):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    parallel_simple_gla_bwd_kernel_dq(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n        dq, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B=B, H=H, T=T, K=K, V=V,\n        BT=BT, BS=BS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    parallel_simple_gla_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, q, k, v, g, do,\n        dk, dv, dg, s_k_h, s_k_t, s_v_h, s_v_t, scale, B, H, T, K, V, BT,\n        BS, BK, BV)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "2273c503-0b1d-4561-b27c-6c3ca58872cd"
  },
  {
    "input": "@triton.jit\ndef _rmsnorm_kernel_fwd(x_ptr, w_ptr, z_ptr, K, eps, BLOCK_SIZE:\n    'tl.constexpr'=8):\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * K\n    w_row_ptr = w_ptr + row_idx * K\n    z_row_ptr = z_ptr + row_idx * K\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for col_index in range(0, K, BLOCK_SIZE):\n        col_offsets = col_index + tl.arange(0, BLOCK_SIZE)\n        x_ptrs = x_row_ptr + col_offsets\n        x = tl.load(x_ptrs, mask=col_offsets < K, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / K\n    rsqrt = 1 / tl.sqrt(var + eps)\n    for col_index in range(0, K, BLOCK_SIZE):\n        col_offsets = col_index + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < K\n        x = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n        w = tl.load(w_ptr + col_offsets, mask=mask)\n        normed = x * rsqrt\n        normed = normed\n        z = normed * w\n        tl.store(z_row_ptr + col_offsets, z, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "c8403be4-0f96-4a22-81b8-9b8b2ec14d18"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, DO, Delta, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    D_HEAD: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_hz = tl.program_id(1)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(O + off_hz * D_HEAD * N_CTX + off_m[:, None] * D_HEAD +\n        off_n[None, :])\n    do = tl.load(DO + off_hz * D_HEAD * N_CTX + off_m[:, None] * D_HEAD +\n        off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "82ba1986-b6eb-4579-9f61-b6d179601077"
  },
  {
    "input": "@triton.jit\ndef _d_softplus(grad, x):\n    z = tl.where(x >= 0, 1 / (1 + tl.exp(-x)), 1 - 1 / (1 + tl.exp(x)))\n    return grad * z\n",
    "category": "Activation Functions",
    "subcategory": "softplus",
    "uuid": "b7f707e3-5332-4835-b41f-30126201db63"
  },
  {
    "input": "@triton.jit\ndef _triton_third_order_fwd(x_ptr: 'tl.tensor', y_ptr: 'tl.tensor', z_ptr:\n    'tl.tensor', sh_1_0_ptr: 'tl.tensor', sh_1_1_ptr: 'tl.tensor',\n    sh_1_2_ptr: 'tl.tensor', sh_2_0_ptr: 'tl.tensor', sh_2_1_ptr:\n    'tl.tensor', sh_2_2_ptr: 'tl.tensor', sh_2_3_ptr: 'tl.tensor',\n    sh_2_4_ptr: 'tl.tensor', sh_3_0_ptr: 'tl.tensor', sh_3_1_ptr:\n    'tl.tensor', sh_3_2_ptr: 'tl.tensor', sh_3_3_ptr: 'tl.tensor',\n    sh_3_4_ptr: 'tl.tensor', sh_3_5_ptr: 'tl.tensor', sh_3_6_ptr:\n    'tl.tensor', BLOCK_SIZE: 'tl.constexpr', vector_length: 'tl.constexpr'):\n    sqrt_3 = 3 ** 0.5\n    block_id = tl.program_id(0)\n    offset = tl.arange(0, BLOCK_SIZE) + BLOCK_SIZE * block_id\n    x_row_start = x_ptr + offset\n    y_row_start = y_ptr + offset\n    z_row_start = z_ptr + offset\n    x = tl.load(x_row_start, mask=offset < vector_length)\n    y = tl.load(y_row_start, mask=offset < vector_length)\n    z = tl.load(z_row_start, mask=offset < vector_length)\n    sh_1_0 = x * sqrt_3\n    sh_1_1 = y * sqrt_3\n    sh_1_2 = z * sqrt_3\n    sqrt_15 = 15 ** 0.5\n    sqrt_5 = 5 ** 0.5\n    sq_x = x * x\n    sq_y = y * y\n    sq_z = z * z\n    sh_2_0 = sqrt_15 * x * z\n    sh_2_1 = sqrt_15 * x * y\n    sh_2_2 = sqrt_5 * (sq_y - 0.5 * (sq_x + sq_z))\n    sh_2_3 = sqrt_15 * y * z\n    sh_2_4 = 0.5 * sqrt_15 * (sq_z - sq_x)\n    sqrt_42 = 42 ** 0.5\n    sqrt_168 = 168 ** 0.5\n    sqrt_7 = 7 ** 0.5\n    sh_3_0 = 1 / 6 * sqrt_42 * (sh_2_0 * z + sh_2_4 * x)\n    sh_3_1 = sqrt_7 * sh_2_0 * y\n    sh_3_2 = 1 / 8 * sqrt_168 * (4 * sq_y - (sq_x + sq_z)) * x\n    sh_3_3 = 0.5 * sqrt_7 * y * (2 * sq_y - 3 * (sq_x + sq_z))\n    sh_3_4 = 1 / 8 * sqrt_168 * z * (4 * sq_y - (sq_x + sq_z))\n    sh_3_5 = sqrt_7 * (sh_2_4 * y)\n    sh_3_6 = 1 / 6 * sqrt_42 * (sh_2_4 * z - sh_2_0 * x)\n    sh_1_0_start = sh_1_0_ptr + offset\n    sh_1_1_start = sh_1_1_ptr + offset\n    sh_1_2_start = sh_1_2_ptr + offset\n    sh_2_0_start = sh_2_0_ptr + offset\n    sh_2_1_start = sh_2_1_ptr + offset\n    sh_2_2_start = sh_2_2_ptr + offset\n    sh_2_3_start = sh_2_3_ptr + offset\n    sh_2_4_start = sh_2_4_ptr + offset\n    sh_3_0_start = sh_3_0_ptr + offset\n    sh_3_1_start = sh_3_1_ptr + offset\n    sh_3_2_start = sh_3_2_ptr + offset\n    sh_3_3_start = sh_3_3_ptr + offset\n    sh_3_4_start = sh_3_4_ptr + offset\n    sh_3_5_start = sh_3_5_ptr + offset\n    sh_3_6_start = sh_3_6_ptr + offset\n    tl.store(sh_1_0_start, sh_1_0, mask=offset < vector_length)\n    tl.store(sh_1_1_start, sh_1_1, mask=offset < vector_length)\n    tl.store(sh_1_2_start, sh_1_2, mask=offset < vector_length)\n    tl.store(sh_2_0_start, sh_2_0, mask=offset < vector_length)\n    tl.store(sh_2_1_start, sh_2_1, mask=offset < vector_length)\n    tl.store(sh_2_2_start, sh_2_2, mask=offset < vector_length)\n    tl.store(sh_2_3_start, sh_2_3, mask=offset < vector_length)\n    tl.store(sh_2_4_start, sh_2_4, mask=offset < vector_length)\n    tl.store(sh_3_0_start, sh_3_0, mask=offset < vector_length)\n    tl.store(sh_3_1_start, sh_3_1, mask=offset < vector_length)\n    tl.store(sh_3_2_start, sh_3_2, mask=offset < vector_length)\n    tl.store(sh_3_3_start, sh_3_3, mask=offset < vector_length)\n    tl.store(sh_3_4_start, sh_3_4, mask=offset < vector_length)\n    tl.store(sh_3_5_start, sh_3_5, mask=offset < vector_length)\n    tl.store(sh_3_6_start, sh_3_6, mask=offset < vector_length)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "96dbda54-0493-465b-8608-7b3f6e347995"
  },
  {
    "input": "@triton.autotune(configs=int8_dynamic_configs(), key=['M', 'N', 'K'],\n    prune_configs_by={'early_config_prune': early_config_prune,\n    'perf_model': estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef _kernel(A, B, C, bias, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, a_scale_ptr, b_scale_ptr, out_dtype:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr', SPLIT_K:\n    'tl.constexpr', EVEN_K: 'tl.constexpr', BIAS_ADD: 'tl.constexpr',\n    A_PER_CHANNEL: 'tl.constexpr', B_PER_CHANNEL: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B (optional + bias).\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    if A_PER_CHANNEL:\n        a_scale = tl.load(a_scale_ptr + ram)\n    else:\n        a_scale = tl.load(a_scale_ptr)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            k_remaining = K - k * BLOCK_K\n            _0 = tl.zeros((1, 1), dtype=tl.int8)\n            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)\n            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)\n        if A_PER_CHANNEL:\n            a = tl.math.llrint(a / a_scale[:, None])\n        else:\n            a = tl.math.llrint(a / a_scale)\n        acc += tl.dot(a, b, allow_tf32=True, out_dtype=tl.int32)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    if B_PER_CHANNEL:\n        b_scale = tl.load(b_scale_ptr + rbn)\n    else:\n        b_scale = tl.load(b_scale_ptr)\n    if A_PER_CHANNEL and B_PER_CHANNEL:\n        acc = acc.to(tl.float32) * (a_scale[:, None] * b_scale[None, :])\n    else:\n        acc = acc.to(tl.float32) * (a_scale * b_scale)\n    if BIAS_ADD:\n        bias = tl.load(bias + rn)\n        acc = acc + bias[None, :]\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "fe607079-aea3-46c6-913f-fa5d3aad4c56"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BK'])\n@triton.jit\ndef fwd_prepare_wy_repr_kernel_chunk32(k, beta, A, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BC: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BT,), (0,))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_A += tl.dot(b_kb, tl.trans(b_k), allow_tf32=False)\n    b_A = -tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :],\n        b_A, 0)\n    for i in range(1, BT):\n        mask = tl.arange(0, BT) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BT) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n    b_A += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]\n    if HEAD_FIRST:\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT), (\n            H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n    b_A = b_A\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "af37418b-7cf4-46df-bf72-61dd2350f6e9"
  },
  {
    "input": "@triton.jit\ndef bwd_preprocess_varlen(Out, DO, Delta, stride_oz, stride_oh, stride_om,\n    stride_on, stride_doz, stride_doh, stride_dom, stride_don, cu_seqlens_q,\n    max_seqlen_q, head_dim, BLOCK_M: 'tl.constexpr', D_HEAD: 'tl.constexpr',\n    PADDED_HEAD: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    num_h = tl.num_programs(1)\n    cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n    cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n    seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n    if off_m >= seqlen_q:\n        return\n    o_offset = off_h * stride_oh + cu_seqlens_q_start * stride_om\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(seqlen_q,\n        head_dim), strides=(stride_om, stride_on), offsets=(off_m, 0),\n        block_shape=(BLOCK_M, D_HEAD), order=(1, 0))\n    do_offset = off_h * stride_doh + cu_seqlens_q_start * stride_dom\n    DO_block_ptr = tl.make_block_ptr(base=DO + do_offset, shape=(seqlen_q,\n        head_dim), strides=(stride_dom, stride_don), offsets=(off_m, 0),\n        block_shape=(BLOCK_M, D_HEAD), order=(1, 0))\n    o = tl.load(O_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    do = tl.load(DO_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    delta = tl.sum(o * do, axis=1)\n    off_zh = off_z * num_h + off_h * 1\n    delta_ptrs = Delta + off_zh * max_seqlen_q + off_m + tl.arange(0, BLOCK_M)\n    overflow = off_m + BLOCK_M - seqlen_q\n    if overflow > 0:\n        boundary = tl.full((BLOCK_M,), BLOCK_M - overflow, dtype=tl.int32)\n        mask = boundary > tl.arange(0, BLOCK_M)\n        tl.store(delta_ptrs, delta, mask=mask)\n    else:\n        tl.store(delta_ptrs, delta)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a1174260-51b3-461b-a9c5-eb7e05535e56"
  },
  {
    "input": "@triton.jit\ndef _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr',\n    H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "fa179c20-0bf3-425d-bc55-9625b8252244"
  },
  {
    "input": "@triton.jit\ndef _ragged_hstu_attn_bwd_one_block(start_m, offs_n, offs_m, q_ptrs_trans,\n    dq_ptrs_trans, mask_n, ts_0_ptrs, ts_1, bias_ptrs_trans,\n    dbias_ptrs_trans, do_ptrs, dk, dv, k, v, pos_offs_n, seq_len, n_targets,\n    TW, PW, DTW, DPW, LOCK, stride_qm, stride_dom, stride_dqm, alpha,\n    MAX_SEQ_LEN, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, MAX_ATTN_LEN: 'tl.constexpr',\n    INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL: 'tl.constexpr', BUCKET_FN:\n    'tl.constexpr', ATTN_BIAS_TYPE: 'tl.constexpr', USE_TIME_BIAS:\n    'tl.constexpr', USE_POS_BIAS: 'tl.constexpr', FUSED_BIAS_BWD:\n    'tl.constexpr', HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS:\n    'tl.constexpr', CONTEXTUAL_SEQ_LEN: 'tl.constexpr', ALLOW_TF32:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    ATOMIC_ADD: 'tl.constexpr'):\n    pos_offs_m = offs_m + start_m\n    mask_m = pos_offs_m < seq_len\n    invalid_mask_trans = pos_offs_m[None, :] == offs_n[:, None]\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            pos_offs_m = tl.where(pos_offs_m < seq_len - n_targets,\n                pos_offs_m, seq_len - n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            pos_offs_m = tl.where(pos_offs_m > n_targets - 1, pos_offs_m, \n                n_targets - 1)\n    q_trans = tl.load(q_ptrs_trans + start_m * stride_qm, mask=mask_m[None,\n        :], other=0.0)\n    qk_trans = tl.dot(k, q_trans, allow_tf32=ALLOW_TF32) * alpha\n    if ATTN_BIAS_TYPE == 'fused':\n        attn_bias_trans = tl.zeros([BLOCK_N, BLOCK_M], dtype=tl.float32)\n        if USE_TIME_BIAS:\n            if CAUSAL:\n                ts_0 = tl.load(ts_0_ptrs + start_m + 1, mask=mask_m)\n            else:\n                ts_0 = tl.load(ts_0_ptrs + start_m, mask=mask_m)\n            ts_trans = ts_0[None, :] - ts_1[:, None]\n            ts_trans = ts_trans + time_delta\n            ts_trans = tl.where(ts_trans > 1e-06, ts_trans, 1e-06)\n            ts_trans = ts_trans * (1.0 / time_bucket_incr)\n            if BUCKET_FN == 'log':\n                ts_trans = tl.log(ts_trans)\n            elif BUCKET_FN == 'sqrt':\n                ts_trans = tl.sqrt(ts_trans)\n            ts_trans = ts_trans * (1.0 / time_bucket_div)\n            ts_trans = ts_trans\n            ts_trans = tl.where(ts_trans > 0, ts_trans, 0)\n            ts_trans = tl.where(ts_trans < num_buckets, ts_trans, num_buckets)\n            ts_w_trans = tl.load(TW + ts_trans, mask=mask_m[None, :] and\n                mask_n[:, None])\n            attn_bias_trans = attn_bias_trans + ts_w_trans\n        if USE_POS_BIAS:\n            offs_pos_w_trans = None\n            if HAS_MAX_POS_IND:\n                offs_pos_w_trans = pos_offs_n[:, None] - pos_offs_m[None, :\n                    ] + max_pos_ind - 1\n                offs_pos_w_trans = tl.where(offs_pos_w_trans > 0,\n                    offs_pos_w_trans, 0)\n                offs_pos_w_trans = tl.where(offs_pos_w_trans < 2 *\n                    max_pos_ind - 2, offs_pos_w_trans, 2 * max_pos_ind - 2)\n            else:\n                offs_pos_w_trans = pos_offs_n[:, None] - pos_offs_m[None, :\n                    ] + MAX_SEQ_LEN - 1\n            pos_w_trans = tl.load(PW + offs_pos_w_trans, mask=mask_m[None,\n                :] and mask_n[:, None])\n            attn_bias_trans = attn_bias_trans + pos_w_trans\n        qk_trans = qk_trans + attn_bias_trans\n    elif ATTN_BIAS_TYPE == 'separate':\n        attn_bias_trans = tl.load(bias_ptrs_trans + start_m * seq_len, mask\n            =mask_m[None, :] & mask_n[:, None], other=0.0)\n        qk_trans = qk_trans + attn_bias_trans\n    sig_trans = fast_dividef(1.0, 1.0 + tl.exp(-qk_trans))\n    silu_trans = qk_trans * sig_trans * (1.0 / MAX_SEQ_LEN)\n    if MAX_ATTN_LEN > 0:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n                ] > pos_offs_n[:, None] and pos_offs_n[:, None] - pos_offs_m[\n                None, :] >= -MAX_ATTN_LEN\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n                ] < pos_offs_n[:, None] and pos_offs_n[:, None] - pos_offs_m[\n                None, :] <= MAX_ATTN_LEN\n    elif INVALID_MASK_TYPE == 'lower_triangular':\n        invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n            ] > pos_offs_n[:, None]\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        invalid_mask_trans = invalid_mask_trans or pos_offs_m[None, :\n            ] < pos_offs_n[:, None]\n    if CONTEXTUAL_SEQ_LEN > 0 and INVALID_MASK_TYPE == 'lower_triangular':\n        row_filter = pos_offs_m < CONTEXTUAL_SEQ_LEN\n        if HAS_MULTIPLE_TARGETS:\n            col_filter = pos_offs_n < seq_len - n_targets\n        else:\n            col_filter = pos_offs_n < seq_len\n        invalid_mask_trans = invalid_mask_trans or row_filter[None, :\n            ] and col_filter[:, None]\n    silu_trans = tl.where(invalid_mask_trans, silu_trans, 0)\n    silu_trans = silu_trans\n    do = tl.load(do_ptrs + start_m * stride_dom, mask=mask_m[:, None],\n        other=0.0)\n    dv += tl.dot(silu_trans, do, allow_tf32=ALLOW_TF32)\n    dqk_trans = tl.dot(v, tl.trans(do), allow_tf32=ALLOW_TF32)\n    dqk_trans = dqk_trans * sig_trans * (1 + qk_trans * (1 - sig_trans)) * (\n        1.0 / MAX_SEQ_LEN)\n    dqk_trans = tl.where(invalid_mask_trans, dqk_trans, 0)\n    dqk_trans = dqk_trans\n    if ATTN_BIAS_TYPE == 'fused' and FUSED_BIAS_BWD:\n        if USE_TIME_BIAS:\n            tl.atomic_add(DTW + ts_trans, dqk_trans, mask=mask_m[None, :] &\n                mask_n[:, None] & invalid_mask_trans, sem='relaxed')\n        if USE_POS_BIAS:\n            tl.atomic_add(DPW + offs_pos_w_trans, dqk_trans, mask=mask_m[\n                None, :] & mask_n[:, None] & invalid_mask_trans, sem='relaxed')\n    elif ATTN_BIAS_TYPE == 'separate':\n        tl.store(dbias_ptrs_trans + start_m * seq_len, dqk_trans, mask=\n            mask_m[None, :] & mask_n[:, None])\n    dk += tl.dot(dqk_trans, tl.trans(q_trans), allow_tf32=ALLOW_TF32)\n    if ATOMIC_ADD:\n        lock_id = start_m // BLOCK_M\n        stride_lock = tl.cdiv(MAX_SEQ_LEN, BLOCK_M)\n        lock = LOCK + tl.program_id(0) * stride_lock + lock_id\n        tl.debug_barrier()\n        while tl.atomic_cas(lock, 0, 1) == 1:\n            pass\n    dq_trans = tl.load(dq_ptrs_trans + start_m * stride_dqm, mask=mask_m[\n        None, :], other=0.0, eviction_policy='evict_last')\n    dq_trans += tl.dot(tl.trans(k), dqk_trans, allow_tf32=ALLOW_TF32) * alpha\n    dq_trans = dq_trans\n    tl.store(dq_ptrs_trans + start_m * stride_dqm, dq_trans, mask=mask_m[\n        None, :], eviction_policy='evict_last')\n    if ATOMIC_ADD:\n        tl.atomic_xchg(lock, 0)\n    return dk, dv\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "b16c0616-bcab-425c-a22f-ce6b0f0cac0e"
  },
  {
    "input": "@triton.jit\ndef _pair_hash(x, h):\n    h = h ^ x\n    h = (h << 24) + h * 403\n    return h\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "39d20512-505f-4ad9-a1cf-b7579add88c0"
  },
  {
    "input": "@triton.jit\ndef splat_grid_rep(feature_grid, grad_image, feature_grid_sizes, grid_idx,\n    sample_x, sample_y, sample_z, C: 'tl.constexpr', NUM_GRIDS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr'):\n    _voxel_grid_splat(feature_grid, grad_image, feature_grid_sizes,\n        grid_idx, sample_x, sample_y, sample_z, C, NUM_GRIDS, BLOCK_SIZE,\n        mask_out_of_bounds_samples)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "e1f61ba5-a82a-4fb0-8ce1-54f1cb37032e"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum(s, c, z, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_c = tl.make_block_ptr(c + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_z = tl.make_block_ptr(z + i_bh * NT * S, (NT * S,), (s_s_d,), (i_t *\n        S + i_s * BS,), (BS,), (0,))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_c = tl.dot(m_s, b_s)\n    b_z = tl.sum(b_s, 0)\n    tl.store(p_c, b_c, boundary_check=(0, 1))\n    tl.store(p_z, b_z, boundary_check=(0,))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "11fd0673-fae3-4d57-acbc-755df7c7436b"
  },
  {
    "input": "@triton.jit\ndef load_full_2d(ptr, sz0: 'const', sz1: 'const', stride0=None, stride1=1):\n    \"\"\"Load 2d block [0,...,sz0-1] x [0,...,sz1-1] \"\"\"\n    stride0 = stride0 or sz1\n    offs = offset_2d(tl.arange(0, sz0), tl.arange(0, sz1), stride0, stride1)\n    mask = mask_2d(tl.arange(0, sz0), tl.arange(0, sz1), sz0, sz1)\n    return tl.load(ptr + offs, mask)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "05b855b0-2eb9-485f-bee2-266d44c6316c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS,\n    NUM_STAGES)], key=['M'])\n@triton.jit\ndef triton_jagged_softmax_kernel_simple_fused_buffer_then_sum(input_ptr_values,\n    input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_b), tl.load(\n        input_ptr_offsets + (pid_b + 1))\n    buffer_max_all = tl.full((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float\n        ('-inf'), dtype=tl.float32)\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf')\n            )\n        buffer_max_all = tl.maximum(buffer_max_all, input)\n    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf')\n            )\n        buffer += tl.exp(input - buffer_max)\n    buffer_exp_sum = tl.sum(buffer, axis=0)\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf')\n            )\n        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)\n        tl.store(output_ptr + idxs, output, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "332040cf-b08a-4667-b020-7da645d933ac"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'NC', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_intra(q, k, g, dA, dq, dk, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_t, i_i = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        if HEAD_FIRST:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n                BT + i_i * BC) * K + o_k, BK), BK)\n        else:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                i_t * BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(0, i_i):\n            if HEAD_FIRST:\n                p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1),\n                    (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, \n                    1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n            else:\n                p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_b * T * H * BT + i_h * BT,\n                    (T, BT), (H * BT, 1), (i_t * BT + i_i * BC, i_j * BC),\n                    (BC, BC), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    if HEAD_FIRST:\n        o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * BT + i_i * BC\n        p_kj = tl.max_contiguous(tl.multiple_of(k + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_gkj = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        o_dA = i_b * T * H * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * H * BT + i_h * BT + i_i * BC\n        p_kj = tl.max_contiguous(tl.multiple_of(k + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gkj = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, mask=m_k, other=0)\n        b_gkj = tl.load(p_gkj, mask=m_k, other=0)\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tl.exp(b_g -\n            b_gkj[None, :]), 0.0)\n        p_kj += K if HEAD_FIRST else H * K\n        p_gkj += K if HEAD_FIRST else H * K\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    if HEAD_FIRST:\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    NC = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < NC - 1:\n        if HEAD_FIRST:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n                BT + i_i * BC + BC - 1) * K + o_k, BK), BK)\n        else:\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                i_t * BT + i_i * BC + BC - 1) * H * K + i_h * K + o_k, BK), BK)\n        b_gn = tl.load(p_gn, mask=m_k, other=0)\n        for i_j in range(i_i + 1, NC):\n            if HEAD_FIRST:\n                p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (\n                    i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (BT, T), (1,\n                    BT), (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n            else:\n                p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T,\n                    K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC,\n                    BK), (1, 0))\n                p_dA = tl.make_block_ptr(dA + i_b * T * H * BT + i_h * BT,\n                    (BT, T), (1, H * BT), (i_i * BC, i_t * BT + i_j * BC),\n                    (BC, BC), (0, 1))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_qg = b_q * tl.exp(b_g - b_gn[None, :])\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dk += tl.dot(b_dA, b_qg)\n        b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    if HEAD_FIRST:\n        o_dA = i_bh * T * BT + (i_t * BT + i_i * BC\n            ) * BT + i_i * BC + tl.arange(0, BC)\n        p_qj = tl.max_contiguous(tl.multiple_of(q + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_gqj = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_i * BC) * K + o_k, BK), BK)\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    else:\n        o_dA = i_b * T * H * BT + (i_t * BT + i_i * BC\n            ) * H * BT + i_h * BT + i_i * BC + tl.arange(0, BC)\n        p_qj = tl.max_contiguous(tl.multiple_of(q + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gqj = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_i * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_dA = tl.load(dA + o_dA + j * (1 if HEAD_FIRST else H) * BT)\n        b_qj = tl.load(p_qj, mask=m_k, other=0)\n        b_gqj = tl.load(p_gqj, mask=m_k, other=0)\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n        p_qj += K if HEAD_FIRST else H * K\n        p_gqj += K if HEAD_FIRST else H * K\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d68d7e66-3fe8-4f79-a0ad-3713ceda485b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({\n    'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({\n    'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({\n    'BLOCK_N': 1024})], key=['ncols'])\n@triton.jit\ndef _swiglu_fwd_kernel(X, Y, OUT, stride_x_row, stride_y_row,\n    stride_out_row, ncols, BLOCK_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    start_col = tl.program_id(1) * BLOCK_N\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    OUT += row * stride_out_row\n    cols = start_col + tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < ncols, other=0.0)\n    y = tl.load(Y + cols, mask=cols < ncols, other=0.0)\n    out = x * tl.sigmoid(x) * y\n    tl.store(OUT + cols, out, mask=cols < ncols)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "613c69ee-eecf-448c-9f2a-2184b83cc97c"
  },
  {
    "input": "@triton.jit\ndef qkv_proj(x_ptr, q_weight_ptr, k_weight_ptr, v_weight_ptr, q_ptr, k_ptr,\n    v_ptr, M, N, K, stride_x_batch, stride_x_m, stride_x_k, stride_q_w_k,\n    stride_q_w_n, stride_k_w_k, stride_k_w_n, stride_v_w_k, stride_v_w_n,\n    stride_q_batch, stride_q_m, stride_q_n, stride_k_batch, stride_k_m,\n    stride_k_n, stride_v_batch, stride_v_m, stride_v_n, USE_FP8:\n    'tl.constexpr', EPS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    matmul_(x_ptr=x_ptr, w_ptr=q_weight_ptr, out_ptr=q_ptr, M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=\n        stride_x_k, stride_w_k=stride_q_w_k, stride_w_n=stride_q_w_n,\n        stride_out_batch=stride_q_batch, stride_out_m=stride_q_m,\n        stride_out_n=stride_q_n, USE_FP8=USE_FP8, EPS=EPS, BLOCK_SIZE_M=\n        BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K)\n    matmul_(x_ptr=x_ptr, w_ptr=k_weight_ptr, out_ptr=k_ptr, M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=\n        stride_x_k, stride_w_k=stride_k_w_k, stride_w_n=stride_k_w_n,\n        stride_out_batch=stride_k_batch, stride_out_m=stride_k_m,\n        stride_out_n=stride_k_n, USE_FP8=USE_FP8, EPS=EPS, BLOCK_SIZE_M=\n        BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K)\n    matmul_(x_ptr=x_ptr, w_ptr=v_weight_ptr, out_ptr=v_ptr, M=M, N=N, K=K,\n        stride_x_batch=stride_x_batch, stride_x_m=stride_x_m, stride_x_k=\n        stride_x_k, stride_w_k=stride_v_w_k, stride_w_n=stride_v_w_n,\n        stride_out_batch=stride_v_batch, stride_out_m=stride_v_m,\n        stride_out_n=stride_v_n, USE_FP8=USE_FP8, EPS=EPS, BLOCK_SIZE_M=\n        BLOCK_SIZE_M, BLOCK_SIZE_N=BLOCK_SIZE_N, BLOCK_SIZE_K=BLOCK_SIZE_K)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "3d672cc8-2950-4d91-b527-a6e95f9e807f"
  },
  {
    "input": "@triton.jit\ndef parallel_rebased_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "da6a00d9-fcda-4fed-8917-cf68f49b7464"
  },
  {
    "input": "@triton.jit\ndef flash_attention_v2_kernel(q_ptr, k_ptr, v_ptr, o_ptr, q_batch_stride,\n    q_heads_stride, q_seq_stride, q_dim_stride, k_batch_stride,\n    k_heads_stride, k_seq_stride, k_dim_stride, v_batch_stride,\n    v_heads_stride, v_seq_stride, v_dim_stride, out_batch_stride,\n    out_heads_stride, out_seq_stride, out_dim_stride, num_kv_groups,\n    n_heads, m_size, n_size, HEAD_DIM: 'tl.constexpr', BLOCK_M_SIZE:\n    'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr', qk_scale, causal_mask):\n    \"\"\"\n    flashattention2 \u5185\u6838\u5b9e\u73b0\n    \"\"\"\n    block_m_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    cur_batch_idx = head_idx // n_heads\n    cur_head_idx = head_idx % n_heads\n    cur_kv_head_idx = cur_head_idx // num_kv_groups\n    m_range_offs = tl.arange(0, BLOCK_M_SIZE)\n    n_range_offs = tl.arange(0, BLOCK_N_SIZE)\n    dhead_range_offs = tl.arange(0, HEAD_DIM)\n    offs_m = block_m_idx * BLOCK_M_SIZE + m_range_offs\n    offs_q = cur_batch_idx * q_batch_stride + cur_head_idx * q_heads_stride + (\n        offs_m[:, None] * q_seq_stride + dhead_range_offs[None, :] *\n        q_dim_stride)\n    offs_k = (cur_batch_idx * k_batch_stride + cur_kv_head_idx *\n        k_heads_stride + (n_range_offs[:, None] * k_seq_stride + \n        dhead_range_offs[None, :] * k_dim_stride))\n    offs_v = (cur_batch_idx * v_batch_stride + cur_kv_head_idx *\n        v_heads_stride + (n_range_offs[:, None] * v_seq_stride + \n        dhead_range_offs[None, :] * v_dim_stride))\n    offs_o = (cur_batch_idx * out_batch_stride + cur_head_idx *\n        out_heads_stride + (offs_m[:, None] * out_seq_stride + \n        dhead_range_offs[None, :] * out_dim_stride))\n    q_ptrs = q_ptr + offs_q\n    k_ptrs = k_ptr + offs_k\n    v_ptrs = v_ptr + offs_v\n    out_ptrs = o_ptr + offs_o\n    q_mask = offs_m[:, None] < m_size\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0)\n    m_i = tl.zeros([BLOCK_M_SIZE], dtype=tl.float32) - float('inf')\n    d_i = tl.zeros([BLOCK_M_SIZE], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M_SIZE, HEAD_DIM], dtype=tl.float32)\n    acc, d_i = _attn_fwd_inner(acc, m_i, d_i, q, k_ptrs, v_ptrs,\n        k_seq_stride, v_seq_stride, offs_m, qk_scale, n_size, causal_mask,\n        BLOCK_M_SIZE, BLOCK_N_SIZE, v_ptr.dtype.element_ty == tl.float8e5)\n    acc = acc / d_i[:, None]\n    out_mask = offs_m[:, None] < m_size\n    tl.store(out_ptrs, acc, mask=out_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "724a7258-64bd-4486-8380-9ef203f2b4a3"
  },
  {
    "input": "@triton.jit\ndef _add_position_embeddings_bwd_kernel(Jagged, seq_offsets, high_inds,\n    DenseOut, JaggedOut, B, D, scale, stride_jn, stride_jon, stride_don,\n    SCALE_JAGGED: 'tl.constexpr', BLOCK_D: 'tl.constexpr'):\n    off_k = tl.program_id(0)\n    offs_d = tl.arange(0, BLOCK_D)\n    accumulator = tl.zeros((BLOCK_D,), dtype=tl.float32)\n    for off_b in range(0, B):\n        max_ind = tl.load(high_inds + off_b)\n        if off_k < max_ind:\n            seq_start = tl.load(seq_offsets + off_b)\n            jagged_ptr = Jagged + seq_start * stride_jn + off_k * stride_jn\n            jagged_ptrs = jagged_ptr + offs_d\n            jg = tl.load(jagged_ptrs, mask=offs_d < D)\n            accumulator += jg\n            if SCALE_JAGGED:\n                out_jagged_ptr = (JaggedOut + seq_start * stride_jon + \n                    off_k * stride_jon)\n                out_jagged_ptrs = out_jagged_ptr + offs_d\n                tl.store(out_jagged_ptrs, jg * scale, mask=offs_d < D)\n        elif off_k == max_ind:\n            seq_start = tl.load(seq_offsets + off_b)\n            seq_end = tl.load(seq_offsets + off_b + 1)\n            for k in range(seq_start + max_ind, seq_end):\n                jagged_ptr = Jagged + k * stride_jn\n                jagged_ptrs = jagged_ptr + offs_d\n                jg = tl.load(jagged_ptrs, mask=offs_d < D)\n                accumulator += jg\n                if SCALE_JAGGED:\n                    out_jagged_ptr = JaggedOut + k * stride_jon\n                    out_jagged_ptrs = out_jagged_ptr + offs_d\n                    tl.store(out_jagged_ptrs, jg * scale, mask=offs_d < D)\n    out = accumulator\n    out_ptrs = DenseOut + off_k * stride_don + offs_d\n    tl.store(out_ptrs, out, mask=offs_d < D)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "43cc4544-95cf-4c86-9621-65a1c80321d9"
  },
  {
    "input": "@triton.jit\ndef _attention_scores_compute(Q, stride_q_n, stride_q_tdst, stride_q_hid, K,\n    stride_k_n, stride_k_tsrc, stride_k_hid, COS, stride_cos_t,\n    stride_cos_hid, SIN, stride_sin_t, stride_sin_hid, INDICES,\n    stride_indices_d, stride_indices_z, VALUES, stride_values_z, N, TDST,\n    TSRC, HID, NUM_SINK, WINDOW_SIZE, BLOCK_HID: 'tl.constexpr'):\n    idx_n = tl.program_id(0)\n    idx_tdst = tl.program_id(1)\n    idx_k = tl.program_id(2)\n    tdst = idx_tdst + TSRC - TDST\n    if idx_k < NUM_SINK:\n        idx_tsrc = idx_k\n    else:\n        window_offset = idx_k - NUM_SINK\n        t_tsrc = tdst - WINDOW_SIZE + 1 + window_offset\n        idx_tsrc = tl.maximum(idx_k, t_tsrc)\n    key, _, _, _, _ = load_rotary_embedded_vector(K, stride_k_n,\n        stride_k_tsrc, stride_k_hid, COS, stride_cos_t, stride_cos_hid, SIN,\n        stride_sin_t, stride_sin_hid, idx_n, idx_tsrc, idx_k, HID, BLOCK_HID)\n    query, _, _, _, _ = load_rotary_embedded_vector(Q, stride_q_n,\n        stride_q_tdst, stride_q_hid, COS, stride_cos_t, stride_cos_hid, SIN,\n        stride_sin_t, stride_sin_hid, idx_n, idx_tdst, tl.minimum(tdst, \n        WINDOW_SIZE + NUM_SINK - 1), HID, BLOCK_HID)\n    score = tl.sum(query * key)\n    score = score * (1 / tl.sqrt(HID))\n    score = tl.where(idx_tsrc <= tdst, score, float('-inf'))\n    idx_z = idx_n * TDST * (WINDOW_SIZE + NUM_SINK) + idx_tdst * (WINDOW_SIZE +\n        NUM_SINK) + idx_k\n    tl.store(VALUES + idx_z * stride_values_z, value=score)\n    tl.store(INDICES + 0 * stride_indices_d + idx_z * stride_indices_z,\n        value=idx_n)\n    tl.store(INDICES + 1 * stride_indices_d + idx_z * stride_indices_z,\n        value=idx_tdst)\n    tl.store(INDICES + 2 * stride_indices_d + idx_z * stride_indices_z,\n        value=idx_tsrc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "0d90a29a-8bb2-429c-8862-5856f53b2137"
  },
  {
    "input": "@triton.jit\ndef _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "bafabc3a-8cdf-4e4c-9f4c-219071fb2bc9"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd(Q, K, V, sm_scale, DO, DQ, DK, DV, M, D, stride_z, stride_h,\n    stride_tok, stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1:\n    'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr',\n    BLK_SLICE_FACTOR: 'tl.constexpr', HEAD_DIM: 'tl.constexpr'):\n    LN2: 'tl.constexpr' = 0.6931471824645996\n    bhid = tl.program_id(2)\n    off_chz = bhid * N_CTX\n    adj = stride_h * (bhid % H) + stride_z * (bhid // H)\n    pid = tl.program_id(0)\n    Q += adj\n    K += adj\n    V += adj\n    DO += adj\n    DQ += adj\n    DK += adj\n    DV += adj\n    M += off_chz\n    D += off_chz\n    offs_k = tl.arange(0, HEAD_DIM)\n    start_n = pid * BLOCK_N1\n    start_m = start_n\n    MASK_BLOCK_M1: 'tl.constexpr' = BLOCK_M1 // BLK_SLICE_FACTOR\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, MASK_BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n,\n        start_m, num_steps, MASK=True)\n    start_m += num_steps * MASK_BLOCK_M1\n    num_steps = (N_CTX - start_m) // BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n, start_m,\n        num_steps, MASK=False)\n    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dv_ptrs, dv)\n    dk *= sm_scale\n    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dk_ptrs, dk)\n    start_m = pid * BLOCK_M2\n    end_n = start_m + BLOCK_M2\n    MASK_BLOCK_N2: 'tl.constexpr' = BLOCK_N2 // BLK_SLICE_FACTOR\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n        )\n    m = tl.load(M + offs_m)\n    m = m[:, None]\n    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, MASK_BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps *\n        MASK_BLOCK_N2, num_steps, MASK=True)\n    end_n -= num_steps * MASK_BLOCK_N2\n    num_steps = end_n // BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps * BLOCK_N2,\n        num_steps, MASK=False)\n    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    dq *= LN2\n    tl.store(dq_ptrs, dq)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "25e26f39-b41c-4582-afe3-c7f5d4543044"
  },
  {
    "input": "@triton.jit\ndef chunk_delta_rule_bwd_kernel_dhv(q, k, d, do, dh, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr',\n    T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    m_s = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False) + tl.dot(b_k, b_dh,\n            allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        b_dh -= tl.dot(b_d, b_dv, allow_tf32=False)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "17772fda-29a8-49b3-a3e6-1a772bc79232"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, logits_ptr, labels_ptr,\n    smoothing, lse_square_scale, ignored_index, total_classes,\n    class_start_idx, n_cols, n_rows, logits_row_stride, BLOCK_SIZE:\n    'tl.constexpr', HAS_SMOOTHING: 'tl.constexpr', SPLIT: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf'))\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols,\n            (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx)\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - smoothing * sum_logits / total_classes - (1 - smoothing\n                    ) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits /\n                total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            loss += lse_square_scale * lse * lse\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "bd3dbf1c-df52-42d9-bbce-0e2095ce3cb3"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignore_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: 'tl.constexpr',\n    HAS_SMOOTHING: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - smooth_positive,\n            probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ea79ebb5-a0a1-48e6-9c16-7b0530f930ff"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_NON_REDUCE_DIM': b,\n    'BLOCK_SIZE_REDUCE_DIM': b}, num_warps=w) for b, w in itertools.product\n    ([2, 4, 8, 16], [2, 4, 8])], key=['M', 'N'])\n@triton.jit\ndef triton_sum_kernel_1D_result_buffer_then_sum(input_ptr, output_ptr, M, N,\n    BLOCK_SIZE_NON_REDUCE_DIM: 'tl.constexpr', BLOCK_SIZE_REDUCE_DIM:\n    'tl.constexpr', dim: 'tl.constexpr'):\n    \"\"\"\n    Add blocks of input to a buffer and sum the buffer using Triton\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    reduce_dim_len = M if dim == 0 else N\n    non_reduce_dim_len = N if dim == 0 else M\n    buffer = tl.zeros((BLOCK_SIZE_REDUCE_DIM, BLOCK_SIZE_NON_REDUCE_DIM),\n        dtype=tl.float32)\n    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM\n    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(0,\n        BLOCK_SIZE_NON_REDUCE_DIM)\n    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len\n    for block_start_reduce_dim in range(0, reduce_dim_len,\n        BLOCK_SIZE_REDUCE_DIM):\n        offsets_reduce_dim = block_start_reduce_dim + tl.arange(0,\n            BLOCK_SIZE_REDUCE_DIM)\n        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len\n        idxs, mask = None, None\n        if dim == 0:\n            idxs = offsets_reduce_dim[:, None\n                ] * non_reduce_dim_len + offsets_non_reduce_dim\n            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim\n        elif dim == 1:\n            idxs = offsets_non_reduce_dim[:, None\n                ] * reduce_dim_len + offsets_reduce_dim\n            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim\n        buffer += tl.load(input_ptr + idxs, mask=mask, other=mask)\n    buffer_sum = tl.sum(buffer, axis=dim)\n    buffer_view = buffer_sum.reshape((BLOCK_SIZE_NON_REDUCE_DIM,))\n    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=\n        mask_non_reduce_dim)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "917eeab6-324f-410b-985e-e61c52695be0"
  },
  {
    "input": "@triton.jit\ndef _kernel_argmax_merge_continuous(alpha_c, alpha_d, marginal_c,\n    stride_alpha_c1, stride_alpha_c2, stride_alpha_c3, stride_alpha_d1,\n    stride_alpha_d2, stride_alpha_d3, stride_alpha_d4, stride_alpha_d5, B, w, L\n    ):\n    b_idx = tl.program_id(0)\n    if b_idx >= B:\n        return\n    start = tl.program_id(1)\n    end = start + w\n    l_ptr = alpha_c + b_idx * stride_alpha_c1 + start * stride_alpha_c2 + (\n        start + 1) * stride_alpha_c3\n    r_ptr = alpha_c + b_idx * stride_alpha_c1 + (start + 1\n        ) * stride_alpha_c2 + end * stride_alpha_c3\n    acc1 = tl.zeros((1,), dtype=tl.float32) - 1000000000.0\n    max_idx = tl.zeros((1,), dtype=tl.float32) - 1\n    for split in range(start + 1, start + w):\n        left = tl.load(l_ptr)\n        right = tl.load(r_ptr)\n        merge = left + right\n        max_idx = update_position(acc1, merge, max_idx, tl.zeros((1,),\n            dtype=tl.float32) + split)\n        acc1 = tl.maximum(merge, acc1)\n        l_ptr += stride_alpha_c3\n        r_ptr += stride_alpha_c2\n    for gap_start in range(start + 1, end - 1):\n        for gap_end in range(gap_start + 1, end):\n            ptr_c = (alpha_c + b_idx * stride_alpha_c1 + gap_start *\n                stride_alpha_c2 + gap_end * stride_alpha_c3)\n            ptr_d = (alpha_d + b_idx * stride_alpha_d1 + start *\n                stride_alpha_d2 + gap_start * stride_alpha_d3 + gap_end *\n                stride_alpha_d4 + end * stride_alpha_d5)\n            cont = tl.load(ptr_c)\n            disco = tl.load(ptr_d)\n            merge = cont + disco\n            max_idx = update_position(acc1, merge, max_idx, tl.zeros((1,),\n                dtype=tl.float32) - (gap_start * L + gap_end))\n            acc1 = tl.maximum(merge, acc1)\n    tl.store(alpha_c + b_idx * stride_alpha_c1 + start * stride_alpha_c2 + \n        (start + w) * stride_alpha_c3 + tl.arange(0, 1), acc1 + tl.load(\n        marginal_c + b_idx * stride_alpha_c1 + start * stride_alpha_c2 + (\n        start + w) * stride_alpha_c3))\n    tl.store(alpha_c + b_idx * stride_alpha_c1 + (start + w) *\n        stride_alpha_c2 + start * stride_alpha_c3 + tl.arange(0, 1), max_idx)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "59b8a1c7-3dc1-4feb-9a92-ca7e962f4924"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=\n    4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=\n    3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages\n    =2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages\n    =4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages\n    =3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16},\n    num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16},\n    num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M':\n    32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16},\n    num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])\n@triton.jit\ndef matmul_kernel(a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr, M, N, K, stride_am,\n    stride_ak, stride_asm, stride_bk, stride_bn, stride_bsn, stride_cm,\n    stride_cn, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', SPLIT_K:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_sp_k = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    as_ptrs = as_ptr + offs_am * stride_asm\n    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K *\n            SPLIT_K, other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n    c = accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(c_ptrs, c, mask=c_mask)\n    else:\n        tl.atomic_add(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "48a65d8d-522b-44eb-96af-5cf1c93106c3"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N_COLS': 32,\n    'BLOCK_SIZE_N_ROWS': 32}, num_stages=3, num_warps=1), triton.Config({\n    'BLOCK_SIZE_N_COLS': 32, 'BLOCK_SIZE_N_ROWS': 32}, num_stages=3,\n    num_warps=1), triton.Config({'BLOCK_SIZE_N_COLS': 64,\n    'BLOCK_SIZE_N_ROWS': 32}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 128, 'BLOCK_SIZE_N_ROWS': 32}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 256,\n    'BLOCK_SIZE_N_ROWS': 32}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 32, 'BLOCK_SIZE_N_ROWS': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 64,\n    'BLOCK_SIZE_N_ROWS': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 128, 'BLOCK_SIZE_N_ROWS': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 256,\n    'BLOCK_SIZE_N_ROWS': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 32, 'BLOCK_SIZE_N_ROWS': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 64,\n    'BLOCK_SIZE_N_ROWS': 128}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 128, 'BLOCK_SIZE_N_ROWS': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 256,\n    'BLOCK_SIZE_N_ROWS': 128}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 32, 'BLOCK_SIZE_N_ROWS': 256}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 64,\n    'BLOCK_SIZE_N_ROWS': 256}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 128, 'BLOCK_SIZE_N_ROWS': 256}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 256,\n    'BLOCK_SIZE_N_ROWS': 256}, num_stages=3, num_warps=8)], key=['n_cols',\n    'n_rows'], reset_to_zero=['per_channel_amax_ptr'])\n@triton.jit\ndef sliced_fast_abs_max_kernel(weights_ptr, split_upper_bounds_ptr,\n    per_channel_amax_ptr, col_stride, row_stride,\n    per_channel_amax_row_stride, n_splits, n_cols, n_rows,\n    BLOCK_SIZE_N_COLS: 'tl.constexpr', BLOCK_SIZE_N_ROWS: 'tl.constexpr'):\n    \"\"\"\n    Computes the per-channel absolute maximum of the weights.\n\n    Args:\n        weights_ptr (pointer): pointer to the weights\n        split_upper_bounds_ptr (pointer): pointer to the split upper bounds\n        per_channel_amax_ptr (pointer): pointer to the per-channel amax output vector\n        col_stride (int): stride for moving to the next row of the matrix (next column)\n        row_stride (int): stride for moving to the next column of the matrix (next row)\n        per_channel_amax_row_stride (int): stride for moving to the next row of the per-channel amax\n        n_splits (int): number of splits\n        n_cols (int): number of columns in the weight matrix (assuming x @ W^T).\n            So n_cols is n_rows of W\n        n_rows (int): number of rows. - same as above -\n        BLOCK_SIZE_N_COLS (tl.constexpr): block size for operating along the columns\n        BLOCK_SIZE_N_ROWS (tl.constexpr): block size for operating along the rows\n    \"\"\"\n    pid = tl.program_id(0)\n    n_pid_rows = tl.cdiv(n_rows, BLOCK_SIZE_N_ROWS)\n    col_block_idx = pid // n_pid_rows\n    row_block_idx = pid % n_pid_rows\n    min_row = row_block_idx * BLOCK_SIZE_N_ROWS\n    max_row = min(min_row + BLOCK_SIZE_N_ROWS, n_rows)\n    col_offs = col_block_idx * BLOCK_SIZE_N_COLS + tl.arange(0,\n        BLOCK_SIZE_N_COLS)\n    row_offs = row_block_idx * BLOCK_SIZE_N_ROWS + tl.arange(0,\n        BLOCK_SIZE_N_ROWS)\n    ptrs = weights_ptr + (col_offs[:, None] * col_stride + row_offs[None, :\n        ] * row_stride)\n    block_weights = tl.load(ptrs, mask=(col_offs[:, None] < n_cols) & (\n        row_offs[None, :] < n_rows), other=float('-inf'))\n    block_weights_abs = tl.where((col_offs[:, None] < n_cols) & (row_offs[\n        None, :] < n_rows), tl.abs(block_weights), float('-inf'))\n    lower_bound = 0\n    slice_idx = 0\n    for upper_bound_idx in range(0, n_splits):\n        upper_bound = tl.load(split_upper_bounds_ptr + upper_bound_idx)\n        start = max(lower_bound, min_row)\n        end = min(upper_bound, max_row)\n        if start < end:\n            masked_weights = tl.where((row_offs[None, :] >= start) & (\n                row_offs[None, :] < end), block_weights_abs, float('-inf'))\n            abs_max_block_weights = tl.max(masked_weights, axis=1)\n            tl.atomic_max(per_channel_amax_ptr + slice_idx *\n                per_channel_amax_row_stride + col_offs,\n                abs_max_block_weights, mask=col_offs < n_cols)\n        slice_idx += 1\n        lower_bound = upper_bound\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "1ff2f365-9168-4ed4-b834-6d70fffbe06e"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "fc07d888-7c56-48a0-87f7-3f0d36270621"
  },
  {
    "input": "@triton.jit\ndef _attention_core(Q, K, V, mask, bias, sm_scale, TMP, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_on, Z, H, N_CTX, BATCH, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    use_mask: 'tl.constexpr', use_bias: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    if use_bias:\n        batch_2 = Z // BATCH\n        off_hz_bias = off_hz // (batch_2 * H) * H + off_hz % H\n        offs_base_bias = off_hz_bias * (N_CTX * N_CTX) + offs_m[:, None\n            ] * N_CTX + offs_n[None, :]\n    if use_mask:\n        off_hz_mask = off_hz // H\n        offs_base_mask = off_hz_mask * N_CTX\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q_load_mask = offs_m[:, None] < N_CTX\n    q = tl.load(q_ptrs, mask=q_load_mask, other=0.0)\n    for start_n in range(0, N_CTX, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        load_mask = (start_n + offs_n)[:, None] < N_CTX\n        k = tl.load(k_ptrs + start_n * stride_kn, mask=load_mask, other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] >= N_CTX, float('-1e20'), qk)\n        qk = tl.where((start_n + offs_n)[None, :] >= N_CTX, float('-1e20'), qk)\n        if use_bias:\n            bias_load_mask = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            bias_load_mask = tl.where(offs_m[:, None] >= N_CTX, 1.0,\n                bias_load_mask)\n            bias_load_mask = tl.where((start_n + offs_n)[None, :] >= N_CTX,\n                1.0, bias_load_mask)\n            bias_data = tl.load(bias + offs_base_bias + start_n, mask=\n                bias_load_mask == 0.0, other=0.0)\n            qk += bias_data\n        if use_mask:\n            mask_data = tl.load(mask + offs_base_mask + offs_n + start_n,\n                mask=start_n + offs_n < N_CTX, other=0.0)\n            qk = tl.where(mask_data[None, :] == 0.0, float('-1e20'), qk)\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale, mask=offs_m < N_CTX)\n        acc_scale = tl.load(TMP + off_hz * N_CTX + start_m * BLOCK_M + tl.\n            arange(0, BLOCK_M), mask=start_m * BLOCK_M + tl.arange(0,\n            BLOCK_M) < N_CTX, other=float(0.0))\n        acc = acc * acc_scale[:, None]\n        load_mask = (start_n + offs_n)[:, None] < N_CTX\n        v = tl.load(v_ptrs + start_n * stride_vn, mask=load_mask, other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    out_store_mask = offs_m[:, None] < N_CTX\n    tl.store(out_ptrs, acc, mask=out_store_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a2553b2d-a865-4b2d-b6ee-cac9be675014"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel_brute_force(x_ptr, z_ptr, N0, N1, T, B0: 'tl.constexpr',\n    B1: 'tl.constexpr'):\n    \"\"\"3 loops ver.\"\"\"\n    block_id_i = tl.program_id(0)\n    log2_e = 1.44269504\n    off_i = block_id_i * B0 + tl.arange(0, B0)\n    mask_i = off_i < N0\n    exp_sum = tl.zeros([B0], dtype=tl.float32)\n    x_max = tl.zeros([B0], dtype=tl.float32)\n    for id_j in tl.range(0, T, B1):\n        off_j = id_j + tl.arange(0, B1)\n        off_ij = off_i[:, None] * T + off_j[None, :]\n        mask_j = off_j < T\n        mask_ij = mask_i[:, None] & mask_j[None, :]\n        x = tl.load(x_ptr + off_ij, mask=mask_ij)\n        x_max = tl.maximum(x_max, tl.max(x, axis=1))\n    for id_j in tl.range(0, T, B1):\n        off_j = id_j + tl.arange(0, B1)\n        off_ij = off_i[:, None] * T + off_j[None, :]\n        mask_j = off_j < T\n        mask_ij = mask_i[:, None] & mask_j[None, :]\n        x = tl.load(x_ptr + off_ij, mask=mask_ij)\n        exp_x = tl.exp2(log2_e * (x - x_max[:, None]))\n        exp_sum += tl.sum(exp_x, axis=1)\n    for id_j in tl.range(0, T, B1):\n        off_j = id_j + tl.arange(0, B1)\n        off_ij = off_i[:, None] * T + off_j[None, :]\n        mask_j = off_j < T\n        mask_ij = mask_i[:, None] & mask_j[None, :]\n        x = tl.load(x_ptr + off_ij, mask=mask_ij)\n        exp_x = tl.exp2(log2_e * (x - x_max[:, None]))\n        z = exp_x / exp_sum[:, None]\n        tl.store(z_ptr + off_ij, z, mask=mask_ij)\n    return\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "bc969b77-e695-44b5-b743-042a34552fd8"
  },
  {
    "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_z_row, M, N, eps, BLOCK_N: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr', NORM_BEFORE_GATE:\n    'tl.constexpr', IS_RMS_NORM: 'tl.constexpr'):\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask)\n        y *= z * tl.sigmoid(z)\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "6eb38c0c-2839-4427-8be9-f1f32282f8f7"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dv(do, v, rv, cv, p, dv, dsv, s_qk_h, s_qk_t,\n    s_qk_d, s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BV:\n    'tl.constexpr', BM: 'tl.constexpr', DV: 'tl.constexpr', DM:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_v, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_do = tl.make_block_ptr(do + i_bh * s_qk_h, (T, DV), (s_qk_t, s_qk_d),\n        ((NT - 1) * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_qk_h, (DV, T), (s_qk_d, s_qk_t), (\n        i_v * BV, (NT - 1) * BT), (BV, BT), (0, 1))\n    p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_cv = tl.make_block_ptr(cv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_p = tl.make_block_ptr(p + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t), (\n        i_m * BM, (NT - 1) * BT), (BM, BT), (0, 1))\n    p_dv = tl.make_block_ptr(dv + (i_m * n_bh + i_bh) * s_qk_h, (T, DV), (\n        s_qk_t, s_qk_d), ((NT - 1) * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dsv = tl.make_block_ptr(dsv + (i_v * n_bh + i_bh) * s_sk_h, (T, DM),\n        (s_sk_t, s_sk_m), ((NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s, m_t = o_i[:, None] <= o_i[None, :], o_i[:, None] >= o_i[None, :]\n    b_dhv = tl.zeros([BM, BV], dtype=tl.float32)\n    for i in range(NT):\n        p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (\n            s_sk_m,), ((NT - i) % NT * DM + i_m * BM,), (BM,), (0,))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_rv = tl.load(p_rv, boundary_check=(0,))\n        b_cv = tl.load(p_cv, boundary_check=(0, 1))\n        b_p = tl.load(p_p, boundary_check=(0, 1))\n        b_inter = tl.dot(b_cv * b_rv[None, :], b_dhv, allow_tf32=False)\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_cv, b_p, allow_tf32=False),\n            0.0), b_do, allow_tf32=False)\n        b_dv = b_inter + b_intra\n        b_inter = tl.dot(b_dhv, b_v, allow_tf32=False) * b_rv[:, None]\n        b_intra = tl.dot(b_p, tl.where(m_t, tl.dot(b_do, b_v, allow_tf32=\n            False), 0.0), allow_tf32=False)\n        b_dsv = b_cv * tl.trans(b_inter + b_intra)\n        b_dhv = b_dhv * b_rv[:, None] + tl.dot(b_p, b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        tl.store(p_dsv, b_dsv, boundary_check=(0, 1))\n        p_do = tl.advance(p_do, (-BT, 0))\n        p_v = tl.advance(p_v, (0, -BT))\n        p_cv = tl.advance(p_cv, (-BT, 0))\n        p_p = tl.advance(p_p, (0, -BT))\n        p_dv = tl.advance(p_dv, (-BT, 0))\n        p_dsv = tl.advance(p_dsv, (-BT, 0))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "6394d866-b53c-412d-b7be-a8146c0494e7"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}, pre_hook=\n    init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({\n    'BLOCK_SIZE_H': 2}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])),\n    triton.Config({'BLOCK_SIZE_H': 4}, pre_hook=init_to_zero(['dA_ptr',\n    'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 8}, pre_hook=\n    init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({\n    'BLOCK_SIZE_H': 16}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])),\n    triton.Config({'BLOCK_SIZE_H': 32}, pre_hook=init_to_zero(['dA_ptr',\n    'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 64}, pre_hook=\n    init_to_zero(['dA_ptr', 'ddt_bias_ptr']))], key=['chunk_size', 'nheads'])\n@triton.jit\ndef _chunk_cumsum_bwd_kernel(ddA_ptr, ddt_out_ptr, dt_ptr, A_ptr,\n    dt_bias_ptr, ddt_ptr, dA_ptr, ddt_bias_ptr, batch, seqlen, nheads,\n    chunk_size, dt_min, dt_max, stride_ddA_batch, stride_ddA_chunk,\n    stride_ddA_head, stride_ddA_csize, stride_ddt_out_batch,\n    stride_ddt_out_chunk, stride_ddt_out_head, stride_ddt_out_csize,\n    stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head,\n    stride_dt_bias_head, stride_ddt_batch, stride_ddt_seqlen,\n    stride_ddt_head, stride_dA_head, stride_ddt_bias_head, DT_SOFTPLUS:\n    'tl.constexpr', HAS_DT_BIAS: 'tl.constexpr', BLOCK_SIZE_H:\n    'tl.constexpr', BLOCK_SIZE_CHUNK: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk\n    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * chunk_size *\n        stride_ddt_seqlen)\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    ddt_out_ptrs = ddt_out_ptr + (offs_h[:, None] * stride_ddt_out_head + \n        offs_c[None, :] * stride_ddt_out_csize)\n    ddA_ptrs = ddA_ptr + (offs_h[:, None] * stride_ddA_head + offs_c[None,\n        :] * stride_ddA_csize)\n    dt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] *\n        stride_dt_seqlen)\n    ddt_ptrs = ddt_ptr + (offs_h[:, None] * stride_ddt_head + offs_c[None,\n        :] * stride_ddt_seqlen)\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    ddA = tl.load(ddA_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None,\n        :] < chunk_size_limit), other=0.0)\n    ddt_out = tl.load(ddt_out_ptrs, mask=(offs_h[:, None] < nheads) & (\n        offs_c[None, :] < chunk_size_limit), other=0.0)\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0)\n    ddt = ddA * A[:, None] + ddt_out\n    dt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :\n        ] < chunk_size_limit), other=0.0)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=\n            offs_h < nheads, other=0.0)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt_presoftplus = dt\n        dt = tl.where(dt <= 20.0, softplus(dt), dt)\n    clamp_mask = (dt < dt_min) | (dt > dt_max)\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] <\n        chunk_size_limit), dt, 0.0)\n    ddt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] <\n        chunk_size_limit), ddt, 0.0)\n    ddt = tl.where(clamp_mask, 0.0, ddt)\n    if DT_SOFTPLUS:\n        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(\n            dt_presoftplus), ddt)\n    tl.store(ddt_ptrs, ddt, mask=(offs_h[:, None] < nheads) & (offs_c[None,\n        :] < chunk_size_limit))\n    dA = tl.sum(ddA * dt, axis=1)\n    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)\n    if HAS_DT_BIAS:\n        ddt_bias = tl.sum(ddt, axis=1)\n        tl.atomic_add(ddt_bias_ptr + offs_h * stride_ddt_bias_head,\n            ddt_bias, mask=offs_h < nheads)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "339eb6fa-fd12-45c1-bd44-6537a5bf255d"
  },
  {
    "input": "@triton.jit\ndef _dq_prob_bwd_kernel(Q, K, dQ, LSE, dLSE, nheads, seqlen_q, seqlen_k,\n    BLOCK_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    ASM: 'tl.constexpr' = 'cvt.rna.tf32.f32 $0, $1;'\n    start_m = tl.program_id(0)\n    ndims = nheads * BLOCK_HEADDIM\n    offs_m = tl.arange(0, BLOCK_M) + start_m * BLOCK_M\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + ndims * offs_m[:, None]\n    dq_ptrs = dQ + ndims * offs_m[:, None]\n    k_ptrs = K + ndims * offs_n[:, None]\n    lse = tl.load(LSE + offs_m, mask=offs_m < seqlen_q, other=0.0)\n    dlse = tl.load(dLSE + offs_m, mask=offs_m < seqlen_q, other=0.0)\n    end_n = seqlen_k\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        for off_h in range(nheads):\n            offs_hd = (offs_d + off_h * BLOCK_HEADDIM)[None, :]\n            q = tl.load(q_ptrs + offs_hd, mask=offs_m[:, None] < seqlen_q,\n                other=0.0)\n            k = tl.load(k_ptrs + offs_hd + start_n * ndims, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n            qk += tl.dot(q, tl.trans(k))\n        qk_grad = tl.exp(qk - lse[:, None])\n        qk_grad = tl.where((start_n + offs_n)[None, :] < seqlen_k, qk_grad, 0.0\n            )\n        qk_grad = qk_grad * dlse[:, None]\n        qk_grad = tl.inline_asm_elementwise(ASM, '=r, r', [qk_grad], dtype=\n            tl.float32, is_pure=True, pack=1)\n        for off_h in range(nheads):\n            offs_hd = (offs_d + off_h * BLOCK_HEADDIM)[None, :]\n            q = tl.load(q_ptrs + offs_hd, mask=offs_m[:, None] < seqlen_q,\n                other=0.0)\n            k = tl.load(k_ptrs + offs_hd + start_n * ndims, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n            k = tl.inline_asm_elementwise(ASM, '=r, r', [k], dtype=tl.\n                float32, is_pure=True, pack=1)\n            q_grad = tl.dot(qk_grad, k)\n            dq_h = tl.load(dq_ptrs + offs_hd, mask=offs_m[:, None] <\n                seqlen_q, other=0.0)\n            tl.store(dq_ptrs + offs_hd, dq_h + q_grad, mask=offs_m[:, None] <\n                seqlen_q)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f7909772-9eb3-4190-be57-5aab63e70f67"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_delta_rule_bwd_kernel(q, k, v, d, do, dq, dk, dv, dd,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s = tl.where(m_s, b_s, 0)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n            b_dh -= tl.dot(b_d, b_dv, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n            b_dh -= tl.dot(b_d, b_dv, allow_tf32=False)\n        tl.store(p_dk, b_dk * scale, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        if i < NT - 1:\n            p_dv = tl.make_block_ptr(dv + i_bh * s_vo_h, (T, DV), (s_vo_t,\n                s_vo_d), ((i + 1) * BT, i_v * BV), (BT, BV), (1, 0))\n            b_dv = tl.load(p_dv, boundary_check=(0, 1))\n            b_dd = tl.dot(b_dv, b_h, allow_tf32=False)\n            p_dd = tl.make_block_ptr(dd + (i_bh + i_v * B * H) * s_qk_h, (T,\n                DK), (s_qk_t, s_qk_d), ((i + 1) * BT, i_k * BK), (BT, BK),\n                (1, 0))\n            tl.store(p_dd, -b_dd, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "e96ca046-6cf4-43e5-8463-bdce809997be"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n    stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr', start_n, start_m, num_steps, MASK: 'tl.constexpr'\n    ):\n    offs_m = start_m + tl.arange(0, BLOCK_M1)\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    offs_k = tl.arange(0, HEAD_DIM)\n    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n    curr_m = start_m\n    step_m = BLOCK_M1\n    for blk_idx in range(num_steps):\n        qT = tl.load(qT_ptrs)\n        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n        m = tl.load(M + offs_m)\n        qkT = tl.dot(k, qT)\n        pT = tl.math.exp2(qkT - m[None, :])\n        if MASK:\n            mask = offs_m[None, :] >= offs_n[:, None]\n            pT = tl.where(mask, pT, 0.0)\n        do = tl.load(do_ptrs)\n        ppT = pT\n        ppT = ppT\n        dv += tl.dot(ppT, do)\n        Di = tl.load(D + offs_m)\n        dpT = tl.dot(v, tl.trans(do))\n        dsT = pT * (dpT - Di[None, :])\n        dsT = dsT\n        dk += tl.dot(dsT, tl.trans(qT))\n        curr_m += step_m\n        qT_ptrs += step_m * stride_tok\n        do_ptrs += step_m * stride_tok\n    return dk, dv\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "95bce53d-bbaf-4bbc-908a-55f85241a355"
  },
  {
    "input": "@triton.jit\ndef forward_scan(gates, tokens, outputs, SEQUENCE_LENGTH: 'tl.constexpr'):\n    sequence_id = tl.num_programs(axis=1) * tl.program_id(axis=0\n        ) + tl.program_id(axis=1)\n    strides = tl.arange(0, SEQUENCE_LENGTH) + sequence_id * SEQUENCE_LENGTH\n    tokens_ = tl.load(tokens + strides)\n    gates_ = tl.load(gates + strides)\n    tuples = pack64(tokens_, gates_)\n    output_tuples_ = tl.associative_scan(tuples, axis=0, combine_fn=\n        first_order_op)\n    output_tokens_, output_gates_ = unpack64(output_tuples_)\n    tl.store(outputs + strides, output_tokens_)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "9bc4c9eb-3814-481e-9c4e-1afc82a50cd5"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef rms_norm_forward_kernel(input_pointer, weight_pointer, inv_rms_pointer,\n    output_pointer, batch_dim, feat_dim, input_batch_stride,\n    input_feat_stride, output_batch_stride, output_feat_stride, eps,\n    scale_by_weight: 'tl.constexpr', save_stats: 'tl.constexpr',\n    BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Root-mean-square-normalizes the input.\n\n    Args:\n        input_pointer: Pointer to the input to root-mean-square-normalize.\n            The input must be of shape [batch_dim, feat_dim].\n        weight_pointer: Pointer to optional weights for linear transform.\n            The weights, if provided, must be of shape [feat_dim].\n        inv_rms_pointer: Pointer to an optional container the input's inverse\n            root mean square is written to if save_stats is True.\n            The container, if provided, must be of shape [batch_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        save_stats: Flag for saving the root mean square.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    inv_rms = tl.rsqrt(tl.sum(input * input, axis=1) / feat_dim + eps)\n    output = input * inv_rms[:, None]\n    if save_stats:\n        tl.store(inv_rms_pointer + batch_offset, inv_rms, mask=batch_mask)\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        output *= weight\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "b5ad919a-c992-4423-a439-8af02030dfcf"
  },
  {
    "input": "@triton.jit\ndef triton_sum_kernel_scalar_result(input_ptr, output_ptr, M, BLOCK_SIZE_M:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE_M\n    offsets = block_start + tl.arange(0, BLOCK_SIZE_M)\n    mask = offsets < M\n    x = tl.load(input_ptr + offsets, mask=mask, other=mask)\n    output = tl.sum(x)\n    output_offsets = tl.arange(0, 1)\n    tl.store(output_ptr + output_offsets, output)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "a9f14b7c-5e36-4bf1-b8a1-3644b4a06216"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dx(dx_ptrs, dx, offs_d, headdim, even_headdim):\n    if even_headdim:\n        tl.store(dx_ptrs, dx)\n    else:\n        tl.store(dx_ptrs, dx, mask=offs_d[None, :] < headdim)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "4fc1ae0e-b52d-4560-813e-d36a9b1c7470"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_o(p, v, o, rv, cv, pv, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, BT: 'tl.constexpr', BM: 'tl.constexpr', BV:\n    'tl.constexpr', DM: 'tl.constexpr', DV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_v, i_m, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_p = tl.make_block_ptr(p + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_qk_h, (T, DV), (s_qk_t, s_qk_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_m * n_bh + i_bh) * s_qk_h, (T, DV), (\n        s_qk_t, s_qk_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_rv = tl.make_block_ptr(rv + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_cv = tl.make_block_ptr(cv + i_bh * s_sk_h, (DM, T), (s_sk_m, s_sk_t),\n        (i_m * BM, 0), (BM, BT), (0, 1))\n    p_pv = tl.make_block_ptr(pv + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hv = tl.zeros([BM, BV], dtype=tl.float32)\n    for _ in range(NT):\n        b_p = tl.load(p_p, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_rv = tl.load(p_rv, boundary_check=(0,))\n        b_cv = tl.load(p_cv, boundary_check=(0, 1))\n        b_pv = tl.load(p_pv, boundary_check=(0, 1))\n        b_p = b_p * b_pv\n        b_inter = tl.dot(b_p * b_rv[None, :], b_hv, allow_tf32=False)\n        b_intra = tl.where(m_s, tl.dot(b_p, b_cv, allow_tf32=False), 0)\n        b_intra = tl.dot(b_intra, b_v, allow_tf32=False)\n        b_o = b_inter + b_intra\n        b_hv = b_hv * b_rv[:, None] + tl.dot(b_cv, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_p = tl.advance(p_p, (BT, 0))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_rv = tl.advance(p_rv, (DM,))\n        p_cv = tl.advance(p_cv, (0, BT))\n        p_pv = tl.advance(p_pv, (BT, 0))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "2a0217d3-dee3-434e-8f0c-875fcb0ad840"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, Out, sqz, sqh, sqm, sqd, skz, skh, skn,\n    skd, svz, svh, svn, svd, soz, soh, som, sod, L, M, Z, H, N_CTX_Q,\n    N_CTX_KV, BLOCK: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    N_PREFIX_Q: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    BLOCK_M: 'tl.constexpr' = BLOCK\n    BLOCK_N: 'tl.constexpr' = BLOCK\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_m_real = (start_m + N_PREFIX_Q) * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_q = off_hz * sqh + offs_m[:, None] * sqm + offs_d[None, :] * sqd\n    offs_k = off_hz * skh + offs_n[None, :] * skn + offs_d[:, None] * skd\n    offs_v = off_hz * svh + offs_n[:, None] * svn + offs_d[None, :] * svd\n    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q_vals = tl.load(Q + offs_q, mask=offs_m[:, None] < N_CTX_Q, other=0)\n    for start_n in range(0, N_PREFIX_Q + start_m):\n        k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n        qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n        qk *= sm_scale\n        qk = tl.where(offs_m_real[:, None] >= offs_n[None, :], qk, float(\n            '-inf'))\n        landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] == \n            BLOCK_N - 1, qk, float('-inf')), 1)\n        normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1,\n            float('-inf'), qk)\n        normal_m = tl.max(normal_qk, 1)\n        normal_p = tl.exp(normal_qk - normal_m[:, None])\n        normal_denom = tl.sum(normal_p, 1)\n        m_curr = tl.maximum(landmark_qk, m_prev)\n        m_curr_ = m_curr\n        l_prev *= tl.exp(m_prev - m_curr_)\n        landmark_p = tl.exp(landmark_qk - m_curr_)\n        l_curr = landmark_p + l_prev\n        l_rcp = 1.0 / l_curr\n        landmark_p *= l_rcp\n        acc *= (l_prev * l_rcp)[:, None]\n        v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n        acc += tl.dot(landmark_p[:, None] * normal_p / normal_denom[:, None\n            ], v_vals, allow_tf32=False)\n        l_prev = l_curr\n        m_prev = m_curr\n        offs_n += BLOCK_N\n        offs_k += BLOCK_N * skn\n        offs_v += BLOCK_N * svn\n    k_vals = tl.load(K + offs_k, mask=offs_n[None, :] < N_CTX_KV, other=0)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=q_vals.dtype)\n    qk += tl.dot(q_vals, k_vals, allow_tf32=False)\n    qk *= sm_scale\n    qk = tl.where(offs_m_real[:, None] >= offs_n[None, :], qk, float('-inf'))\n    m_curr = tl.maximum(tl.max(qk, 1), m_prev)\n    m_curr_ = m_curr\n    l_prev *= tl.exp(m_prev - m_curr_)\n    p = tl.exp(qk - m_curr_[:, None])\n    l_curr = tl.sum(p, 1) + l_prev\n    l_rcp = 1.0 / l_curr\n    p *= l_rcp[:, None]\n    acc *= (l_prev * l_rcp)[:, None]\n    p = p\n    v_vals = tl.load(V + offs_v, mask=offs_n[:, None] < N_CTX_KV, other=0)\n    acc += tl.dot(p, v_vals, allow_tf32=False)\n    l_prev = l_curr\n    m_prev = m_curr\n    offs_L = off_hz * N_CTX_Q + offs_m\n    offs_M = off_hz * N_CTX_Q + offs_m\n    tl.store(L + offs_L, l_prev, mask=offs_m < N_CTX_Q)\n    tl.store(M + offs_M, m_prev, mask=offs_m < N_CTX_Q)\n    offs_o = off_hz * soh + offs_m[:, None] * som + offs_d[None, :] * sod\n    tl.store(Out + offs_o, acc, mask=offs_m[:, None] < N_CTX_Q)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "8268ed0f-fdff-43f8-a7c6-c51b63192c0f"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Logics, V, Out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs, stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od, stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_v = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len\n        ) * stride_b_loc_s\n    v_ptrs = V + off_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) *\n            stride_b_loc_s, mask=start_n + offs_n < cur_batch_seq_len,\n            other=other_kv_index)\n        qk = tl.load(Logics + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, mask\n            =start_n + offs_n < cur_batch_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5f6fb4c8-0547-400a-9875-8b038f3a1edf"
  },
  {
    "input": "@triton.jit\ndef chunk_simple_gla_bwd_kernel_dh(q, g, do, dh, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale * tl.math.exp2(tl.load(g + i_bh * T + i_t * BT +\n            tl.arange(0, BT)))[None, :]\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh *= tl.math.exp2(tl.load(g + i_bh * T + i_t * BT + BT - 1))\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a07b07cc-ce59-4da0-b87f-c5de8476a44b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "670ceeb9-147b-4855-b048-a80d2accaaf2"
  },
  {
    "input": "@triton.jit\ndef _swiglu_forward_kernel(a_ptr, b_ptr, c_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0).cast(tl.int64)\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    c_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    c_row = silu(a_row) * b_row\n    tl.store(c_ptr + col_offsets, c_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "3bc5455b-4da9-4d95-a4fe-2b231ad9ca7e"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f6662d59-265b-4f08-bdb8-f9bf5f77cb36"
  },
  {
    "input": "@triton.jit\ndef _d_softplus(grad, x):\n    z = tl.where(x >= 0, 1 / (1 + tl.exp(-x)), 1 - 1 / (1 + tl.exp(x)))\n    return grad * z\n",
    "category": "Activation Functions",
    "subcategory": "softplus",
    "uuid": "bc4e9586-1b48-450a-a260-dbf18373334d"
  },
  {
    "input": "@triton.jit\ndef mload1d(REGS: 'tl.constexpr', i_base, i_start, i_nums):\n    offs = tl.arange(0, REGS) + i_start\n    i_ptrs = i_base + offs\n    overflow = i_start + REGS - i_nums\n    i_ptrs_mask = tl.full([REGS], 1, dtype=tl.int1)\n    i_ptrs_mask = i_ptrs_mask & (offs < i_nums)\n    return tl.load(i_ptrs, mask=i_ptrs_mask, other=0.0)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "18b08eec-a8f7-48e4-843e-616308ffd4a0"
  },
  {
    "input": "@triton.jit\ndef fwd_sequential_scan_fused(v, f1, hidden, B, L, C, BLOCK_M: 'tl.constexpr'):\n    offset_b = tl.program_id(0)\n    if offset_b >= B:\n        return\n    offset_n = tl.program_id(1)\n    ptr = tl.arange(0, BLOCK_M) + offset_b * L * C + offset_n * BLOCK_M\n    h1 = tl.zeros([BLOCK_M], dtype=tl.float32)\n    for _ in range(L):\n        x0 = tl.load(v + ptr)\n        decay1 = tl.load(f1 + ptr)\n        decay1 = tl.sigmoid(decay1)\n        h1 = (h1 - x0) * decay1 + x0\n        tl.store(hidden + ptr, h1)\n        ptr += C\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "0a54abaa-de91-467d-a02b-971ec0ce4b10"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _rms_norm_bwd_kernel_sm(X, stride_x, W, DY, stride_dy, DX, stride_dx,\n    Rstd, DW, eps, M, N, rows_per_program, block_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, block_N)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    dw = tl.zeros((block_N,), dtype=tl.float32)\n    row_end = min(row_start + rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0)\n        dy = tl.load(DY + row * stride_dy + cols, mask=mask, other=0.0)\n        rstd = tl.load(Rstd + row)\n        x_hat = x * rstd\n        wdy = w * dy\n        dw += dy * x_hat\n        c1 = tl.sum(x_hat * wdy, axis=0) / N\n        dx = (wdy - x_hat * c1) * rstd\n        tl.store(DX + row * stride_dx + cols, dx, mask=mask)\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "4723601b-fb2f-49a9-8675-0eba3e13b31d"
  },
  {
    "input": "@triton.jit\ndef relu(x):\n    return tl.where(x >= 0, x, 0)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "76ea9a67-532d-4644-869b-29557029819c"
  },
  {
    "input": "@triton.jit\ndef one_shot_reduce_scatter_kernel(buffer_ptrs, signal_pad_ptrs, output_ptr,\n    numel: 'tl.constexpr', rank: 'tl.constexpr', world_size: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', NUMEL_PER_THREAD: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    per_rank_numel = numel // world_size\n    buffer_ptrs = buffer_ptrs\n    output_ptr = output_ptr\n    block_start = pid * BLOCK_SIZE\n    while block_start < per_rank_numel // NUMEL_PER_THREAD:\n        offsets = (block_start + tl.arange(0, BLOCK_SIZE)) * 2\n        mask = block_start + tl.arange(0, BLOCK_SIZE\n            ) < per_rank_numel // NUMEL_PER_THREAD\n        acc_hi = tl.zeros((BLOCK_SIZE,), tl.uint64)\n        acc_lo = tl.zeros((BLOCK_SIZE,), tl.uint64)\n        for i in range(world_size):\n            buffer_ptr = tl.load(buffer_ptrs + i\n                ) + rank * per_rank_numel // NUMEL_PER_THREAD * 2\n            hi, lo = load_128(buffer_ptr + offsets, mask=mask)\n            acc_hi, acc_lo = add_v8_bf16(acc_hi, acc_lo, hi, lo)\n        tl.store(output_ptr + offsets + 0, acc_hi, mask=mask)\n        tl.store(output_ptr + offsets + 1, acc_lo, mask=mask)\n        block_start += tl.num_programs(axis=0) * BLOCK_SIZE\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "cbcaa283-5cba-462f-8844-3a438fdface6"
  },
  {
    "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "c485b36b-dc83-46ac-b5b9-4a445b4ab560"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "ef5e8100-f36e-45f5-add7-bd85717f51fd"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "42c823b0-c548-4bd7-a6c8-70f0ae88ec9a"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr', fp8_v: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        qk = tl.dot(q, k, allow_tf32=False)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr)\n        p = p\n        acc = tl.dot(p, v, acc, allow_tf32=False)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "a7f6bf59-0ace-4366-b245-83f5456be18d"
  },
  {
    "input": "@triton.jit\ndef _fwd_none_diag_kernel(Q, K, V, Out, S, KV, GKV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', D_FBLOCK:\n    'tl.constexpr', E_FBLOCK: 'tl.constexpr', NUM_FBLOCK: 'tl.constexpr',\n    CBLOCK: 'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_h = off_bh % h\n    off_nc = tl.program_id(1)\n    off_n = off_nc // NUM_CBLOCK\n    off_c = off_nc % NUM_CBLOCK\n    off_e = tl.program_id(2)\n    n_offset = off_n * BLOCK\n    c_offset = off_c * CBLOCK\n    e_offset = off_e * E_FBLOCK\n    q_offset = off_bh * n * d + (n_offset + c_offset) * d\n    o_offset = off_bh * n * e + (n_offset + c_offset) * e + e_offset\n    kv_offset = off_bh * (NUM_BLOCK + 1) * d * e + off_n * d * e + e_offset\n    gkv_offset = off_bh * d * e + e_offset\n    Q_block_ptr = Q + q_offset + tl.arange(0, CBLOCK)[:, None] * d + tl.arange(\n        0, d)[None, :]\n    O_block_ptr = Out + o_offset + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, E_FBLOCK)[None, :]\n    KV_block_ptr = KV + kv_offset + tl.arange(0, d)[:, None] * e + tl.arange(\n        0, E_FBLOCK)[None, :]\n    GKV_block_ptr = GKV + gkv_offset + tl.arange(0, d)[:, None\n        ] * e + tl.arange(0, E_FBLOCK)[None, :]\n    S_block_ptr = S + off_h\n    s = tl.load(S_block_ptr)\n    c_array = tl.arange(0, CBLOCK)\n    GKV = tl.load(GKV_block_ptr)\n    kv = tl.load(KV_block_ptr)\n    q = tl.load(Q_block_ptr)\n    q_decay = tl.exp(-s * (c_offset + c_array[:, None]))\n    qkv_none_diag = tl.dot(q, kv) * q_decay + tl.dot(q, GKV) * tl.exp(-s *\n        (c_offset + c_array[:, None] + n_offset))\n    qkv_diag = tl.load(O_block_ptr)\n    qkv = qkv_diag + qkv_none_diag\n    tl.store(O_block_ptr, qkv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "b9929e1c-e67b-46b7-bdda-99913a148579"
  },
  {
    "input": "@triton.jit\ndef _rotary_kernel(Q, Cos, Sin, stride_qbs, stride_qh, stride_qd,\n    stride_cosbs, stride_cosd, stride_sinbs, stride_sind, max_total_len, H,\n    BLOCK_HEAD: 'tl.constexpr', BLOCK_SEQ: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr'):\n    cur_head_index = tl.program_id(0)\n    cur_seq_index = tl.program_id(1)\n    cur_head_range = cur_head_index * BLOCK_HEAD + tl.arange(0, BLOCK_HEAD)\n    cur_seq_range = cur_seq_index * BLOCK_SEQ + tl.arange(0, BLOCK_SEQ)\n    dim_range0 = tl.arange(0, BLOCK_DMODEL // 2)\n    dim_range1 = tl.arange(BLOCK_DMODEL // 2, BLOCK_DMODEL)\n    off_q0 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range0[None, None, :] * stride_qd\n    off_q1 = cur_seq_range[:, None, None] * stride_qbs + cur_head_range[\n        None, :, None] * stride_qh + dim_range1[None, None, :] * stride_qd\n    off_dimcos_sin = cur_seq_range[:, None, None] * stride_cosbs + dim_range0[\n        None, None, :] * stride_cosd\n    q0 = tl.load(Q + off_q0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    q1 = tl.load(Q + off_q1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H), other=0.0)\n    cos = tl.load(Cos + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    sin = tl.load(Sin + off_dimcos_sin, mask=cur_seq_range[:, None, None] <\n        max_total_len, other=0.0)\n    out0 = q0 * cos - q1 * sin\n    out1 = q0 * sin + q1 * cos\n    tl.store(Q + off_q0, out0, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    tl.store(Q + off_q1, out1, mask=(cur_seq_range[:, None, None] <\n        max_total_len) & (cur_head_range[None, :, None] < H))\n    return\n",
    "category": "Helper Functions",
    "subcategory": "shape manipulation",
    "uuid": "f0346c52-6252-4355-a504-92eb29b260b6"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_intra(s, z, ss, doo, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BS: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_s, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    o_i = tl.arange(0, BC)\n    m_o = tl.full([BC, BC], 1.0, dtype=tl.float32)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (s_s_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * S + i_s * BS,), (BS,), (0,))\n    p_doo = tl.make_block_ptr(doo + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n        i_t * BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_doo = tl.zeros([BC, BS], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_ss = tl.load(p_ss, boundary_check=(0, 1))\n        b_doo += b_ss * tl.exp(b_zn[None, :] - b_z)\n    b_doo = tl.exp(b_s - b_zn[None, :]) * tl.dot(m_o, b_doo, allow_tf32=False)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        p_ss = tl.make_block_ptr(ss + i_bh * s_s_h, (T * S,), (1,), ((i_t *\n            BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_ss = tl.load(p_ss, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_doo += tl.where(m_i, tl.exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)\n    b_doo += tl.load(p_doo, boundary_check=(0, 1))\n    tl.store(p_doo, b_doo, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "eed74fef-5492-44dd-b65b-95b5082358fe"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_attn_kernel(Q, K, V, B, Do, L, D, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_dob,\n    stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm, stride_dkb,\n    stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn, stride_lb,\n    stride_lh, seqlen_q, seqlen_k, headdim, nheads, Dq, Dk, Dv, HAVE_BIAS:\n    'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M: 'tl.constexpr',\n    EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    off_n, off_b, off_h = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    offs_n = off_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    offs_m = tl.arange(0, BLOCK_M)\n    q_ptrs = Q + (off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :]))\n    k_ptrs = K + (off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :]))\n    v_ptrs = V + (off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :]))\n    if HAVE_BIAS:\n        b_ptrs = B + (off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :]))\n    dq_ptrs = Dq + (off_b * stride_dqb + off_h * stride_dqh + (offs_m[:,\n        None] * stride_dqm + offs_d[None, :]))\n    dk_ptrs = Dk + (off_b * stride_dkb + off_h * stride_dkh + (offs_n[:,\n        None] * stride_dkn + offs_d[None, :]))\n    dv_ptrs = Dv + (off_b * stride_dvb + off_h * stride_dvh + (offs_n[:,\n        None] * stride_dvn + offs_d[None, :]))\n    do_ptrs = Do + (off_b * stride_dob + off_h * stride_doh + (offs_m[:,\n        None] * stride_dom + offs_d[None, :]))\n    lse_ptrs = L + (off_b * stride_lb + off_h * stride_lh + offs_m)\n    del_ptrs = D + (off_b * stride_lb + off_h * stride_lh + offs_m)\n    k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    for start_m in range(0, seqlen_q, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        m_loop_offs = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs + start_m)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=m_loop_offs[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs + start_m, mask=(m_loop_offs[:, None] <\n                seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k.T)\n        if not EVEN_N:\n            qk += tl.where(offs_n[None, :] < seqlen_k, 0, float('-inf'))\n        l = tl.load(lse_ptrs + start_m, mask=m_loop_offs < seqlen_q, other=0.0\n            )[:, None]\n        if HAVE_BIAS:\n            if EVEN_N & EVEN_M:\n                b = tl.load(b_ptrs + start_m)\n            else:\n                b = tl.load(b_ptrs + start_m, mask=(m_loop_offs[:, None] <\n                    seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + b\n            p = tl.exp(qk - l)\n        else:\n            p = tl.exp(qk * softmax_scale - l)\n        if EVEN_N & EVEN_HEADDIM:\n            do = tl.load(do_ptrs + start_m)\n        else:\n            do = tl.load(do_ptrs + start_m, mask=(m_loop_offs[:, None] <\n                seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n        dv = dv + tl.dot(p.T, do)\n        dp = tl.dot(do, v.T)\n        di = tl.load(del_ptrs + start_m, mask=m_loop_offs < seqlen_q, other=0.0\n            )\n        ds = p * (dp - di[:, None]) * softmax_scale\n        foqi = tl.dot(ds.T, q)\n        dk += foqi\n        dq = tl.dot(ds, k)\n        pq = tl.load(dq_ptrs, mask=(m_loop_offs[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0, eviction_policy='evict_last'\n            )\n        res = dq + pq\n        tl.store(dq_ptrs, value=res, mask=(m_loop_offs[:, None] < seqlen_q) &\n            (offs_d[None, :] < headdim), eviction_policy='evict_last')\n        epq = tl.load(dq_ptrs, mask=(m_loop_offs[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim), other=0.0, eviction_policy='evict_last'\n            )\n        tl.device_print('epq', epq)\n    tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n    tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[None,\n        :] < headdim))\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "1988e218-39fa-406a-92cb-1df85383b452"
  },
  {
    "input": "@triton.jit\ndef _ragged_hstu_attn_fwd_compute(Q, K, V, seq_offsets, TS, TW, PW, Bias,\n    seq2_offsets, delta_x_offsets, num_targets, Out, stride_qm, stride_qh,\n    stride_kn, stride_kh, stride_vn, stride_vh, stride_ts, stride_om,\n    stride_oh, alpha, Z, H, MAX_SEQ_LEN, DimQ, DimV, DeltaSize, num_buckets,\n    max_pos_ind, time_bucket_incr, time_bucket_div, time_delta, off_z,\n    off_h, pid, INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL: 'tl.constexpr',\n    BUCKET_FN: 'tl.constexpr', ATTN_BIAS_TYPE: 'tl.constexpr',\n    USE_TIME_BIAS: 'tl.constexpr', USE_POS_BIAS: 'tl.constexpr',\n    HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS: 'tl.constexpr',\n    IS_DELTA_Q: 'tl.constexpr', ALLOW_TF32: 'tl.constexpr', BLOCK_D_Q:\n    'tl.constexpr', BLOCK_D_V: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', MAX_ATTN_LEN: 'tl.constexpr',\n    CONTEXTUAL_SEQ_LEN: 'tl.constexpr'):\n    seq_start = tl.load(seq_offsets + off_z)\n    off_h = off_h\n    off_z = off_z\n    seq_end = tl.load(seq_offsets + off_z + 1)\n    seq_len = seq_end - seq_start\n    if IS_DELTA_Q:\n        start_m_delta = pid * BLOCK_M\n        delta_start = tl.load(delta_x_offsets + off_z * DeltaSize)\n        start_m = start_m_delta + delta_start - seq_start\n    else:\n        start_m_delta = 0\n        start_m = pid * BLOCK_M\n    if start_m < seq_len:\n        if HAS_MULTIPLE_TARGETS:\n            n_targets = tl.load(num_targets + off_z)\n        else:\n            n_targets = None\n        offs_m = start_m + tl.arange(0, BLOCK_M)\n        offs_n = tl.arange(0, BLOCK_N)\n        if IS_DELTA_Q:\n            Q_block_ptr = tl.make_block_ptr(base=Q + off_h * stride_qh + \n                off_z * DeltaSize * stride_qm, shape=(DeltaSize, BLOCK_D_Q),\n                strides=(stride_qm, 1), offsets=(start_m_delta, 0),\n                block_shape=(BLOCK_M, BLOCK_D_Q), order=(1, 0))\n        else:\n            Q_block_ptr = tl.make_block_ptr(base=Q + off_h * stride_qh + \n                seq_start * stride_qm, shape=(seq_len, BLOCK_D_Q), strides=\n                (stride_qm, 1), offsets=(start_m, 0), block_shape=(BLOCK_M,\n                BLOCK_D_Q), order=(1, 0))\n        K_block_ptr = tl.make_block_ptr(base=K + off_h * stride_kh + \n            seq_start * stride_kn, shape=(BLOCK_D_Q, seq_len), strides=(1,\n            stride_kn), offsets=(0, 0), block_shape=(BLOCK_D_Q, BLOCK_N),\n            order=(0, 1))\n        V_block_ptr = tl.make_block_ptr(base=V + off_h * stride_vh + \n            seq_start * stride_vn, shape=(seq_len, BLOCK_D_V), strides=(\n            stride_vn, 1), offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_D_V),\n            order=(1, 0))\n        mask_m = offs_m < seq_len\n        if ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS:\n            ts_0_ptrs = TS + off_z * stride_ts + offs_m\n            ts_1_ptrs = TS + off_z * stride_ts + offs_n\n            if CAUSAL:\n                ts_0 = tl.load(ts_0_ptrs + 1, mask=mask_m)\n            else:\n                ts_0 = tl.load(ts_0_ptrs, mask=mask_m)\n        elif ATTN_BIAS_TYPE == 'separate':\n            seq2_start = tl.load(seq2_offsets + off_z)\n            bias_start = seq2_start * H + off_h * seq_len * seq_len\n            off_bias = offs_m[:, None] * seq_len + offs_n[None, :]\n            bias_ptrs = Bias + bias_start + off_bias\n        q = tl.load(Q_block_ptr, boundary_check=(0,), padding_option='zero')\n        acc = tl.zeros([BLOCK_M, BLOCK_D_V], dtype=tl.float32)\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            if HAS_MULTIPLE_TARGETS:\n                if MAX_ATTN_LEN > 0:\n                    start_m_index = (seq_len - n_targets if start_m > \n                        seq_len - n_targets else start_m)\n                    low = start_m_index - MAX_ATTN_LEN\n                    low = low if low > 0 else 0\n                else:\n                    low = 0\n                uih_end = (seq_len - n_targets + BLOCK_N - 1\n                    ) // BLOCK_N * BLOCK_N\n                if uih_end < start_m:\n                    high = seq_len - n_targets\n                else:\n                    high = start_m + BLOCK_M\n                if CONTEXTUAL_SEQ_LEN > 0:\n                    if start_m < CONTEXTUAL_SEQ_LEN:\n                        high = seq_len - n_targets\n            else:\n                if MAX_ATTN_LEN > 0:\n                    low = start_m - MAX_ATTN_LEN\n                    low = low if low > 0 else 0\n                else:\n                    low = 0\n                high = start_m + BLOCK_M\n                if CONTEXTUAL_SEQ_LEN > 0:\n                    if start_m < CONTEXTUAL_SEQ_LEN:\n                        high = seq_len\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            low = start_m\n            high = seq_len\n        if low > 0:\n            K_block_ptr = tl.advance(K_block_ptr, (0, low))\n            V_block_ptr = tl.advance(V_block_ptr, (low, 0))\n        for start_n in range(low, high, BLOCK_N):\n            cur_offs_n = offs_n + start_n\n            mask_n = cur_offs_n < seq_len\n            acc += _ragged_hstu_attn_fwd_one_block(start_n=start_n, seq_len\n                =seq_len, offs_m=offs_m, offs_n=cur_offs_n, mask_m=mask_m,\n                mask_n=mask_n, q=q, K_block_ptr=K_block_ptr, V_block_ptr=\n                V_block_ptr, n_targets=n_targets if HAS_MULTIPLE_TARGETS else\n                None, ts_1_ptrs=ts_1_ptrs if ATTN_BIAS_TYPE == 'fused' and\n                USE_TIME_BIAS else None, ts_0=ts_0 if ATTN_BIAS_TYPE ==\n                'fused' and USE_TIME_BIAS else None, TW=TW, PW=PW, alpha=\n                alpha, MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=num_buckets,\n                max_pos_ind=max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN,\n                time_bucket_incr=time_bucket_incr, time_bucket_div=\n                time_bucket_div, time_delta=time_delta, bias_ptrs=bias_ptrs if\n                ATTN_BIAS_TYPE == 'separate' else None, CONTEXTUAL_SEQ_LEN=\n                CONTEXTUAL_SEQ_LEN, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n                HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS, IS_DELTA_Q=\n                IS_DELTA_Q, ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M, BLOCK_N\n                =BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        if HAS_MULTIPLE_TARGETS and INVALID_MASK_TYPE == 'lower_triangular':\n            if uih_end < start_m:\n                low_delta = start_m\n                high_delta = start_m + BLOCK_M\n                offset = low_delta - uih_end\n                K_block_ptr = tl.advance(K_block_ptr, (0, offset))\n                V_block_ptr = tl.advance(V_block_ptr, (offset, 0))\n                for start_delta in tl.range(low_delta, high_delta, BLOCK_N,\n                    num_stages=0):\n                    cur_offs_n = offs_n + start_delta\n                    mask_n = cur_offs_n < seq_len\n                    acc += _ragged_hstu_attn_fwd_one_block(start_n=\n                        start_delta, seq_len=seq_len, offs_m=offs_m, offs_n\n                        =cur_offs_n, mask_m=mask_m, mask_n=mask_n, q=q,\n                        K_block_ptr=K_block_ptr, V_block_ptr=V_block_ptr,\n                        n_targets=n_targets if HAS_MULTIPLE_TARGETS else\n                        None, ts_1_ptrs=ts_1_ptrs if ATTN_BIAS_TYPE ==\n                        'fused' and USE_TIME_BIAS else None, ts_0=ts_0 if \n                        ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS else\n                        None, TW=TW, PW=PW, alpha=alpha, MAX_SEQ_LEN=\n                        MAX_SEQ_LEN, num_buckets=num_buckets, max_pos_ind=\n                        max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN,\n                        time_bucket_incr=time_bucket_incr, time_bucket_div=\n                        time_bucket_div, time_delta=time_delta, bias_ptrs=\n                        bias_ptrs if ATTN_BIAS_TYPE == 'separate' else None,\n                        CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n                        INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL,\n                        BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n                        USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                        USE_POS_BIAS, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n                        HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS,\n                        IS_DELTA_Q=IS_DELTA_Q, ALLOW_TF32=ALLOW_TF32,\n                        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n                    K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n                    V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        if IS_DELTA_Q:\n            start_m_delta = pid * BLOCK_M\n            offs_m_delta = start_m_delta + tl.arange(0, BLOCK_M)\n            offs_v_d = tl.arange(0, BLOCK_D_V)\n            off_o = Out + off_z * DeltaSize * stride_om + off_h * stride_oh\n            out_ptrs = off_o + offs_m_delta[:, None] * stride_om + offs_v_d[\n                None, :]\n            tl.store(out_ptrs, acc, mask=(offs_m_delta < DeltaSize)[:, None])\n        else:\n            start_m = pid * BLOCK_M\n            offs_m = start_m + tl.arange(0, BLOCK_M)\n            offs_v_d = tl.arange(0, BLOCK_D_V)\n            off_o = Out + seq_start * stride_om + off_h * stride_oh\n            out_ptrs = off_o + offs_m[:, None] * stride_om + offs_v_d[None, :]\n            tl.store(out_ptrs, acc, mask=(offs_m < seq_len)[:, None])\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "0fd9c877-63a7-45c7-b1a2-e7f81dc496fa"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.\n    Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}),\n    triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_unstable_kernel(dout_ptr, out_ptr, dt_ptr,\n    ddt_ptr, x_ptr, D_ptr, ddA_cumsum_ptr, dD_ptr, chunk_size, hdim, batch,\n    seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head,\n    stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head,\n    stride_ddt_csize, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_D_head, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_ddA_cs_csize, stride_dD_batch,\n    stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D:\n    'tl.constexpr', D_HAS_HDIM: 'tl.constexpr', SUBTRACT_DDTDT:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    if HAS_D:\n        x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size *\n            stride_x_seqlen + pid_h * stride_x_head)\n        dD_ptr += (pid_b * stride_dD_batch + pid_c * stride_dD_chunk + \n            pid_h * stride_dD_head + pid_m * stride_dD_csize)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    out_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None,\n        :] * stride_out_hdim)\n    if HAS_D:\n        x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None,\n            :] * stride_x_hdim)\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    out = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    if HAS_D:\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        out -= x * D\n    ddA_cs = tl.sum(dout * out, axis=1)\n    if SUBTRACT_DDTDT:\n        dt = tl.load(dt_ptr + offs_m * stride_dt_csize, mask=offs_m <\n            chunk_size, other=0.0)\n        ddt = tl.load(ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m <\n            chunk_size, other=0.0)\n        ddA_cs -= dt * ddt\n    tl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=\n        offs_m < chunk_size)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "866b8bf7-c5cf-4c2b-87a0-5a5630a89d6b"
  },
  {
    "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_z_row, M, N, eps, BLOCK_N: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr', NORM_BEFORE_GATE:\n    'tl.constexpr', IS_RMS_NORM: 'tl.constexpr'):\n    row = tl.program_id(0)\n    group = tl.program_id(1)\n    X += row * stride_x_row + group * N\n    Y += row * stride_y_row + group * N\n    if HAS_Z:\n        Z += row * stride_z_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    if HAS_BIAS:\n        B += group * N\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_Z and not NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=cols < N)\n        x *= z * tl.sigmoid(z)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask)\n        y *= z * tl.sigmoid(z)\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "95d79c64-2028-4962-9401-3b1b4629cc68"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_X': 64}, num_warps=2),\n    triton.Config({'BLOCK_X': 128}, num_warps=2), triton.Config({'BLOCK_X':\n    256}, num_warps=2), triton.Config({'BLOCK_X': 128}, num_warps=4),\n    triton.Config({'BLOCK_X': 256}, num_warps=4)], key=['NUM_COLUMNS'])\n@triton.jit\ndef _padded_copy(a, b, indices, bin_ids, weights, bins, padded_bins,\n    NUM_COLUMNS: 'tl.constexpr', TOP_K: 'tl.constexpr', BLOCK_X:\n    'tl.constexpr', A_TO_B: 'tl.constexpr', SCALE: 'tl.constexpr'):\n    index_a = tl.load(indices + tl.program_id(0))\n    bin_idx = tl.load(bin_ids + tl.program_id(0))\n    offset_in_bin = tl.program_id(0)\n    if bin_idx > 0:\n        offset_in_bin -= tl.load(bins + bin_idx - 1)\n    index_b = offset_in_bin\n    if bin_idx > 0:\n        index_b += tl.load(padded_bins + bin_idx - 1)\n    offset = index_a // TOP_K if A_TO_B else index_a\n    a += tl.multiple_of(offset * NUM_COLUMNS, NUM_COLUMNS)\n    b += tl.multiple_of(index_b * NUM_COLUMNS, NUM_COLUMNS)\n    offsets = tl.max_contiguous(tl.arange(0, BLOCK_X), BLOCK_X)\n    scale = tl.load(weights + index_a) if SCALE else 1\n    iptr = a if A_TO_B else b\n    optr = b if A_TO_B else a\n    for i in range(tl.cdiv(NUM_COLUMNS, BLOCK_X)):\n        mask = offsets < NUM_COLUMNS\n        x = tl.load(iptr + offsets, mask=mask)\n        x = x * scale\n        tl.store(optr + offsets, x, mask=mask)\n        offsets += BLOCK_X\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "c8421e3d-c249-42e1-83e4-ab4d35ef07d2"
  },
  {
    "input": "@triton.jit\ndef _contract_pi_one(x, n, perc_foreground):\n    x_c = tl.where(n <= 1.0, x, tl.where(tl.abs(tl.abs(x) - n) <= 1e-08, (\n        1.0 / perc_foreground - (1.0 / perc_foreground - 1) / tl.abs(x)) *\n        (x / tl.abs(x)), x / n))\n    x_c = x_c * perc_foreground\n    return x_c\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "184abbb3-5937-4d12-aa48-6d5767a38b4f"
  },
  {
    "input": "@triton.jit\ndef pack64(a, b):\n    tl.static_assert(a.dtype == tl.float32)\n    tl.static_assert(b.dtype == tl.float32)\n    a = a.to(dtype=tl.uint32, bitcast=True)\n    a = a << 32\n    b = b.to(dtype=tl.uint32, bitcast=True)\n    return a | b\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "dce55332-6061-47aa-9ba4-344c91c001e5"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_gla_bwd_kernel(q, k, v, gk, gv, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', REVERSE:\n    'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * DK if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if REVERSE else 0)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[:, None] & mask_bv[None, :]\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            h = h * _gk[:, None]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            h = h * _gv[None, :]\n        h += _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += -DK if REVERSE else DK\n        p_v += -DV if REVERSE else DV\n        p_q += -DK if REVERSE else DK\n        p_do += -DV if REVERSE else DV\n        p_dq += -DK if REVERSE else DK\n        if USE_GK:\n            p_gk += -DK if REVERSE else DK\n        if USE_GV:\n            p_gv += -DV if REVERSE else DV\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        not REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        not REVERSE else 0)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        not REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        not REVERSE else 0)\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * DK if not REVERSE else 0)\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * DV if not REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if not REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if not REVERSE else 0)\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            d_h *= _gk[:, None]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            d_h *= _gv[None, :]\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do += DV if REVERSE else -DV\n        p_q += DK if REVERSE else -DK\n        p_k += DK if REVERSE else -DK\n        p_v += DV if REVERSE else -DV\n        p_dk += DK if REVERSE else -DK\n        p_dv += DV if REVERSE else -DV\n        if USE_GK:\n            p_gk += DK if REVERSE else -DK\n        if USE_GV:\n            p_gv += DV if REVERSE else -DV\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "12b7afa3-193c-40bb-8187-09ef72d8fb42"
  },
  {
    "input": "@triton.jit\ndef _exact_forward_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = 0.5 * e_row * (tl.math.erf(tl.math.rsqrt(2.0) * e_row) + 1.0)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "612f28eb-88c7-4a74-818c-6dc7aec03c51"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr',\n    offs_n: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    else:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr)\n        acc += tl.dot(p, v)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "4eee2304-12c8-4eed-b7e0-2aff233e01f9"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, stride_oz, stride_oh, stride_om,\n    stride_ok, stride_doz, stride_doh, stride_dom, stride_dok, stride_dz,\n    stride_dh, stride_dm, M, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr'):\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    Out += off_z * stride_oz + off_h * stride_oh\n    DO += off_z * stride_doz + off_h * stride_doh\n    Delta += off_z * stride_dz + off_h * stride_dh\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o_ptrs = Out + off_m[:, None] * stride_om + off_n[None, :] * stride_ok\n    do_ptrs = DO + off_m[:, None] * stride_dom + off_n[None, :] * stride_dok\n    if DIVISIBLE_M:\n        o = tl.load(o_ptrs)\n        do = tl.load(do_ptrs)\n    else:\n        mask_m = off_m < M\n        o = tl.load(o_ptrs, mask=mask_m[:, None])\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n    delta = tl.sum(o * do, axis=1)\n    d_ptrs = Delta + off_m * stride_dm\n    if DIVISIBLE_M:\n        tl.store(d_ptrs, delta)\n    else:\n        tl.store(d_ptrs, delta, mask=mask_m)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d75ff124-283b-4963-a5a6-6ef1ae054d4b"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(q_ptr, k_ptr, v_ptr, bias_ptr, mask_ptr, k_start_ptr,\n    k_end_ptr, q_offset, k_offset, v_offset, q_stride_b, q_stride_s,\n    q_stride_h, q_stride_d, k_stride_b, k_stride_s, k_stride_h, k_stride_d,\n    v_stride_b, v_stride_s, v_stride_h, v_stride_d, bias_stride_b,\n    bias_stride_h, bias_stride_sq, bias_stride_sk, mask_stride_b,\n    mask_stride_h, mask_stride_sq, mask_stride_sk, k_start_stride_b,\n    k_start_stride_h, k_start_stride_sq, k_end_stride_b, k_end_stride_h,\n    k_end_stride_sq, o_stride_b, o_stride_s, o_stride_h, o_stride_d,\n    num_heads_q, num_heads_k, seq_len_q, seq_len_k, o_ptr, is_causal:\n    'tl.constexpr', use_attention_mask: 'tl.constexpr', use_k_start:\n    'tl.constexpr', use_k_end: 'tl.constexpr', use_bias: 'tl.constexpr',\n    sm_scale: 'tl.constexpr', block_q: 'tl.constexpr', block_k:\n    'tl.constexpr', head_dim: 'tl.constexpr', use_mask_q: 'tl.constexpr',\n    use_mask_k: 'tl.constexpr', bias_bcast_sq: 'tl.constexpr',\n    mask_bcast_sq: 'tl.constexpr', dot_fn_qk: 'tl.constexpr', dot_fn_kv:\n    'tl.constexpr'):\n    \"\"\"Triton MHA forward kernel.\"\"\"\n    block_d: 'tl.constexpr' = jt.utils.next_power_of_2(head_dim.value)\n    start_q = tl.program_id(1) * block_q\n    off_h = tl.program_id(0)\n    off_b = tl.program_id(2)\n    off_h_k = off_h // (num_heads_q // num_heads_k)\n    q_ptr += off_h * q_stride_h + off_b * q_stride_b + q_offset\n    k_ptr += off_h_k * k_stride_h + off_b * k_stride_b + k_offset\n    v_ptr += off_h_k * v_stride_h + off_b * v_stride_b + v_offset\n    o_ptr += off_h * o_stride_h + off_b * o_stride_b\n    if use_bias:\n        bias_ptr += off_b * bias_stride_b + off_h * bias_stride_h\n    if use_attention_mask:\n        mask_ptr += off_b * mask_stride_b + off_h * mask_stride_h\n    if use_k_start:\n        k_start_ptr += off_b * k_start_stride_b + off_h * k_start_stride_h\n    if use_k_end:\n        k_end_ptr += off_b * k_end_stride_b + off_h * k_end_stride_h\n    q_block_ptr = tl.make_block_ptr(q_ptr, shape=(seq_len_q, head_dim),\n        strides=(q_stride_s, q_stride_d), offsets=(start_q, 0), block_shape\n        =(block_q, block_d), order=(1, 0))\n    k_block_ptr = tl.make_block_ptr(k_ptr, shape=(head_dim, seq_len_k),\n        strides=(k_stride_d, k_stride_s), offsets=(0, 0), block_shape=(\n        block_d, block_k), order=(0, 1))\n    v_block_ptr = tl.make_block_ptr(v_ptr, shape=(seq_len_k, head_dim),\n        strides=(v_stride_s, v_stride_d), offsets=(0, 0), block_shape=(\n        block_k, block_d), order=(1, 0))\n    q_boundary_check0: 'tl.constexpr' = (0,) if use_mask_q else ()\n    q_boundary_check1: 'tl.constexpr' = (1,) if head_dim != block_d else ()\n    q_boundary_check: 'tl.constexpr' = q_boundary_check0 + q_boundary_check1\n    q_padding_option: 'tl.constexpr' = 'zero' if len(q_boundary_check.value\n        ) else ''\n    k_boundary_check: 'tl.constexpr' = (0,) if head_dim != block_d else ()\n    v_boundary_check: 'tl.constexpr' = (0,) if use_mask_k else ()\n    bias_start_dim: 'tl.constexpr' = 1 if bias_bcast_sq else 0\n    bias_block_ptr = tl.make_block_ptr(bias_ptr, shape=(seq_len_q,\n        seq_len_k)[bias_start_dim:], strides=(bias_stride_sq,\n        bias_stride_sk)[bias_start_dim:], offsets=(start_q, 0)[\n        bias_start_dim:], block_shape=(block_q, block_k)[bias_start_dim:],\n        order=(1, 0)[bias_start_dim:])\n    bias_advance: 'tl.constexpr' = (0, block_k)[bias_start_dim:]\n    mask_start_dim: 'tl.constexpr' = 1 if mask_bcast_sq else 0\n    mask_block_ptr = tl.make_block_ptr(mask_ptr, shape=(seq_len_q,\n        seq_len_k)[mask_start_dim:], strides=(mask_stride_sq,\n        mask_stride_sk)[mask_start_dim:], offsets=(start_q, 0)[\n        mask_start_dim:], block_shape=(block_q, block_k)[mask_start_dim:],\n        order=(1, 0)[mask_start_dim:])\n    mask_advance: 'tl.constexpr' = (0, block_k)[mask_start_dim:]\n    k_start_block_ptr = tl.make_block_ptr(k_start_ptr, shape=(seq_len_q,),\n        strides=(k_start_stride_sq,), offsets=(start_q,), block_shape=(\n        block_q,), order=(0,))\n    k_end_block_ptr = tl.make_block_ptr(k_end_ptr, shape=(seq_len_q,),\n        strides=(k_end_stride_sq,), offsets=(start_q,), block_shape=(\n        block_q,), order=(0,))\n    span_q = start_q + tl.arange(0, block_q)\n    m_i = tl.full([block_q], float('-inf'), dtype=tl.float32)\n    l_i = tl.zeros([block_q], dtype=tl.float32)\n    acc = tl.zeros([block_q, block_d], dtype=tl.float32)\n    q = tl.load(q_block_ptr, boundary_check=q_boundary_check,\n        padding_option=q_padding_option)\n    q *= sm_scale\n    k_start = None\n    if use_k_start:\n        k_start = tl.load(k_start_block_ptr)\n        start_loop = tl.maximum(tl.min(k_start), 0)\n        blocks_to_skip = start_loop // block_k\n        start_loop = block_k * blocks_to_skip\n        for _ in range(blocks_to_skip):\n            k_block_ptr = tl.advance(k_block_ptr, (0, block_k))\n            v_block_ptr = tl.advance(v_block_ptr, (block_k, 0))\n            bias_block_ptr = tl.advance(bias_block_ptr, bias_advance.value)\n            mask_block_ptr = tl.advance(mask_block_ptr, mask_advance.value)\n    else:\n        start_loop = 0\n    k_end = None\n    if is_causal:\n        end_loop = tl.minimum(start_q // block_k * block_k, seq_len_k)\n    elif use_k_end:\n        k_end = tl.load(k_end_block_ptr)\n        end_loop = tl.minimum(tl.max(k_end), seq_len_k)\n    else:\n        end_loop = seq_len_k\n    (k_block_ptr, v_block_ptr, bias_block_ptr, mask_block_ptr, acc, m_i, l_i\n        ) = (_fwd_kernel_inner(start_loop, end_loop, q, span_q, k_block_ptr,\n        v_block_ptr, bias_block_ptr, mask_block_ptr, k_start, k_end,\n        seq_len_k, acc, m_i, l_i, bias_advance, mask_advance, False,\n        use_attention_mask, use_k_start, use_k_end, use_bias, block_k,\n        use_mask_k, k_boundary_check, v_boundary_check, dot_fn_qk, dot_fn_kv))\n    if is_causal:\n        tl.debug_barrier()\n        start_loop, end_loop = end_loop, tl.minimum(end_loop + block_k,\n            seq_len_k)\n        _, _, _, _, acc, _, l_i = _fwd_kernel_inner(start_loop, end_loop, q,\n            span_q, k_block_ptr, v_block_ptr, bias_block_ptr,\n            mask_block_ptr, k_start, k_end, seq_len_k, acc, m_i, l_i,\n            bias_advance, mask_advance, True, use_attention_mask,\n            use_k_start, use_k_end, use_bias, block_k, use_mask_k,\n            k_boundary_check, v_boundary_check, dot_fn_qk, dot_fn_kv)\n    l_i += float(jnp.finfo(jnp.float32).tiny)\n    acc /= l_i[:, None]\n    o_block_ptr = tl.make_block_ptr(o_ptr, shape=(seq_len_q, head_dim),\n        strides=(o_stride_s, o_stride_d), offsets=(start_q, 0), block_shape\n        =(block_q, block_d), order=(1, 0))\n    acc = acc\n    tl.store(o_block_ptr, acc, boundary_check=q_boundary_check)\n",
    "category": "Attention Mechanisms",
    "subcategory": "multi-head attention",
    "uuid": "4704d071-0ad7-46f8-993a-1d5fe7f1c1e4"
  },
  {
    "input": "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None})\n@triton.jit\ndef fused_recurrent_retention_bwd_kernel(q, k, v, h0, do, dq, dk, dv, dh0,\n    dht, scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    USE_FINAL_STATE_GRADIENT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = 1 - tl.math.exp2(-5 - i_h * 1.0)\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B * H + i_bh) * T * K + i_k * BK + tl.arange(0, BK)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_dq = dq + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[:, None] & mask_v[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_h, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_do = tl.load(p_do, mask=mask_v, other=0)\n        b_h = b_b * b_h + b_k[:, None] * b_v[None, :]\n        b_dq = tl.sum(b_h * b_do[None, :], axis=1) * scale\n        tl.store(p_dq, b_dq, mask=mask_k)\n        p_k += K if HEAD_FIRST else H * K\n        p_v += V if HEAD_FIRST else H * V\n        p_do += V if HEAD_FIRST else H * V\n        p_dq += K if HEAD_FIRST else H * K\n    tl.debug_barrier()\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n        p_dk = dk + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK\n            ) + (T - 1) * K\n        p_dv = dv + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV\n            ) + (T - 1) * V\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T\n             - 1) * H * K\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T\n             - 1) * H * V\n        p_do = do + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (\n            T - 1) * H * V\n        p_dk = dk + (i_v * B + i_b\n            ) * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK) + (T - 1\n            ) * H * K\n        p_dv = dv + (i_k * B + i_b\n            ) * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV) + (T - 1\n            ) * H * V\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_ht = dht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_dh += tl.load(p_ht, mask=mask_h, other=0)\n    for _ in range(T):\n        b_q = tl.load(p_q, mask=mask_k, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_do = tl.load(p_do, mask=mask_v, other=0)\n        b_dh += b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n        b_dh *= b_b\n        tl.store(p_dk, b_dk, mask=mask_k)\n        tl.store(p_dv, b_dv, mask=mask_v)\n        p_q -= K if HEAD_FIRST else H * K\n        p_k -= K if HEAD_FIRST else H * K\n        p_v -= V if HEAD_FIRST else H * V\n        p_do -= V if HEAD_FIRST else H * V\n        p_dk -= K if HEAD_FIRST else H * K\n        p_dv -= V if HEAD_FIRST else H * V\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh, mask=mask_h)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "94d25db2-3304-461f-94fc-f146d90b54ad"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef softmax_forward_kernel(input_pointer, output_pointer, batch_dim,\n    feat_dim, input_batch_stride, input_feat_stride, output_batch_stride,\n    output_feat_stride, log: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Normalizes the input using softmax.\n\n    Args:\n        input_pointer: Pointer to the input to normalize.\n            The input must be of shape [batch_dim, feat_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        log: Flag for indicating if the log of softmax should be taken.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    output_pointer += output_batch_stride * batch_offset[:, None\n        ] + output_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :], other=-float('inf'))\n    input -= tl.max(input, axis=1)[:, None]\n    numerator = tl.exp(input)\n    denominator = tl.sum(numerator, axis=1)[:, None]\n    if log:\n        output = input - tl.log(denominator)\n    else:\n        output = numerator / denominator\n    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "9bb860f5-68e3-42e2-a3b1-eb3add34c18b"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU), only support inference.\"\"\"\n    return x * 0.5 * (1.0 + tl.libdevice.erf(x / sqrt2))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "b7d56488-f697-4186-8a02-fb88d192ad1e"
  },
  {
    "input": "@triton.jit\ndef add_grad(left, right):\n    right = triton_unbroadcast(right, left.shape)\n    return left + right\n",
    "category": "Gradient Operations",
    "subcategory": "gradient accumulation",
    "uuid": "b1b4c748-e21b-4ff8-8338-b9b210153e0a"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'TILE_SIZE_M': 64, 'TILE_SIZE_N': \n    32, 'TILE_SIZE_K': 64, 'GROUP_SIZE_M': 1}, num_stages=5, num_warps=4)],\n    key=['N', 'C', 'H', 'W', 'R', 'S', 'K'])\n@triton.jit\ndef implicit_gemm_fprop_kernel(a_ptr, b_ptr, c_ptr, N, C, H, W, R, S, K,\n    stride_An, stride_Ah, stride_Aw, stride_Ac, stride_Bk, stride_Br,\n    stride_Bs, stride_Bc, stride_Cn, stride_Cp, stride_Cq, stride_Ck,\n    TILE_SIZE_M: 'tl.constexpr', TILE_SIZE_N: 'tl.constexpr', TILE_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    Pad_H = 1\n    Pad_W = 1\n    Stride_H = 1\n    Stride_W = 1\n    Dilation_H = 1\n    Dilation_W = 1\n    P = (H + Pad_H * 2 - R * Dilation_H) // Stride_H + 1\n    Q = (W + Pad_W * 2 - S * Dilation_W) // Stride_W + 1\n    GEMM_M = N * P * Q\n    GEMM_N = K\n    GEMM_K = C * R * S\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(GEMM_M, TILE_SIZE_M)\n    num_pid_n = tl.cdiv(GEMM_N, TILE_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    pq = pid_m * TILE_SIZE_M + tl.arange(0, TILE_SIZE_M)\n    q = pq % Q\n    p = pq // Q\n    k = pid_n * TILE_SIZE_N + tl.arange(0, TILE_SIZE_N)\n    crs = tl.arange(0, TILE_SIZE_K)\n    s = crs % S\n    c = crs // S // R\n    r = crs // S % R\n    a_ptrs = a_ptr + q[:, None] * stride_Aw + p[:, None] * stride_Ah + r[\n        None, :] * stride_Ah + s[None, :] * stride_Aw + c[None, :] * stride_Ac\n    b_ptrs = b_ptr + r[:, None] * stride_Br + s[:, None] * stride_Bs + c[:,\n        None] * stride_Bc + k[None, :] * stride_Bk\n    accumulator = tl.zeros((TILE_SIZE_M, TILE_SIZE_N), dtype=tl.float32)\n    for gemm_k in range(0, GEMM_K, TILE_SIZE_K):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        accumulator += tl.dot(a, b)\n        crs += TILE_SIZE_K\n        s = crs % S\n        c = crs // S // R\n        r = crs // S % R\n        a_ptrs = a_ptr + q[:, None] * stride_Aw + p[:, None] * stride_Ah + r[\n            None, :] * stride_Ah + s[None, :] * stride_Aw + c[None, :\n            ] * stride_Ac\n        b_ptrs = b_ptr + r[:, None] * stride_Br + s[:, None] * stride_Bs + c[\n            :, None] * stride_Bc + k[None, :] * stride_Bk\n    c = accumulator\n    offs_cm = pid_m * TILE_SIZE_M + tl.arange(0, TILE_SIZE_M)\n    offs_cn = pid_n * TILE_SIZE_N + tl.arange(0, TILE_SIZE_N)\n    c_ptrs = c_ptr + stride_Cq * offs_cm[:, None] + stride_Ck * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < GEMM_M) & (offs_cn[None, :] < GEMM_N)\n    tl.store(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8d98716a-520e-4a20-9603-35c8c619d8fe"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_local_cumsum_scalar_kernel(s, o, offsets, T: 'tl.constexpr', H:\n    'tl.constexpr', BT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr',\n    USE_OFFSETS: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b), tl.load(offsets + i_b + 1)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n    else:\n        p_s = tl.make_block_ptr(s + start * H + i_h, (T,), (H,), (i_t * BT,\n            ), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + start * H + i_h, (T,), (H,), (i_t * BT,\n            ), (BT,), (0,))\n    b_s = tl.load(p_s, boundary_check=(0,))\n    b_o = tl.cumsum(b_s, axis=0)\n    tl.store(p_o, b_o, boundary_check=(0,))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "205e4046-a93c-4908-b44a-f1bd8db3b090"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_apply_penalty(Logits, presence_penalty, freqency_penalty,\n    p_token_ids, p_token_counts, p_cumsum_seq_len, stride_logit_b,\n    stride_logit_s, BLOCK_P: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset <\n        cur_batch_end_index, other=0.0)\n    freq_logits = cur_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset <\n        cur_batch_end_index)\n    return\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "6ab494c0-dbf9-4102-8db4-5a370b5dc5dc"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_with_bias_calculation(Q, K, V, BW, sm_scale, L, O,\n    cu_seqlens_q, cu_seqlens_k, mid_batch, mid_start, stride_qz, stride_qh,\n    stride_qk, stride_kz, stride_kh, stride_kk, stride_vz, stride_vh,\n    stride_vk, stride_oz, stride_oh, stride_ok, stride_wn, Z, H, M, N,\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', HAS_BIAS: 'tl.constexpr',\n    NUM_BUCKETS: 'tl.constexpr', MAX_DISTANCE: 'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_z = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_b = tl.load(mid_batch + start_z)\n    off_m = tl.load(mid_start + start_z)\n    q_start = tl.load(cu_seqlens_q + off_b)\n    q_end = tl.load(cu_seqlens_q + off_b + 1)\n    k_start = tl.load(cu_seqlens_k + off_b)\n    k_end = tl.load(cu_seqlens_k + off_b + 1)\n    lM = q_end - q_start\n    lN = k_end - k_start\n    P_SEQ = lM - lN\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    L += off_m * H + off_h\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_m = offs_m_base + off_m\n    offs_m_relative = offs_m - q_start\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qz + off_h * stride_qh + offs_k[\n        None, :] * stride_qk)\n    o_ptrs = O + (offs_m[:, None] * stride_oz + off_h * stride_oh + offs_k[\n        None, :] * stride_ok)\n    l_ptrs = L + offs_m_base * H\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    mask_m = offs_m < q_end\n    q = tl.load(q_ptrs, mask=mask_m[:, None], cache_modifier='.cg')\n    if BLOCK_DMODEL < 128:\n        I = tl.where(offs_k[:, None] == offs_k, tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 1.0, dtype=input_dtype), tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 0.0, dtype=input_dtype))\n        q = tl.dot(q, I)\n    if IS_CAUSAL:\n        hi = tl.minimum(lN, P_SEQ + (off_m + 1) * BLOCK_M)\n        if lM > lN:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = lN\n    offs_n_init = k_start + offs_n_base\n    k_ptrs = K + (offs_k[:, None] * stride_vk + offs_n_init[None, :] *\n        stride_vz + off_h * stride_kh)\n    v_ptrs = V + (offs_n_init[:, None] * stride_kz + offs_k[None, :] *\n        stride_kk + off_h * stride_vh)\n    for start_n in range(0, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        mask_n = offs_n < lN\n        k = tl.load(k_ptrs, mask=mask_n[None, :], cache_modifier='.cg')\n        v = tl.load(v_ptrs, mask=mask_n[:, None], cache_modifier='.cg')\n        if HAS_BIAS:\n            relative_positions = offs_n[None, :] - offs_m_relative[:, None]\n            relative_buckets = tl.zeros_like(relative_positions)\n            num_buckets = NUM_BUCKETS\n            if not IS_CAUSAL:\n                num_buckets //= 2\n                relative_buckets += (relative_positions > 0) * num_buckets\n                relative_positions = tl.abs(relative_positions)\n            else:\n                relative_positions = tl.maximum(-relative_positions, tl.\n                    zeros_like(relative_positions))\n            max_exact = num_buckets // 2\n            is_small = relative_positions < max_exact\n            relative_position_if_large = max_exact + tl.log(\n                relative_positions.to(tl.float32) / max_exact) / tl.log(\n                MAX_DISTANCE / max_exact) * (num_buckets - max_exact)\n            relative_position_if_large = tl.minimum(relative_position_if_large,\n                num_buckets - 1)\n            relative_buckets += tl.where(is_small, relative_positions,\n                relative_position_if_large)\n            bucket_offs = relative_buckets * stride_wn + off_h\n            bias_ptrs = BW + bucket_offs\n            bias_values = tl.load(bias_ptrs, mask_m[:, None] & mask_n[None, :])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, k) * sm_scale\n        if HAS_BIAS:\n            s += bias_values\n        s = tl.where(mask_n[None, :], s, float('-inf'))\n        if IS_CAUSAL:\n            causal_mask = P_SEQ + offs_m_relative[:, None] >= offs_n[None, :]\n            s = tl.where(causal_mask, s, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(s, 1))\n        alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n        p = tl.math.exp2((s - m_i_new[:, None]) * log2e)\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        k_ptrs += BLOCK_N * stride_kz\n        v_ptrs += BLOCK_N * stride_vz\n    if IS_CAUSAL and lM > lN:\n        is_empty_line = offs_m_relative + P_SEQ < 0\n        acc = tl.where(is_empty_line[:, None], 0.0, acc * (1.0 / l_i[:, None]))\n        l = tl.where(is_empty_line, float('-inf'), m_i + tl.log(l_i))\n    else:\n        acc = acc * (1.0 / l_i[:, None])\n        l = m_i + tl.log(l_i)\n    tl.store(l_ptrs, l, mask=mask_m, cache_modifier='.cg')\n    tl.store(o_ptrs, acc, mask=mask_m[:, None], cache_modifier='.cg')\n",
    "category": "Attention Mechanisms",
    "subcategory": "multi-head attention",
    "uuid": "fd126ae7-41e5-47a7-96fc-26381a7e9538"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "6f9cf310-4ff8-415e-9e86-af4876c595db"
  },
  {
    "input": "@triton.jit\ndef fused_cross_entropy_fwd_bwd_kernel(output_loss_ptr,\n    output_logit_grad_ptr, input_logit_ptr, input_targ_ptr,\n    input_divisor_ptr, output_loss_stride, output_logit_grad_stride,\n    input_logit_stride, input_targ_stride, n_cols, ignore_index, BLOCK_SIZE:\n    'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    logit_grad_row_start_ptr = (output_logit_grad_ptr + row_idx *\n        output_logit_grad_stride)\n    logit_row_start_ptr = input_logit_ptr + row_idx * input_logit_stride\n    targ_ptr = input_targ_ptr + row_idx * input_targ_stride\n    loss_ptr = output_loss_ptr + row_idx * output_loss_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    logit_row_ptrs = logit_row_start_ptr + col_offsets\n    logit_grad_row_ptrs = logit_grad_row_start_ptr + col_offsets\n    logit_row_unnormalized = tl.load(logit_row_ptrs, mask=col_offsets <\n        n_cols, other=float('-Inf'))\n    targ = tl.load(targ_ptr)\n    divisor = tl.load(input_divisor_ptr)\n    logit_row = logit_row_unnormalized - tl.max(logit_row_unnormalized, axis=0)\n    exp_logit_row = tl.exp(logit_row)\n    sum_exp_logit_row = tl.sum(exp_logit_row, axis=0)\n    log_sum_exp_logit_row = tl.log(sum_exp_logit_row)\n    logit_gt_logit = tl.sum(tl.where(targ == col_offsets, logit_row, 0.0))\n    loss = log_sum_exp_logit_row - logit_gt_logit\n    loss = loss / divisor\n    loss = tl.where(targ == ignore_index, 0.0, loss)\n    tl.store(loss_ptr, loss)\n    targ_one_hot = tl.where(targ == col_offsets, 1.0, 0.0)\n    grad = exp_logit_row / sum_exp_logit_row - targ_one_hot\n    grad = grad / divisor\n    grad = tl.where(targ == ignore_index, 0.0, grad)\n    tl.store(logit_grad_row_ptrs, grad, mask=col_offsets < n_cols)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "29c8f262-cb88-40aa-90f2-da7c99910adc"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})\n@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})\n@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, X1, W1, B1, Y1,\n    RESIDUAL_OUT, ROWSCALE, SEEDS, DROPOUT_MASK, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_res_row, stride_res_out_row, stride_x1_row,\n    stride_y1_row, M, N, eps, dropout_p, IS_RMS_NORM: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', HAS_RESIDUAL: 'tl.constexpr',\n    STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS: 'tl.constexpr',\n    HAS_DROPOUT: 'tl.constexpr', STORE_DROPOUT_MASK: 'tl.constexpr',\n    HAS_ROWSCALE: 'tl.constexpr', HAS_X1: 'tl.constexpr', HAS_W1:\n    'tl.constexpr', HAS_B1: 'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    if HAS_X1:\n        X1 += row * stride_x1_row\n    if HAS_W1:\n        Y1 += row * stride_y1_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row)\n        x *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + row), cols, n_rounds=7) > dropout_p\n        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\n    if HAS_X1:\n        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + M + row)\n            x1 *= rowscale\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + M + row), cols, n_rounds=7\n                ) > dropout_p\n            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n            if STORE_DROPOUT_MASK:\n                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask,\n                    mask=cols < N)\n        x += x1\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n    if HAS_W1:\n        w1 = tl.load(W1 + cols, mask=mask)\n        if HAS_B1:\n            b1 = tl.load(B1 + cols, mask=mask)\n        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n        tl.store(Y1 + cols, y1, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "74042a76-26b6-4eec-96e2-c1ff9cd72fbf"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_fwd_kernel_o(q, k, v, h, o, scale, T: 'tl.constexpr',\n    H: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    if HEAD_FIRST:\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "5aee1161-1a9e-4649-873e-53dc407377b4"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, head_dim,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=(offs_h[:, None] < head_num) & (offs_d[None, :\n        ] < head_dim), other=0.0)\n    tl.store(o_ptrs, k, mask=(offs_h[:, None] < head_num) & (offs_d[None, :\n        ] < head_dim))\n    return\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "dd3cad04-4dbd-4b56-b3a5-ef906871b307"
  },
  {
    "input": "@triton.jit\ndef load_2d(ptr, sz0: 'const', sz1: 'const', n0, n1, max0, max1, stride0=\n    None, stride1=1):\n    \"\"\"Chunk 2d matrix (defined by ptr) into 2d grid, where each chunk has size (sz0,sz1). Load the (n0,n1)th chunk. Ie, load [n0*sz0,...,(n0+1)*sz0-1] x [n1*sz1,...,(n1+1)*sz1-1].\"\"\"\n    stride0 = stride0 or sz1\n    offs0 = offset_1d(sz0, n0)\n    offs1 = offset_1d(sz1, n1)\n    offs = offset_2d(offs0, offs1, stride0, stride1)\n    mask = mask_2d(offs0, offs1, max0, max1)\n    return tl.load(ptr + offs, mask)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "83175500-fe1d-48ce-aaa8-734739e87528"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd(X, Y, Mean, Rstd, D, eps, stride_x, stride_y, TRAINING:\n    'tl.constexpr', BLOCK_D: 'tl.constexpr', COMPUTE_MEAN_AND_RSTD:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x\n    Y += row * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    x = tl.load(X + cols, mask=cols < D, other=0.0)\n    if COMPUTE_MEAN_AND_RSTD:\n        mean = tl.sum(x, axis=0) / D\n    else:\n        mean = tl.load(Mean + row)\n    x_mean = tl.where(cols < D, x - mean, 0.0)\n    if COMPUTE_MEAN_AND_RSTD:\n        _var = tl.zeros([BLOCK_D], dtype=tl.float32)\n        _var += x_mean * x_mean\n        var = tl.sum(_var, axis=0) / D\n        rstd = 1 / tl.sqrt(var + eps)\n        if TRAINING:\n            tl.store(Mean + row, mean)\n            tl.store(Rstd + row, rstd)\n    else:\n        rstd = tl.load(Rstd + row)\n    mask = cols < D\n    y = x_mean * rstd\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "c9da7713-cc26-4745-bdc2-6fb2810de0e5"
  },
  {
    "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x(x, w, y, stride_xn, stride_xc, stride_xh, stride_xw,\n    stride_wn, stride_wc, stride_wh, stride_ww, stride_yn, stride_yc,\n    stride_yh, stride_yw, stride_biasn, delta_x_ptr, BATCH, IN_C, IN_H,\n    IN_W, KERNEL_N, KERNEL_H, KERNEL_W, OUT_H, OUT_W, stride_h, stride_w,\n    padding_h, padding_w, dilation_h, dilation_w, output_padding_h,\n    output_padding_w, groups, ACC_TYPE: 'tl.constexpr', CONV1X1_NHWC:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', GROUP_H: 'tl.constexpr'):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_x_ptrs = delta_x_ptr + off_x_crs\n        off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n    mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n        off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[None, :]\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w)\n        w_ptrs += BLOCK_K\n        if not CONV1X1_NHWC:\n            delta_x_ptrs += BLOCK_K\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            off_x_crs_unpacked = tl.load(delta_x_ptrs, mask=off_x_crs < CRS,\n                other=0)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n            x_ptrs += BLOCK_K\n        mask_x = ((off_x_n < BATCH) & (off_x_h >= 0) & (off_x_h < IN_H) & (\n            off_x_w >= 0) & (off_x_w < IN_W))[:, None] & (off_x_crs < CRS)[\n            None, :]\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = acc\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "9fafa88d-3117-4f6e-87a5-31fe9734f288"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['chunk_size', 'hdim', 'dstate', 'IS_CAUSAL'])\n@triton.jit\ndef _chunk_scan_fwd_kernel(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, C_ptr, prev_states_ptr, D_ptr, chunk_size,\n    hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_cb_batch,\n    stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k,\n    stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim,\n    stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim,\n    stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim,\n    stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate,\n    stride_states_batch, stride_states_chunk, stride_states_head,\n    stride_states_hdim, stride_states_dstate, stride_D_head, IS_CAUSAL:\n    'tl.constexpr', HAS_D: 'tl.constexpr', D_HAS_HDIM: 'tl.constexpr',\n    HAS_Z: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr', IS_TRITON_22:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_C_head)\n    prev_states_ptr += (pid_b * stride_states_batch + pid_c *\n        stride_states_chunk + pid_h * stride_states_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if IS_TRITON_22 or pid_c > -1:\n        offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <=\n            128 else BLOCK_SIZE_K)\n        C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate\n            [None, :] * stride_C_dstate)\n        prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n            stride_states_hdim + offs_k_dstate[:, None] * stride_states_dstate)\n        if not HAS_SEQ_IDX:\n            scale_m = tl.exp(dA_cs_m)\n        else:\n            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        if BLOCK_SIZE_DSTATE <= 128:\n            C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_k_dstate[None, :] < dstate), other=0.0)\n            prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:,\n                None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n            prev_states = prev_states\n            acc = tl.dot(C, prev_states) * scale_m[:, None]\n        else:\n            for k in range(0, dstate, BLOCK_SIZE_K):\n                C = tl.load(C_ptrs, mask=(offs_m[:, None] <\n                    chunk_size_limit) & (offs_k_dstate[None, :] < dstate -\n                    k), other=0.0)\n                prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate\n                    [:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                    other=0.0)\n                prev_states = prev_states\n                acc += tl.dot(C, prev_states)\n                C_ptrs += BLOCK_SIZE_K\n                prev_states_ptrs += BLOCK_SIZE_K\n            acc *= scale_m[:, None]\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None,\n        :] * stride_cb_csize_k)\n    x_ptrs = x_ptr + (offs_k[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    K_MAX = chunk_size_limit if not IS_CAUSAL else min((pid_m + 1) *\n        BLOCK_SIZE_M, chunk_size_limit)\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k\n            [None, :] < chunk_size - k), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k,\n            other=0.0)\n        cb *= tl.exp(dA_cs_m[:, None] - dA_cs_k[None, :])\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0)\n        cb *= dt_k\n        if IS_CAUSAL:\n            mask = offs_m[:, None] >= k + offs_k[None, :]\n            cb = tl.where(mask, cb, 0.0)\n        cb = cb\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) &\n            (offs_n[None, :] < hdim), other=0.0)\n        acc += tl.dot(cb, x)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_D:\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        x_residual = tl.load(x_ptr + (offs_m[:, None] * stride_x_seqlen + \n            offs_n[None, :] * stride_x_hdim), mask=(offs_m[:, None] <\n            chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0)\n        acc += x_residual * D\n    if HAS_Z:\n        out_x_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n            stride_out_seqlen + pid_h * stride_out_head)\n        out_x_ptrs = out_x_ptr + (stride_out_seqlen * offs_out_m[:, None] +\n            offs_out_n[None, :])\n        tl.store(out_x_ptrs, acc, mask=(offs_out_m[:, None] <\n            chunk_size_limit) & (offs_out_n[None, :] < hdim))\n        z_ptr += (pid_b * stride_z_batch + pid_c * chunk_size *\n            stride_z_seqlen + pid_h * stride_z_head)\n        z_ptrs = z_ptr + (stride_z_seqlen * offs_out_m[:, None] + \n            stride_z_hdim * offs_out_n[None, :])\n        z = tl.load(z_ptrs, mask=(offs_out_m[:, None] < chunk_size_limit) &\n            (offs_out_n[None, :] < hdim), other=0.0)\n        acc *= z * tl.sigmoid(z)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    out_ptrs = out_ptr + (stride_out_seqlen * offs_out_m[:, None] + \n        offs_out_n[None, :] * stride_out_hdim)\n    tl.store(out_ptrs, acc, mask=(offs_out_m[:, None] < chunk_size_limit) &\n        (offs_out_n[None, :] < hdim))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b47af2ba-fe84-47de-b795-0c60297f6c18"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4, pre_hook=init_to_zero([\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128},\n    num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size',\n    'dstate', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dc_kernel(dout_ptr, prev_states_ptr, C_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, dc_ptr, ddA_cumsum_ptr, chunk_size, dstate,\n    hdim, batch, seqlen, nheads, nheads_per_program, ngroups,\n    stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_prev_states_batch, stride_prev_states_chunk,\n    stride_prev_states_head, stride_prev_states_hdim,\n    stride_prev_states_dstate, stride_C_batch, stride_C_seqlen,\n    stride_C_head, stride_C_dstate, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_dc_batch, stride_dc_seqlen,\n    stride_dc_split, stride_dc_group, stride_dc_dstate, stride_ddA_cs_batch,\n    stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize,\n    HAS_DDA_CS: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dout_head)\n    dc_ptr += (pid_b * stride_dc_batch + pid_c * chunk_size *\n        stride_dc_seqlen + pid_g * stride_dc_group + pid_s * stride_dc_split)\n    prev_states_ptr += (pid_b * stride_prev_states_batch + pid_c *\n        stride_prev_states_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_prev_states_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dA_cs_head)\n    if HAS_DDA_CS:\n        C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size *\n            stride_C_seqlen + pid_g * stride_C_head)\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n            nheads_per_program) * stride_ddA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n        stride_prev_states_dstate + offs_k[:, None] * stride_prev_states_hdim)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    if HAS_DDA_CS:\n        C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None,\n            :] * stride_C_dstate)\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        c = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < dstate), other=0.0)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n    nheads_iter = min(nheads_per_program, nheads // ngroups - pid_s *\n        nheads_per_program)\n    for h in range(nheads_iter):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_k[None, :] < hdim), other=0.0)\n        prev_states = tl.load(prev_states_ptrs, mask=(offs_k[:, None] <\n            hdim) & (offs_n[None, :] < dstate), other=0.0)\n        prev_states = prev_states\n        dc = tl.dot(dout, prev_states)\n        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit,\n            other=0.0)\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_m)\n        else:\n            scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        dc *= scale[:, None]\n        if HAS_DDA_CS:\n            ddA_cs = tl.sum(dc * c, axis=1)\n            tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n        acc += dc\n        dout_ptrs += stride_dout_head\n        prev_states_ptrs += stride_prev_states_head\n        dA_cumsum_ptrs += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dc_ptrs = dc_ptr + (offs_m[:, None] * stride_dc_seqlen + offs_n[None, :\n        ] * stride_dc_dstate)\n    tl.store(dc_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < dstate))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "452bd387-c5f4-437d-a223-cee6725c92b3"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dx(dx_ptrs, dx, offs_n, offs_d, seqlen, headdim, EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', even_headdim):\n    if EVEN_N & EVEN_M:\n        if even_headdim:\n            tl.store(dx_ptrs, dx)\n        else:\n            tl.store(dx_ptrs, dx, mask=offs_d[None, :] < headdim)\n    elif even_headdim:\n        tl.store(dx_ptrs, dx, mask=offs_n[:, None] < seqlen)\n    else:\n        tl.store(dx_ptrs, dx, mask=(offs_n[:, None] < seqlen) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "3908ce65-ad18-46e5-93a6-943161fe7bdb"
  },
  {
    "input": "@triton.jit\ndef _grid_splat(to_splat, grad_image, batch_index, ix, iy, N:\n    'tl.constexpr', C: 'tl.constexpr', IH: 'tl.constexpr', IW:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    ix_nw = ix - ix % 1\n    iy_nw = iy - iy % 1\n    ix_ne = ix_nw + 1\n    iy_ne = iy_nw\n    ix_sw = ix_nw\n    iy_sw = iy_nw + 1\n    ix_se = ix_nw + 1\n    iy_se = iy_nw + 1\n    nw = (ix_se - ix) * (iy_se - iy)\n    ne = (ix - ix_sw) * (iy_sw - iy)\n    sw = (ix_ne - ix) * (iy - iy_ne)\n    se = (ix - ix_nw) * (iy - iy_nw)\n    _splat_2d(to_splat, grad_image, nw, batch_index, ix_nw, iy_nw, IH, IW,\n        C, BLOCK_SIZE)\n    _splat_2d(to_splat, grad_image, ne, batch_index, ix_ne, iy_ne, IH, IW,\n        C, BLOCK_SIZE)\n    _splat_2d(to_splat, grad_image, sw, batch_index, ix_sw, iy_sw, IH, IW,\n        C, BLOCK_SIZE)\n    _splat_2d(to_splat, grad_image, se, batch_index, ix_se, iy_se, IH, IW,\n        C, BLOCK_SIZE)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "d5d3dc85-66c6-4705-9430-098c710b8e90"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16)], key=['BK'])\n@triton.jit\ndef fwd_prepare_wy_repr_kernel_chunk64(k, beta, A, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BC: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A2 = tl.zeros([BC, BC], dtype=tl.float32)\n    b_A3 = tl.zeros([BC, BC], dtype=tl.float32)\n    if HEAD_FIRST:\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,),\n            (BC,), (0,))\n    else:\n        p_beta = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT,), (BC,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    if HEAD_FIRST:\n        p_beta2 = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT +\n            BC,), (BC,), (0,))\n    else:\n        p_beta2 = tl.make_block_ptr(beta + i_b * T * H + i_h, (T,), (H,), (\n            i_t * BT + BC,), (BC,), (0,))\n    b_beta2 = tl.load(p_beta2, boundary_check=(0,))\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BC, BK), (1, 0))\n            p_k2 = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT + BC, i_k * BK), (BC, BK), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\n            p_k2 = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT + BC, i_k * BK), (BC, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kb = b_k * b_beta[:, None]\n        b_k2 = tl.load(p_k2, boundary_check=(0, 1))\n        b_kb2 = b_k2 * b_beta2[:, None]\n        b_A += tl.dot(b_kb, tl.trans(b_k), allow_tf32=False)\n        b_A2 += tl.dot(b_kb2, tl.trans(b_k2), allow_tf32=False)\n        b_A3 += tl.dot(b_kb2, tl.trans(b_k), allow_tf32=False)\n    b_A = -tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :],\n        b_A, 0)\n    b_A2 = -tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :],\n        b_A2, 0)\n    for i in range(1, BC):\n        mask = tl.arange(0, BC) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)\n        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)\n        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)\n        b_A = tl.where(mask[:, None], b_a, b_A)\n        b_A2 = tl.where(mask[:, None], b_a2, b_A2)\n    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\n    b_A3 = -tl.dot(tl.dot(b_A2, b_A3, allow_tf32=False), b_A, allow_tf32=False)\n    if HEAD_FIRST:\n        p_A1 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BC, BC), (1, 0))\n        p_A2 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + BC, BC), (BC, BC), (1, 0))\n        p_A3 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + BC, 0), (BC, BC), (1, 0))\n        p_A4 = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, BC), (BC, BC), (1, 0))\n    else:\n        p_A1 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0))\n        p_A2 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT + BC, BC), (BC, BC), (1, 0))\n        p_A3 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT + BC, 0), (BC, BC), (1, 0))\n        p_A4 = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT, BC), (BC, BC), (1, 0))\n    tl.store(p_A1, b_A, boundary_check=(0, 1))\n    tl.store(p_A2, b_A2, boundary_check=(0, 1))\n    tl.store(p_A3, b_A3, boundary_check=(0, 1))\n    tl.store(p_A4, tl.zeros([BC, BC], dtype=tl.float32), boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "63e48929-b9d5-4591-8340-d557aaab6239"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "60cd2ea9-6fa8-4ba9-9464-e5687c303c71"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B, H, T, K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "6a56ef35-3f70-45d8-a00a-c22eab6aa8af"
  },
  {
    "input": "@triton.jit\ndef triton_cross_merge_unidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr',\n    BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp2\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp2\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _y1 = tl.load(p_y1 + _idx, mask=_mask_hw)\n        _y2 = tl.load(p_y2 + _idx, mask=_mask_hw)\n        _y3 = tl.load(p_y3 + _idx, mask=_mask_hw)\n        _y4 = tl.load(p_y4 + _idx, mask=_mask_hw)\n        tl.store(p_x + _idx, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "d82ee6fb-b9d3-4b46-b9e0-de2d58ffb189"
  },
  {
    "input": "@triton.jit\ndef swiglu_backward(grad_output_ptr, grad_e_ptr, grad_g_ptr, e_ptr, g_ptr,\n    n_cols, sigmoid_ptr, f_ptr, grad_output_stride, grad_e_stride,\n    grad_g_stride, e_stride, g_stride, sigmoid_stride, f_stride, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    col_offset = tl.arange(0, BLOCK_SIZE)\n    mask = col_offset < n_cols\n    grad_output_row = tl.load(grad_output_ptr + pid * grad_output_stride +\n        col_offset, mask=mask)\n    e_row = tl.load(e_ptr + pid * e_stride + col_offset, mask=mask)\n    g_row = tl.load(g_ptr + pid * g_stride + col_offset, mask=mask)\n    sigmoid_row = tl.load(sigmoid_ptr + pid * sigmoid_stride + col_offset,\n        mask=mask)\n    f_row = tl.load(f_ptr + pid * f_stride + col_offset, mask=mask)\n    grad_g_row = grad_output_row * f_row\n    grad_e_row = grad_output_row * g_row * sigmoid_row * (1.0 + e_row * (\n        1.0 - sigmoid_row))\n    tl.store(grad_e_ptr + pid * grad_e_stride + col_offset, grad_e_row,\n        mask=mask)\n    tl.store(grad_g_ptr + pid * grad_g_stride + col_offset, grad_g_row,\n        mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "93662b1f-bc3c-4bd2-a189-d2d53bbf0c6a"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, M, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    num_block, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_qz + off_h * stride_qh\n    V += off_z * stride_qz + off_h * stride_qh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_qz + off_h * stride_qh\n    DV += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        lo = start_n * BLOCK_M\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        D_ptrs = D + off_hz * N_CTX\n        m_ptrs = M + off_hz * N_CTX\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, k, trans_b=True)\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n            m = tl.load(m_ptrs + offs_m_curr)\n            p = tl.exp(qk * sm_scale - m[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(p, do, trans_a=True)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, v, trans_b=True)\n            ds = p * dp * sm_scale\n            dk += tl.dot(ds, q, trans_a=True)\n            dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n            dq += tl.dot(ds, k)\n            tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "0266ba98-fb0b-4094-b2c5-367089705386"
  },
  {
    "input": "@triton.jit\ndef _ln_mul_dropout_bwd_dx_du(DX, DU, DY, DW, DB, X, U, Y, W, B, Mean, Rstd,\n    stride_dx, stride_du, stride_dy, stride_x, stride_u, stride_y, D, eps,\n    seed, dropout_ratio, N, BLOCK_D: 'tl.constexpr', TRAINING:\n    'tl.constexpr', CONCAT_UX: 'tl.constexpr', COMPUTE_Y: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    tile_num = tl.num_programs(0)\n    rows_per_tile = N // tile_num\n    if pid < N % tile_num:\n        rows_per_tile += 1\n    if rows_per_tile == 0:\n        return\n    cols = tl.arange(0, BLOCK_D)\n    mask = cols < D\n    row = pid\n    X += row * stride_x\n    U += row * stride_u\n    if COMPUTE_Y:\n        Y += row * stride_y\n    DY += row * stride_dy\n    DX += row * stride_dx\n    DU += row * stride_du\n    DW = DW + pid * D + cols\n    DB = DB + pid * D + cols\n    for idx in range(0, rows_per_tile):\n        x = tl.load(X + cols, mask=mask, other=0)\n        if CONCAT_UX:\n            du = tl.load(DY + cols, mask=mask, other=0)\n            dx = tl.load(DY + D + cols, mask=mask, other=0)\n            dy = tl.load(DY + 2 * D + cols, mask=mask, other=0)\n        else:\n            du = tl.zeros([BLOCK_D], dtype=tl.float32)\n            dx = tl.zeros([BLOCK_D], dtype=tl.float32)\n            dy = tl.load(DY + cols, mask=mask, other=0)\n        if TRAINING:\n            random_offsets = row * BLOCK_D + cols\n            if CONCAT_UX:\n                random_du = tl.rand(seed, random_offsets)\n                du_keep = random_du > dropout_ratio\n                du = tl.where(du_keep, du / (1.0 - dropout_ratio), 0.0)\n                random_dx = tl.rand(seed, random_offsets + D)\n                dx_keep = random_dx > dropout_ratio\n                dx = tl.where(dx_keep, dx / (1.0 - dropout_ratio), 0.0)\n                random_dy = tl.rand(seed, random_offsets + 2 * D)\n                dy_keep = random_dy > dropout_ratio\n                dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n            else:\n                random = tl.rand(seed, random_offsets)\n                dy_keep = random > dropout_ratio\n                dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n        mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        ln = xhat * w + b\n        du += dy * ln\n        tl.store(DU + cols, du, mask=mask)\n        u = tl.load(U + cols, mask=mask, other=0)\n        dy = dy * u\n        wdy = w * dy\n        if COMPUTE_Y:\n            y = ln * u\n            if TRAINING:\n                if CONCAT_UX:\n                    u = tl.where(du_keep, u / (1.0 - dropout_ratio), 0.0)\n                    x = tl.where(dx_keep, x / (1.0 - dropout_ratio), 0.0)\n                    y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n                else:\n                    y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n            if CONCAT_UX:\n                tl.store(Y + cols, u, mask=mask)\n                tl.store(Y + D + cols, x, mask=mask)\n                tl.store(Y + 2 * D + cols, y, mask=mask)\n            else:\n                tl.store(Y + cols, y, mask=mask)\n            Y += tile_num * stride_y\n        xhat = tl.where(mask, xhat, 0.0)\n        wdy = tl.where(mask, wdy, 0.0)\n        c1 = tl.sum(xhat * wdy, axis=0) / D\n        c2 = tl.sum(wdy, axis=0) / D\n        dx += (wdy - (xhat * c1 + c2)) * rstd\n        tl.store(DX + cols, dx, mask=mask)\n        partial_dw = dy * xhat\n        partial_db = dy\n        if idx > 0:\n            partial_dw += tl.load(DW, mask=mask)\n            partial_db += tl.load(DB, mask=mask)\n        tl.store(DW, partial_dw, mask=mask)\n        tl.store(DB, partial_db, mask=mask)\n        X += tile_num * stride_x\n        U += tile_num * stride_u\n        DY += tile_num * stride_dy\n        DX += tile_num * stride_dx\n        DU += tile_num * stride_du\n        row += tile_num\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "45f5a1b8-8fa9-4fba-9281-fffd1e3e24eb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BS': 32}, num_warps=2), triton.\n    Config({'BS': 32}, num_warps=4), triton.Config({'BS': 32}, num_warps=8),\n    triton.Config({'BS': 64}, num_warps=2), triton.Config({'BS': 64},\n    num_warps=4), triton.Config({'BS': 64}, num_warps=8), triton.Config({\n    'BS': 128}, num_warps=2), triton.Config({'BS': 128}, num_warps=4),\n    triton.Config({'BS': 128}, num_warps=8)], key=['S'])\n@triton.jit\ndef recurrent_reversed_cumsum_bwd_kernel(ds, dz, s_s_h, s_s_t, T:\n    'tl.constexpr', S: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_s = i_s * BS + tl.arange(0, BS)\n    mask = o_s < S\n    b_ds = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(0, T):\n        b_dz = tl.load(dz + i_bh * s_s_h + i_t * s_s_t + o_s, mask=mask,\n            other=0)\n        b_ds = b_ds + b_dz\n        tl.store(ds + i_bh * s_s_h + i_t * s_s_t + o_s, b_ds, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "9e57eeda-c70a-4150-9565-4f396d188c43"
  },
  {
    "input": "@triton.jit\ndef _triton_gemm_a16w4_per_channel_kernel(A, B, C, scale_b, bias,\n    zero_points, M, N, K, rescale_m, rescale_n, rescale_k, stride_am,\n    stride_ak, stride_bn, stride_bk, stride_cm, stride_cn, stride_zpk,\n    stride_zpn, stride_scalek, stride_scalen, add_bias: 'tl.constexpr',\n    add_zero_points: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr',\n    SPLIT_K: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rbn[:, None] * stride_bn + rk[None, :] * stride_bk)\n    acc_l = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)\n    acc_h = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)\n    _A0 = tl.zeros((1, 1), dtype=A.dtype.element_ty)\n    _B0 = tl.zeros((1, 1), dtype=B.dtype.element_ty)\n    if add_zero_points:\n        offs_zero_points = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n        zero_points_ptrs = zero_points + offs_zero_points\n        _ZERO_POINT0 = tl.zeros([1], dtype=zero_points.dtype.element_ty)\n        zero_points_vals = tl.load(zero_points_ptrs, mask=offs_zero_points <\n            2 * N, other=_ZERO_POINT0)\n        zero_points_vals = tl.reshape(zero_points_vals, (BLOCK_N, 2))\n        zp_l, zp_h = tl.split(zero_points_vals)\n    offs_scale = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n    scale_ptrs = scale_b + offs_scale\n    _SCALE0 = tl.zeros([1], dtype=scale_b.dtype.element_ty)\n    scales = tl.load(scale_ptrs, mask=offs_scale < 2 * N, other=_SCALE0)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        k_remaining = K - k * (BLOCK_K * SPLIT_K)\n        b_int4_two = tl.load(B, mask=rk[None, :] < k_remaining, other=_B0)\n        b_int4_l = b_int4_two.__lshift__(4).to(tl.int8).__rshift__(4)\n        b_int4_h = b_int4_two.__rshift__(4)\n        a = tl.load(A, mask=rk[None, :] < k_remaining, other=_A0)\n        a = tl.trans(a)\n        if add_zero_points:\n            b_int4_l -= zp_l[:, None]\n            b_int4_h -= zp_h[:, None]\n        acc_l += tl.dot(b_int4_l, a, out_dtype=tl.float32, allow_tf32=True)\n        acc_h += tl.dot(b_int4_h, a, out_dtype=tl.float32, allow_tf32=True)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    acc_l = tl.trans(acc_l)\n    acc_h = tl.trans(acc_h)\n    acc = tl.interleave(acc_l, acc_h)\n    offs_scale = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n    scale_ptrs = scale_b + offs_scale\n    _SCALE0 = tl.zeros([1], dtype=scale_b.dtype.element_ty)\n    scales = tl.load(scale_ptrs, mask=offs_scale < 2 * N, other=_SCALE0)\n    acc *= scales[None, :]\n    acc = acc\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n    mask = (rm < M)[:, None] & (rn < 2 * N)[None, :]\n    if add_bias:\n        offs_bias = pid_n * BLOCK_N * 2 + tl.arange(0, 2 * BLOCK_N)\n        bias_ptrs = bias + offs_bias\n        _BIAS0 = tl.zeros([1], dtype=bias.dtype.element_ty)\n        bias_vals = tl.load(bias_ptrs, mask=offs_bias < 2 * N, other=_BIAS0)\n        if pid_z == 0:\n            acc += bias_vals[None, :]\n    if SPLIT_K == 1:\n        tl.store(C + rm[:, None] * stride_cm + rn[None, :], acc, mask=mask)\n    else:\n        tl.atomic_add(C + rm[:, None] * stride_cm + rn[None, :], acc, mask=mask\n            )\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "7e8ee0f7-f124-4fd9-bc98-71f20287985d"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_bwd_kernel(q, k, v, beta, do, dq, dk, dv, dbeta,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_beta = beta + i_bh * T + T - 1\n    p_dbeta = dbeta + (i_bh + i_v * B * H) * T + T - 1\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _beta = tl.load(p_beta)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :] * _beta, axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        d_beta = tl.sum(d_v * _v)\n        d_v = d_v * _beta\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        tl.store(p_dbeta, d_beta)\n        d_h -= _k[:, None] * d_v[None, :]\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n        p_dbeta -= 1\n        p_beta -= 1\n    tl.debug_barrier()\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_beta = beta + i_bh * T\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV\n        ) + DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK\n        ) + DK\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _beta = tl.load(p_beta)\n        _v *= _beta\n        h += _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        if i < T - 1:\n            d_k = tl.load(p_dk, mask=mask_bk, other=0)\n            d_v = tl.load(p_dv, mask=mask_bv, other=0)\n            d_k -= tl.sum(d_v[None, :] * h, axis=1)\n            tl.store(p_dk, d_k, mask=mask_bk)\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dk += DK\n        p_dv += DV\n        p_dq += DK\n        p_beta += 1\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "31381fe8-c1e5-42fa-a3f2-8f71ab2cdf4b"
  },
  {
    "input": "@triton.autotune(configs=configs, key=['CACHE_KEY_M', 'CACHE_KEY_N',\n    'BATCHSIZE', 'SPARSITY_BIN'])\n@triton.jit\ndef qkv_kernel(Y, A, X, threshold_q, threshold_k, threshold_v, N, N_q, N_kv,\n    M, CACHE_KEY_N, CACHE_KEY_M, BATCHSIZE: 'tl.constexpr', SPARSITY_BIN:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    start_m = tl.program_id(1)\n    is_q = start_n * BLOCK_N < N_q\n    is_v = N_q + N_kv <= start_n * BLOCK_N\n    rm = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    A_ptr = A + rm[:, None] * N + rn[None, :]\n    X_ptr = X + rm\n    Y_ptr = Y + rn\n    threshold = tl.where(is_q, threshold_q, tl.where(is_v, threshold_v,\n        threshold_k))\n    if BATCHSIZE == 1:\n        x0 = tl.load(X_ptr, mask=rm < M, other=0.0, eviction_policy=\n            'evict_last')\n        idx = tl.abs(x0) > threshold\n        a = tl.load(A_ptr, mask=idx[:, None], other=0.0, eviction_policy=\n            'evict_first')\n        acc = tl.sum(a * x0[:, None], 0)\n    rn = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    mask_n = rn < N\n    tl.atomic_add(Y_ptr, acc, mask=mask_n)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "9469e729-cdfc-41b8-87fa-5bf69eaed3b4"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages\n    =3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.\n    Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'hdim',\n    'dstate'])\n@triton.jit\ndef _chunk_state_bwd_ddAcs_stable_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, hdim, dstate,\n    batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen,\n    stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen,\n    stride_b_head, stride_b_dstate, stride_dstates_batch,\n    stride_dstates_chunk, stride_states_head, stride_states_hdim,\n    stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + pid_h * stride_states_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    offs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else\n        BLOCK_SIZE_K)\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] *\n        stride_b_dstate)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + \n        offs_k[:, None] * stride_states_dstate)\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < dstate), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_k[None, :] < dstate - k), other=0.0)\n            dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate -\n                k) & (offs_n[None, :] < hdim), other=0.0)\n            dstates = dstates\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize)\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last -\n            dA_cs_m), 0.0)\n    acc *= scale[:, None]\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0)\n    ddt = tl.sum(acc * x, axis=1)\n    ddA_cs = ddt * dt_m\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=\n        offs_m < chunk_size - 1)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "9f8cea07-d626-4ae1-bcd7-ecdb9c89e408"
  },
  {
    "input": "@triton.jit\ndef _bwd_save_into_alpha_d(alpha_d, x, stride_alpha_d1, stride_alpha_d2,\n    stride_alpha_d3, stride_alpha_d4, stride_alpha_d5, stride_merge1,\n    stride_merge2, stride_merge3, B, L, w, r, BLOCK_RD: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    if b_idx >= B:\n        return\n    span_length_left = tl.program_id(1) + 1\n    tid = tl.program_id(2)\n    start = 0\n    to_save_ptr = x + b_idx * stride_merge1 + tl.program_id(1\n        ) * stride_merge2 + tid * stride_merge3 + tl.arange(0, BLOCK_RD)\n    mask = tl.arange(0, BLOCK_RD) < r\n    while tid >= L - w - start:\n        tid -= L - w - start\n        start += 1\n    gap_start = start + span_length_left\n    gap_end = gap_start + (tid + 1)\n    end = gap_end + (w - span_length_left)\n    save = tl.load(alpha_d + b_idx * stride_alpha_d1 + gap_start *\n        stride_alpha_d2 + start * stride_alpha_d3 + gap_end *\n        stride_alpha_d4 + end * stride_alpha_d5 + tl.arange(0, BLOCK_RD),\n        mask=mask, other=0)\n    tl.store(to_save_ptr, save, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "5f934082-e618-4075-b576-67703f78b1c3"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dx(dk_ptrs, dk, offs_n, offs_d, seqlen_k, headdim,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n        _bwd_store_dx(dv_ptrs, dv, offs_n, offs_d, seqlen_k, headdim,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dx(dk_ptrs, dk, offs_n, offs_d, seqlen_k, headdim, EVEN_M=\n        EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n    _bwd_store_dx(dv_ptrs, dv, offs_n, offs_d, seqlen_k, headdim, EVEN_M=\n        EVEN_M, EVEN_N=EVEN_N, even_headdim=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "c02041f4-6ade-4946-9321-b5393a24b893"
  },
  {
    "input": "@triton.jit\ndef silu(input):\n    \"\"\"\n    Applies SiLU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by SiLU.\n    \"\"\"\n    return input * sigmoid(input)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "f0fd47cd-6ef8-44b7-8aae-7fb826f5ced0"
  },
  {
    "input": "@triton.jit\ndef _fused_moe_kernel(A, B, C, topk_weights_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens,\n    stride_am, stride_ak, stride_be, stride_bn, stride_bk, stride_cm,\n    stride_cn, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr',\n    MUL_ROUTED_WEIGHT: 'tl.constexpr', top_k: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_token[:, None] // top_k * stride_am + offs_k[None, :\n        ] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = B + off_experts * stride_be + (offs_k[:, None] * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    _A0 = tl.zeros([1, 1], dtype=a_ptrs.dtype.element_ty)\n    _B0 = tl.zeros([1, 1], dtype=b_ptrs.dtype.element_ty)\n    for k in range(tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=_A0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=_B0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    accumulator = accumulator\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = C + stride_cm * offs_token[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "5f576566-9349-425e-8f96-b7bf3b7d9448"
  },
  {
    "input": "@triton.jit\ndef _bwd_inter_kernel(Q, K, V, S, DO, DQ, DK, DV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_h = off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    S_block_ptr = S + off_h\n    DQ_block_ptr = DQ + qk_offset + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    K_block_ptr = K + qk_offset + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + tl.arange(0, CBLOCK)[None, :\n        ] * e + tl.arange(0, e)[:, None]\n    DO_block_ptr = DO + o_offset + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    off_block1 = tl.arange(0, CBLOCK)\n    off_block2 = tl.arange(0, CBLOCK)\n    c_array = tl.arange(0, CBLOCK)\n    s = tl.load(S_block_ptr)\n    block_decay = tl.exp(-s * BLOCK)\n    kv_trans = tl.zeros([e, d], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        for j in range(NUM_CBLOCK):\n            if i > 0:\n                q_decay = tl.exp(-s * (j * CBLOCK + c_array[:, None]))\n                do = tl.load(DO_block_ptr, mask=off_block1[:, None] < n,\n                    other=0.0)\n                dq_inter = tl.dot(do, kv_trans) * q_decay\n                dq = dq_inter + tl.load(DQ_block_ptr, mask=off_block1[:,\n                    None] < n, other=0.0)\n                tl.store(DQ_block_ptr, dq, mask=off_block1[:, None] < n)\n            DQ_block_ptr += CBLOCK * d\n            DO_block_ptr += CBLOCK * e\n            off_block1 += CBLOCK\n        kv_trans_current = tl.zeros([e, d], dtype=tl.float32)\n        for j in range(NUM_CBLOCK):\n            v_trans = tl.load(V_trans_block_ptr, mask=off_block2[None, :] <\n                n, other=0.0)\n            k = tl.load(K_block_ptr, mask=off_block2[:, None] < n, other=0.0)\n            k_decay = tl.exp(-s * (BLOCK - (j * CBLOCK + c_array[:, None])))\n            kv_trans_current += tl.dot(v_trans, k * k_decay)\n            K_block_ptr += CBLOCK * d\n            V_trans_block_ptr += CBLOCK * e\n            off_block2 += CBLOCK\n        kv_trans = block_decay * kv_trans + kv_trans_current\n    m = NUM_BLOCK * BLOCK\n    off_block1 = m + tl.arange(0, CBLOCK)\n    off_block2 = m + tl.arange(0, CBLOCK)\n    Q_trans_block_ptr = Q + qk_offset + m * d + tl.arange(0, CBLOCK)[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    K_block_ptr = K + qk_offset + m * d + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + m * e + tl.arange(0, CBLOCK)[None, :\n        ] * e + tl.arange(0, e)[:, None]\n    DK_trans_block_ptr = DK + qk_offset + m * d + tl.arange(0, CBLOCK)[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + m * e + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + m * e + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    dkv = tl.zeros([d, e], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            K_block_ptr -= CBLOCK * d\n            V_trans_block_ptr -= CBLOCK * e\n            DK_trans_block_ptr -= CBLOCK * d\n            DV_block_ptr -= CBLOCK * e\n            off_block1 -= CBLOCK\n            if i < NUM_BLOCK - 1:\n                k = tl.load(K_block_ptr, mask=off_block1[:, None] < n,\n                    other=0.0)\n                v_trans = tl.load(V_trans_block_ptr, mask=off_block1[None,\n                    :] < n, other=0.0)\n                k_decay_trans = tl.exp(-s * (BLOCK - (j * CBLOCK + c_array[\n                    None, :])))\n                k_decay = tl.exp(-s * (BLOCK - (j * CBLOCK + c_array[:, None]))\n                    )\n                dk_inter_trans = tl.dot(dkv, v_trans) * k_decay_trans\n                dv_inter = tl.dot(k, dkv) * k_decay\n                dk_trans = dk_inter_trans + tl.load(DK_trans_block_ptr,\n                    mask=off_block1[None, :] < n, other=0.0)\n                dv = dv_inter + tl.load(DV_block_ptr, mask=off_block1[:,\n                    None] < n, other=0.0)\n                tl.store(DK_trans_block_ptr, dk_trans, mask=off_block1[None,\n                    :] < n)\n                tl.store(DV_block_ptr, dv, mask=off_block1[:, None] < n)\n        dkv_current = tl.zeros([d, e], dtype=tl.float32)\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            DO_block_ptr -= CBLOCK * e\n            Q_trans_block_ptr -= CBLOCK * d\n            off_block2 -= CBLOCK\n            do = tl.load(DO_block_ptr, mask=off_block2[:, None] < n, other=0.0)\n            q_trans = tl.load(Q_trans_block_ptr, mask=off_block2[None, :] <\n                n, other=0.0)\n            q_decay_trans = tl.exp(-s * (j * CBLOCK + c_array[None, :]))\n            dkv_current += tl.dot(q_trans * q_decay_trans, do)\n        dkv = block_decay * dkv + dkv_current\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a6f0129f-c123-477e-9731-91fa80638adc"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "c56e44a5-0cd7-44c8-8e13-178dbb872391"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dx(dx_ptrs, dx, offs_n, offs_d, seqlen, headdim, EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', even_headdim):\n    if EVEN_N & EVEN_M:\n        if even_headdim:\n            tl.store(dx_ptrs, dx)\n        else:\n            tl.store(dx_ptrs, dx, mask=offs_d[None, :] < headdim)\n    elif even_headdim:\n        tl.store(dx_ptrs, dx, mask=offs_n[:, None] < seqlen)\n    else:\n        tl.store(dx_ptrs, dx, mask=(offs_n[:, None] < seqlen) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "5ade5358-c2ab-4a79-8737-a58ead1af3dd"
  },
  {
    "input": "@triton.jit\ndef _geglu_tanh_backward_kernel(dc, a, b, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0).cast(tl.int64)\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    db_row = dc_row * geglu_a\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = 0.5 * a_row * (1 - tanh_sq) * (sqrt_2_over_pi * (1 + 3 * \n        0.044715 * a_row * a_row))\n    da_row = dc_row * b_row * (term1 + term2)\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "ae7c61ff-3d61-4c7d-af82-de1e7250b548"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2)], key=['BT'])\n@triton.jit\ndef save_intra_chunk_attn(A, A_local, T: 'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_A = tl.make_block_ptr(A + i_bh * T * T, (T, T), (T, 1), (i_t * BT, \n        i_t * BT), (BT, BT), (1, 0))\n    p_A_local = tl.make_block_ptr(A_local + i_bh * T * BT, (T, BT), (BT, 1),\n        (i_t * BT, 0), (BT, BT), (1, 0))\n    b_A_local = tl.load(p_A_local, boundary_check=(0, 1))\n    tl.store(p_A, b_A_local, boundary_check=(0, 1))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "01a88d01-3634-4c75-bb69-91e5ea06384d"
  },
  {
    "input": "@triton.jit\ndef _fused_moe_a8w8_kernel(A, B, C, alpha_row_ptr, alpha_col_ptr,\n    topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr,\n    num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am,\n    stride_ak, stride_be, stride_bn, stride_bk, stride_cm, stride_cn,\n    stride_scale_be, stride_scale_bn, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr', MUL_ROUTED_WEIGHT: 'tl.constexpr', top_k:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_token[:, None] // top_k * stride_am + offs_k[None, :\n        ] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = B + off_experts * stride_be + (offs_bn[None, :] * stride_bn + \n        offs_k[:, None] * stride_bk)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    _A0 = tl.zeros([1, 1], dtype=a_ptrs.dtype.element_ty)\n    _B0 = tl.zeros([1, 1], dtype=b_ptrs.dtype.element_ty)\n    lo = 0\n    hi = tl.cdiv(K, BLOCK_SIZE_K)\n    for k in range(lo, hi - 1):\n        a = tl.load(a_ptrs, mask=token_mask[:, None], other=_A0)\n        b = tl.load(b_ptrs)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    for k in range(hi - 1, hi):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=_A0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=_B0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    alpha_row_ptrs = alpha_row_ptr + offs_token // top_k\n    alpha_col_ptrs = alpha_col_ptr + off_experts * stride_scale_be + offs_cn\n    _ALPHA0 = tl.zeros([1], dtype=alpha_row_ptr.dtype.element_ty)\n    alpha_row = tl.load(alpha_row_ptrs, mask=token_mask, other=_ALPHA0)\n    alpha_col = tl.load(alpha_col_ptrs, mask=offs_cn < N, other=_ALPHA0)\n    accumulator = accumulator * alpha_row[:, None]\n    accumulator = accumulator * alpha_col[None, :]\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0)\n        accumulator = accumulator * moe_weight[:, None]\n    accumulator = accumulator\n    c_ptrs = C + stride_cm * offs_token[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "29138c82-62a3-4545-88a5-16b2a136699e"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4, pre_hook=init_to_zero([\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128},\n    num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size',\n    'dstate', 'hdim'])\n@triton.jit\ndef _chunk_state_bwd_db_kernel(x_ptr, dstates_ptr, b_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, db_ptr, ddA_cumsum_ptr, chunk_size, dstate,\n    hdim, batch, seqlen, nheads, nheads_per_program, ngroups,\n    stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim,\n    stride_dstates_batch, stride_dstates_chunk, stride_states_head,\n    stride_states_hdim, stride_states_dstate, stride_b_batch,\n    stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dt_batch,\n    stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch,\n    stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize,\n    stride_seq_idx_batch, stride_seq_idx_seqlen, stride_db_batch,\n    stride_db_seqlen, stride_db_split, stride_db_group, stride_db_dstate,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize, HAS_DDA_CS: 'tl.constexpr', HAS_SEQ_IDX:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (\n        pid_g * (nheads // ngroups) + pid_s * nheads_per_program\n        ) * stride_x_head\n    db_ptr += (pid_b * stride_db_batch + pid_c * chunk_size *\n        stride_db_seqlen + pid_g * stride_db_group + pid_s * stride_db_split)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_states_head)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g *\n        (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dA_cs_head)\n    if HAS_DDA_CS:\n        b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size *\n            stride_b_seqlen + pid_g * stride_b_head)\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n            nheads_per_program) * stride_ddA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_k[None, :] *\n        stride_x_hdim)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_dstate + \n        offs_k[:, None] * stride_states_hdim)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    if HAS_DDA_CS:\n        b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_n[None,\n            :] * stride_b_dstate)\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < dstate), other=0.0)\n    if HAS_SEQ_IDX:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n    nheads_iter = min(nheads_per_program, nheads // ngroups - pid_s *\n        nheads_per_program)\n    for h in range(nheads_iter):\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < hdim), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < hdim) & (\n            offs_n[None, :] < dstate), other=0.0)\n        dstates = dstates\n        db = tl.dot(x, dstates)\n        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) *\n            stride_dA_cs_csize)\n        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0)\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0)\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_last - dA_cs_m)\n        else:\n            scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last -\n                dA_cs_m), 0.0)\n        db *= (scale * dt_m)[:, None]\n        if HAS_DDA_CS:\n            ddA_cs = tl.sum(db * b, axis=1)\n            tl.atomic_add(ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs,\n                mask=offs_m < chunk_size - 1)\n        acc += db\n        x_ptrs += stride_x_head\n        dstates_ptrs += stride_states_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_ptr += stride_dA_cs_head\n        dA_cumsum_ptrs += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    db_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :\n        ] * stride_db_dstate)\n    tl.store(db_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < dstate))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "8ac1a67a-bd90-428c-94d1-4e82db33be6b"
  },
  {
    "input": "@triton.jit\ndef element_mul_kernel(X_ptr, X_stride, grad_output_ptr, n_cols, BLOCK_SIZE:\n    'tl.constexpr'):\n    program_id = tl.program_id(0)\n    X_ptr += program_id * X_stride\n    grad_output = tl.load(grad_output_ptr)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols)\n        tl.store(X_ptr + X_offsets, X_block * grad_output, mask=X_offsets <\n            n_cols)\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "94fc1620-bbad-4824-be46-3102d33e5305"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _rms_norm_fwd_kernel(X, stride_x, Y, stride_y, W, Rstd, eps, M, N,\n    block_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, block_N)\n    mask = cols < N\n    x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0)\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    x_hat = x * rstd\n    y = x_hat * w\n    tl.store(Y + row * stride_y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "e688e6ac-c6f6-43ee-a318-913dffb587ab"
  },
  {
    "input": "@triton.jit\ndef gelu_approx_grad(x):\n    tanh_out = tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n    return 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + \n        0.1070322243 * x * x)) + 0.5 * (1 + tanh_out)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "b9cc02c0-1843-4c17-a479-a913aa0b4ba1"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_fwd_kernel_h(k, v, h, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, \n            1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = d_b * b_h + tl.dot(b_k, b_v * d_i[:, None], allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "79d57db1-23ae-4254-bc09-3090588c5e78"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n    stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', start_n, start_m, num_steps, MASK:\n    'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M1)\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n    curr_m = start_m\n    step_m = BLOCK_M1\n    for blk_idx in range(num_steps):\n        qT = tl.load(qT_ptrs)\n        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n        m = tl.load(M + offs_m)\n        qkT = tl.dot(k, qT)\n        pT = tl.math.exp2(qkT - m[None, :])\n        if MASK:\n            mask = offs_m[None, :] >= offs_n[:, None]\n            pT = tl.where(mask, pT, 0.0)\n        do = tl.load(do_ptrs)\n        ppT = pT\n        ppT = ppT\n        dv += tl.dot(ppT, do)\n        Di = tl.load(D + offs_m)\n        dpT = tl.dot(v, tl.trans(do))\n        dsT = pT * (dpT - Di[None, :])\n        dsT = dsT\n        dk += tl.dot(dsT, tl.trans(qT))\n        curr_m += step_m\n        qT_ptrs += step_m * stride_tok\n        do_ptrs += step_m * stride_tok\n    return dk, dv\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e15e4413-fb69-4dc5-ad81-a9e871c25916"
  },
  {
    "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV']),\n    'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None})\n@triton.jit\ndef parallel_simple_gla_fwd_kernel(q, k, v, g, o, attn, s_k_h, s_k_t, s_v_h,\n    s_v_t, scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr',\n    K: 'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NV:\n    'tl.constexpr', OUTPUT_ATTENTIONS: 'tl.constexpr'):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        i_k * BK), (BT, BK), (1, 0))\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, 0), (BT, BS), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_s in range(0, i_t * BT, BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_s,\n            i_v * BV), (BS, BV), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_gn = tl.load(g + i_bh * T + min(i_s + BS, T) - 1)\n        b_gp = tl.load(g + i_bh * T + i_s - 1) if i_s % BT > 0 else 0.0\n        b_kg = b_k * tl.exp(b_gn - b_g)\n        b_s = tl.dot(b_q, b_kg, allow_tf32=False)\n        if i_s > 0:\n            b_o = b_o * tl.exp(b_gn - b_gp)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s, boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n    tl.debug_barrier()\n    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    b_gq = tl.load(p_g, boundary_check=(0,))\n    b_o *= tl.exp(b_gq)[:, None]\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, i_t * BT), (BT, BS), (1, 0))\n    o_q = i_t * BT + tl.arange(0, BT)\n    o_k = i_t * BT + tl.arange(0, BS)\n    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_s,\n            i_v * BV), (BS, BV), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_s,), (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.where(m_s, tl.dot(b_q, b_k, allow_tf32=False) * tl.exp(\n            b_gq[:, None] - b_gk[None, :]), 0)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s, boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n        o_k += BS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "1893636e-f9fd-4f59-866a-cf58d5bf1a35"
  },
  {
    "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "ac82dea3-fe07-4cea-b10a-03e4a96e4dec"
  },
  {
    "input": "@triton.jit\ndef _fused_moe_kernel_a16w4_perchannel(A, B, C, scale_b_ptr,\n    zero_points_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr,\n    num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am,\n    stride_ak, stride_be, stride_bn, stride_bk, stride_cm, stride_cn,\n    stride_scale_be, stride_scale_bn, stride_scale_bk, stride_zero_points_e,\n    stride_zero_points_n, stride_zero_points_k, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', MUL_ROUTED_WEIGHT:\n    'tl.constexpr', top_k: 'tl.constexpr', add_zero_points: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N * 2) // 2) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_token[:, None] // top_k * stride_am + offs_k[None, :\n        ] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = B + off_experts * stride_be + (offs_k[None, :] * stride_bk + \n        offs_bn[:, None] * stride_bn)\n    if add_zero_points:\n        offs_zero_points = pid_n * BLOCK_SIZE_N * 2 + tl.arange(0, 2 *\n            BLOCK_SIZE_N)\n        zero_points_ptrs = (zero_points_ptr + off_experts *\n            stride_zero_points_e + offs_zero_points)\n        _ZERO_POINT0 = tl.zeros([1], dtype=zero_points_ptr.dtype.element_ty)\n        zero_points_vals = tl.load(zero_points_ptrs, mask=offs_zero_points <\n            2 * N, other=_ZERO_POINT0)\n    _A0 = tl.zeros([1, 1], dtype=A.dtype.element_ty)\n    _B0 = tl.zeros([1, 1], dtype=B.dtype.element_ty)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N * 2), dtype=tl.float32)\n    l_shifter = (1 - tl.arange(0, BLOCK_SIZE_N * 2) % 2) * 4\n    for k in range(tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=_A0)\n        b = tl.load(b_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=_B0)\n        b = (b << l_shifter[:, None]).__rshift__(4)\n        if add_zero_points:\n            b -= zero_points_vals[:, None]\n        b = tl.trans(b)\n        b = b\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    offs_scale = pid_n * BLOCK_SIZE_N * 2 + tl.arange(0, BLOCK_SIZE_N * 2)\n    scale_ptrs = (scale_b_ptr + off_experts * stride_scale_be + offs_scale *\n        stride_scale_bn)\n    _SCALE0 = tl.zeros([1], dtype=scale_b_ptr.dtype.element_ty)\n    scales = tl.load(scale_ptrs, mask=offs_scale < 2 * N, other=_SCALE0)\n    accumulator *= scales[None, :]\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0.0)\n        accumulator = accumulator * moe_weight[:, None]\n    accumulator = accumulator\n    offs_cn = pid_n * BLOCK_SIZE_N * 2 + tl.arange(0, BLOCK_SIZE_N * 2)\n    c_ptrs = C + stride_cm * offs_token[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N * 2)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "bc885294-6cb0-47e6-a5f0-e77ee5af0ae8"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8,\n    pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=\n    init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=\n    init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=\n    init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages\n    =5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])\n@triton.jit\ndef _chunk_state_bwd_dx_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr,\n    dA_cumsum_ptr, dx_ptr, ddt_ptr, ddA_cumsum_ptr, chunk_size, hdim,\n    dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch,\n    stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch,\n    stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_batch,\n    stride_dstates_chunk, stride_states_head, stride_states_hdim,\n    stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_dx_batch,\n    stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch,\n    stride_ddt_chunk, stride_ddt_head, stride_ddt_csize,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr', BLOCK_SIZE_DSTATE:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + pid_h * stride_states_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    offs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else\n        BLOCK_SIZE_K)\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] *\n        stride_b_dstate)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + \n        offs_k[:, None] * stride_states_dstate)\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < dstate), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_k[None, :] < dstate - k), other=0.0)\n            dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate -\n                k) & (offs_n[None, :] < hdim), other=0.0)\n            dstates = dstates\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0)\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0)\n    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n    ddA_cs = -(ddt * dt_m)\n    ddA_cs_last = -tl.sum(ddA_cs)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize,\n        ddA_cs_last)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (pid_b * stride_dx_batch + pid_c * chunk_size *\n        stride_dx_seqlen + pid_h * stride_dx_head)\n    dx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :\n        ] * stride_dx_hdim)\n    tl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "8d045bdb-da59-42da-8bdb-a7b6a1600aa7"
  },
  {
    "input": "@triton.jit\ndef forward_scan(gates, tokens, outputs, SEQUENCE_LENGTH: 'tl.constexpr'):\n    sequence_id = tl.num_programs(axis=1) * tl.program_id(axis=0\n        ) + tl.program_id(axis=1)\n    strides = tl.arange(0, SEQUENCE_LENGTH) + sequence_id * SEQUENCE_LENGTH\n    tokens_ = tl.load(tokens + strides)\n    gates_ = tl.load(gates + strides)\n    tuples = pack64(tokens_, gates_)\n    output_tuples_ = tl.associative_scan(tuples, axis=0, combine_fn=\n        first_order_op)\n    output_tokens_, output_gates_ = unpack64(output_tuples_)\n    tl.store(outputs + strides, output_tokens_)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "da84f8e9-3394-4144-968d-02b11b342528"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['chunk_size', 'hdim', 'dstate', 'IS_CAUSAL'])\n@triton.jit\ndef _chunk_scan_fwd_kernel(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, C_ptr, prev_states_ptr, D_ptr, chunk_size,\n    hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_cb_batch,\n    stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k,\n    stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim,\n    stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim,\n    stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim,\n    stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate,\n    stride_states_batch, stride_states_chunk, stride_states_head,\n    stride_states_hdim, stride_states_dstate, stride_D_head, IS_CAUSAL:\n    'tl.constexpr', HAS_D: 'tl.constexpr', D_HAS_HDIM: 'tl.constexpr',\n    HAS_Z: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr', IS_TRITON_22:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_C_head)\n    prev_states_ptr += (pid_b * stride_states_batch + pid_c *\n        stride_states_chunk + pid_h * stride_states_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if IS_TRITON_22 or pid_c > -1:\n        offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <=\n            128 else BLOCK_SIZE_K)\n        C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate\n            [None, :] * stride_C_dstate)\n        prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n            stride_states_hdim + offs_k_dstate[:, None] * stride_states_dstate)\n        if not HAS_SEQ_IDX:\n            scale_m = tl.exp(dA_cs_m)\n        else:\n            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        if BLOCK_SIZE_DSTATE <= 128:\n            C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_k_dstate[None, :] < dstate), other=0.0)\n            prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:,\n                None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n            prev_states = prev_states\n            acc = tl.dot(C, prev_states) * scale_m[:, None]\n        else:\n            for k in range(0, dstate, BLOCK_SIZE_K):\n                C = tl.load(C_ptrs, mask=(offs_m[:, None] <\n                    chunk_size_limit) & (offs_k_dstate[None, :] < dstate -\n                    k), other=0.0)\n                prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate\n                    [:, None] < dstate - k) & (offs_n[None, :] < hdim),\n                    other=0.0)\n                prev_states = prev_states\n                acc += tl.dot(C, prev_states)\n                C_ptrs += BLOCK_SIZE_K\n                prev_states_ptrs += BLOCK_SIZE_K\n            acc *= scale_m[:, None]\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None,\n        :] * stride_cb_csize_k)\n    x_ptrs = x_ptr + (offs_k[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    K_MAX = chunk_size_limit if not IS_CAUSAL else min((pid_m + 1) *\n        BLOCK_SIZE_M, chunk_size_limit)\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k\n            [None, :] < chunk_size - k), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k,\n            other=0.0)\n        cb *= tl.exp(dA_cs_m[:, None] - dA_cs_k[None, :])\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0)\n        cb *= dt_k\n        if IS_CAUSAL:\n            mask = offs_m[:, None] >= k + offs_k[None, :]\n            cb = tl.where(mask, cb, 0.0)\n        cb = cb\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) &\n            (offs_n[None, :] < hdim), other=0.0)\n        acc += tl.dot(cb, x)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_D:\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        x_residual = tl.load(x_ptr + (offs_m[:, None] * stride_x_seqlen + \n            offs_n[None, :] * stride_x_hdim), mask=(offs_m[:, None] <\n            chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0)\n        acc += x_residual * D\n    if HAS_Z:\n        out_x_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n            stride_out_seqlen + pid_h * stride_out_head)\n        out_x_ptrs = out_x_ptr + (stride_out_seqlen * offs_out_m[:, None] +\n            offs_out_n[None, :])\n        tl.store(out_x_ptrs, acc, mask=(offs_out_m[:, None] <\n            chunk_size_limit) & (offs_out_n[None, :] < hdim))\n        z_ptr += (pid_b * stride_z_batch + pid_c * chunk_size *\n            stride_z_seqlen + pid_h * stride_z_head)\n        z_ptrs = z_ptr + (stride_z_seqlen * offs_out_m[:, None] + \n            stride_z_hdim * offs_out_n[None, :])\n        z = tl.load(z_ptrs, mask=(offs_out_m[:, None] < chunk_size_limit) &\n            (offs_out_n[None, :] < hdim), other=0.0)\n        acc *= z * tl.sigmoid(z)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    out_ptrs = out_ptr + (stride_out_seqlen * offs_out_m[:, None] + \n        offs_out_n[None, :] * stride_out_hdim)\n    tl.store(out_ptrs, acc, mask=(offs_out_m[:, None] < chunk_size_limit) &\n        (offs_out_n[None, :] < hdim))\n",
    "category": "Memory Management",
    "subcategory": "memory padding",
    "uuid": "ebd759b7-42d4-41a8-901e-e69e72878afe"
  },
  {
    "input": "@triton.jit\ndef rope_kernel_fw(input_ptr, in_seq_len_stride, in_batch_stride,\n    output_ptr, cos_ptr, sin_ptr, cos_stride, sin_stride, seq_len, head_dim,\n    BLOCK_SIZE: 'tl.constexpr', BATCH_NUM: 'tl.constexpr'):\n    pid_seq = tl.program_id(axis=0)\n    pid_head = tl.program_id(axis=1)\n    head_dim_offset = tl.arange(0, BLOCK_SIZE)\n    head_dim_mid = head_dim // 2\n    mask = head_dim_offset < head_dim_mid\n    cos_offset = pid_seq % seq_len * cos_stride + head_dim_offset\n    sin_offset = pid_seq % seq_len * sin_stride + head_dim_offset\n    cos = tl.load(cos_ptr + cos_offset, mask=mask, other=0.0)\n    sin = tl.load(sin_ptr + sin_offset, mask=mask, other=0.0)\n    for batch_idx in tl.static_range(0, BATCH_NUM):\n        x1_offset = (pid_seq * in_seq_len_stride + batch_idx *\n            in_batch_stride + pid_head * head_dim + head_dim_offset)\n        x2_offset = (pid_seq * in_seq_len_stride + batch_idx *\n            in_batch_stride + pid_head * head_dim + head_dim_mid +\n            head_dim_offset)\n        x1 = tl.load(input_ptr + x1_offset, mask=mask, other=0.0)\n        x2 = tl.load(input_ptr + x2_offset, mask=mask, other=0.0)\n        y1 = x1 * cos - x2 * sin\n        y2 = x1 * sin + x2 * cos\n        tl.store(output_ptr + x1_offset, y1, mask=mask)\n        tl.store(output_ptr + x2_offset, y2, mask=mask)\n    return\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "0d6bedd0-29b8-4714-bad1-b3616a1f70cf"
  },
  {
    "input": "@triton.jit\ndef coor_descent_kernel_backward(dk_ptr, input_ptr, a_ptr, b_ptr, mask_ptr,\n    ds_ptr, db_ptr, k_ptr, last_da_ptr, input_row_stride, b_row_stride,\n    mask_row_stride, ds_row_stride, db_row_stride, n_iters, eps_init,\n    eps_decay, eps, n_cols, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    mask_start_ptr = mask_ptr + row_idx * mask_row_stride\n    mask_ptrs = mask_start_ptr + col_offsets\n    mask_ints = tl.load(mask_ptrs, mask=col_mask, other=0)\n    mask = mask_ints == 1\n    a_ptr = a_ptr + row_idx\n    init_a = tl.load(a_ptr)\n    b_start_ptr = b_ptr + row_idx * b_row_stride\n    b_ptrs = b_start_ptr + col_offsets\n    init_b = tl.load(b_ptrs, mask=mask, other=0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    input_ptrs = row_start_ptr + col_offsets\n    s = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    k_ptr = k_ptr + row_idx\n    k = tl.load(k_ptr)\n    logk = tl.log(k)\n    last_da_ptr = last_da_ptr + row_idx\n    last_da = tl.load(last_da_ptr)\n    ds_row_start_ptr = ds_ptr + row_idx * ds_row_stride\n    ds_ptrs = ds_row_start_ptr + col_offsets\n    ds = tl.load(ds_ptrs, mask=mask, other=0.0)\n    db_row_start_ptr = db_ptr + row_idx * db_row_stride\n    db_ptrs = db_row_start_ptr + col_offsets\n    db = tl.load(db_ptrs, mask=mask, other=0.0)\n    dk_ptr = dk_ptr + row_idx\n    dk = tl.load(dk_ptr)\n    for ind in range(n_iters):\n        a = init_a\n        b = init_b\n        sa = s * 0\n        softmax = s * 0\n        current_eps = eps_init / eps_decay\n        for _ in range(n_iters - ind):\n            current_eps *= eps_decay\n            if current_eps < eps:\n                current_eps = eps\n            sb = (s + b) / current_eps\n            sb = tl.where(mask, sb, -float('inf'))\n            sb_max = tl.max(sb, axis=0)\n            sb_minus_max = tl.where(mask, sb - sb_max, -float('inf'))\n            exp = tl.exp(sb_minus_max)\n            sum_exp = tl.sum(exp, axis=0)\n            softmax = exp / sum_exp\n            log_sum_exp = tl.log(sum_exp) + sb_max\n            a = current_eps * (logk - log_sum_exp)\n            sa = s + a\n            b = tl.where(sa > 0.0, -sa, 0.0)\n        dsa = db * tl.where(sa > 0, -1.0, 0.0)\n        ds += dsa\n        da = tl.sum(dsa, axis=0) + last_da\n        dk += da * current_eps\n        dsb = da * -softmax\n        ds += dsb\n        db = dsb\n        last_da *= 0.0\n    tl.store(dk_ptr, dk)\n    tl.store(ds_ptrs, ds, mask=col_mask)\n    tl.store(db_ptrs, db, mask=col_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "d18fc135-c4b2-476f-91b6-a9c04fcc2843"
  },
  {
    "input": "@triton.jit\ndef _reduce(c_ptr, c_buf_ptr, M, N, stride_cm, stride_cn, stride_cb_m,\n    stride_cb_n, stride_cb_k, PK: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    pid_m = pid // num_pid_m\n    pid_n = pid % num_pid_n\n    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, PK)\n    c_buf_ptrs = c_buf_ptr + (offs_m[:, None, None] * stride_cb_m + offs_n[\n        None, :, None] * stride_cb_n + offs_k[None, None, :] * stride_cb_k)\n    c_buf = tl.load(c_buf_ptrs)\n    reduced_k = tl.sum(c_buf, axis=2)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    tl.store(c_ptrs, reduced_k)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "fca89caf-6e59-4d9b-a569-34acd9b7be1b"
  },
  {
    "input": "@triton.jit\ndef _geglu_tanh_backward_kernel(dc, a, b, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    db_row = dc_row * geglu_a\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = 0.5 * a_row * (1 - tanh_sq) * (sqrt_2_over_pi * (1 + 3 * \n        0.044715 * a_row * a_row))\n    da_row = dc_row * b_row * (term1 + term2)\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "58a57c35-f04b-4544-93c5-b113bc28e8bb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['BLOCK_M', 'BLOCK_M', 'MAX_INTERP'])\n@triton.jit\ndef __scan_col_2_compute(PIXEL_INDICES, stride_pixel_n, stride_pixel_m,\n    V_STARTS, stride_vs_tdst, stride_vs_tm, COL_INDICES, stride_col_n,\n    stride_col_z, N, M, H, T_M, TARGET_WIDTH_MAX, BLOCK_N: 'tl.constexpr',\n    GROUP_M: 'tl.constexpr', BLOCK_M: 'tl.constexpr', MAX_INTERP:\n    'tl.constexpr'):\n    pid_n = tl.program_id(0)\n    pid_m = tl.program_id(1)\n    grid_m = tl.program_id(1)\n    for _n in range(BLOCK_N):\n        for _m in range(0, GROUP_M):\n            n = pid_n * BLOCK_N + _n\n            ms = pid_m * BLOCK_M * GROUP_M + _m * BLOCK_M + tl.arange(0,\n                BLOCK_M)\n            ms_mask = ms < M\n            idx_tdst = ms // (H * T_M)\n            idx_h = ms % (H * T_M) // T_M\n            idx_tm = ms % T_M\n            v_start = tl.load(V_STARTS + idx_tdst * stride_vs_tdst + idx_tm *\n                stride_vs_tm, mask=ms_mask)\n            col_start = tl.load(PIXEL_INDICES + n * stride_pixel_n + (ms - \n                1) * stride_pixel_m, mask=(ms - 1 >= 0 and ms < M) and ms_mask)\n            col_end = tl.load(PIXEL_INDICES + n * stride_pixel_n + ms *\n                stride_pixel_m, mask=(ms >= 0 and ms < M) and ms_mask)\n            col_len = col_end - col_start\n            range_start = v_start + idx_h * TARGET_WIDTH_MAX\n            tl.store(COL_INDICES + n * stride_col_n + (tl.arange(0,\n                MAX_INTERP)[None, :] + col_start[:, None]) * stride_col_z, \n                tl.arange(0, MAX_INTERP)[None, :] + range_start[:, None],\n                mask=tl.arange(0, MAX_INTERP)[None, :] < col_len[:, None] and\n                ms_mask[:, None])\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "e50c4e61-5349-41cf-b851-2c83ccd0b588"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef cross_entropy_loss_forward_kernel(input_pointer, target_pointer,\n    weight_pointer, sum_weights_pointer, output_pointer, batch_dim,\n    feat_dim, input_batch_stride, input_feat_stride, weighted:\n    'tl.constexpr', BLOCK_SIZE_BATCH: 'tl.constexpr', BLOCK_SIZE_FEAT:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the mean cross entropy loss between the input and target,\n    with optional reweighing of each class.\n\n    Args:\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to a container the sum of the class weights is written to.\n            The container must be of shape [batch_dim/BLOCK_SIZE_BATCH].\n        output_pointer: Pointer to a container the loss is written to.\n            The container must be of shape [batch_dim/BLOCK_SIZE_BATCH].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    target = tl.load(target_pointer + batch_offset, mask=batch_mask)\n    pred_pointer = (input_pointer + input_feat_stride * target + \n        input_batch_stride * batch_offset)\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :], other=-float('inf'))\n    pred = tl.load(pred_pointer, mask=batch_mask)\n    mx = tl.max(input, axis=1)\n    input -= mx[:, None]\n    loss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask)\n        loss *= weight\n        tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))\n    else:\n        loss /= batch_dim\n    tl.store(output_pointer + batch_pid, tl.sum(loss))\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "692c5ec5-d337-4a01-92bb-45e681dfa9ce"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef logcumsumexp_fwd_kernel(s, z, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_mp = tl.full([S], float('-inf'), dtype=tl.float32)\n    b_zp = tl.zeros([S], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, 0), (BT, S), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_mc = tl.max(b_s, 0)\n        if i_t > 0:\n            b_mc = tl.maximum(b_mp, b_mc)\n        b_zp = b_zp * tl.exp(b_mp - b_mc)\n        b_s = tl.exp(b_s - b_mc)\n        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp\n        b_zc = tl.max(b_z, 0)\n        b_mp = b_mc\n        b_zp = b_zc\n        b_z = tl.log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc\n        tl.store(p_z, b_z, boundary_check=(0, 1))\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "87a68e2a-431d-417e-99cd-12fa80247071"
  },
  {
    "input": "@triton.jit\ndef layernorm_backward(dY, dY_row_stride, X, X_row_stride, W, b, r, mu,\n    n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    inv_var = tl.load(r)\n    mean = tl.load(mu)\n    normed = (X_row - mean) * inv_var\n    dY_W = dY_row * W_row\n    dX_row = dY_W - tl.sum(dY_W, axis=0) / n_cols - normed * tl.sum(dY_W *\n        normed, axis=0) / n_cols\n    dX_row = dX_row * inv_var\n    tl.store(dY + col_offsets, dX_row, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "6e5ac41b-7c70-48b7-af66-7a3bdfff2729"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['chunk_size', 'K', 'IS_CAUSAL'])\n@triton.jit\ndef _bmm_chunk_fwd_kernel(a_ptr, b_ptr, out_ptr, seq_idx_ptr, seqlen,\n    chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head,\n    stride_ak, stride_b_batch, stride_b_seqlen, stride_b_head, stride_bk,\n    stride_out_batch, stride_out_chunk, stride_out_head, stride_outm,\n    stride_outn, stride_seq_idx_batch, stride_seq_idx_seqlen, IS_CAUSAL:\n    'tl.constexpr', dot_dtype: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    if IS_CAUSAL:\n        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n            return\n    a_ptr += (pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen +\n        pid_h * stride_a_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h * stride_b_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] *\n        stride_b_seqlen)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < K - k * BLOCK_SIZE_K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) &\n            (offs_n[None, :] < chunk_size_limit), other=0.0)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit, other=-2)\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    out = acc\n    out_ptr += (pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h *\n        stride_out_head)\n    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] *\n        stride_outn)\n    tl.store(out_ptrs, out, mask=(offs_m[:, None] < chunk_size) & (offs_n[\n        None, :] < chunk_size))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0ae18b22-bcad-4ada-a4cb-d5e68462dc00"
  },
  {
    "input": "@triton.jit\ndef fused_embeddings_kernel(x_ptr, wte_ptr, wpe_ptr, z_ptr, B, L, V, P, H,\n    dropout_prob=0.0, seed=1337, BLOCK_SIZE: 'tl.constexpr'=512):\n    pid = tl.program_id(0)\n    wte_ptr += tl.load(x_ptr + pid) * H\n    wpe_ptr += pid % L * H\n    z_ptr += pid * H\n    for k in range(0, H, BLOCK_SIZE):\n        offset = k + tl.arange(0, BLOCK_SIZE)\n        mask = offset < H\n        z = tl.load(wte_ptr + offset, mask=mask, other=0.0)\n        z += tl.load(wpe_ptr + offset, mask=mask, other=0.0)\n        z = dropout(z, dropout_prob, seed, offset)\n        tl.store(z_ptr + offset, z, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "2a61bc02-f441-4b53-9ef7-ce2dec53d885"
  },
  {
    "input": "@triton.jit\ndef _geglu_tanh_forward_kernel(a, b, c, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0).cast(tl.int64)\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "e500fafe-5654-47a4-bf51-0e5700d602a4"
  },
  {
    "input": "@triton.autotune(configs=_generate_reduce_configs(), key=['N', 'K'], rep=1,\n    use_cuda_graph=True)\n@triton.jit\ndef split_reduce_kernel(slice_to_tiles, grad_other_tiles, grad_other,\n    stride_grad_other_b, stride_grad_other_k, stride_grad_other_n, K, N,\n    TILE_SIZE_N: 'tl.constexpr', TILE_SIZE_K: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    grid_k = tl.cdiv(K, TILE_SIZE_K)\n    grid_n = tl.cdiv(N, TILE_SIZE_N)\n    slice_id = pid // (grid_k * grid_n)\n    pid_k = pid % (grid_k * grid_n) // grid_n\n    pid_n = pid % (grid_k * grid_n) % grid_n\n    type_id = tl.load(slice_to_tiles + slice_id * 3 + 0)\n    start_tile_id = tl.load(slice_to_tiles + slice_id * 3 + 1)\n    end_tile_id = tl.load(slice_to_tiles + slice_id * 3 + 2)\n    if start_tile_id == end_tile_id or end_tile_id - start_tile_id == 1:\n        return\n    acc = tl.zeros((TILE_SIZE_K, TILE_SIZE_N), dtype=grad_other.dtype.\n        element_ty)\n    k_offs = pid_k * TILE_SIZE_K + tl.arange(0, TILE_SIZE_K)[:, None]\n    n_offs = pid_n * TILE_SIZE_N + tl.arange(0, TILE_SIZE_N)[None, :]\n    grad_other_tiles_ptrs = (grad_other_tiles + k_offs *\n        stride_grad_other_k + n_offs * stride_grad_other_n)\n    mask = (k_offs < K) & (n_offs < N)\n    for i in range(start_tile_id, end_tile_id):\n        acc += tl.load(grad_other_tiles_ptrs + stride_grad_other_b * i,\n            mask=mask)\n    tl.store(grad_other + type_id * stride_grad_other_b + k_offs *\n        stride_grad_other_k + n_offs * stride_grad_other_n, acc, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "4e204092-28bc-41de-8ff6-2dd0aff2fb6b"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_o(q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "1ca7a4ea-a983-4c63-83f2-bfa2f15eff83"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC', 'BK'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split(q, k, gi, ge, u, A,\n    s_k_h, s_k_t, s_k_d, scale, H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    i_t, i_i = i_tc // NC, i_tc % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    i_j = i_i\n    i_h = i_bh % H\n    o_i = tl.arange(0, BC)\n    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0,\n        BC)) * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    p_u = tl.make_block_ptr(u + i_h * s_k_t, (s_k_t,), (1,), i_k * BK, (BK,\n        ), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        p_qi = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qi = tl.load(p_qi, boundary_check=(0,))\n        A_jj = tl.sum(b_qi * b_k * b_u * scale)\n        b_A = tl.where(o_i != j, b_A, A_jj)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "74b5edc5-7104-4da9-8a68-c38febaaa2e2"
  },
  {
    "input": "@triton.jit\ndef squared_relu(x):\n    \"\"\"\n    Squared ReLU activation, as proposed in the Primer_ paper.\n\n    .. _Primer: https://arxiv.org/abs/2109.08668\n    \"\"\"\n    x_ = relu(x)\n    return x_ * x_\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "ed5d43bf-876d-47c4-a322-79f86a3545f6"
  },
  {
    "input": "@triton.jit\ndef blora_bp_kernel_with_loraB_mask(dr_ptr, dr_stride_bs, dr_stride_n,\n    dx_ptr, dx_stride_bsk, lA_ptr, lA_stride_l, lA_stride_h, lB_ptr,\n    lB_stride_l, lB_stride_rn, lB_mask1_ptr, lB_mask1_stride_rn,\n    lB_mask2_ptr, lB_mask2_stride_r, sel_ptr, k: 'tl.constexpr', n:\n    'tl.constexpr', r: 'tl.constexpr', rn: 'tl.constexpr', hn:\n    'tl.constexpr', h: 'tl.constexpr', block_size_r: 'tl.constexpr',\n    block_size_hn: 'tl.constexpr', block_size_h: 'tl.constexpr'):\n    \"\"\" \n    dr shape = (bs, n, hout//n)\n    lora_B_weights shape = (loras, r, hout) -> (loras, rn, hout//n)\n    lora_A_weights shape = (loras, h, r)\n    \"\"\"\n    block_idx_bsk = tl.program_id(0)\n    block_idx_bs = block_idx_bsk // k\n    block_idx_h = tl.program_id(1)\n    offsets_h = block_idx_h * block_size_h + tl.arange(0, block_size_h)\n    offsets_hn = tl.arange(0, block_size_hn)\n    offsets_r = tl.arange(0, block_size_r)\n    offsets_n = tl.arange(0, n)\n    offsets_rn = tl.arange(0, rn)\n    block_mask_h = offsets_h < h\n    block_mask_h_col = offsets_h[:, None] < h\n    block_mask_r_col = offsets_r[:, None] < r\n    block_mask_r_row = offsets_r[None, :] < r\n    sel_ptr += block_idx_bsk\n    sel_idx = tl.load(sel_ptr)\n    dr_block_ptrs = dr_ptr + block_idx_bs * dr_stride_bs + (offsets_n[:,\n        None] * dr_stride_n + offsets_hn[None, :])\n    lB_block_ptrs = lB_ptr + sel_idx * lB_stride_l + (offsets_rn[:, None] *\n        lB_stride_rn + offsets_hn[None, :])\n    lA_block_ptrs = lA_ptr + sel_idx * lA_stride_l + (offsets_h[:, None] *\n        lA_stride_h + offsets_r[None, :])\n    lB_mask1_ptrs = lB_mask1_ptr + (offsets_rn[:, None] *\n        lB_mask1_stride_rn + offsets_n[None, :])\n    lB_mask2_ptrs = lB_mask2_ptr + (offsets_r[:, None] * lB_mask2_stride_r +\n        offsets_rn[None, :])\n    dx_block_ptrs = dx_ptr + block_idx_bsk * dx_stride_bsk + offsets_h\n    compute_olB_dtype = tl.float16\n    olB = tl.zeros((rn, n), dtype=compute_olB_dtype)\n    for block_idx_hn in range(tl.cdiv(hn, block_size_hn)):\n        block_mask_hn_col = offsets_hn[:, None] < hn\n        block_mask_hn_row = offsets_hn[None, :] < hn\n        dr = tl.load(dr_block_ptrs, mask=block_mask_hn_row, other=0.0)\n        lB = tl.load(lB_block_ptrs, mask=block_mask_hn_row, other=0.0)\n        olB += tl.dot(lB, dr.T)\n        offsets_hn += block_size_hn\n        dr_block_ptrs += block_size_hn\n        lB_block_ptrs += block_size_hn\n    lB_mask1 = tl.load(lB_mask1_ptrs)\n    lB_mask2 = tl.load(lB_mask2_ptrs, mask=block_mask_r_col, other=0.0)\n    olB = tl.dot(lB_mask2, olB * lB_mask1)\n    compute_olA_dtype = tl.float16\n    lA = tl.load(lA_block_ptrs, mask=block_mask_h_col & block_mask_r_row,\n        other=0.0)\n    olA = tl.sum(tl.dot(lA, olB), axis=1)\n    tl.store(dx_block_ptrs, olA, mask=block_mask_h)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "b47df6e6-4e11-4922-a893-aecd890b4a52"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_fwd_kernel(q, k, v, beta, o, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BK:\n    'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_beta = beta + i_bh * T\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _v_minus = tl.sum(h * _k[None, :], axis=1)\n        _v -= _v_minus\n        _beta = tl.load(p_beta)\n        tl.store(p_v, _v, mask=mask_bv)\n        _v *= _beta\n        h += _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n        p_beta += 1\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "b1c0afcd-7209-45c9-af04-3299a1f0f959"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_delta_rule_fwd_kernel(q, k, v, u, beta, o, h0, ht,\n    scale, B, T, H, K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', IS_BETA_HEADWISE: 'tl.constexpr',\n    HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        p_u = u + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_bh * T * V + i_v * BV + tl.arange(0, BV)\n        else:\n            p_beta = beta + i_bh * T\n        p_o = o + (i_k * B * H + i_bh) * T * V + i_v * BV + tl.arange(0, BV)\n    else:\n        p_q = q + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_k = k + i_b * T * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        p_v = v + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        p_u = u + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        if IS_BETA_HEADWISE:\n            p_beta = beta + i_b * T * H * V + i_h * V + i_v * BV + tl.arange(\n                0, BV)\n        else:\n            p_beta = beta + i_b * T * H + i_h\n        p_o = o + (i_k * B + i_b) * T * H * V + i_h * V + i_v * BV + tl.arange(\n            0, BV)\n    mask_k = i_k * BK + tl.arange(0, BK) < K\n    mask_v = i_v * BV + tl.arange(0, BV) < V\n    mask_h = mask_k[None, :] & mask_v[:, None]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_h, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_k, other=0)\n        b_v = tl.load(p_v, mask=mask_v, other=0)\n        b_q = tl.load(p_q, mask=mask_k, other=0) * scale\n        b_v_minus = tl.sum(b_h * b_k[None, :], axis=1)\n        b_v -= b_v_minus\n        if IS_BETA_HEADWISE:\n            b_beta = tl.load(p_beta, mask=mask_v, other=0)\n        else:\n            b_beta = tl.load(p_beta)\n        tl.store(p_u, b_v, mask=mask_v)\n        b_v *= b_beta\n        b_h += b_k[None, :] * b_v[:, None]\n        b_o = b_h * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        tl.store(p_o, b_o, mask=mask_v)\n        p_q += K if HEAD_FIRST else H * K\n        p_k += K if HEAD_FIRST else H * K\n        p_o += V if HEAD_FIRST else H * V\n        p_v += V if HEAD_FIRST else H * V\n        p_u += V if HEAD_FIRST else H * V\n        p_beta += (1 if HEAD_FIRST else H) * (V if IS_BETA_HEADWISE else 1)\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_h)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "f8f08e3d-9c5d-499a-ad8c-41752d2d1876"
  },
  {
    "input": "@triton.jit\ndef triton_bid_scan(x, y, BC: 'tl.constexpr', BT: 'tl.constexpr', d_head:\n    'tl.constexpr', n_heads: 'tl.constexpr', batch_size: 'tl.constexpr',\n    seq_len: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_c, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    batch_idx = i_bh // n_heads\n    head_idx = i_bh % n_heads\n    block_start_seq = i_t * BT\n    block_start_depth = i_c * BC\n    seq_range = tl.arange(0, BT)\n    depth_range = tl.arange(0, BC)\n    seq_idx = block_start_seq + seq_range\n    depth_idx = block_start_depth + depth_range\n    mask = (seq_idx < seq_len)[:, None] & (depth_idx < d_head)\n    offset_normal = (batch_idx * n_heads * seq_len * d_head + head_idx *\n        seq_len * d_head + seq_idx[:, None] * d_head + depth_idx)\n    offset_mirrored = (batch_idx * n_heads * seq_len * d_head + head_idx *\n        seq_len * d_head + (seq_len - seq_idx - 1)[:, None] * d_head +\n        depth_idx + batch_size * n_heads * seq_len * d_head)\n    x_values = tl.load(x + offset_normal, mask=mask)\n    tl.store(y + offset_normal, x_values, mask=mask)\n    tl.store(y + offset_mirrored, x_values, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "3dd7f810-9f92-4443-a817-eb77ccbf2498"
  },
  {
    "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate,\n    stride_state_batch, stride_state_dim, stride_state_dstate,\n    stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim,\n    stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch,\n    stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim,\n    stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim,\n    DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS:\n    'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    state_ptr += pid_b * stride_state_batch\n    x_ptr += pid_b * stride_x_batch\n    dt_ptr += pid_b * stride_dt_batch\n    B_ptr += pid_b * stride_B_batch\n    C_ptr += pid_b * stride_C_batch\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch\n    out_ptr += pid_b * stride_out_batch\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n    if DT_SOFTPLUS:\n        dt = tl.where(dt <= 20.0, tl.math.log1p(tl.exp(dt)), dt)\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n        dstate), other=0.0)\n    dA = tl.exp(A * dt[:, None])\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    dB = B[None, :] * dt[:, None]\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "5f55775f-9d19-48c4-8c17-546ea3637c13"
  },
  {
    "input": "@triton.jit\ndef dropout_offsets(philox_seed, philox_offset, dropout_p, m, n, stride):\n    ms = tl.arange(0, m)\n    ns = tl.arange(0, n)\n    return philox_offset + ms[:, None] * stride + ns[None, :]\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "e627a6b3-941e-4014-a91e-3a0a456fd657"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_quant_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_WEIGHT:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    if HAS_WEIGHT:\n        w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w if HAS_WEIGHT else x_hat\n    if HAS_BIAS:\n        y = y + b\n    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-05)\n    y = tl.math.round(y * scale)\n    y = tl.maximum(tl.minimum(y, 127), -128) / scale\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "8fe05eb8-b2bd-44da-a79b-05112b2a03ce"
  },
  {
    "input": "@triton.jit\ndef flash_attention_v1_kernel(q_ptr, k_ptr, v_ptr, z_ptr, BN, Lq, Lk, scale,\n    H: 'tl.constexpr', dropout_prob=0.0, seed=1337, BLOCK_SIZE_L:\n    'tl.constexpr'=64):\n    q_ptr += tl.program_id(0) * (Lq * H)\n    z_ptr += tl.program_id(0) * (Lq * H)\n    k_ptr += tl.program_id(0) * (Lk * H)\n    v_ptr += tl.program_id(0) * (Lk * H)\n    offs_lq = tl.program_id(1) * BLOCK_SIZE_L + tl.arange(0, BLOCK_SIZE_L)\n    offs_h = tl.arange(0, H)\n    q_mask = offs_lq[:, None] < Lq\n    q_offs = offs_lq[:, None] * H + offs_h[None, :]\n    q = tl.load(q_ptr + q_offs, mask=q_mask, other=0.0)\n    q = q\n    z = tl.zeros((BLOCK_SIZE_L, H), dtype=tl.float32)\n    max_value = tl.zeros((BLOCK_SIZE_L, 1), dtype=tl.float32) + float('-inf')\n    denominator = tl.zeros((BLOCK_SIZE_L, 1), dtype=tl.float32)\n    for i in range(0, Lk, BLOCK_SIZE_L):\n        offs_lk = i + tl.arange(0, BLOCK_SIZE_L)\n        kv_mask = offs_lk[:, None] < Lk\n        kv_offs = offs_lk[:, None] * H + offs_h[None, :]\n        k = tl.load(k_ptr + kv_offs, mask=kv_mask, other=0.0)\n        k = k\n        qk = tl.dot(q, k.trans(1, 0)) * scale\n        qk = tl.where(offs_lq[:, None] >= offs_lk[None, :], qk, float('-inf'))\n        block_max_value = tl.max(qk, axis=1, keep_dims=True)\n        new_max_value = tl.where(block_max_value > max_value,\n            block_max_value, max_value)\n        qk = tl.exp(qk - new_max_value)\n        multiplier = tl.exp(max_value - new_max_value)\n        denominator *= multiplier\n        z *= multiplier\n        denominator += tl.sum(qk, axis=1, keep_dims=True)\n        max_value = new_max_value\n        if dropout_prob > 0.0:\n            qk_offs = offs_lq[:, None] * Lk + offs_lk[None, :]\n            qk = dropout(qk, dropout_prob, seed, qk_offs)\n        v = tl.load(v_ptr + kv_offs, mask=kv_mask, other=0.0)\n        v = v\n        qk = qk\n        z = tl.dot(qk, v, acc=z)\n    z /= denominator\n    z = z\n    tl.store(z_ptr + q_offs, z, mask=q_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "4e253afb-4b39-4097-beef-4cb9f7410967"
  },
  {
    "input": "@triton.jit\ndef scaled_index_add_fwd_kernel(input_ptr, index_ptr, source_ptr,\n    scaling_ptr, alpha, num_inp_indices, num_src_indices, num_rows,\n    num_cols, stride0, stride1, stride2, BLOCK_SIZE_INDEX: 'tl.constexpr',\n    BLOCK_SIZE_ROW: 'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr',\n    HAS_SCALING: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    pid2 = tl.program_id(axis=2)\n    rows = pid1 * BLOCK_SIZE_ROW + tl.arange(0, BLOCK_SIZE_ROW)\n    cols = pid2 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    source_offsets = source_ptr + source_indices[:, None, None\n        ] * stride0 + rows[None, :, None] * stride1 + cols[None, None, :\n        ] * stride2\n    source_mask = (source_indices[:, None, None] < num_src_indices) & (rows\n        [None, :, None] < num_rows) & (cols[None, None, :] < num_cols)\n    source = tl.load(source_offsets, mask=source_mask)\n    input_indices = tl.load(index_ptr + source_indices, mask=source_indices <\n        num_src_indices)\n    input_offsets = input_ptr + input_indices[:, None, None] * stride0 + rows[\n        None, :, None] * stride1 + cols[None, None, :] * stride2\n    x = tl.load(input_offsets, mask=source_mask)\n    if HAS_SCALING:\n        scaling = tl.load(scaling_ptr + cols[None, None, :] * stride2, mask\n            =cols[None, None, :] < num_cols)\n        tl.store(input_offsets, x + alpha * scaling * source, mask=source_mask)\n    else:\n        tl.store(input_offsets, x + alpha * source, mask=source_mask)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "ffc5df33-a0c2-4cf4-8fcc-ae073dca2818"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'bsy': 128, 'bsx': 256, 'bsk': 64,\n    'group_sz': 8}, num_stages=3, num_warps=8), triton.Config({'bsy': 64,\n    'bsx': 256, 'bsk': 32, 'group_sz': 8}, num_stages=4, num_warps=4),\n    triton.Config({'bsy': 128, 'bsx': 64, 'bsk': 32, 'group_sz': 8},\n    num_stages=4, num_warps=4), triton.Config({'bsy': 64, 'bsx': 128, 'bsk':\n    32, 'group_sz': 8}, num_stages=4, num_warps=4), triton.Config({'bsy': \n    64, 'bsx': 32, 'bsk': 32, 'group_sz': 8}, num_stages=5, num_warps=2),\n    triton.Config({'bsy': 32, 'bsx': 64, 'bsk': 32, 'group_sz': 8},\n    num_stages=5, num_warps=2), triton.Config({'bsy': 128, 'bsx': 256,\n    'bsk': 128, 'group_sz': 8}, num_stages=3, num_warps=8), triton.Config({\n    'bsy': 256, 'bsx': 128, 'bsk': 128, 'group_sz': 8}, num_stages=3,\n    num_warps=8), triton.Config({'bsy': 256, 'bsx': 64, 'bsk': 128,\n    'group_sz': 8}, num_stages=4, num_warps=4), triton.Config({'bsy': 64,\n    'bsx': 256, 'bsk': 128, 'group_sz': 8}, num_stages=4, num_warps=4),\n    triton.Config({'bsy': 128, 'bsx': 128, 'bsk': 128, 'group_sz': 8},\n    num_stages=4, num_warps=4), triton.Config({'bsy': 64, 'bsx': 128, 'bsk':\n    64, 'group_sz': 8}, num_stages=4, num_warps=4), triton.Config({'bsy': \n    128, 'bsx': 32, 'bsk': 64, 'group_sz': 4}, num_stages=4, num_warps=4),\n    triton.Config({'bsy': 128, 'bsx': 256, 'bsk': 64, 'group_sz': 4},\n    num_stages=3, num_warps=8), triton.Config({'bsy': 64, 'bsx': 256, 'bsk':\n    32, 'group_sz': 4}, num_stages=4, num_warps=4), triton.Config({'bsy': \n    128, 'bsx': 64, 'bsk': 32, 'group_sz': 4}, num_stages=4, num_warps=4),\n    triton.Config({'bsy': 64, 'bsx': 128, 'bsk': 32, 'group_sz': 4},\n    num_stages=4, num_warps=4), triton.Config({'bsy': 128, 'bsx': 32, 'bsk':\n    32, 'group_sz': 4}, num_stages=4, num_warps=4), triton.Config({'bsy': \n    32, 'bsx': 64, 'bsk': 32, 'group_sz': 4}, num_stages=5, num_warps=2),\n    triton.Config({'bsy': 128, 'bsx': 256, 'bsk': 128, 'group_sz': 4},\n    num_stages=3, num_warps=8), triton.Config({'bsy': 256, 'bsx': 128,\n    'bsk': 128, 'group_sz': 4}, num_stages=3, num_warps=8), triton.Config({\n    'bsy': 64, 'bsx': 256, 'bsk': 128, 'group_sz': 4}, num_stages=4,\n    num_warps=4), triton.Config({'bsy': 128, 'bsx': 128, 'bsk': 128,\n    'group_sz': 4}, num_stages=4, num_warps=4), triton.Config({'bsy': 64,\n    'bsx': 128, 'bsk': 64, 'group_sz': 4}, num_stages=4, num_warps=4),\n    triton.Config({'bsy': 128, 'bsx': 32, 'bsk': 64, 'group_sz': 4},\n    num_stages=4, num_warps=4)], key=['batch_size', 'seq_len', 'dim',\n    'dim_out'])\n@triton.jit\ndef matmul_kernel(A_ptr, B_ptr, O_ptr, A_stride_batch, A_stride_height,\n    A_stride_width, B_stride_batch, B_stride_height, B_stride_width,\n    O_stride_batch, O_stride_height, O_stride_width, batch_size, seq_len,\n    dim, dim_out, bsx: 'tl.constexpr', bsy: 'tl.constexpr', bsk:\n    'tl.constexpr', group_sz: 'tl.constexpr', apply_scaling: 'tl.constexpr',\n    scale_factor: 'tl.constexpr'):\n    \"\"\"\n    Matrix multiplication by loading rows of A\n    and columns of B to calculate a block of O.\n\n    This can be further improved by implementing tiling, however\n    I am yet to figure out how to use L2 cache in Triton.\n    \"\"\"\n    batch_idx = tl.program_id(axis=0)\n    row_idx = tl.program_id(axis=1)\n    col_idx = tl.program_id(axis=2)\n    num_row_programs = tl.num_programs(1)\n    num_col_programs = tl.num_programs(2)\n    row_idxnew, col_idxnew = tl.swizzle2d(row_idx, col_idx,\n        num_row_programs, num_col_programs, group_sz)\n    a_offset_batch = batch_idx * A_stride_batch\n    b_offset_batch = batch_idx * B_stride_batch\n    output = tl.zeros((bsy, bsx), dtype=tl.float32)\n    for offset in range(0, dim, bsk):\n        offset_k = offset + tl.arange(0, bsk)\n        offset_a = row_idxnew * bsy + tl.arange(0, bsy)\n        mask_a = (offset_a[:, None] < seq_len) & (offset_k[None, :] < dim)\n        offset_a = offset_a[:, None] * A_stride_height + offset_k[None, :\n            ] * A_stride_width\n        a = tl.load(A_ptr + a_offset_batch + offset_a, mask_a)\n        offset_b = col_idxnew * bsx + tl.arange(0, bsx)\n        mask_b = (offset_k[:, None] < dim) & (offset_b[None, :] < dim_out)\n        offset_b = offset_k[:, None] * B_stride_height + offset_b[None, :\n            ] * B_stride_width\n        b = tl.load(B_ptr + b_offset_batch + offset_b, mask_b)\n        output = tl.dot(a, b, output, allow_tf32=True)\n    offset_out_batch = batch_idx * O_stride_batch\n    offset_or = row_idxnew * bsy + tl.arange(0, bsy)\n    offset_oc = col_idxnew * bsx + tl.arange(0, bsx)\n    offset_o = offset_or[:, None] * O_stride_height + offset_oc[None, :\n        ] * O_stride_width\n    mask_o = (offset_or[:, None] < seq_len) & (offset_oc[None, :] < dim_out)\n    if apply_scaling:\n        output = scale_factor * output\n    tl.store(O_ptr + offset_out_batch + offset_o, output, mask_o)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b746e455-492d-4c96-bf80-afcaa69d5ecd"
  },
  {
    "input": "@triton.jit\ndef _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    d_h = tl.math.exp2((BTS - tl.arange(0, BTS)) * b_b)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        b_dq *= d_b\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= tl.math.exp2(tl.arange(0, BTL) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "2e604eb5-4227-4856-b421-2855a1a012ae"
  },
  {
    "input": "@triton.jit\ndef matmul_kernel(A, B, C, M, N, K, stride_am, stride_ak, stride_bn,\n    stride_bk, stride_cn, stride_cm):\n    m = tl.program_id(0)\n    n = tl.program_id(1)\n    k = tl.arange(0, BLOCK_SIZE)\n    a_ptrs = A + m * stride_am + k * stride_ak\n    b_ptrs = B + k * stride_bk + n * stride_bn\n    c_ptrs = C + m * stride_cm + n * stride_cn\n    a = tl.load(a_ptrs)\n    b = tl.load(b_ptrs)\n    c = tl.dot(a, b)\n    tl.atomic_add(c_ptrs, c)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8c830909-8928-414b-816a-6d7945eba4e7"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4)], key=['BT', 'BK',\n    'BV'])\n@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(q, k, v, h, g, o, scale, T:\n    'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    if HEAD_FIRST:\n        p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,\n            ), (0,))\n        p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_g = tl.make_block_ptr(g + i_b * T * H + i_h, (T,), (H,), (i_t *\n            BT,), (BT,), (0,))\n        p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_o = tl.make_block_ptr(o + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_g)[:, None]\n    b_s = b_s * tl.exp(b_g[:, None] - b_g[None, :])\n    b_s = tl.where(m_s, b_s, 0)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "43843f32-ab52-4bf9-b9be-e8de76f45a47"
  },
  {
    "input": "@triton.jit\ndef sample_grid_rep(feature_grid, feature_grid_sizes, grid_idx, sample_x,\n    sample_y, sample_z, C: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples: 'tl.constexpr'):\n    vec = _voxel_grid_sample(feature_grid, feature_grid_sizes, grid_idx,\n        sample_x, sample_y, sample_z, C, NUM_GRIDS, BLOCK_SIZE,\n        mask_out_of_bounds_samples)\n    return vec\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "15c93ce5-1d90-4043-ab53-ab19b24f375c"
  },
  {
    "input": "@triton.jit\ndef _rms_layer_norm_bwd_dwdb(DW, FINAL_DW, M, N, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f72bd318-0f42-4829-80d6-6976dec4fd0b"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_modulation_fwd(X, Y, W, B, Mean, Rstd, stride, seq_len, N,\n    eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    batch_idx = row // seq_len\n    Y += row * stride\n    X += row * stride\n    W += batch_idx * stride\n    B += batch_idx * stride\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    x = tl.load(X + cols, mask=mask, other=0.0)\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    b = tl.load(B + cols, mask=mask, other=0.0)\n    mean = tl.sum(x, axis=0) / N\n    var = tl.sum(x * x, axis=0) / N - mean * mean\n    rstd = tl.rsqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    y = (x - mean) * rstd * (1 + w) + b\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "3019523c-a11e-47e3-b84d-dfb45b367f44"
  },
  {
    "input": "@triton.jit\ndef _rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Fast RMS Layernorm kernel\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    normed = normed\n    output = normed * W_row\n    tl.store(Y + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "2dfa1169-c1a8-4ad0-aa6d-6f4b59c9931a"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_cum(s, r, c, p, s_sk_h, s_sk_t, s_sk_m, T, BT:\n    'tl.constexpr', BM: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    p_r = tl.make_block_ptr(r + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_c = tl.make_block_ptr(c + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    p_p = tl.make_block_ptr(p + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        0, i_m * BM), (BT, BM), (1, 0))\n    b_mp = tl.zeros([BM], dtype=tl.float32)\n    b_zp = tl.zeros([BM], dtype=tl.float32)\n    for i in range(NT):\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_m = tl.max(b_s, 0)\n        if i == 0:\n            b_r = tl.exp(-b_m)\n        else:\n            b_m = tl.maximum(b_mp, b_m)\n            b_r = tl.exp(b_mp - b_m)\n        b_c = tl.exp(b_s - b_m[None, :])\n        b_z = tl.cumsum(b_c, 0) + (b_zp * b_r)[None, :]\n        b_p = tl.exp(-tl.log(b_z))\n        b_mp = b_m\n        b_zp = tl.max(b_z, 0)\n        tl.store(p_r, b_r, boundary_check=(0,))\n        tl.store(p_c, b_c, boundary_check=(0, 1))\n        tl.store(p_p, b_p, boundary_check=(0, 1))\n        p_s = tl.advance(p_s, (BT, 0))\n        p_r = tl.advance(p_r, (DM,))\n        p_c = tl.advance(p_c, (BT, 0))\n        p_p = tl.advance(p_p, (BT, 0))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "d55e0913-fd33-451b-a3f4-6264b49c4dbe"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "38fdee43-371f-40f1-864a-15dc69f3f27c"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel(q, k, v, w, u, o, h0, ht, s_k_h, s_v_h,\n    scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "1c9b0971-0126-4857-b9c8-95dcab5310f1"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_delta_rule_bwd_kernel_dqkw(q, k, v, w, h, do, dh, dq, dk, dv, dw,\n    scale, T: 'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    o_i = tl.arange(0, BT)\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK,\n            i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T), (1, \n            H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dw = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V, (V, NT * K), (1, V),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (V, NT * K), (1, V\n            ), (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, b_dh, allow_tf32=False)\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dw += tl.dot(b_dv, b_h, allow_tf32=False)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dq *= scale\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    if HEAD_FIRST:\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dw = tl.make_block_ptr(dw + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dw = tl.make_block_ptr(dw + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dw, -b_dw, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "e9556efc-8b00-48b3-a927-f7a06a11f88d"
  },
  {
    "input": "@triton.jit\ndef double_tree_all_reduce_kernel(buffer_ptrs, signal_pad_ptrs, output_ptr,\n    tree0_parent, tree0_child0, tree0_child1, tree1_parent, tree1_child0,\n    tree1_child1, numel: 'tl.constexpr', rank: 'tl.constexpr', world_size:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', NUMEL_PER_THREAD:\n    'tl.constexpr'):\n    blockwise_barrier(signal_pad_ptrs, None, rank, world_size)\n    block_id = tl.program_id(2) * tl.num_programs(1) * tl.num_programs(0\n        ) + tl.program_id(1) * tl.num_programs(0) + tl.program_id(0)\n    pid = tl.program_id(axis=0)\n    buffer_ptrs = buffer_ptrs\n    output_ptr = output_ptr\n    signal_pad_ptrs = signal_pad_ptrs\n    block_start = pid * BLOCK_SIZE\n    if tree0_child0 != -1 and tree0_child0 < 8:\n        local_signal_pad_addr = tl.load(signal_pad_ptrs + rank)\n        wait_addrs = (local_signal_pad_addr + block_id * world_size +\n            tree0_child0)\n        triton_wait(wait_addrs)\n    if tree0_child1 != -1 and tree0_child1 < 8:\n        local_signal_pad_addr = tl.load(signal_pad_ptrs + rank)\n        wait_addrs = (local_signal_pad_addr + block_id * world_size +\n            tree0_child1)\n        triton_wait(wait_addrs)\n    while block_start < numel // NUMEL_PER_THREAD:\n        offsets = (block_start + tl.arange(0, BLOCK_SIZE)) * 2\n        mask = block_start + tl.arange(0, BLOCK_SIZE\n            ) < numel // NUMEL_PER_THREAD\n        acc_hi = tl.zeros((BLOCK_SIZE,), tl.uint64)\n        acc_lo = tl.zeros((BLOCK_SIZE,), tl.uint64)\n        if tree0_child0 != -1 and tree0_child0 < 8:\n            if block_id == 0:\n                if rank == 0:\n                    if block_start == pid * BLOCK_SIZE:\n                        buffer_ptr = tl.load(buffer_ptrs + tree0_child0)\n                        None\n                        hi, lo = load_128(buffer_ptr + offsets, mask=tl.\n                            full((1,), 4294967295, dtype=tl.uint32))\n        buffer_ptr = tl.load(buffer_ptrs + rank)\n        hi, lo = load_128(buffer_ptr + offsets, mask=mask)\n        acc_hi, acc_lo = add_v8_bf16(acc_hi, acc_lo, hi, lo)\n        tl.store(buffer_ptr + offsets + 0, acc_hi, mask=mask)\n        tl.store(buffer_ptr + offsets + 1, acc_lo, mask=mask)\n        block_start += tl.num_programs(axis=0) * BLOCK_SIZE\n    if tree0_parent != -1:\n        remote_signal_pad_addrs = tl.load(signal_pad_ptrs + tree0_parent)\n        send_addrs = remote_signal_pad_addrs + block_id * world_size + rank\n        triton_send(send_addrs)\n        local_signal_pad_addr = tl.load(signal_pad_ptrs + rank)\n        wait_addrs = (local_signal_pad_addr + block_id * world_size +\n            tree0_parent)\n        triton_wait(wait_addrs)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "5330dd11-e60a-4393-ac15-a3efca149906"
  },
  {
    "input": "@triton.jit\ndef _group_norm_mul_dropout_bwd_dx_du(DX, DU, DY, DW, DB, X, U, Y, W, B,\n    Mean, Rstd, stride_dx, stride_du, stride_dy, stride_x, stride_u,\n    stride_y, D, Heads, eps, seed, dropout_ratio, GROUP_N: 'tl.constexpr',\n    BLOCK_D: 'tl.constexpr', BLOCK_H: 'tl.constexpr', TRAINING:\n    'tl.constexpr', CONCAT_UX: 'tl.constexpr', COMPUTE_Y: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_D)\n    off_heads = tl.arange(0, BLOCK_H)\n    mask_c = cols < D\n    mask_h = off_heads < Heads\n    mask = mask_c[None, :] & mask_h[:, None]\n    X += row * stride_x\n    U += row * stride_u\n    DY += row * stride_dy\n    DX += row * stride_dx\n    DU += row * stride_du\n    offsets = off_heads[:, None] * D + cols[None, :]\n    x = tl.load(X + offsets, mask=mask, other=0)\n    if CONCAT_UX:\n        du = tl.load(DY + offsets, mask=mask, other=0)\n        dx = tl.load(DY + Heads * D + offsets, mask=mask, other=0)\n        dy = tl.load(DY + 2 * Heads * D + offsets, mask=mask, other=0)\n    else:\n        du = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)\n        dx = tl.zeros([BLOCK_H, BLOCK_D], dtype=tl.float32)\n        dy = tl.load(DY + offsets, mask=mask, other=0)\n    if TRAINING:\n        if CONCAT_UX:\n            random_offsets = row * 3 * D * Heads + offsets\n            random_du = tl.rand(seed, random_offsets)\n            du_keep = random_du > dropout_ratio\n            du = tl.where(du_keep, du / (1.0 - dropout_ratio), 0.0)\n            random_dx = tl.rand(seed, random_offsets + Heads * D)\n            dx_keep = random_dx > dropout_ratio\n            dx = tl.where(dx_keep, dx / (1.0 - dropout_ratio), 0.0)\n            random_dy = tl.rand(seed, random_offsets + 2 * Heads * D)\n            dy_keep = random_dy > dropout_ratio\n            dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n        else:\n            random_offsets = row * D * Heads + offsets\n            random = tl.rand(seed, random_offsets)\n            dy_keep = random > dropout_ratio\n            dy = tl.where(dy_keep, dy / (1.0 - dropout_ratio), 0.0)\n    mean = tl.load(Mean + row * Heads + off_heads)\n    rstd = tl.load(Rstd + row * Heads + off_heads)\n    xhat = (x - mean[:, None]) * rstd[:, None]\n    w = tl.load(W + off_heads, mask=mask_h)\n    b = tl.load(B + off_heads, mask=mask_h)\n    ln = xhat * w[:, None] + b[:, None]\n    du += dy * ln\n    tl.store(DU + offsets, du, mask=mask)\n    u = tl.load(U + offsets, mask=mask, other=0)\n    dy = dy * u\n    wdy = w[:, None] * dy\n    if COMPUTE_Y:\n        Y += row * stride_y\n        y = ln * u\n        if TRAINING:\n            if CONCAT_UX:\n                u = tl.where(du_keep, u / (1.0 - dropout_ratio), 0.0)\n                x = tl.where(dx_keep, x / (1.0 - dropout_ratio), 0.0)\n                y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n            else:\n                y = tl.where(dy_keep, y / (1.0 - dropout_ratio), 0.0)\n        if CONCAT_UX:\n            tl.store(Y + offsets, u, mask=mask)\n            tl.store(Y + Heads * D + offsets, x, mask=mask)\n            tl.store(Y + 2 * Heads * D + offsets, y, mask=mask)\n        else:\n            tl.store(Y + offsets, y, mask=mask)\n    xhat = tl.where(mask, xhat, 0.0)\n    wdy = tl.where(mask, wdy, 0.0)\n    c1 = tl.sum(xhat * wdy, axis=1) / D\n    c2 = tl.sum(wdy, axis=1) / D\n    dx += (wdy - (xhat * c1[:, None] + c2[:, None])) * rstd[:, None]\n    tl.store(DX + offsets, dx, mask=mask)\n    lock_id = row % GROUP_N\n    DW = DW + lock_id * Heads + off_heads\n    DB = DB + lock_id * Heads + off_heads\n    partial_dw = tl.sum(dy * xhat, axis=1)\n    partial_dw = tl.ravel(partial_dw)\n    partial_db = tl.sum(dy, axis=1)\n    partial_db = tl.ravel(partial_db)\n    tl.atomic_add(DW, partial_dw, mask=mask_h, sem='relaxed')\n    tl.atomic_add(DB, partial_db, mask=mask_h, sem='relaxed')\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "08229b91-f737-42c9-b8ea-5da16c5cc29f"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    num_block, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', CAUSAL: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qk_scale = sm_scale * 1.44269504\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_qz + off_h * stride_qh\n    V += off_z * stride_qz + off_h * stride_qh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_qz + off_h * stride_qh\n    DV += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        if CAUSAL:\n            lo = start_n * BLOCK_M\n        else:\n            lo = 0\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        D_ptrs = D + off_hz * N_CTX\n        l_ptrs = L + off_hz * N_CTX\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            if CAUSAL:\n                qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :],\n                    float(0.0), float('-inf'))\n            else:\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, tl.trans(k))\n            qk *= qk_scale\n            l_i = tl.load(l_ptrs + offs_m_curr)\n            p = tl.math.exp2(qk - l_i[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(tl.trans(p), do)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, tl.trans(v))\n            ds = p * dp * sm_scale\n            dk += tl.dot(tl.trans(ds), q)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k)\n            tl.store(dq_ptrs, dq)\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "3ec87142-0794-4524-9e3a-0da504520099"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BT'])\n@triton.jit\ndef chunk_gla_fwd_A_kernel_intra_sub_intra(q, k, g, A, scale, T:\n    'tl.constexpr', H: 'tl.constexpr', K: 'tl.constexpr', BT:\n    'tl.constexpr', BC: 'tl.constexpr', BK: 'tl.constexpr', HEAD_FIRST:\n    'tl.constexpr'):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    i_j = i_i\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_i = tl.arange(0, BC)\n    o_k = tl.arange(0, BK)\n    m_k = o_k < K\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    if HEAD_FIRST:\n        o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * BT + i_j * BC\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, 0), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t * BT +\n            i_i * BC, 0), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_bh * T * K + (i_t * BT +\n            i_j * BC) * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + (i_t *\n            BT + i_j * BC) * K + o_k, BK), BK)\n    else:\n        o_A = i_b * T * H * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n            ) * H * BT + i_h * BT + i_j * BC\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\n        p_k = tl.max_contiguous(tl.multiple_of(k + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n        p_gk = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (i_t *\n            BT + i_j * BC) * H * K + i_h * K + o_k, BK), BK)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_k = tl.load(p_k, mask=m_k, other=0)\n        b_gk = tl.load(p_gk, mask=m_k, other=0)\n        b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n        p_k += K if HEAD_FIRST else H * K\n        p_gk += K if HEAD_FIRST else H * K\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "4a99f3bf-056d-4958-8ffc-3e34609f8298"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "a63694bf-005b-4e30-88db-e0174b6b7cb9"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "2df9239f-d95e-4a05-adf1-4a646a287606"
  },
  {
    "input": "@triton.jit\ndef mish_grad(input):\n    \"\"\"\n    Calculates the gradient of Mish.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of Mish.\n    \"\"\"\n    exp = tl.exp(input)\n    delta = exp * (exp + 2) + 2\n    return exp * (exp * (4 * input + 6 + exp * (exp + 4)) + 4 * (input + 1)\n        ) / (delta * delta)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "9bc0677b-3761-4ab8-a618-907fc3e7dd1f"
  },
  {
    "input": "@triton.jit\ndef second_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    CONST_00 = 3.87298334620742\n    CONST_01 = 2.23606797749979\n    CONST_02 = 4.47213595499958\n    g_Y20 = tl.load(sph_grad_ptr + output_row_offset, mask=\n        output_row_offset < output_numel)\n    g_Y21 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_Y22 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_Y23 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_Y24 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += (CONST_00 * g_Y20 * z + CONST_00 * g_Y21 * y - CONST_01 * g_Y22 *\n        x - CONST_00 * g_Y24 * x)\n    g_y += CONST_00 * g_Y21 * x + CONST_02 * g_Y22 * y + CONST_00 * g_Y23 * z\n    g_z += (CONST_00 * g_Y20 * x - CONST_01 * g_Y22 * z + CONST_00 * g_Y23 *\n        y + CONST_00 * g_Y24 * z)\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a5f57289-ff2e-43d8-bed2-693c3c0545d3"
  },
  {
    "input": "@triton.jit\ndef _triton_attn_fwd(Q, K, V, sm_scale, Out, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_km, stride_kk,\n    stride_vz, stride_vh, stride_vm, stride_vk, stride_oz, stride_oh,\n    stride_om, stride_ok, Z, H, N_CTX, POWER_OF_2_N_CTX: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', GROUPS: 'tl.constexpr', ORDER_12:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_g = tl.program_id(2)\n    q_offset = off_z * stride_qz + (off_h * GROUPS + off_g) * stride_qh\n    k_offset = off_z * stride_kz + off_h * stride_kh\n    v_offset = off_z * stride_vz + off_h * stride_vh\n    o_offset = off_z * stride_oz + (off_h * GROUPS + off_g) * stride_oh\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + v_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vm, stride_vk), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        N_CTX), strides=(stride_kk, stride_km), offsets=(0, 0), block_shape\n        =(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_ok), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n    if ORDER_12:\n        if STAGE & 1:\n            acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n                V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL,\n                BLOCK_N, 4 - STAGE, offs_m, offs_n, N_CTX)\n        if STAGE & 2:\n            acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n                V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL,\n                BLOCK_N, 2, offs_m, offs_n, N_CTX)\n    else:\n        if STAGE & 2:\n            acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n                V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL,\n                BLOCK_N, 2, offs_m, offs_n, N_CTX)\n        if STAGE & 1:\n            acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n                V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL,\n                BLOCK_N, 4 - STAGE, offs_m, offs_n, N_CTX)\n    acc = acc / l_i[:, None]\n    tl.store(O_block_ptr, acc, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "86424863-ac4b-4758-a281-14b73aea08ce"
  },
  {
    "input": "@triton.jit\ndef triton_adam_kernel(params_ptr, grads_ptr, exp_avgs_ptr, exp_avg_sqs_ptr,\n    noop_flag_ptr, scale_ptr, step_size, beta1, beta2, bias_correction,\n    decay_factor, epsilon, numel: 'tl.constexpr', block_size: 'tl.constexpr'):\n    noop_flag = tl.load(noop_flag_ptr)\n    if noop_flag != 0:\n        return\n    scale = tl.load(scale_ptr)\n    block_start = tl.program_id(axis=0) * block_size\n    offsets = block_start + tl.arange(0, block_size)\n    mask = offsets < numel\n    params = tl.load(params_ptr + offsets, mask=mask)\n    grads = tl.load(grads_ptr + offsets, mask=mask)\n    grads = scale * grads\n    exp_avgs = tl.load(exp_avgs_ptr + offsets, mask=mask)\n    exp_avgs = beta1 * exp_avgs + (1 - beta1) * grads\n    tl.store(exp_avgs_ptr + offsets, exp_avgs, mask=mask)\n    exp_avg_sqs = tl.load(exp_avg_sqs_ptr + offsets, mask=mask)\n    exp_avg_sqs = beta2 * exp_avg_sqs + (1 - beta2) * grads * grads\n    tl.store(exp_avg_sqs_ptr + offsets, exp_avg_sqs, mask=mask)\n    params = decay_factor * params - step_size * exp_avgs / (tl.sqrt(\n        exp_avg_sqs) / bias_correction + epsilon)\n    tl.store(params_ptr + offsets, params, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "c4028f31-5cc4-4711-9ec6-15536d910cdc"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU)\"\"\"\n    return x * 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "63c1debf-3c53-4d95-83da-eb051afd53d0"
  },
  {
    "input": "@triton.jit\ndef _single2scatter(X_ptr, stride_xm, stride_xk, W_ptr, stride_we,\n    stride_wk, stride_wn, Y_ptr, stride_ym, stride_yn, expert_idxs_ptr,\n    FAN_OUT: 'tl.constexpr', K: 'tl.constexpr', N: 'tl.constexpr', E:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr',\n    ACC_TYPE: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    N_block_id = pid0\n    if FAN_OUT == 1:\n        in_idx = pid1\n    else:\n        in_idx = 0\n    out_idx = pid1\n    K_block = tl.arange(0, BLOCK_K)\n    N_block = tl.max_contiguous(tl.multiple_of((N_block_id * BLOCK_N + tl.\n        arange(0, BLOCK_N)) % N, BLOCK_N), BLOCK_N)\n    E_idx = tl.load(expert_idxs_ptr + pid1)\n    X_blk_ptrs = X_ptr + in_idx * stride_xm + K_block[:, None] * stride_xk\n    W_blk_ptrs = W_ptr + E_idx * stride_we + K_block[:, None\n        ] * stride_wk + N_block[None, :] * stride_wn\n    acc = tl.zeros((1, BLOCK_N), dtype=ACC_TYPE)\n    for K_block_id in range(0, tl.cdiv(K, BLOCK_K)):\n        x = tl.load(X_blk_ptrs)\n        w = tl.load(W_blk_ptrs)\n        acc += tl.sum(x * w, axis=0)[None, :]\n        X_blk_ptrs += BLOCK_K * stride_xk\n        W_blk_ptrs += BLOCK_K * stride_wk\n    Y_blk_ptrs = Y_ptr + out_idx * stride_ym + N_block[None, :] * stride_yn\n    tl.store(Y_blk_ptrs, acc)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4dac278e-3d5f-48b4-b2e2-ffe3c73234be"
  },
  {
    "input": "@triton.jit\ndef add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'\n    ):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    breakpoint()\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "8b8bb361-976c-4f69-83bd-f2bac0bfd2a9"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, smoothing, lse_square_scale, ignored_index, total_classes,\n    class_start_idx, n_cols, logits_row_stride, dlogits_row_stride,\n    dloss_row_stride, BLOCK_SIZE: 'tl.constexpr', HAS_SMOOTHING: 'tl.constexpr'\n    ):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf'))\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_positive = 1.0 - smoothing\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing),\n            probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * probs, mask=col_offsets <\n        n_cols)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "ac0da87b-45b4-4b04-a748-2138c0ab601f"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    \"\"\"Gaussian Error Linear Unit (GELU)\"\"\"\n    return x * 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "c661c03a-0ef5-4ec9-b1f8-adea83b3e0d1"
  },
  {
    "input": "@triton.jit\ndef _splat_grid_rep(to_splat, xy, yz, zx, batch_index, sample_x, sample_y,\n    sample_z, batch_size: 'tl.constexpr', C: 'tl.constexpr', D:\n    'tl.constexpr', H: 'tl.constexpr', W: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', shape_representation: 'tl.constexpr'):\n    if shape_representation == 0:\n        _grid_splat(to_splat, xy, batch_index, sample_x, sample_y,\n            batch_size, C, H, W, BLOCK_SIZE)\n        _grid_splat(to_splat, yz, batch_index, sample_y, sample_z,\n            batch_size, C, D, H, BLOCK_SIZE)\n        _grid_splat(to_splat, zx, batch_index, sample_z, sample_x,\n            batch_size, C, W, D, BLOCK_SIZE)\n    else:\n        _voxel_grid_splat(to_splat, xy, batch_index, sample_x, sample_y,\n            sample_z, batch_size, C, D, H, W, BLOCK_SIZE)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "9fcd4a84-3d27-4a1d-8723-1dc110a189e4"
  },
  {
    "input": "@triton.jit\ndef bwd_recurrence(A, B, C, U, Dt, DO, H, start, DA, DB, DC, dDt, dU, batch,\n    initial_state, grad_detach, T: 'tl.constexpr', D: 'tl.constexpr', K:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    i_v = tl.program_id(1)\n    NV = tl.cdiv(D, BV)\n    dt_ptr = Dt + i_bh * T * D + i_v * BV + tl.arange(0, BV) + (T - 1) * D\n    ddt_ptr = dDt + i_bh * T * D + i_v * BV + tl.arange(0, BV) + (T - 1) * D\n    u_ptr = U + i_bh * T * D + i_v * BV + tl.arange(0, BV) + (T - 1) * D\n    du_ptr = dU + i_bh * T * D + i_v * BV + tl.arange(0, BV) + (T - 1) * D\n    do_ptr = DO + i_bh * T * D + i_v * BV + tl.arange(0, BV) + (T - 1) * D\n    start_ptr = start + i_bh * T + (T - 1)\n    grad_detach_ptr = grad_detach + i_bh * T + (T - 1)\n    dh = tl.zeros([BV, K], dtype=tl.float32)\n    dA = tl.zeros([BV, K], dtype=tl.float32)\n    b_ptr = B + i_bh * T * K + tl.arange(0, K) + (T - 1) * K\n    c_ptr = C + i_bh * T * K + tl.arange(0, K) + (T - 1) * K\n    dc_ptr = DC + (i_bh + batch * i_v) * T * K + tl.arange(0, K) + (T - 1) * K\n    db_ptr = DB + (i_bh + batch * i_v) * T * K + tl.arange(0, K) + (T - 1) * K\n    A = A + (i_v * BV + tl.arange(0, BV)[:, None]) * K + tl.arange(0, K)[\n        None, :]\n    _A = tl.load(A)\n    H_ptr = H + i_bh * T * D * K + (i_v * BV + tl.arange(0, BV)[:, None]\n        ) * K + tl.arange(0, K)[None, :] + (T - 1) * D * K\n    for i in range(T):\n        h = tl.load(H_ptr)\n        if i < T - 1:\n            next_h = tl.load(H_ptr - D * K)\n        else:\n            next_h = tl.load(initial_state + i_bh * D * K + (i_v * BV + tl.\n                arange(0, BV)[:, None]) * K + tl.arange(0, K)[None, :])\n        b = tl.load(b_ptr)\n        c = tl.load(c_ptr)\n        do = tl.load(do_ptr)\n        u = tl.load(u_ptr)\n        dt = tl.load(dt_ptr)\n        start_flag = tl.load(start_ptr)\n        grad_detach_flag = tl.load(grad_detach_ptr)\n        dh = dh * (1 - grad_detach_flag)\n        dc = tl.sum(h * do[:, None], axis=0)\n        tl.store(dc_ptr, dc)\n        dh += do[:, None] * c[None, :]\n        dt_u = dt * u\n        db = tl.sum(dh * dt_u[:, None], axis=0)\n        tl.store(db_ptr, db)\n        ddt_u = tl.sum(dh * b[None, :], axis=1)\n        ddt = ddt_u * u\n        du = ddt_u * dt\n        tl.store(du_ptr, du)\n        dt_a = tl.exp(dt[:, None] * _A) * (1 - start_flag)\n        dh *= dt_a\n        d_decay = dh * next_h\n        dA += d_decay * dt[:, None]\n        ddt += tl.sum(d_decay * _A, axis=1)\n        tl.store(ddt_ptr, ddt)\n        b_ptr -= K\n        c_ptr -= K\n        dc_ptr -= K\n        db_ptr -= K\n        dt_ptr -= D\n        ddt_ptr -= D\n        u_ptr -= D\n        du_ptr -= D\n        do_ptr -= D\n        H_ptr -= D * K\n        start_ptr -= 1\n        grad_detach_ptr -= 1\n    DA_ptr = DA + i_bh * D * K + (i_v * BV + tl.arange(0, BV)[:, None]\n        ) * K + tl.arange(0, K)[None, :]\n    tl.store(DA_ptr, dA)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "df9184f5-0171-4fc3-b8f3-19f4f2b2b2d3"
  },
  {
    "input": "@triton.jit\ndef fwbw_init(directions, origins, grid_idx, near, far, rays_encoding,\n    inject_noise_seed, DIM_IN_COLOR: 'tl.constexpr', DIM_OUT_COLOR:\n    'tl.constexpr', num_samples: 'tl.constexpr', num_samples_inf:\n    'tl.constexpr', num_rays: 'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    tot_num_samples = num_samples + num_samples_inf\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = offs < num_rays\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = (pid * BLOCK_SIZE * DIM_OUT_COLOR + DIM_OUT_COLOR * tl.\n        arange(0, BLOCK_SIZE)[:, None] + tl.arange(0, DIM_OUT_COLOR)[None, :])\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    center_x = tl.load(origins + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(origins + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(origins + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(directions + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(directions + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(directions + offs_z, mask=offs_z < num_rays * 3)\n    near_buffer = tl.load(near + offs, mask=offs_mask)\n    far_buffer = tl.load(far + offs, mask=offs_mask)\n    grid_idx_buffer = tl.load(grid_idx + offs, mask=offs_mask)\n    seed_buffer = tl.load(inject_noise_seed + offs, mask=offs < num_rays)\n    sample_index_buffer = tl.arange(0, BLOCK_SIZE\n        ) * tot_num_samples + pid * BLOCK_SIZE * tot_num_samples + 1\n    rays_encoding_buffer = tl.load(rays_encoding + pid * BLOCK_SIZE *\n        DIM_IN_COLOR + DIM_IN_COLOR * tl.arange(0, BLOCK_SIZE)[:, None] +\n        tl.arange(0, DIM_IN_COLOR)[None, :], mask=offs_features_mask)\n    one_scaffold = tl.full((BLOCK_SIZE,), 1.0, tl.float32)\n    zero_value = tl.zeros((BLOCK_SIZE,), tl.float32)\n    one_vec = tl.full((BLOCK_SIZE, C), 1.0, tl.float32)\n    zero_color = tl.zeros((BLOCK_SIZE, DIM_OUT_COLOR), tl.float32)\n    return (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer, seed_buffer,\n        sample_index_buffer, rays_encoding_buffer, one_scaffold, zero_value,\n        one_vec, zero_color)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "678bc05f-3181-497d-afa7-cb7a57db010d"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel(input_ptr, output_ptr, input_row_stride: 'tl.constexpr',\n    output_row_stride: 'tl.constexpr', n_cols: 'tl.constexpr', block_size:\n    'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, block_size)\n    input_ptrs = row_start_ptr + col_offsets\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "6486e6ed-927e-4aec-8d95-a5850b8b539c"
  },
  {
    "input": "@autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'N'],\n    nearest_power_of_two=True)\n@triton.jit\ndef matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M,\n    N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm,\n    stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N) float16\n    g_ptr is of shape (K) int32\n    \"\"\"\n    infearure_per_bits = 32 // bits\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    g_ptrs = g_ptr + offs_k\n    scales_ptrs = scales_ptr + offs_bn[None, :]\n    zeros_ptrs = zeros_ptr + offs_bn[None, :] // infearure_per_bits\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        g_idx = tl.load(g_ptrs)\n        scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)\n        zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)\n        zeros = zeros >> zeros_shifter[None, :] & maxq\n        zeros = zeros + 1\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        b = b >> shifter[:, None] & maxq\n        b = (b - zeros) * scales\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n        g_ptrs += BLOCK_SIZE_K\n    c = accumulator\n    c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :\n        ]\n    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "485c4cd1-87de-4080-b24f-7e3ecec82cda"
  },
  {
    "input": "@triton.jit\ndef _fwd_combine_kv_splits(multiple_o, multiple_l, final_o, final_l,\n    stride_mul_oz, stride_mul_oh, stride_mul_os, stride_mul_om,\n    stride_mul_ok, stride_fin_oz, stride_fin_oh, stride_fin_om,\n    stride_fin_ok, Z, H, M, S, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    offs_h = tl.program_id(1)\n    offs_z = tl.program_id(2)\n    multiple_o += offs_z * stride_mul_oz + offs_h * stride_mul_oh\n    multiple_l += (offs_z * H + offs_h) * S * M\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not DIVISIBLE_M:\n        mask_m = offs_m < M\n    m = tl.full([BLOCK_M], value=float('-inf'), dtype=tl.float32)\n    acc = tl.full([BLOCK_M], value=float(0.0), dtype=tl.float32)\n    l_ptrs = multiple_l + offs_m\n    for _ in range(0, S):\n        if DIVISIBLE_M:\n            l = tl.load(l_ptrs)\n        else:\n            l = tl.load(l_ptrs, mask=mask_m)\n        m_new = tl.maximum(m, l)\n        acc = acc * tl.exp(m - m_new) + tl.exp(l - m_new)\n        m = m_new\n        l_ptrs += M\n    l_acc = m + tl.log(acc)\n    o_acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    l_ptrs = multiple_l + offs_m\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    o_ptrs = multiple_o + offs_m[:, None] * stride_mul_om + offs_k[None, :\n        ] * stride_mul_ok\n    for _ in range(0, S):\n        l = tl.load(l_ptrs, mask=offs_m < M)\n        rescale = tl.exp(l - l_acc)\n        if DIVISIBLE_M:\n            o = tl.load(o_ptrs)\n        else:\n            o = tl.load(o_ptrs, mask=mask_m[:, None])\n        o_acc += o * rescale[:, None]\n        l_ptrs += M\n        o_ptrs += stride_mul_os\n    final_o += offs_z * stride_fin_oz + offs_h * stride_fin_oh\n    final_l += (offs_z * H + offs_h) * M\n    a_ptrs = final_o + offs_m[:, None] * stride_fin_om + offs_k * stride_fin_ok\n    b_ptrs = final_l + offs_m\n    if DIVISIBLE_M:\n        tl.store(a_ptrs, o_acc)\n        tl.store(b_ptrs, l_acc)\n    else:\n        tl.store(a_ptrs, o_acc, mask=mask_m[:, None])\n        tl.store(b_ptrs, l_acc, mask=mask_m)\n",
    "category": "Attention Mechanisms",
    "subcategory": "multi-head attention",
    "uuid": "27858201-1281-4bd1-843b-20c652a9d8aa"
  },
  {
    "input": "@triton.jit\ndef add_vec_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: 'tl.constexpr', B1:\n    'tl.constexpr'):\n    off_x = tl.arange(0, B0)\n    off_y = tl.arange(0, B1)\n    off_z = off_y[:, None] * B0 + off_x[None, :]\n    x = tl.load(x_ptr + off_x)\n    y = tl.load(y_ptr + off_y)\n    z = y[:, None] + x[None, :]\n    tl.store(z_ptr + off_z, z)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "4ab97c7c-6ebf-4d23-9726-f8928f7d6865"
  },
  {
    "input": "@triton.jit\ndef _gemma_rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = 1.0 / tl.sqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    output = normed * (W_row + 1.0)\n    tl.store(Y + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "e07ce250-b132-4758-808b-4313a06c63b7"
  },
  {
    "input": "@triton.jit\ndef add_kernel(x_ptr, z_ptr, N0, B0: 'tl.constexpr'):\n    off_x = tl.arange(0, B0)\n    x = tl.load(x_ptr + off_x)\n    x = x + 10.0\n    tl.store(z_ptr + off_x, x)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "bffe92e6-d72c-4cf6-94d8-54e856e5d08c"
  },
  {
    "input": "@triton.jit\ndef _fg_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = e_row * tl.sigmoid(e_row)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "a7d3a8e3-2d1b-4350-a67c-e7bc0433d50f"
  },
  {
    "input": "@triton.jit\ndef _class_indices_forward(LOGITS, PROBS, IDX, LOSS, weight, N,\n    WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS:\n    'tl.constexpr', CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING:\n    'tl.constexpr', IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE:\n    'tl.constexpr', BLOCK: 'tl.constexpr'):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK)\n    logit_start_ptrs = LOGITS + row * N\n    logit_ptrs = logit_start_ptrs + cols\n    m_prev = -float('inf')\n    l_prev = 0.0\n    m_prev = m_prev\n    l_prev = l_prev\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=-float('inf'))\n        m_curr = tl.maximum(tl.max(row_logits, 0), m_prev)\n        l_prev *= tl.exp(m_prev - m_curr)\n        p = tl.exp(row_logits - m_curr)\n        l_curr = tl.sum(p, 0) + l_prev\n        l_prev = l_curr\n        m_prev = m_curr\n        logit_ptrs += BLOCK\n    logit_ptrs = logit_start_ptrs + cols\n    output_ptrs = PROBS + row * N + cols\n    WRIT_PROBS = PROBS + row * N + cols\n    if LABEL_SMOOTHING:\n        sum_total = 0.0\n        sum_total = sum_total\n        weights_total = 0.0\n        weights_total = weights_total\n        if WEIGHTS:\n            weight_ptr = weight + cols\n    l_prev_log = tl.log(l_prev)\n    for start_n in range(0, tl.cdiv(N, BLOCK)):\n        row_logits = tl.load(logit_ptrs, mask=cols < N - start_n * BLOCK,\n            other=l_prev_log + m_prev)\n        if LABEL_SMOOTHING and WEIGHTS:\n            full_weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            weights_total += tl.sum(full_weights_val, 0)\n        row_minus_max = row_logits - m_prev\n        log_softmax = l_prev_log - row_minus_max\n        if LABEL_SMOOTHING and WEIGHTS:\n            log_softmax *= full_weights_val\n        if LABEL_SMOOTHING:\n            sum_total += tl.sum(log_softmax, 0)\n        tl.store(WRIT_PROBS, log_softmax, mask=cols < N - start_n * BLOCK)\n        logit_ptrs += BLOCK\n        WRIT_PROBS += BLOCK\n        if LABEL_SMOOTHING and WEIGHTS:\n            weight_ptr += BLOCK\n    idx = tl.load(IDX + row)\n    use_class = 0.0\n    if IGNORE_INDEX >= 0:\n        use_class = idx == IGNORE_INDEX\n    READ_PROBS = PROBS + row * N + idx\n    tl.debug_barrier()\n    probs = tl.load(READ_PROBS)\n    if WEIGHTS and not LABEL_SMOOTHING:\n        weight_ptr = weight + idx\n        weights_val = tl.load(weight_ptr)\n        probs = weights_val * probs\n    if LABEL_SMOOTHING:\n        tl.store(WEIGHT_BUFFER + row, weights_total)\n        probs = (1 - smoothing_factor\n            ) * probs + smoothing_factor * sum_total / N\n    probs = probs * (1.0 - use_class)\n    tl.store(LOSS + row, probs)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "99ab9b85-b56e-47fe-bfb2-5f037f3716e0"
  },
  {
    "input": "@triton.autotune(configs=get_cuda_autotune_config(), key=['N_ITER', 'M',\n    'N', 'K'], reset_to_zero=['output_ptr'])\n@triton.jit\ndef fused_unpack_and_reconstruct_kernel_v2(packed_sign_ptr, u_ptr, vt_ptr,\n    output_ptr, N_ITER, M, N, K, n_sign_elements, stride_packed_sign_iter,\n    stride_packed_sign_m, stride_packed_sign_n, stride_u_iter, stride_u_m,\n    stride_u_k, stride_vt_iter, stride_vt_k, stride_vt_n, stride_output_m,\n    stride_output_n, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'\n    ):\n    pid_spatial = tl.program_id(axis=0)\n    pid_iter = tl.program_id(axis=1)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid_spatial // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    local_pid = pid_spatial % num_pid_in_group\n    pid_n = local_pid // group_size_m\n    pid_m = first_pid_m + local_pid % group_size_m\n    offsets_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M))[:, None]\n    offsets_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N))[None, :]\n    offsets_k = tl.arange(0, BLOCK_SIZE_K)\n    element_indices = pid_iter * M * N + offsets_m * N + offsets_n\n    element_indices = tl.reshape(element_indices, (BLOCK_SIZE_M *\n        BLOCK_SIZE_N,))\n    byte_indices = element_indices // 8\n    bit_indices = element_indices % 8\n    byte_ptrs = packed_sign_ptr + byte_indices\n    byte_mask = byte_indices < (n_sign_elements + 7) // 8\n    packed_bytes = tl.load(byte_ptrs, mask=byte_mask, other=0)\n    bits = packed_bytes >> 7 - bit_indices & 1\n    signs = bits * 2 - 1\n    signs = tl.reshape(signs, (BLOCK_SIZE_M, BLOCK_SIZE_N))\n    u_ptrs = u_ptr + pid_iter * stride_u_iter + offsets_m * stride_u_m\n    vt_ptrs = vt_ptr + pid_iter * stride_vt_iter + offsets_n * stride_vt_n\n    iter_acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        u_block_ptrs = u_ptrs + offsets_k[None, :] * stride_u_k\n        vt_block_ptrs = vt_ptrs + offsets_k[:, None] * stride_vt_k\n        u = tl.load(u_block_ptrs, mask=offsets_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        vt = tl.load(vt_block_ptrs, mask=offsets_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        iter_acc += tl.dot(u, vt, out_dtype=tl.float32)\n    output = signs * iter_acc\n    offsets_output_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_output_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    output_ptrs = output_ptr + stride_output_m * offsets_output_m[:, None\n        ] + stride_output_n * offsets_output_n[None, :]\n    output_mask = (offsets_output_m[:, None] < M) & (offsets_output_n[None,\n        :] < N)\n    tl.atomic_add(output_ptrs, output, mask=output_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4812ff17-f45c-4068-84b9-297191150097"
  },
  {
    "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV'])})\n@triton.jit\ndef parallel_retention_bwd_kernel(q, k, v, do, dq, dk, dv, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NV: 'tl.constexpr'):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    i_h = i_bh % H\n    parallel_retention_bwd_kernel_dq(i_bh, i_t, i_k, i_v, i_h, k, v, do, dq,\n        scale, B=B, H=H, T=T, K=K, V=V, BT=BT, BS=BS, BK=BK, BV=BV)\n    tl.debug_barrier()\n    parallel_retention_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, i_h, q, k, v, do,\n        dk, dv, scale, B, H, T, K, V, BT, BS, BK, BV)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "49191400-05df-425d-8e1b-2f3900837395"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum_inter(c, z, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_c = tl.make_block_ptr(c + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * NT * S, (NT * S,), (s_s_d,), (\n            i_t * S + i_s * BS,), (BS,), (0,))\n        b_c = tl.load(p_c, boundary_check=(0, 1)) + b_z\n        b_z += tl.load(p_z, boundary_check=(0,))\n        tl.store(p_c, b_c, boundary_check=(0, 1))\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "493d4c9d-b9d0-4f26-ac6a-6ac50af6346f"
  },
  {
    "input": "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD, key=['BLOCK_DMODEL',\n    'max_seqlen_q', 'max_seqlen_k'])\n@triton.jit\ndef tuned_bwd_kernel_dk_dv(Q, K, V, B, sm_scale, Out, DO, DK, DV, L, D,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n    stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn, stride_dkk,\n    stride_dvz, stride_dvh, stride_dvk, stride_dvn, cu_seqlens_q,\n    cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim,\n    dropout_p, philox_seed, philox_offset_base, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    bare_bwd_kernel_dk_dv(Q, K, V, B, sm_scale, Out, DO, DK, DV, L, D,\n        stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n        stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n        stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n        stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn,\n        stride_dkk, stride_dvz, stride_dvh, stride_dvk, stride_dvn,\n        cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k,\n        head_dim, dropout_p, philox_seed, philox_offset_base, BLOCK_M,\n        BLOCK_DMODEL, BLOCK_N, CAUSAL, ENABLE_DROPOUT, PADDED_HEAD=\n        PADDED_HEAD, BIAS_TYPE=BIAS_TYPE)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "1e049534-6bef-48f6-9feb-7b9c943e1f97"
  },
  {
    "input": "@triton.jit\ndef one_shot_all_gather_kernel(buffer_ptrs, signal_pad_ptrs, output_ptr,\n    numel: 'tl.constexpr', rank: 'tl.constexpr', world_size: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', NUMEL_PER_THREAD: 'tl.constexpr'):\n    blockwise_barrier(signal_pad_ptrs, None, rank, world_size)\n    pid = tl.program_id(axis=0)\n    buffer_ptrs = buffer_ptrs\n    output_ptr = output_ptr\n    block_start = pid * BLOCK_SIZE\n    while block_start < numel // NUMEL_PER_THREAD:\n        offsets = (block_start + tl.arange(0, BLOCK_SIZE)) * 2\n        mask = block_start + tl.arange(0, BLOCK_SIZE\n            ) < numel // NUMEL_PER_THREAD\n        for i in range(world_size):\n            buffer_ptr = tl.load(buffer_ptrs + i)\n            hi, lo = load_128(buffer_ptr + offsets, mask=mask)\n            scale_factor_for_uint64_ptr = (tl.uint64.primitive_bitwidth //\n                tl.bfloat16.primitive_bitwidth)\n            tl.store(output_ptr + i * numel // scale_factor_for_uint64_ptr +\n                offsets + 0, hi, mask=mask)\n            tl.store(output_ptr + i * numel // scale_factor_for_uint64_ptr +\n                offsets + 1, lo, mask=mask)\n        block_start += tl.num_programs(axis=0) * BLOCK_SIZE\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "f627f42c-9bd1-44ba-8852-0db5afded576"
  },
  {
    "input": "@triton.jit\ndef _kernel_inside_merge_discontinuous_v1(alpha_c, tmp_merge,\n    tmp_merge_normalizer, w, batch, L, stride_alpha_c1, stride_alpha_c2,\n    stride_alpha_c3, stride_tmp_merge1, stride_tmp_merge2,\n    stride_tmp_merge3, stride_normalizer1, stride_normalizer2, r1, r2, r3,\n    r4, BLOCK_R3: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    if b_idx >= batch:\n        return\n    span_length_left = tl.program_id(1) + 1\n    tid = tl.program_id(2)\n    start = 0\n    while tid >= L - w - start:\n        tid -= L - w - start\n        start += 1\n    gap_start = start + span_length_left\n    gap_end = gap_start + (tid + 1)\n    end = gap_end + (w - span_length_left)\n    l_ptr = (alpha_c + b_idx * stride_alpha_c1 + start * stride_alpha_c2 + \n        gap_start * stride_alpha_c3 + 2 * r1 + r2 + tl.arange(0, BLOCK_R3))\n    r_ptr = (alpha_c + b_idx * stride_alpha_c1 + gap_end * stride_alpha_c2 +\n        end * stride_alpha_c3 + 2 * r1 + r2 + r3 + tl.arange(0, BLOCK_R3))\n    mask = tl.arange(0, BLOCK_R3) < r3\n    child_l = tl.load(l_ptr, mask=mask, other=-1000000000.0)\n    child_r = tl.load(r_ptr, mask=mask, other=-1000000000.0)\n    acc1 = child_l + child_r\n    acc_max = tl.max(acc1, 0)\n    tl.store(tmp_merge_normalizer + b_idx * stride_normalizer1 + tl.\n        program_id(1) * stride_normalizer2 + tl.program_id(2), acc_max)\n    acc = tl.exp(acc1 - acc_max)\n    tl.store(tmp_merge + b_idx * stride_tmp_merge1 + tl.program_id(1) *\n        stride_tmp_merge2 + tl.program_id(2) * stride_tmp_merge3 + tl.\n        arange(0, BLOCK_R3), acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "96bb8752-2825-4ceb-892b-9a0ebf1eda23"
  },
  {
    "input": "@triton.jit\ndef dropout_rng(philox_seed, philox_offset, dropout_p, m, n, stride):\n    rng_offsets = dropout_offsets(philox_seed, philox_offset, dropout_p, m,\n        n, stride)\n    return tl.rand(philox_seed, rng_offsets)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "464b7ece-860d-468e-b6b2-4b3e649c2717"
  },
  {
    "input": "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV']),\n    'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None})\n@triton.jit\ndef parallel_retention_fwd_kernel(q, k, v, o, attn, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NV: 'tl.constexpr',\n    OUTPUT_ATTENTIONS: 'tl.constexpr'):\n    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_k, i_v = i_kv // NV, i_kv % NV\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_k = tl.arange(0, BS)\n    d_h = tl.math.exp2((BS - o_k) * b_b)\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0),\n        (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV),\n        (BS, BV), (1, 0))\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, 0), (BT, BS), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i in range(0, i_t * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_h\n        if i > 0:\n            b_o = b_o * tl.math.exp2(b_b * BS)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BS))\n        p_v = tl.advance(p_v, (BS, 0))\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s, boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BT)\n    d_q = tl.math.exp2(tl.arange(0, BT) * b_b)\n    b_o *= d_q[:, None]\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, \n        i_t * BT), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BS, BV), (1, 0))\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + (i_k * B * H + i_bh) * T * T, (T, T),\n            (T, 1), (i_t * BT, i_t * BT), (BT, BS), (1, 0))\n    for _ in range(i_t * BT, (i_t + 1) * BT, BS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            tl.store(p_a, b_s, boundary_check=(0, 1))\n            p_a = tl.advance(p_a, (0, BS))\n        p_k = tl.advance(p_k, (0, BS))\n        p_v = tl.advance(p_v, (BS, 0))\n        o_k += BS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * T * V, (T, V), (V, 1\n        ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "959498e1-846d-4f83-a942-c33b487f56ac"
  },
  {
    "input": "@triton.jit\ndef _prob_fwd_kernel(Q, K, LSE, nheads, seqlen_q, seqlen_k, BLOCK_HEADDIM:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    ndims = nheads * BLOCK_HEADDIM\n    offs_m = tl.arange(0, BLOCK_M) + start_m * BLOCK_M\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + ndims * offs_m[:, None]\n    k_ptrs = K + ndims * offs_n[:, None]\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    end_n = seqlen_k\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        for off_h in range(nheads):\n            offs_hd = (offs_d + off_h * BLOCK_HEADDIM)[None, :]\n            q = tl.load(q_ptrs + offs_hd, mask=offs_m[:, None] < seqlen_q,\n                other=0.0)\n            k = tl.load(k_ptrs + offs_hd + start_n * ndims, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n            qk += tl.dot(q, tl.trans(k))\n        m_ij = tl.maximum(tl.max(qk, 1), m_i)\n        p = tl.exp(qk - m_ij[:, None])\n        p = tl.where((start_n + offs_n)[None, :] < seqlen_k, p, 0.0)\n        lse_i = tl.exp(m_i - m_ij) * lse_i + tl.sum(p, 1)\n        m_i = m_ij\n    lse_i = m_i + tl.log(lse_i)\n    lse_i = tl.where(offs_m < seqlen_q, lse_i, 0.0)\n    tl.store(LSE + offs_m, lse_i)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "7290978d-54bb-4ffc-be5a-48c19eb9249c"
  },
  {
    "input": "@triton.jit\ndef relu(x):\n    \"\"\"\n    ReLU_ activation function\n\n    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n    \"\"\"\n    zero = 0.0\n    return tl.where(x >= 0, x, zero)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "154a095e-7aa3-473e-9ddf-cb648fb96e1c"
  },
  {
    "input": "@triton.jit\ndef _triton_second_order_fwd(x_ptr: 'tl.tensor', y_ptr: 'tl.tensor', z_ptr:\n    'tl.tensor', sh_1_0_ptr: 'tl.tensor', sh_1_1_ptr: 'tl.tensor',\n    sh_1_2_ptr: 'tl.tensor', sh_2_0_ptr: 'tl.tensor', sh_2_1_ptr:\n    'tl.tensor', sh_2_2_ptr: 'tl.tensor', sh_2_3_ptr: 'tl.tensor',\n    sh_2_4_ptr: 'tl.tensor', BLOCK_SIZE: 'tl.constexpr', vector_length:\n    'tl.constexpr'):\n    sqrt_3 = 3 ** 0.5\n    block_id = tl.program_id(0)\n    offset = tl.arange(0, BLOCK_SIZE) + BLOCK_SIZE * block_id\n    x_row_start = x_ptr + offset\n    y_row_start = y_ptr + offset\n    z_row_start = z_ptr + offset\n    x = tl.load(x_row_start, mask=offset < vector_length)\n    y = tl.load(y_row_start, mask=offset < vector_length)\n    z = tl.load(z_row_start, mask=offset < vector_length)\n    sh_1_0 = x * sqrt_3\n    sh_1_1 = y * sqrt_3\n    sh_1_2 = z * sqrt_3\n    sqrt_15 = 15 ** 0.5\n    sqrt_5 = 5 ** 0.5\n    sq_x = x * x\n    sq_y = y * y\n    sq_z = z * z\n    sh_2_0 = sqrt_15 * x * z\n    sh_2_1 = sqrt_15 * x * y\n    sh_2_2 = sqrt_5 * (sq_y - 0.5 * (sq_x + sq_z))\n    sh_2_3 = sqrt_15 * y * z\n    sh_2_4 = 0.5 * sqrt_15 * (sq_z - sq_x)\n    sh_1_0_start = sh_1_0_ptr + offset\n    sh_1_1_start = sh_1_1_ptr + offset\n    sh_1_2_start = sh_1_2_ptr + offset\n    sh_2_0_start = sh_2_0_ptr + offset\n    sh_2_1_start = sh_2_1_ptr + offset\n    sh_2_2_start = sh_2_2_ptr + offset\n    sh_2_3_start = sh_2_3_ptr + offset\n    sh_2_4_start = sh_2_4_ptr + offset\n    tl.store(sh_1_0_start, sh_1_0, mask=offset < vector_length)\n    tl.store(sh_1_1_start, sh_1_1, mask=offset < vector_length)\n    tl.store(sh_1_2_start, sh_1_2, mask=offset < vector_length)\n    tl.store(sh_2_0_start, sh_2_0, mask=offset < vector_length)\n    tl.store(sh_2_1_start, sh_2_1, mask=offset < vector_length)\n    tl.store(sh_2_2_start, sh_2_2, mask=offset < vector_length)\n    tl.store(sh_2_3_start, sh_2_3, mask=offset < vector_length)\n    tl.store(sh_2_4_start, sh_2_4, mask=offset < vector_length)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "386b70ff-264e-45eb-ab69-52a419244383"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel(input_ptr, input_batch_stride, input_row_stride,\n    output_ptr, num_rows, num_cols, BLOCK_SIZE: 'tl.constexpr'):\n    batch_id = tl.program_id(axis=0)\n    row_id = tl.program_id(axis=1)\n    batch_offset = batch_id * input_batch_stride\n    row_offset = row_id * input_row_stride + tl.arange(0, BLOCK_SIZE)\n    mask = tl.arange(0, BLOCK_SIZE) < num_cols\n    data = tl.load(input_ptr + batch_offset + row_offset, mask, other=-\n        float('inf'))\n    data = data - tl.max(data, axis=0)\n    row_wise_exp = tl.exp(data)\n    row_wise_sum = tl.sum(row_wise_exp, axis=0)\n    output = row_wise_exp / row_wise_sum\n    tl.store(output_ptr + batch_offset + row_offset, output, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "e5522389-7b90-4f1c-b4ba-27472d8c82dd"
  },
  {
    "input": "@triton.jit\ndef relu_grad(x):\n    zero = 0.0\n    one = 1.0\n    return tl.where(x >= 0, one, zero)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "70b82a66-b4cd-4b7a-982e-4045ab66ab2e"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    v_headdim, BLOCK_M: 'tl.constexpr', V_BLOCK_HEADDIM: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, V_BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=offs_d[None, :] <\n        v_headdim, other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=offs_d[None, :] <\n        v_headdim, other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q + offs_m, delta)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "77249fc3-38cd-4e9b-8b00-8d555e04a0c2"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_rcum(s, r, c, o, s_sk_h, s_sk_t, s_sk_m, T, BT:\n    'tl.constexpr', BM: 'tl.constexpr', DM: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_m, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        (NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_c = tl.make_block_ptr(c + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        (NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m), (\n        (NT - 1) * BT, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_t = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BM], dtype=tl.float32)\n    for i in range(NT):\n        p_r = tl.make_block_ptr(r + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m\n            ,), ((NT - i) % NT * DM + i_m * BM,), (BM,), (0,))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_r = tl.load(p_r, boundary_check=(0,))\n        b_c = tl.load(p_c, boundary_check=(0, 1))\n        b_o = tl.load(p_o, boundary_check=(0, 1))\n        b_z = b_z * b_r\n        b_o -= b_c * (b_z[None, :] + tl.dot(m_t, b_s, allow_tf32=False))\n        b_z += tl.sum(b_s, 0)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_s = tl.advance(p_s, (-BT, 0))\n        p_c = tl.advance(p_c, (-BT, 0))\n        p_o = tl.advance(p_o, (-BT, 0))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "02dbe545-7cbc-4551-b50b-936ba0d44b1c"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, L, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    qvk_offset = off_hz * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(\n        BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n        ), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if IS_CAUSAL:\n            qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n                float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "28318cc0-d278-4216-a638-70a78d576250"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "30a3f484-b64b-4c29-bbc8-7ca712c5cd0c"
  },
  {
    "input": "@triton.jit\ndef leaky_relu_grad(x):\n    min_grad = 0.01\n    max_grad = 1\n    min_grad = min_grad\n    max_grad = max_grad\n    return tl.where(x >= 0, max_grad, min_grad)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "ad59321d-062a-440b-9899-f969593043c1"
  },
  {
    "input": "@triton.jit\ndef combine_add(a, b):\n    return a + b\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "dc07e6ec-07a6-4512-bc75-ac6dbe0172b6"
  },
  {
    "input": "@triton.jit\ndef _splat_3d(to_splat, grad_image, w, batch_index, ix, iy, iz, ID, IH, IW,\n    C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW) * (iz < ID) * (\n        iz >= 0))\n    w = tl.view(w[:, None], (BLOCK_SIZE, 1))\n    offs = tl.view((batch_index * ID * IW * IH * C + iz_ * IW * IH * C + \n        iy_ * IW * C + ix_ * C)[:, None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "b20c1657-a163-4a59-b0ec-f49274afd5c8"
  },
  {
    "input": "@triton.jit\ndef backward_scan(gates, tokens, outputs, SEQUENCE_LENGTH: 'tl.constexpr'):\n    sequence_id = tl.num_programs(axis=1) * tl.program_id(axis=0\n        ) + tl.program_id(axis=1)\n    forward_strides = tl.arange(0, SEQUENCE_LENGTH\n        ) + sequence_id * SEQUENCE_LENGTH\n    reverse_strides = tl.num_programs(axis=0) * tl.num_programs(axis=1\n        ) * SEQUENCE_LENGTH - 1 - forward_strides\n    tokens_ = tl.load(tokens + reverse_strides)\n    gates_ = tl.load(gates + reverse_strides)\n    tuples = pack64(tokens_, gates_)\n    output_tuples_ = tl.associative_scan(tuples, axis=0, combine_fn=\n        first_order_op)\n    output_tokens_, output_gates_ = unpack64(output_tuples_)\n    tl.store(outputs + reverse_strides, output_tokens_)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "f1ff2bf1-c31e-4031-80ef-77b149908282"
  },
  {
    "input": "@triton.jit\ndef rms_kernel(audios, audios_real_lens, audios_max_len, batch_idx,\n    BLOCK_SIZE_RMS: 'tl.constexpr'):\n    audios_real_lens_vals = tl.load(audios_real_lens + batch_idx)\n    _mean = tl.zeros([BLOCK_SIZE_RMS], dtype=tl.float32)\n    for offset in range(0, audios_max_len, BLOCK_SIZE_RMS):\n        audios_block_ptr = offset + tl.arange(0, BLOCK_SIZE_RMS)\n        audios_mask = audios_block_ptr < audios_real_lens_vals\n        audios_vals = tl.load(audios + batch_idx * audios_max_len +\n            audios_block_ptr, mask=audios_mask)\n        audios_partial_sum_sq = tl.where(audios_mask, tl.math.pow(\n            audios_vals, 2.0), 0)\n        _mean += audios_partial_sum_sq\n    audios_global_sum_sq = tl.sum(_mean, axis=0)\n    return tl.sqrt(audios_global_sum_sq / audios_real_lens_vals)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "1179d4d9-609f-40c3-b4c1-6e7714e51f19"
  },
  {
    "input": "@triton.jit\ndef print_grid():\n    pid = tl.program_id(0)\n    tl.device_print('pid: ', pid)\n",
    "category": "Helper Functions",
    "subcategory": "",
    "uuid": "8792a7b6-3043-4b2d-baa1-5cf4b78a5492"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BC'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge(A, A2, T: 'tl.constexpr',\n    BT: 'tl.constexpr', BC: 'tl.constexpr', NK: 'tl.constexpr'):\n    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_c * BC >= T:\n        return\n    n_bh = tl.num_programs(2)\n    b_A = tl.zeros([BC, BC], dtype=tl.float32)\n    for i_k in range(0, NK):\n        p_A = tl.make_block_ptr(A + (i_bh + i_k * n_bh) * T * BC, (T, BC),\n            (BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n        b_A += tl.load(p_A, boundary_check=(0, 1))\n    p_A2 = tl.make_block_ptr(A2 + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\n    tl.store(p_A2, b_A, boundary_check=(0, 1))\n",
    "category": "Kernel Operations",
    "subcategory": "kernel optimization",
    "uuid": "ad9fd892-7d8d-4be9-94fc-0b3b146565b1"
  },
  {
    "input": "@triton.jit\ndef sum_kernel(x_ptr, z_ptr, N0, N1, T, B0: 'tl.constexpr', B1: 'tl.constexpr'\n    ):\n    block_id_i = tl.program_id(0)\n    off_i = block_id_i * B0 + tl.arange(0, B0)\n    mask_i = off_i < N0\n    z = tl.zeros([B0], dtype=tl.float32)\n    for id_j in tl.range(0, T, B1):\n        off_j = id_j + tl.arange(0, B1)\n        off_ij = off_i[:, None] * T + off_j[None, :]\n        mask_j = off_j < T\n        mask_ij = mask_i[:, None] & mask_j[None, :]\n        x = tl.load(x_ptr + off_ij, mask=mask_ij)\n        z += tl.sum(x, axis=1)\n    tl.store(z_ptr + off_i, z, mask=mask_i)\n    return\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "25050036-6a4c-46d3-b414-1fb9672548c4"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "6db67b9e-835c-4d8a-ada8-a67d7ad90547"
  },
  {
    "input": "@triton.jit\ndef tl_softcapping(v: 'tl.tensor', softcap: 'float') ->tl.tensor:\n    return tl_tanh(v / softcap) * softcap\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "0208d1a0-e068-46d3-bdc9-5803c8efe24c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_NON_REDUCE_DIM': b_nr,\n    'BLOCK_SIZE_REDUCE_DIM': b_r}, num_warps=w) for b_nr, b_r, w in\n    itertools.product([2, 4, 8, 16], [2, 4, 8, 16], [2, 4, 8])], key=['M', 'N']\n    )\n@triton.jit\ndef triton_sum_kernel_1D_result_sum_then_buffer(input_ptr, output_ptr, M, N,\n    BLOCK_SIZE_NON_REDUCE_DIM: 'tl.constexpr', BLOCK_SIZE_REDUCE_DIM:\n    'tl.constexpr', dim: 'tl.constexpr'):\n    \"\"\"\n    Sum blocks of input using Triton and store in buffer\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    reduce_dim_len = M if dim == 0 else N\n    non_reduce_dim_len = N if dim == 0 else M\n    buffer = tl.zeros((1, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32)\n    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM\n    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(0,\n        BLOCK_SIZE_NON_REDUCE_DIM)\n    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len\n    for block_start_reduce_dim in range(0, reduce_dim_len,\n        BLOCK_SIZE_REDUCE_DIM):\n        offsets_reduce_dim = block_start_reduce_dim + tl.arange(0,\n            BLOCK_SIZE_REDUCE_DIM)\n        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len\n        idxs, mask = None, None\n        if dim == 0:\n            idxs = offsets_reduce_dim[:, None\n                ] * non_reduce_dim_len + offsets_non_reduce_dim\n            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim\n        elif dim == 1:\n            idxs = offsets_non_reduce_dim[:, None\n                ] * reduce_dim_len + offsets_reduce_dim\n            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim\n        input = tl.load(input_ptr + idxs, mask=mask, other=mask)\n        buffer += tl.sum(input, axis=dim)\n    buffer_view = buffer.reshape((BLOCK_SIZE_NON_REDUCE_DIM,))\n    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=\n        mask_non_reduce_dim)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "c1c572a5-a578-485d-b992-4f797b7bce88"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}), triton.Config\n    ({'BLOCK_SIZE_H': 2}), triton.Config({'BLOCK_SIZE_H': 4}), triton.\n    Config({'BLOCK_SIZE_H': 8}), triton.Config({'BLOCK_SIZE_H': 16}),\n    triton.Config({'BLOCK_SIZE_H': 32}), triton.Config({'BLOCK_SIZE_H': 64}\n    )], key=['chunk_size', 'nheads'])\n@triton.jit\ndef _chunk_cumsum_fwd_kernel(dt_ptr, A_ptr, dt_bias_ptr, dt_out_ptr,\n    dA_cumsum_ptr, batch, seqlen, nheads, chunk_size, dt_min, dt_max,\n    stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head,\n    stride_dt_bias_head, stride_dt_out_batch, stride_dt_out_chunk,\n    stride_dt_out_head, stride_dt_out_csize, stride_dA_cs_batch,\n    stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, DT_SOFTPLUS:\n    'tl.constexpr', HAS_DT_BIAS: 'tl.constexpr', BLOCK_SIZE_H:\n    'tl.constexpr', BLOCK_SIZE_CHUNK: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\n    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    dt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] *\n        stride_dt_seqlen)\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    dt_out_ptrs = dt_out_ptr + (offs_h[:, None] * stride_dt_out_head + \n        offs_c[None, :] * stride_dt_out_csize)\n    dA_cs_ptrs = dA_cumsum_ptr + (offs_h[:, None] * stride_dA_cs_head + \n        offs_c[None, :] * stride_dA_cs_csize)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :\n        ] < chunk_size_limit), other=0.0)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=\n            offs_h < nheads, other=0.0)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt = softplus(dt)\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] <\n        chunk_size_limit), dt, 0.0)\n    tl.store(dt_out_ptrs, dt, mask=(offs_h[:, None] < nheads) & (offs_c[\n        None, :] < chunk_size))\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0)\n    dA = dt * A[:, None]\n    dA_cs = tl.cumsum(dA, axis=1)\n    tl.store(dA_cs_ptrs, dA_cs, mask=(offs_h[:, None] < nheads) & (offs_c[\n        None, :] < chunk_size))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "c7388a38-3913-4e8a-a32f-cd7d8fe6cfbb"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_linear_attn_fwd_kernel(q, k, v, o, h0, ht, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B, H, T, K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "31b73ce4-bd93-450a-9054-d7dccbbdcb57"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, Rstd, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "7fbefc9a-ddb1-4a41-bd52-5673f05c7bc5"
  },
  {
    "input": "@triton.jit\ndef _bwd_recurrence(S, p2, DS, Dp2, NUM_BLOCK, NUM_SPLIT_K, NUM_SPLIT_V,\n    D_MODEL_K: 'tl.constexpr', D_MODEL_V: 'tl.constexpr', BLOCK_MODEL:\n    'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)\n    S = (S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None,\n        :] + (NUM_BLOCK - 2) * D_MODEL_K * D_MODEL_V)\n    DS = (DS + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None,\n        :] + (NUM_BLOCK - 1) * D_MODEL_K * D_MODEL_V)\n    p2 = p2 + offset_bh * NUM_BLOCK * D_MODEL_V + tl.arange(0, BLOCK_MODEL\n        ) + offset_s * BLOCK_MODEL + (NUM_BLOCK - 2) * D_MODEL_V\n    Dp2 = (Dp2 + offset_bh * NUM_BLOCK * D_MODEL_V * NUM_SPLIT_K + offset_d *\n        D_MODEL_V + tl.arange(0, BLOCK_MODEL) + offset_s * BLOCK_MODEL + (\n        NUM_BLOCK - 2) * D_MODEL_V * NUM_SPLIT_K)\n    Dacc = tl.zeros([BLOCK_MODEL, BLOCK_MODEL], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1):\n        p_value = tl.load(p2)\n        S_i = tl.load(S)\n        DS_i = tl.load(DS)\n        Dacc += DS_i\n        dp_i = Dacc * S_i\n        dp_value = tl.sum(dp_i, axis=0)\n        tl.store(Dp2, dp_value)\n        tl.store(S, Dacc)\n        Dacc *= p_value[None, :]\n        S -= D_MODEL_K * D_MODEL_V\n        DS -= D_MODEL_K * D_MODEL_V\n        p2 -= D_MODEL_V\n        Dp2 -= D_MODEL_V * NUM_SPLIT_K\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "720c789a-6289-482e-abb3-96f6f4cf01dd"
  },
  {
    "input": "@triton.jit\ndef _block_is_filtered(check_val: 'tl.tensor', filter_eps: 'tl.tensor'\n    ) ->tl.tensor:\n    return tl.reduce(check_val < filter_eps, None, tl_and_reduce_fn)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "4286b659-9331-4ee1-852f-255877bd32a5"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, L, NewDO, Delta, BLOCK_M: 'tl.constexpr',\n    D_HEAD: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    denom = tl.load(L + off_m)\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    tl.store(NewDO + off_m[:, None] * D_HEAD + off_n[None, :], do)\n    tl.store(Delta + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b0e75d91-eb38-47c8-b689-9ebc091b575f"
  },
  {
    "input": "@triton.jit\ndef leaky_relu(x):\n    return tl.where(x >= 0, x, 0.01 * x)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "55d07641-9624-47a6-a82b-00a83a53c828"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_dh(q, g, do, dh, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t,\n    s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NORMK: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        if NORMK:\n            p_g = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (s_k_d, s_k_t\n                ), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,),\n                ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            b_dh *= tl.exp(b_gn)[:, None]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_q = b_q * tl.exp(b_g)\n        else:\n            p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d\n                ), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,),\n                ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            b_gn = tl.load(p_gn, boundary_check=(0,))\n            b_dh *= tl.exp(b_gn)[None, :]\n            b_g = tl.load(p_g, boundary_check=(0, 1))\n            b_do = b_do * tl.exp(b_g)\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "ed67098f-2b86-492b-92b0-061fd96bc2e0"
  },
  {
    "input": "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K: 'tl.constexpr', stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    tl.static_assert(K % (4 * BLOCK_SIZE_K) == 0,\n        'K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K'\n        )\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    \"\"\"\n        This part of the code generates pointers to the specific blocks of matrices A and B that the current thread block will process.\n\n        As described in the PyTorch documentation, a stride refers to the step size needed to move from one element to the next along a given dimension:\n\n        For matrix A: stride_am = A.stride(0) = K (stride along the rows), and stride_ak = A.stride(1) = 1 (stride along the columns).\n        For matrix B: stride_bk = B.stride(0) = N (stride along the rows), and stride_bn = B.stride(1) = 1 (stride along the columns).\n        Now, let's break down the pointer generation:\n\n        offs_am[:, None] creates a column of shape [BLOCK_SIZE_M, 1], which represents the row indices of matrix A that this block is processing. It is multiplied by K (the number of columns in matrix A) since A is stored in row-major order. So, the element at position (i, j) in A is located at index i*K + j in memory.\n        offs_k[None, BLOCK_SIZE_K] creates a row vector representing the column indices of the block, i.e., a range from 0 to BLOCK_SIZE_K. This is used to compute the positions of the columns within the block.\n        When combined, the result has the shape [BLOCK_SIZE_M, BLOCK_SIZE_K], where each entry (i, j) points to the element in matrix A at position (i, j) for the current block.\n\n        The same logic is applied to matrix B, but the resulting shape is [BLOCK_SIZE_K, BLOCK_SIZE_N], representing the block of matrix B that the thread block will work on.\n    \"\"\"\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\n    \"\"\"\n        We split the loop into two layers. The outer loop runs 4 times, and each iteration focuses on a specific portion of matrix A.\n\n        For example, when i = 0, we\u2019re only concerned with the blocks of matrix A that cover the range from 0 to K // (4 * BLOCK_SIZE_K).\n        Since matrix B is packed, its first dimension is effectively divided by 4. So, while we process the first segment of matrix A,\n        we still iterate over the entire first dimension of matrix B.\n\n        In each of the 4 iterations of the outer loop, we go through the full blocks of matrix B, but what changes is the data we extract.\n        Matrix B elements contain 4 weights, all packed into an int8 format, and during each iteration of the outer loop,\n        we extract a different weight by using bitwise shifting operations. This way, we access a unique weight on each pass.\n    \"\"\"\n    for i in range(4):\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n            stride_bn)\n        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n                other=0)\n            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n            mask = 3 << 2 * i\n            b = (b_uint8 & mask) >> 2 * i\n            tensor_full = tl.full((1,), 1, dtype=tl.int8)\n            accumulator += tl.dot(a, b - tensor_full, out_dtype=tl.int32)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "dbdf73e1-2681-48a9-9786-3c7428d5ccdf"
  },
  {
    "input": "@triton.jit\ndef _get_voxel_grid_sample_info(gi, ix_in, iy_in, iz_in, ID, IH, IW,\n    feature_grid_size, C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    BS = tl.load(feature_grid_size + offs + 0)\n    grid_numel = BS * ID * IH * IW * C\n    grid_numel = tl.sum(grid_numel, axis=0) // BLOCK_SIZE\n    ix11 = (ix_in + 1) / 2 * IW - 0.5\n    iy11 = (iy_in + 1) / 2 * IH - 0.5\n    iz11 = (iz_in + 1) / 2 * ID - 0.5\n    ix = ix11 * (IW > 1)\n    iy = iy11 * (IH > 1)\n    iz = iz11 * (ID > 1)\n    ix0 = _floor(ix)\n    iy0 = _floor(iy)\n    iz0 = _floor(iz)\n    return ix, iy, iz, ix0, iy0, iz0, grid_numel\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "d443285e-8457-41ef-a2f7-b50d18e77b9f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)],\n    key=['M'])\n@triton.jit\ndef triton_jagged_sum_kernel_variable_length_loop_buffer_then_sum(\n    input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_ragged\n        ), tl.load(input_ptr_offsets + (pid_ragged + 1))\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED\n        ):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer_sum = tl.sum(buffer, axis=0)\n    buffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\n    output_offsets = offsets_m + pid_ragged * M\n    output_mask = output_offsets < M * (pid_ragged + 1)\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "6e594c9c-83b4-4b9d-b76c-eb3330445363"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_gla_fwd_kernel(q, k, v, gk, gv, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * DV if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if REVERSE else 0)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            h = h * _gk[None, :]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            h = h * _gv[:, None]\n        h += _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += -DK if REVERSE else DK\n        p_k += -DK if REVERSE else DK\n        p_o += -DV if REVERSE else DV\n        p_v += -DV if REVERSE else DV\n        if USE_GK:\n            p_gk += -DK if REVERSE else DK\n        if USE_GV:\n            p_gv += -DV if REVERSE else DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "79c70999-bde5-440f-8996-23808edcab4f"
  },
  {
    "input": "@triton.jit\ndef fused_ffn_kernel(x_ptr, w_ptr, z_ptr, M, N, K, b_ptr=None, r_ptr=None,\n    apply_gelu=False, dropout_prob=0.0, seed=1337, BLOCK_SIZE_M:\n    'tl.constexpr'=128, BLOCK_SIZE_N: 'tl.constexpr'=128, BLOCK_SIZE_K:\n    'tl.constexpr'=64):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)[:, None]\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)[None, :]\n    z = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, K, BLOCK_SIZE_K):\n        x_k = tl.arange(0, BLOCK_SIZE_K)[None, :] + k\n        x = tl.load(x_ptr + offs_m * K + x_k, mask=(offs_m < M) & (x_k < K),\n            other=0.0)\n        x = x\n        w_k = tl.arange(0, BLOCK_SIZE_K)[:, None] + k\n        w = tl.load(w_ptr + w_k * N + offs_n, mask=(w_k < K) & (offs_n < N),\n            other=0.0)\n        w = w\n        z = tl.dot(x, w, acc=z)\n    if b_ptr is not None:\n        b = tl.load(b_ptr + offs_n, mask=offs_n < N, other=0.0)\n        z += b\n    z_offset = offs_m * N + offs_n\n    z_mask = (offs_m < M) & (offs_n < N)\n    if apply_gelu:\n        z = gelu_new(z)\n    if dropout_prob > 0.0:\n        z = dropout(z, dropout_prob, seed, z_offset)\n    if r_ptr is not None:\n        r = tl.load(r_ptr + z_offset, mask=z_mask)\n        z += r\n    tl.store(z_ptr + z_offset, z, mask=z_mask)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "dca8c259-7965-469b-9d6f-b3fd79277365"
  },
  {
    "input": "@triton.jit\ndef index_select_cat_fwd_kernel(output_ptr, source_ptr, index_ptr,\n    num_indices, num_cols, stride0, stride1, BLOCK_SIZE_INDEX:\n    'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    indices = pid0 * BLOCK_SIZE_INDEX + tl.arange(0, BLOCK_SIZE_INDEX)\n    rows = tl.load(index_ptr + indices, mask=indices < num_indices)\n    cols = pid1 * BLOCK_SIZE_COL + tl.arange(0, BLOCK_SIZE_COL)\n    source_offsets = source_ptr + rows[:, None] * stride0 + cols[None, :\n        ] * stride1\n    mask = (indices[:, None] < num_indices) & (cols[None, :] < num_cols)\n    output = tl.load(source_offsets, mask=mask)\n    output_offsets = output_ptr + indices[:, None] * stride0 + cols[None, :\n        ] * stride1\n    tl.store(output_offsets, output, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "1950cb55-7669-4c0c-b079-4208075d263f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}, pre_hook=\n    init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({\n    'BLOCK_SIZE_H': 2}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])),\n    triton.Config({'BLOCK_SIZE_H': 4}, pre_hook=init_to_zero(['dA_ptr',\n    'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 8}, pre_hook=\n    init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({\n    'BLOCK_SIZE_H': 16}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])),\n    triton.Config({'BLOCK_SIZE_H': 32}, pre_hook=init_to_zero(['dA_ptr',\n    'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 64}, pre_hook=\n    init_to_zero(['dA_ptr', 'ddt_bias_ptr']))], key=['chunk_size', 'nheads'])\n@triton.jit\ndef _chunk_cumsum_bwd_kernel(ddA_ptr, ddt_out_ptr, dt_ptr, A_ptr,\n    dt_bias_ptr, ddt_ptr, dA_ptr, ddt_bias_ptr, batch, seqlen, nheads,\n    chunk_size, dt_min, dt_max, stride_ddA_batch, stride_ddA_chunk,\n    stride_ddA_head, stride_ddA_csize, stride_ddt_out_batch,\n    stride_ddt_out_chunk, stride_ddt_out_head, stride_ddt_out_csize,\n    stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head,\n    stride_dt_bias_head, stride_ddt_batch, stride_ddt_seqlen,\n    stride_ddt_head, stride_dA_head, stride_ddt_bias_head, DT_SOFTPLUS:\n    'tl.constexpr', HAS_DT_BIAS: 'tl.constexpr', BLOCK_SIZE_H:\n    'tl.constexpr', BLOCK_SIZE_CHUNK: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=0)\n    pid_c = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk\n    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk\n    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * chunk_size *\n        stride_ddt_seqlen)\n    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\n    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\n    ddt_out_ptrs = ddt_out_ptr + (offs_h[:, None] * stride_ddt_out_head + \n        offs_c[None, :] * stride_ddt_out_csize)\n    ddA_ptrs = ddA_ptr + (offs_h[:, None] * stride_ddA_head + offs_c[None,\n        :] * stride_ddA_csize)\n    dt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] *\n        stride_dt_seqlen)\n    ddt_ptrs = ddt_ptr + (offs_h[:, None] * stride_ddt_head + offs_c[None,\n        :] * stride_ddt_seqlen)\n    A_ptrs = A_ptr + offs_h * stride_A_head\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    ddA = tl.load(ddA_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None,\n        :] < chunk_size_limit), other=0.0)\n    ddt_out = tl.load(ddt_out_ptrs, mask=(offs_h[:, None] < nheads) & (\n        offs_c[None, :] < chunk_size_limit), other=0.0)\n    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0)\n    ddt = ddA * A[:, None] + ddt_out\n    dt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :\n        ] < chunk_size_limit), other=0.0)\n    if HAS_DT_BIAS:\n        dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=\n            offs_h < nheads, other=0.0)\n        dt += dt_bias[:, None]\n    if DT_SOFTPLUS:\n        dt_presoftplus = dt\n        dt = softplus(dt)\n    clamp_mask = (dt < dt_min) | (dt > dt_max)\n    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\n    dt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] <\n        chunk_size_limit), dt, 0.0)\n    ddt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] <\n        chunk_size_limit), ddt, 0.0)\n    ddt = tl.where(clamp_mask, 0.0, ddt)\n    if DT_SOFTPLUS:\n        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(\n            dt_presoftplus), ddt)\n    tl.store(ddt_ptrs, ddt, mask=(offs_h[:, None] < nheads) & (offs_c[None,\n        :] < chunk_size_limit))\n    dA = tl.sum(ddA * dt, axis=1)\n    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)\n    if HAS_DT_BIAS:\n        ddt_bias = tl.sum(ddt, axis=1)\n        tl.atomic_add(ddt_bias_ptr + offs_h * stride_ddt_bias_head,\n            ddt_bias, mask=offs_h < nheads)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "11cf0609-705b-483b-8d12-933a8fe9c4cb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_K': b}, num_warps=w) for\n    b, w in itertools.product([2, 4, 16, 32, 128, 256], [2, 4, 8])], key=['N'])\n@triton.jit\ndef triton_sum_kernel_2D_result_dim_1(input_ptr, output_ptr, M:\n    'tl.constexpr', N: 'tl.constexpr', K: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\n    block_start_n = 0\n    block_start_k = pid_k * BLOCK_SIZE_K\n    offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\n    mask_n = offsets_n < N\n    mask_k = offsets_k < K\n    idxs_base = offsets_n[:, None] * K + offsets_k\n    idxs = idxs_base + pid_m * N * K\n    mask = mask_n[:, None] & mask_k\n    input = tl.load(input_ptr + idxs, mask=mask, other=0)\n    output = tl.sum(input, axis=0)\n    output_offsets = pid_m * K + offsets_k\n    tl.store(output_ptr + output_offsets, output, mask=mask_k)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "6a2ae29f-2f91-47ce-8a19-21edf7decf4e"
  },
  {
    "input": "@triton.jit\ndef third_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST000 = 2.64575131106459\n    CONST002 = 5.1234753829798\n    CONST004 = 6.48074069840786\n    CONST005 = 10.2469507659596\n    CONST006 = -2.09165006633519\n    CONST007 = -1\n    CONST008 = -6.27495019900557\n    CONST009 = -3.96862696659689\n    CONST010 = -1.62018517460197\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR25 = z * z * z\n    VAR26 = z * z\n    Y00 = CONST006 * VAR07 - CONST008 * VAR26 * x\n    Y01 = CONST005 * x * y * z\n    Y02 = CONST010 * VAR07 + x * (CONST004 * VAR17 + CONST010 * VAR26)\n    Y03 = CONST000 * VAR16 + CONST009 * VAR08 * y + CONST009 * VAR26 * y\n    Y04 = CONST010 * VAR25 + z * (CONST004 * VAR17 + CONST010 * VAR08)\n    Y05 = CONST002 * y * (CONST007 * VAR08 + VAR26)\n    Y06 = -CONST006 * VAR25 + CONST008 * VAR08 * z\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "255b1f7d-802f-4f21-9212-4d068bb54500"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_intra_K(v, g, o, A, T: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BV:\n    'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_t, i_i = i_c // NC, i_c % NC\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n    if i_t * BT + i_i * BC > T:\n        return\n    p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT + \n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + min(i_t * BT +\n        i_i * BC, T) * V + o_v, BV), BV)\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n    b_o = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t * BT +\n            i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_gv = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t *\n            BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = b_v * tl.exp(b_gn[None, :] - b_gv)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_o += tl.dot(b_A, b_vg)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_o *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_v = tl.max_contiguous(tl.multiple_of(v + i_bg * T * V + (i_t * BT +\n            i_i * BC + j) * V + o_v, BV), BV)\n        p_gv = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t *\n            BT + i_i * BC + j) * V + o_v, BV), BV)\n        b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n        b_v = tl.load(p_v, mask=m_v, other=0)\n        b_gv = tl.load(p_gv, mask=m_v, other=0)\n        b_vg = b_v[None, :] * tl.exp(b_g - b_gv[None, :])\n        b_o += tl.where(o_i[:, None] >= j, b_A[:, None] * b_vg, 0.0)\n    p_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT + \n        i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "0e322028-940f-4a64-a30b-377e52a40504"
  },
  {
    "input": "@triton.autotune(configs=get_cuda_autotune_config(), key=['N_ITER', 'M',\n    'N', 'K'])\n@triton.jit\ndef fused_reconstruct_kernel(sign_ptr, u_ptr, vt_ptr, output_ptr, N_ITER, M,\n    N, K, stride_sign_iter, stride_sign_m, stride_sign_n, stride_u_iter,\n    stride_u_m, stride_u_k, stride_vt_iter, stride_vt_k, stride_vt_n,\n    stride_output_m, stride_output_n, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr', num_warps: 'tl.constexpr', num_stages:\n    'tl.constexpr'):\n    \"\"\"Kernel for computing (sign * u @ vt).sum(dim=0).\n    sign: (N_ITER, M, N)\n    u: (N_ITER, M, K)\n    vt: (N_ITER, K, N)\n    output: (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offsets_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offsets_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offsets_k = tl.arange(0, BLOCK_SIZE_K)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for n_iter in range(N_ITER):\n        sign_ptrs = sign_ptr + (n_iter * stride_sign_iter + offsets_m[:,\n            None] * stride_sign_m + offsets_n[None, :] * stride_sign_n)\n        u_ptrs = u_ptr + (n_iter * stride_u_iter + offsets_m[:, None] *\n            stride_u_m)\n        vt_ptrs = vt_ptr + (n_iter * stride_vt_iter + offsets_n[None, :] *\n            stride_vt_n)\n        iter_acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            u_block_ptrs = u_ptrs + offsets_k[None, :] * stride_u_k\n            vt_block_ptrs = vt_ptrs + offsets_k[:, None] * stride_vt_k\n            u = tl.load(u_block_ptrs, mask=offsets_k[None, :] < K - k *\n                BLOCK_SIZE_K, other=0.0)\n            vt = tl.load(vt_block_ptrs, mask=offsets_k[:, None] < K - k *\n                BLOCK_SIZE_K, other=0.0)\n            iter_acc += tl.dot(u, vt, out_dtype=tl.float32)\n        sign = tl.load(sign_ptrs, mask=(offsets_m[:, None] < M) & (\n            offsets_n[None, :] < N), other=0.0)\n        acc += sign * iter_acc\n    output = acc\n    offsets_output_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_output_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    output_ptrs = output_ptr + stride_output_m * offsets_output_m[:, None\n        ] + stride_output_n * offsets_output_n[None, :]\n    output_mask = (offsets_output_m[:, None] < M) & (offsets_output_n[None,\n        :] < N)\n    tl.store(output_ptrs, output, mask=output_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "866416b8-75c9-47a8-b7e5-ced0dd5c9a37"
  },
  {
    "input": "@triton.autotune(configs=get_cuda_autotune_config(), key=['N_ITER', 'M',\n    'N', 'K'])\n@triton.jit\ndef fused_unpack_and_reconstruct_kernel(packed_sign_ptr, u_ptr, vt_ptr,\n    output_ptr, N_ITER, M, N, K, n_sign_elements, stride_u_iter, stride_u_m,\n    stride_u_k, stride_vt_iter, stride_vt_k, stride_vt_n, stride_output_m,\n    stride_output_n, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr', GROUP_SIZE_M:\n    'tl.constexpr', num_warps: 'tl.constexpr', num_stages: 'tl.constexpr'):\n    \"\"\"Kernel for computing (sign * u @ vt).sum(dim=0) with sign unpacking fused.\"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offsets_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offsets_k = tl.arange(0, BLOCK_SIZE_K)\n    base_m = offsets_m[:, None] * N\n    base_n = offsets_n[None, :]\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for n_iter in range(N_ITER):\n        element_indices = n_iter * M * N + base_m + base_n\n        element_indices = tl.reshape(element_indices, (BLOCK_SIZE_M *\n            BLOCK_SIZE_N,))\n        byte_indices = element_indices // 8\n        bit_indices = element_indices % 8\n        byte_ptrs = packed_sign_ptr + byte_indices\n        byte_mask = byte_indices < (n_sign_elements + 7) // 8\n        packed_bytes = tl.load(byte_ptrs, mask=byte_mask, other=0)\n        bits = packed_bytes >> 7 - bit_indices & 1\n        signs = bits * 2 - 1\n        signs = tl.reshape(signs, (BLOCK_SIZE_M, BLOCK_SIZE_N))\n        u_ptrs = u_ptr + (n_iter * stride_u_iter + offsets_m[:, None] *\n            stride_u_m)\n        vt_ptrs = vt_ptr + (n_iter * stride_vt_iter + offsets_n[None, :] *\n            stride_vt_n)\n        iter_acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n            u_block_ptrs = u_ptrs + offsets_k[None, :] * stride_u_k\n            vt_block_ptrs = vt_ptrs + offsets_k[:, None] * stride_vt_k\n            u = tl.load(u_block_ptrs, mask=offsets_k[None, :] < K - k *\n                BLOCK_SIZE_K, other=0.0)\n            vt = tl.load(vt_block_ptrs, mask=offsets_k[:, None] < K - k *\n                BLOCK_SIZE_K, other=0.0)\n            iter_acc += tl.dot(u, vt, out_dtype=tl.float32)\n        acc += signs * iter_acc\n    output = acc\n    offsets_output_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offsets_output_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    output_ptrs = output_ptr + stride_output_m * offsets_output_m[:, None\n        ] + stride_output_n * offsets_output_n[None, :]\n    output_mask = (offsets_output_m[:, None] < M) & (offsets_output_n[None,\n        :] < N)\n    tl.store(output_ptrs, output, mask=output_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "31f19480-5213-4f74-a1e4-c96f86ab8ce2"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd(Q, K, V, sm_scale, DO, DQ, DK, DV, M, D, stride_z, stride_h,\n    stride_tok, stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1:\n    'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr',\n    BLK_SLICE_FACTOR: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr'):\n    LN2: 'tl.constexpr' = 0.6931471824645996\n    bhid = tl.program_id(2)\n    off_chz = bhid * N_CTX\n    adj = stride_h * (bhid % H) + stride_z * (bhid // H)\n    pid = tl.program_id(0)\n    Q += adj\n    K += adj\n    V += adj\n    DO += adj\n    DQ += adj\n    DK += adj\n    DV += adj\n    M += off_chz\n    D += off_chz\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    start_n = pid * BLOCK_N1\n    start_m = start_n\n    MASK_BLOCK_M1: 'tl.constexpr' = BLOCK_M1 // BLK_SLICE_FACTOR\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    dv = tl.zeros([BLOCK_N1, BLOCK_DMODEL], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N1, BLOCK_DMODEL], dtype=tl.float32)\n    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, MASK_BLOCK_M1, BLOCK_N1, BLOCK_DMODEL, start_n,\n        start_m, num_steps, MASK=True)\n    start_m += num_steps * MASK_BLOCK_M1\n    num_steps = (N_CTX - start_m) // BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, BLOCK_M1, BLOCK_N1, BLOCK_DMODEL, start_n,\n        start_m, num_steps, MASK=False)\n    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dv_ptrs, dv)\n    dk *= sm_scale\n    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dk_ptrs, dk)\n    start_m = pid * BLOCK_M2\n    end_n = start_m + BLOCK_M2\n    MASK_BLOCK_N2: 'tl.constexpr' = BLOCK_N2 // BLK_SLICE_FACTOR\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    dq = tl.zeros([BLOCK_M2, BLOCK_DMODEL], dtype=tl.float32)\n    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n        )\n    m = tl.load(M + offs_m)\n    m = m[:, None]\n    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, MASK_BLOCK_N2, BLOCK_DMODEL, start_m, end_n - num_steps *\n        MASK_BLOCK_N2, num_steps, MASK=True)\n    end_n -= num_steps * MASK_BLOCK_N2\n    num_steps = end_n // BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, BLOCK_N2, BLOCK_DMODEL, start_m, end_n - num_steps *\n        BLOCK_N2, num_steps, MASK=False)\n    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    dq *= LN2\n    tl.store(dq_ptrs, dq)\n",
    "category": "Attention Mechanisms",
    "subcategory": "multi-head attention",
    "uuid": "575cda3b-f0dc-4c26-bad5-0dbba52ecf18"
  },
  {
    "input": "@triton.jit\ndef _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    e = e.float()\n    se = 1.0 / (1.0 + torch.exp(-e))\n    f = (se * e).to(dtype)\n    h = f * g\n    df = DW * f\n    dg = DW * g\n    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    se_row = tl.sigmoid(e_row)\n    f_row = se_row * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row * se_row * (1.0 + e_row * (1.0 - se_row))\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b30dff14-b22c-41fb-96c2-9192446f7fc0"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _rms_norm_bwd_kernel_sm(X, stride_x, W, DY, stride_dy, DX, stride_dx,\n    Rstd, DW, eps, M, N, rows_per_program, block_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, block_N)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    dw = tl.zeros((block_N,), dtype=tl.float32)\n    row_end = min(row_start + rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0)\n        dy = tl.load(DY + row * stride_dy + cols, mask=mask, other=0.0)\n        rstd = tl.load(Rstd + row)\n        x_hat = x * rstd\n        wdy = w * dy\n        dw += dy * x_hat\n        c1 = tl.sum(x_hat * wdy, axis=0) / N\n        dx = (wdy - x_hat * c1) * rstd\n        tl.store(DX + row * stride_dx + cols, dx, mask=mask)\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "3f85638f-91c1-44ee-96b2-0c4202cca6d4"
  },
  {
    "input": "@triton.jit\ndef d_linear_relu(d_y, w, b, xwb, x):\n    d_y_relu = d_y * (xwb > 0.0)\n    return d_linear(d_y_relu, w, b, x)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a722d091-f362-46a9-8574-aa718a1a1edd"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S', 'BT'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_local_cumsum_vector_kernel(s, o, offsets, T: 'tl.constexpr', H:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr', USE_OFFSETS: 'tl.constexpr'):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b), tl.load(offsets + i_b + 1)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t * BT,\n            i_s * BS), (BT, BS), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * T * S, (T, S), (S, 1), (i_t * BT,\n            i_s * BS), (BT, BS), (1, 0))\n    else:\n        p_s = tl.make_block_ptr(s + start * H * S + i_h * S, (T, S), (H * S,\n            1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_o = tl.make_block_ptr(o + start * H * S + i_h * S, (T, S), (H * S,\n            1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "f0323e4e-948f-4aeb-9795-3256c7377162"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 8, 'BLOCK_N': 128},\n    num_warps=2, pre_hook=init_to_zero('Y')), triton.Config({'BLOCK_M': 16,\n    'BLOCK_N': 256}, num_warps=4, pre_hook=init_to_zero('Y')), triton.\n    Config({'BLOCK_M': 16, 'BLOCK_N': 256}, num_warps=4, pre_hook=\n    init_to_zero('Y')), triton.Config({'BLOCK_M': 16, 'BLOCK_N': 512},\n    num_warps=4, pre_hook=init_to_zero('Y')), triton.Config({'BLOCK_M': 16,\n    'BLOCK_N': 1024}, num_warps=4, pre_hook=init_to_zero('Y')), triton.\n    Config({'BLOCK_M': 32, 'BLOCK_N': 256}, num_warps=4, pre_hook=\n    init_to_zero('Y')), triton.Config({'BLOCK_M': 32, 'BLOCK_N': 512},\n    num_warps=4, pre_hook=init_to_zero('Y')), triton.Config({'BLOCK_M': 32,\n    'BLOCK_N': 1024}, num_warps=4, pre_hook=init_to_zero('Y')), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256}, num_warps=4, pre_hook=\n    init_to_zero('Y')), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 512},\n    num_warps=4, pre_hook=init_to_zero('Y')), triton.Config({'BLOCK_M': 64,\n    'BLOCK_N': 1024}, num_warps=4, pre_hook=init_to_zero('Y')), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 16}, num_warps=4, pre_hook=\n    init_to_zero('Y')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32},\n    num_warps=4, pre_hook=init_to_zero('Y')), triton.Config({'BLOCK_M': 128,\n    'BLOCK_N': 64}, num_warps=4, pre_hook=init_to_zero('Y')), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 128}, num_warps=4, pre_hook=init_to_zero(\n    'Y')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256}, num_warps=4,\n    pre_hook=init_to_zero('Y')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    512}, num_warps=4, pre_hook=init_to_zero('Y')), triton.Config({\n    'BLOCK_M': 128, 'BLOCK_N': 1024}, num_warps=4, pre_hook=init_to_zero(\n    'Y'))], key=['CACHE_KEY_M', 'CACHE_KEY_N', 'BATCHSIZE', 'SPARSITY_BIN'])\n@triton.jit\ndef gather_transposed_gemv_flag_atomicadd_kernel(Y, A, X, IDX, M, N,\n    CACHE_KEY_M, CACHE_KEY_N, stride_am, BATCHSIZE: 'tl.constexpr',\n    SPARSITY_BIN: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Y = A[IDX, :]^T @ X + BIAS, where A is a dense matrix\n    with Z rows and N columns. We also batch across the batch dimension of the input X.\n    We will not check that the indices are valid, for performance reason.\n    - Input X has shape (BATCHSIZE, M)\n    - Weight has shape (Z, N)\n    - IDX has shape (M), where M is the number of non-zero rows in A\n    - Bias has shape (N)\n    - Output has shape (BATCHSIZE, N)\n    \"\"\"\n    start_m = tl.program_id(0)\n    start_n = tl.program_id(1)\n    rm = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    IDX = IDX + rm\n    idx = tl.load(IDX, mask=rm < M, other=0) > 0\n    A = A + (rm[:, None] * stride_am + rn[None, :])\n    X = X + rm\n    Y = Y + rn\n    if BATCHSIZE == 1:\n        a = tl.load(A, mask=idx[:, None], other=0.0)\n        x0 = tl.load(X)\n        acc0 = tl.sum(a * x0[:, None], 0)\n    rn = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    tl.atomic_add(Y, acc0, mask=rn < N)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "7d273783-b16f-4d24-89a2-4b2ce19fa1b2"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages\n    =3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.\n    Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'hdim',\n    'dstate'])\n@triton.jit\ndef _chunk_state_bwd_ddAcs_stable_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, hdim, dstate,\n    batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen,\n    stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen,\n    stride_b_head, stride_b_dstate, stride_dstates_batch,\n    stride_dstates_chunk, stride_states_head, stride_states_hdim,\n    stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + pid_h * stride_states_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    offs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else\n        BLOCK_SIZE_K)\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] *\n        stride_b_dstate)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + \n        offs_k[:, None] * stride_states_dstate)\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < dstate), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_k[None, :] < dstate - k), other=0.0)\n            dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate -\n                k) & (offs_n[None, :] < hdim), other=0.0)\n            dstates = dstates\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize)\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last -\n            dA_cs_m), 0.0)\n    acc *= scale[:, None]\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0)\n    ddt = tl.sum(acc * x, axis=1)\n    ddA_cs = ddt * dt_m\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=\n        offs_m < chunk_size - 1)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d85d05ca-fb03-41af-8892-9681922c25bb"
  },
  {
    "input": "@triton.jit\ndef layer_norm_kernel(x_ptr, weight_ptr, bias_ptr, y_ptr, N, eps:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    x_offset = x_ptr + row_idx * N + cols\n    x = tl.load(x_offset, mask=mask, other=0.0)\n    mean = tl.sum(x, axis=0) / N\n    x_centered = x - mean\n    var = tl.sum(x_centered * x_centered, axis=0) / N\n    rstd = 1.0 / tl.sqrt(var + eps)\n    w = tl.load(weight_ptr + cols, mask=mask, other=1.0)\n    b = tl.load(bias_ptr + cols, mask=mask, other=0.0)\n    y = x_centered * rstd * w + b\n    tl.store(y_ptr + row_idx * N + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "e19bf2bf-d124-4e95-92d3-b73e474caf09"
  },
  {
    "input": "@triton.jit\ndef softmax_grad_kernel(d_output_ptr, output_ptr, d_input_ptr,\n    d_output_row_stride, output_row_stride, d_input_row_stride, n_cols,\n    BLOCK_SIZE: 'tl.constexpr', is_bf16: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    output_row_ptr = output_ptr + row_idx * output_row_stride\n    d_output_row_ptr = d_output_ptr + row_idx * d_output_row_stride\n    d_input_row_ptr = d_input_ptr + row_idx * d_input_row_stride\n    output_ptrs = output_row_ptr + col_offsets\n    d_output_ptrs = d_output_row_ptr + col_offsets\n    d_input_ptrs = d_input_row_ptr + col_offsets\n    _softmax_grad_core(output_ptrs, d_output_ptrs, d_input_ptrs,\n        col_offsets, n_cols, is_bf16)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "bcb38b06-6eaa-4a37-9692-ce297b38a163"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0,\n    s_k_h, s_v_h, scale, B, H, T, K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_h += b_k[:, None] * b_v[None, :]\n        _d_q = b_h * b_do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += K\n        p_do += V\n        p_v += V\n        p_dq += K\n    tl.debug_barrier()\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + (T - 1) * K\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + (T - 1) * V\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (T\n         - 1) * K\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (T\n         - 1) * V\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += b_q[:, None] * b_do[None, :]\n        d_k = tl.sum(d_h * b_v[None, :], axis=1)\n        d_v = tl.sum(d_h * b_k[:, None], axis=0)\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do -= V\n        p_q -= K\n        p_k -= K\n        p_v -= V\n        p_dk -= K\n        p_dv -= V\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "92445d16-fc62-4cf0-9350-35b3becb7de3"
  },
  {
    "input": "@triton.jit\ndef layer_norm_xformers(output_ptr, a_ptr, weight_ptr, bias_ptr, mean_ptr,\n    rstd_ptr, output_row_stride, output_col_stride, a_row_stride,\n    a_col_stride, N_SIZE, eps, HAS_BIAS: 'tl.constexpr', IS_RMSNORM:\n    'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr'):\n    \"\"\"\n    LayerNorm forward pass for a single feature.\n    Requires that a whole row of X is loaded into shared memory -> won't work for large tensors.\n    based on:\n    https://github.com/facebookresearch/xformers/blob/main/xformers/triton/k_layer_norm.py\n    (arg names modified to match other implementation)\n    -> only used in benchmarks\n    \"\"\"\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_N_SIZE)\n    mask = cols < N_SIZE\n    x_ptrs = a_ptr + row * a_row_stride + cols * a_col_stride\n    x = tl.load(x_ptrs, mask=mask, other=0.0, eviction_policy='evict_first')\n    w = tl.load(weight_ptr + cols, mask=mask, other=1.0)\n    b = tl.load(bias_ptr + cols, mask=mask, other=0.0)\n    mean = tl.sum(x, axis=0) / N_SIZE\n    x_zm = tl.where(mask, x - mean, 0.0)\n    tl.store(mean_ptr + row, mean)\n    x_var = tl.sum(x_zm * x_zm, axis=0) / N_SIZE\n    rstd = 1.0 / tl.sqrt(x_var + eps)\n    y = x_zm * rstd\n    tl.store(rstd_ptr + row, rstd)\n    y = y * w + b\n    y_ptrs = output_ptr + row * output_row_stride + cols * output_col_stride\n    tl.store(y_ptrs, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "237b8d69-09d9-4062-aa0a-be6e72343c71"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, DO, Delta, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_hz = tl.program_id(1)\n    off_n = tl.arange(0, HEAD_DIM)\n    o = tl.load(O + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    do = tl.load(DO + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "category": "Attention Mechanisms",
    "subcategory": "multi-head attention",
    "uuid": "b24e1d3b-4356-421f-a9a1-7cf6a575f591"
  },
  {
    "input": "@triton.jit\ndef seventh_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST002 = 3.87298334620742\n    CONST008 = 11.7655316231354\n    CONST010 = 16.5555704843566\n    CONST012 = 20.4939015319192\n    CONST013 = 20.4939015319192\n    CONST014 = 22.0740939791422\n    CONST015 = 23.5310632462709\n    CONST017 = 36.7901566319036\n    CONST019 = 38.4260653723485\n    CONST020 = 38.4260653723485\n    CONST021 = 38.4260653723485\n    CONST023 = -4.9916923169903\n    CONST025 = 47.0621264925418\n    CONST026 = 50.8329064189723\n    CONST028 = 55.1852349478554\n    CONST029 = 56.2781179722634\n    CONST030 = 56.2781179722634\n    CONST032 = 66.5558975598707\n    CONST033 = 75.2994023880668\n    CONST037 = 101.665812837945\n    CONST038 = 110.370469895711\n    CONST041 = 147.160626527614\n    CONST042 = -1.66389743899677\n    CONST043 = -9.37968632871057\n    CONST044 = -1.66389743899677\n    CONST045 = -220.740939791422\n    CONST046 = -220.740939791422\n    CONST047 = -1.60108605718119\n    CONST048 = -187.593726574211\n    CONST049 = -9.1975391579759\n    CONST050 = -1.83950783159518\n    CONST051 = -1.83950783159518\n    CONST052 = -4.80325817154356\n    CONST053 = -147.160626527614\n    CONST054 = -140.695294930659\n    CONST055 = -133.111795119741\n    CONST056 = -125.499003980111\n    CONST057 = -125.499003980111\n    CONST058 = -99.833846339806\n    CONST059 = -87.7389315936062\n    CONST060 = -76.852130744697\n    CONST061 = -66.5558975598707\n    CONST062 = -62.7495019900557\n    CONST063 = -52.6433589561637\n    CONST064 = -44.1481879582843\n    CONST065 = -44.3705983732471\n    CONST066 = -40.6663251351779\n    CONST067 = -40.6663251351779\n    CONST068 = -8.31948719498384\n    CONST069 = -37.6497011940334\n    CONST070 = -33.2779487799353\n    CONST071 = -25.4164532094862\n    CONST072 = -25.4164532094862\n    CONST073 = -17.5477863187212\n    CONST074 = -11.7655316231354\n    CONST075 = -11.0370469895711\n    CONST076 = -9.1975391579759\n    CONST077 = -8.47215106982872\n    CONST078 = -4.80325817154356\n    CONST079 = -2.50682661696018\n    CONST080 = -1.60108605718119\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR03 = VAR06 * VAR07\n    VAR04 = VAR07 * VAR07\n    VAR05 = VAR07 * VAR08\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR12 = VAR15 * VAR16\n    VAR13 = VAR16 * VAR16\n    VAR14 = VAR16 * VAR17\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR21 = VAR24 * VAR25\n    VAR22 = VAR25 * VAR25\n    VAR23 = VAR25 * VAR26\n    Y00 = (CONST059 * VAR07 * VAR24 - CONST063 * VAR05 * VAR26 - CONST073 *\n        VAR22 * x + CONST079 * VAR03)\n    Y01 = y * (CONST029 * VAR23 * x + CONST030 * VAR05 * z + CONST048 *\n        VAR07 * VAR25)\n    Y02 = CONST050 * VAR03 + VAR05 * (CONST010 * VAR26 + CONST014 * VAR17\n        ) + VAR07 * (CONST045 * VAR17 * VAR26 - CONST076 * VAR24) + x * (\n        CONST038 * VAR17 * VAR24 + CONST076 * VAR22)\n    Y03 = VAR16 * (CONST041 * VAR25 * x + CONST053 * VAR07 * z) + y * (-\n        CONST064 * VAR05 * z + CONST064 * VAR23 * x)\n    Y04 = CONST042 * VAR03 + VAR05 * (-CONST042 * VAR26 - CONST070 * VAR17\n        ) + VAR07 * (CONST061 * VAR17 * VAR26 + CONST065 * VAR15 - CONST068 *\n        VAR24) + x * (-CONST023 * VAR22 - CONST055 * VAR15 * VAR26 + \n        CONST058 * VAR17 * VAR24)\n    Y05 = CONST015 * VAR05 * y * z + VAR07 * (CONST025 * VAR25 * y + \n        CONST057 * VAR16 * z) + x * (CONST015 * VAR23 * y + CONST033 *\n        VAR14 * z + CONST056 * VAR16 * VAR25)\n    Y06 = CONST047 * VAR03 + VAR05 * (CONST020 * VAR17 + CONST078 * VAR26\n        ) + VAR07 * (CONST052 * VAR24 + CONST060 * VAR15 - CONST060 * VAR17 *\n        VAR26) + x * (CONST012 * VAR13 + CONST019 * VAR17 * VAR24 + \n        CONST060 * VAR15 * VAR26 + CONST080 * VAR22)\n    Y07 = CONST002 * VAR12 + VAR14 * (CONST066 * VAR08 + CONST067 * VAR26\n        ) + VAR16 * (CONST026 * VAR06 + CONST026 * VAR24 + CONST037 * VAR08 *\n        VAR26) + y * (CONST071 * VAR06 * VAR26 + CONST072 * VAR08 * VAR24 +\n        CONST077 * VAR04 + CONST077 * VAR22)\n    Y08 = CONST047 * VAR21 + VAR23 * (CONST020 * VAR17 + CONST052 * VAR08\n        ) + VAR25 * (CONST052 * VAR06 - CONST060 * VAR08 * VAR17 + CONST060 *\n        VAR15) + z * (CONST013 * VAR13 + CONST021 * VAR06 * VAR17 + \n        CONST047 * VAR04 + CONST060 * VAR08 * VAR15)\n    Y09 = VAR14 * (CONST069 * VAR08 - CONST069 * VAR26) + VAR16 * (-\n        CONST062 * VAR06 + CONST062 * VAR24) + y * (CONST008 * VAR08 *\n        VAR24 + CONST074 * VAR04 + CONST074 * VAR06 * VAR26 - CONST074 * VAR22)\n    Y10 = -CONST042 * VAR21 + VAR23 * (CONST044 * VAR08 + CONST070 * VAR17\n        ) + VAR25 * (CONST032 * VAR08 * VAR17 - CONST065 * VAR15 + CONST068 *\n        VAR06) + z * (CONST023 * VAR04 + CONST055 * VAR08 * VAR15 - \n        CONST058 * VAR06 * VAR17)\n    Y11 = VAR16 * (CONST017 * VAR06 + CONST017 * VAR24 + CONST046 * VAR08 *\n        VAR26) + y * (CONST028 * VAR06 * VAR26 + CONST028 * VAR08 * VAR24 +\n        CONST075 * VAR04 + CONST075 * VAR22)\n    Y12 = CONST051 * VAR21 + VAR23 * (CONST010 * VAR08 + CONST014 * VAR17\n        ) + VAR25 * (CONST045 * VAR08 * VAR17 - CONST049 * VAR06) + z * (\n        CONST038 * VAR06 * VAR17 + CONST049 * VAR04)\n    Y13 = y * (CONST043 * VAR04 - CONST043 * VAR22 - CONST054 * VAR06 *\n        VAR26 + CONST054 * VAR08 * VAR24)\n    Y14 = (-CONST059 * VAR06 * VAR25 + CONST063 * VAR08 * VAR23 + CONST073 *\n        VAR04 * z - CONST079 * VAR21)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y07, mask=\n        output_row_offset + 7 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y08, mask=\n        output_row_offset + 8 < output_numel)\n    tl.store(output_ptr + output_row_offset + 9, Y09, mask=\n        output_row_offset + 9 < output_numel)\n    tl.store(output_ptr + output_row_offset + 10, Y10, mask=\n        output_row_offset + 10 < output_numel)\n    tl.store(output_ptr + output_row_offset + 11, Y11, mask=\n        output_row_offset + 11 < output_numel)\n    tl.store(output_ptr + output_row_offset + 12, Y12, mask=\n        output_row_offset + 12 < output_numel)\n    tl.store(output_ptr + output_row_offset + 13, Y13, mask=\n        output_row_offset + 13 < output_numel)\n    tl.store(output_ptr + output_row_offset + 14, Y14, mask=\n        output_row_offset + 14 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "586c4142-cd0f-4fda-961b-8a71681c9244"
  },
  {
    "input": "@triton.jit\ndef nll_loss(input, size, reduction: 'tl.constexpr'):\n    \"\"\"\n    Measures the negative log likelihood loss given log-probabilities of target class.\n\n    Args:\n        input: Input containing predicted log-probabilities corresponding to target class.\n            The input can have arbitrary shape.\n        size: Number of elements in the input.\n            This value is used only if reduction is 'mean'.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the loss\n            across all entries, and 'sum' for summing the loss across all entries.\n\n    Returns:\n        Loss.\n    \"\"\"\n    input = input\n    if reduction == 'none':\n        output = -input\n    elif reduction == 'mean':\n        output = -tl.sum(input) / size\n    elif reduction == 'sum':\n        output = -tl.sum(input)\n    return output\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "db85264a-bd72-4248-8343-e2efc41fcc61"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "c5e10010-dcfc-4569-ab37-b3b2a47513f1"
  },
  {
    "input": "@triton.jit\ndef _inner_paged_attn_unroll_8_kernel(q, k_cache, v_cache, stride_km,\n    block_base_ptrs, base_offs_kv, alibi_slope, block_offs, seq_len, qkv,\n    qk_max, exp_sum, BLOCK_SIZE: 'tl.constexpr', LO: 'tl.constexpr', HI:\n    'tl.constexpr'):\n    for block_idx in range(LO, HI, 8):\n        offs_kv_0 = tl.load(block_base_ptrs + block_idx + 0\n            ) * stride_km + base_offs_kv\n        offs_kv_1 = tl.load(block_base_ptrs + block_idx + 1\n            ) * stride_km + base_offs_kv\n        offs_kv_2 = tl.load(block_base_ptrs + block_idx + 2\n            ) * stride_km + base_offs_kv\n        offs_kv_3 = tl.load(block_base_ptrs + block_idx + 3\n            ) * stride_km + base_offs_kv\n        offs_kv_4 = tl.load(block_base_ptrs + block_idx + 4\n            ) * stride_km + base_offs_kv\n        offs_kv_5 = tl.load(block_base_ptrs + block_idx + 5\n            ) * stride_km + base_offs_kv\n        offs_kv_6 = tl.load(block_base_ptrs + block_idx + 6\n            ) * stride_km + base_offs_kv\n        offs_kv_7 = tl.load(block_base_ptrs + block_idx + 7\n            ) * stride_km + base_offs_kv\n        k_0 = tl.load(k_cache + offs_kv_0)\n        k_1 = tl.load(k_cache + offs_kv_1)\n        k_2 = tl.load(k_cache + offs_kv_2)\n        k_3 = tl.load(k_cache + offs_kv_3)\n        k_4 = tl.load(k_cache + offs_kv_4)\n        k_5 = tl.load(k_cache + offs_kv_5)\n        k_6 = tl.load(k_cache + offs_kv_6)\n        k_7 = tl.load(k_cache + offs_kv_7)\n        v_0 = tl.load(v_cache + offs_kv_0)\n        v_1 = tl.load(v_cache + offs_kv_1)\n        v_2 = tl.load(v_cache + offs_kv_2)\n        v_3 = tl.load(v_cache + offs_kv_3)\n        v_4 = tl.load(v_cache + offs_kv_4)\n        v_5 = tl.load(v_cache + offs_kv_5)\n        v_6 = tl.load(v_cache + offs_kv_6)\n        v_7 = tl.load(v_cache + offs_kv_7)\n        _qk_0 = tl.sum(q[None, :] * k_0, axis=1)\n        _qk_1 = tl.sum(q[None, :] * k_1, axis=1)\n        _qk_2 = tl.sum(q[None, :] * k_2, axis=1)\n        _qk_3 = tl.sum(q[None, :] * k_3, axis=1)\n        _qk_4 = tl.sum(q[None, :] * k_4, axis=1)\n        _qk_5 = tl.sum(q[None, :] * k_5, axis=1)\n        _qk_6 = tl.sum(q[None, :] * k_6, axis=1)\n        _qk_7 = tl.sum(q[None, :] * k_7, axis=1)\n        if alibi_slope is not None:\n            _qk_0 += alibi_slope * ((block_idx + 0) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_1 += alibi_slope * ((block_idx + 1) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_2 += alibi_slope * ((block_idx + 2) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_3 += alibi_slope * ((block_idx + 3) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_4 += alibi_slope * ((block_idx + 4) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_5 += alibi_slope * ((block_idx + 5) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_6 += alibi_slope * ((block_idx + 6) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_7 += alibi_slope * ((block_idx + 7) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n        _qk_max = tl.maximum(tl.max(_qk_0, axis=0), qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_1, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_2, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_3, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_4, axis=0), qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_5, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_6, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_7, axis=0), _qk_max)\n        exp_tmp = tl.exp(_qk_0 - _qk_max) + tl.exp(_qk_1 - _qk_max) + tl.exp(\n            _qk_2 - _qk_max) + tl.exp(_qk_3 - _qk_max) + tl.exp(_qk_4 - _qk_max\n            ) + tl.exp(_qk_5 - _qk_max) + tl.exp(_qk_6 - _qk_max) + tl.exp(\n            _qk_7 - _qk_max)\n        _exp_sum = exp_sum * tl.exp(qk_max - _qk_max) + tl.sum(exp_tmp, axis=0)\n        qkv_sum_tmp = tl.exp(_qk_0[:, None] - _qk_max) * v_0 + tl.exp(_qk_1\n            [:, None] - _qk_max) * v_1 + tl.exp(_qk_2[:, None] - _qk_max\n            ) * v_2 + tl.exp(_qk_3[:, None] - _qk_max) * v_3 + tl.exp(_qk_4\n            [:, None] - _qk_max) * v_4 + tl.exp(_qk_5[:, None] - _qk_max\n            ) * v_5 + tl.exp(_qk_6[:, None] - _qk_max) * v_6 + tl.exp(_qk_7\n            [:, None] - _qk_max) * v_7\n        qkv = (qkv * (exp_sum * tl.exp(qk_max - _qk_max)) + qkv_sum_tmp\n            ) / _exp_sum\n        qk_max = _qk_max\n        exp_sum = _exp_sum\n    return qkv, qk_max, exp_sum\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "21fc8989-ab2c-49df-a6de-974744ac3e69"
  },
  {
    "input": "@triton.jit\ndef dot(BLOCK_M: 'tl.constexpr', QDIM: 'tl.constexpr', KDIM: 'tl.constexpr',\n    q, k):\n    if BLOCK_M == 1:\n        return tl.sum(tl.view(q, [QDIM]) * tl.view(k, [KDIM]))\n    else:\n        return tl.dot(q, k)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d046974c-efd0-460c-87de-a80f76b2cfa9"
  },
  {
    "input": "@triton.jit\ndef _triton_first_order_fwd(x_ptr: 'tl.tensor', y_ptr: 'tl.tensor', z_ptr:\n    'tl.tensor', sph_1_0_ptr: 'tl.tensor', sph_1_1_ptr: 'tl.tensor',\n    sph_1_2_ptr: 'tl.tensor', BLOCK_SIZE: 'tl.constexpr', vector_length:\n    'tl.constexpr'):\n    \"\"\"\n    First order spherical harmonics in Triton.\n\n    Computationally not that intensive, as we're just applying\n    a sqrt 3 to the coordinates, but also good for validating\n    the kernel performs as intended.\n\n    Parameters\n    ----------\n    x_ptr, y_ptr, z_ptr : tl.tensor\n        Pointers to the coordinate tensors.\n    sph_1_0_ptr, sph_1_1_ptr, sph_1_2_ptr : tl.tensor\n        Points to tensors to write outputs to. Assumed to\n        be the same length as the input tensors.\n    block_size : tl.constexpr\n        Vector length of contiguous elements to load into memory\n        within a given block.\n    vector_length : tl.constexpr\n        The maximum/total length of the vectors, assumed to\n        be the same for every one. This is used to calculate\n        the mask to keep operations within bounds.\n    \"\"\"\n    sqrt_3 = 3 ** 0.5\n    block_id = tl.program_id(0)\n    offset = tl.arange(0, BLOCK_SIZE) + BLOCK_SIZE * block_id\n    x_row_start = x_ptr + offset\n    y_row_start = y_ptr + offset\n    z_row_start = z_ptr + offset\n    x = tl.load(x_row_start, mask=offset < vector_length)\n    y = tl.load(y_row_start, mask=offset < vector_length)\n    z = tl.load(z_row_start, mask=offset < vector_length)\n    sph_1_0 = sqrt_3 * x\n    sph_1_1 = sqrt_3 * y\n    sph_1_2 = sqrt_3 * z\n    sph_1_0_start = sph_1_0_ptr + offset\n    sph_1_1_start = sph_1_1_ptr + offset\n    sph_1_2_start = sph_1_2_ptr + offset\n    tl.store(sph_1_0_start, sph_1_0, mask=offset < vector_length)\n    tl.store(sph_1_1_start, sph_1_1, mask=offset < vector_length)\n    tl.store(sph_1_2_start, sph_1_2, mask=offset < vector_length)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "2736b77b-29a9-4e15-9e5b-3aea974cdd30"
  },
  {
    "input": "@triton.jit\ndef _triton_rope(q_ptr, q_row_stride, k_ptr, k_row_stride, cos,\n    cos_row_stride, sin, sin_row_stride, sl, bs: 'tl.constexpr', n_qh:\n    'tl.constexpr', n_kh: 'tl.constexpr', hd: 'tl.constexpr', pad_n_qh:\n    'tl.constexpr', pad_n_kh: 'tl.constexpr', pad_hd: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', BACKWARD_PASS: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n    cos_row_idx = pid % sl\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        )\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        )\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "6a9b9aad-4c57-4f4a-8720-122a4ba2414e"
  },
  {
    "input": "@triton.jit\ndef _bw_kernel(xy, yz, zx, xy_color, yz_color, zx_color, rays, centers,\n    weights, biases, weight_opacity, bias_opacity, weight_color, bias_color,\n    rays_encoding, negative_log_transmittance, expected_depth,\n    expected_features, near, far, effective_num_samples, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', gain: 'tl.constexpr',\n    batch_size: 'tl.constexpr', num_rays_per_batch: 'tl.constexpr', C:\n    'tl.constexpr', OUT_C: 'tl.constexpr', H: 'tl.constexpr', W:\n    'tl.constexpr', D: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    transmittance_thr: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr', inject_noise: 'tl.constexpr', inject_noise_sigma:\n    'tl.constexpr', inject_noise_seed, contract_coords: 'tl.constexpr',\n    contract_perc_foreground: 'tl.constexpr', disparity_at_inf:\n    'tl.constexpr', shape_representation: 'tl.constexpr', activation_fun:\n    'tl.constexpr', use_separate_color_rep: 'tl.constexpr',\n    grad_negative_log_transmittance, grad_expected_depth,\n    grad_expected_features, grad_xy, grad_yz, grad_zx, grad_xy_color,\n    grad_yz_color, grad_zx_color, grad_weights, grad_biases,\n    grad_weight_opacity, grad_bias_opacity, grad_weight_color,\n    grad_bias_color, grad_rays_enc):\n    tot_num_samples = num_samples + num_samples_inf\n    num_rays = num_rays_per_batch * batch_size\n    pid = tl.program_id(axis=0)\n    offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    offs_mask = offs < num_rays\n    offs_x = pid * BLOCK_SIZE * 3 + tl.arange(0, BLOCK_SIZE) * 3\n    offs_y = offs_x + 1\n    offs_z = offs_y + 1\n    offs_features = pid * BLOCK_SIZE * OUT_C + OUT_C * tl.arange(0, BLOCK_SIZE\n        )[:, None] + tl.arange(0, OUT_C)[None, :]\n    offs_features_mask = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)[:, None\n        ] < num_rays\n    offs_CC = tl.arange(0, C)[:, None] * C + tl.arange(0, C)[None, :]\n    center_x = tl.load(centers + offs_x, mask=offs_x < num_rays * 3)\n    center_y = tl.load(centers + offs_y, mask=offs_y < num_rays * 3)\n    center_z = tl.load(centers + offs_z, mask=offs_z < num_rays * 3)\n    ray_x = tl.load(rays + offs_x, mask=offs_x < num_rays * 3)\n    ray_y = tl.load(rays + offs_y, mask=offs_y < num_rays * 3)\n    ray_z = tl.load(rays + offs_z, mask=offs_z < num_rays * 3)\n    rays_enc_offs = pid * BLOCK_SIZE * C + C * tl.arange(0, BLOCK_SIZE)[:, None\n        ] + tl.arange(0, C)[None, :]\n    rays_enc_mask = rays_enc_offs < num_rays * C\n    rays_encoding_buffer = tl.load(rays_encoding + rays_enc_offs, mask=\n        rays_enc_mask)\n    batch_index = (pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n        ) // num_rays_per_batch\n    near_buffer = tl.load(near + offs, mask=offs < num_rays)\n    far_buffer = tl.load(far + offs, mask=offs < num_rays)\n    seed_buffer = tl.load(inject_noise_seed + offs, mask=offs < num_rays)\n    sample_index_buffer = (tl.arange(0, BLOCK_SIZE) * tot_num_samples + pid *\n        BLOCK_SIZE * tot_num_samples + 1 + tot_num_samples - 1)\n    depth = far_buffer\n    grad_negative_log_transmittance_buffer = tl.load(\n        grad_negative_log_transmittance + offs, mask=offs_mask, other=0.0)\n    grad_expected_features_buffer = tl.load(grad_expected_features +\n        offs_features, mask=offs_features_mask, other=0.0)\n    grad_expected_depth_buffer = tl.load(grad_expected_depth + offs, mask=\n        offs_mask, other=0.0)\n    prev_proj_depth = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    prev_proj_features = tl.zeros((BLOCK_SIZE, OUT_C), dtype=tl.float32)\n    negative_log_transmittance_buffer = tl.load(negative_log_transmittance +\n        offs, mask=offs_mask, other=0.0)\n    transmittance = tl.exp(-negative_log_transmittance_buffer)\n    prev_grad_opacity = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    d_w2 = tl.zeros((C, C), dtype=tl.float32)\n    d_w1 = tl.zeros((C, C), dtype=tl.float32)\n    d_b1 = tl.zeros((C,), dtype=tl.float32)\n    d_b2 = tl.zeros((C,), dtype=tl.float32)\n    d_w2c = tl.zeros((C, C), dtype=tl.float32)\n    d_b2c = tl.zeros((C,), dtype=tl.float32)\n    d_wr = tl.zeros((C, C), dtype=tl.float32)\n    d_br = tl.zeros((C,), dtype=tl.float32)\n    d_wo = tl.zeros((C,), dtype=tl.float32)\n    d_bo = tl.zeros((1,), dtype=tl.float32)\n    d_wc = tl.zeros((C, OUT_C), dtype=tl.float32)\n    d_wc = tl.zeros((OUT_C, C), dtype=tl.float32)\n    d_bc = tl.zeros((OUT_C,), dtype=tl.float32)\n    d_rays_enc = tl.zeros((BLOCK_SIZE, C), dtype=tl.float32)\n    zero_w = tl.zeros((C, C), dtype=tl.float32)\n    zero_b = tl.zeros((C,), dtype=tl.float32)\n    zero_vec = tl.zeros((BLOCK_SIZE, C), dtype=tl.float32)\n    w1, w2, wr, wo, wc, b1, b2, br, bo, bc, w2c, b2c = _load_mlp_weights(\n        weights, biases, weight_opacity, bias_opacity, weight_color,\n        bias_color, C, OUT_C, BLOCK_SIZE)\n    prev_transmittance = transmittance\n    for step in range(tot_num_samples):\n        if step < num_samples_inf:\n            depth = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, num_samples_inf - step - 1)\n            depth_prev = _depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, num_samples_inf - step - 2)\n        else:\n            depth = _depth_lin(near_buffer, far_buffer, num_samples, \n                num_samples - (step - num_samples_inf) - 1)\n            depth_prev = _depth_lin(near_buffer, far_buffer, num_samples, \n                num_samples - (step - num_samples_inf) - 2)\n        delta = depth - depth_prev\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = _contract_pi(sample_x, sample_y,\n                sample_z, contract_perc_foreground)\n        vec = _sample_grid_rep(xy, yz, zx, batch_index, sample_x, sample_y,\n            sample_z, batch_size, C, D, H, W, BLOCK_SIZE, shape_representation)\n        if mask_out_of_bounds_samples:\n            in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z, W,\n                H, D, C, BLOCK_SIZE)\n            vec = vec * in_bounds_mask\n        vec1 = tl.maximum(tl.dot(vec, w1, allow_tf32=ALLOW_TF32) + b1, 0.0)\n        vec2 = tl.maximum(tl.dot(vec1, w2, allow_tf32=ALLOW_TF32) + b2, 0.0)\n        value = tl.sum(vec2 * wo, axis=1) + bo\n        if inject_noise:\n            r = _int_to_randn(sample_index_buffer, sample_index_buffer + \n                num_rays * tot_num_samples, seed_buffer)\n            value = value + r * inject_noise_sigma\n        if activation_fun == 0:\n            value_act = _softplus(value)\n        else:\n            value_act = tl.maximum(value, 0.0)\n        delta_value = gain * value_act * delta\n        if use_separate_color_rep:\n            vec_color = _sample_grid_rep(xy_color, yz_color, zx_color,\n                batch_index, sample_x, sample_y, sample_z, batch_size, C, D,\n                H, W, BLOCK_SIZE, shape_representation)\n            vec_color = vec_color + rays_encoding_buffer\n            if mask_out_of_bounds_samples:\n                in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z,\n                    W, H, D, C, BLOCK_SIZE)\n                vec_color = vec_color * in_bounds_mask\n            vec_color1 = tl.maximum(tl.dot(vec_color, wr, allow_tf32=\n                ALLOW_TF32) + br, 0.0)\n            vecr = tl.maximum(tl.dot(vec_color1, w2c, allow_tf32=ALLOW_TF32\n                ) + b2c, 0.0)\n        else:\n            vecr = tl.maximum(tl.dot(vec2, wr, allow_tf32=ALLOW_TF32) + br +\n                rays_encoding_buffer, 0.0)\n        log_color = tl.dot(vecr, wc, allow_tf32=ALLOW_TF32) + bc\n        color = _color_activation(log_color)\n        proj_features = color * grad_expected_features_buffer\n        proj_depth = depth * grad_expected_depth_buffer\n        prev_transmittance = transmittance\n        opacity_grad_now = prev_transmittance * (proj_depth -\n            prev_proj_depth + tl.sum(proj_features - prev_proj_features,\n            axis=1))\n        prev_grad_opacity = prev_grad_opacity + opacity_grad_now\n        grad_value_act = delta * (prev_grad_opacity +\n            grad_negative_log_transmittance_buffer)\n        if activation_fun == 0:\n            grad_value_act = _d_softplus(grad_value_act, value)\n        else:\n            grad_value_act = grad_value_act * (value > 0.0)\n        grad_value = gain * grad_value_act\n        grad_value = tl.expand_dims(grad_value, 1)\n        d_wo_ = tl.sum(vec2 * tl.broadcast_to(grad_value, (BLOCK_SIZE, C)),\n            axis=0)\n        d_bo_ = tl.sum(grad_value, axis=0)\n        d_vec2_1 = wo * grad_value\n        negative_log_transmittance_buffer = (\n            negative_log_transmittance_buffer - delta_value)\n        transmittance = tl.exp(-negative_log_transmittance_buffer)\n        \"\"\"\n        transmittance_diff = tl.broadcast_to(\n            tl.view(transmittance, (BLOCK_SIZE, 1)), (BLOCK_SIZE, OUT_C)\n        ) - tl.broadcast_to(\n            tl.view(prev_transmittance, (BLOCK_SIZE, 1)), (BLOCK_SIZE, OUT_C)\n        )  # = rendering weights for the given step\n        \"\"\"\n        transmittance_diff = tl.broadcast_to(tl.expand_dims(transmittance, \n            1), (BLOCK_SIZE, OUT_C)) - tl.broadcast_to(tl.expand_dims(\n            prev_transmittance, 1), (BLOCK_SIZE, OUT_C))\n        d_color = grad_expected_features_buffer * transmittance_diff\n        d_log_color = _d_color_activation(d_color, log_color)\n        d_vecr, d_wc_, d_bc_ = _d_linear(d_log_color, wc, bc, vecr)\n        if use_separate_color_rep:\n            d_vec2_12 = tl.view(d_vec2_1, (BLOCK_SIZE, C))\n        else:\n            d_vec2_2, d_wr_, d_br_ = _d_linear_relu(d_vecr, wr, br, vecr, vec2)\n            d_vec2_12 = tl.view(d_vec2_1, (BLOCK_SIZE, C)) + tl.view(d_vec2_2,\n                (BLOCK_SIZE, C))\n        d_vec1, d_w2_, d_b2_ = _d_linear_relu(d_vec2_12, w2, b2, vec2, vec1)\n        d_vec, d_w1_, d_b1_ = _d_linear_relu(d_vec1, w1, b1, vec1, vec)\n        if mask_out_of_bounds_samples:\n            in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z, W,\n                H, D, C, BLOCK_SIZE)\n            d_vec = d_vec * in_bounds_mask\n        _splat_grid_rep(d_vec, grad_xy, grad_yz, grad_zx, batch_index,\n            sample_x, sample_y, sample_z, batch_size, C, D, H, W,\n            BLOCK_SIZE, shape_representation)\n        if use_separate_color_rep:\n            d_vec_color1, d_w2c_, d_b2c_ = _d_linear_relu(d_vecr, w2c, b2c,\n                vecr, vec_color1)\n            d_vec_color, d_wr_, d_br_ = _d_linear_relu(d_vec_color1, wr, br,\n                vec_color1, vec_color)\n            if mask_out_of_bounds_samples:\n                in_bounds_mask = _is_in_bounds(sample_x, sample_y, sample_z,\n                    W, H, D, C, BLOCK_SIZE)\n                d_vec_color = d_vec_color * in_bounds_mask\n            d_rays_enc_ = tl.view(d_vec_color, (BLOCK_SIZE, C))\n            _splat_grid_rep(d_vec_color, grad_xy_color, grad_yz_color,\n                grad_zx_color, batch_index, sample_x, sample_y, sample_z,\n                batch_size, C, D, H, W, BLOCK_SIZE, shape_representation)\n        else:\n            d_vec_color = zero_vec\n            d_w2c_ = zero_w\n            d_b2c_ = zero_b\n            d_rays_enc_ = d_vecr * (vecr > 0.0)\n        d_wc += d_wc_\n        d_bc += d_bc_\n        d_wr += d_wr_\n        d_br += d_br_\n        d_w2 += d_w2_\n        d_w1 += d_w1_\n        d_b1 += d_b1_\n        d_b2 += d_b2_\n        d_wo += d_wo_\n        d_bo += d_bo_\n        d_w2c += d_w2c_\n        d_b2c += d_b2c_\n        d_rays_enc += d_rays_enc_\n        prev_proj_depth = proj_depth\n        prev_proj_features = proj_features\n        sample_index_buffer = sample_index_buffer - 1\n    tl.atomic_add(grad_weights + offs_CC, d_w1)\n    tl.atomic_add(grad_weights + C * C + offs_CC, d_w2)\n    tl.atomic_add(grad_weights + 2 * C * C + offs_CC, d_wr)\n    tl.atomic_add(grad_weights + 3 * C * C + offs_CC, d_w2c)\n    tl.atomic_add(grad_biases + tl.arange(0, C), d_b1)\n    tl.atomic_add(grad_biases + C + tl.arange(0, C), d_b2)\n    tl.atomic_add(grad_biases + 2 * C + tl.arange(0, C), d_br)\n    tl.atomic_add(grad_biases + 3 * C + tl.arange(0, C), d_b2c)\n    tl.atomic_add(grad_weight_opacity + tl.arange(0, C), d_wo)\n    tl.atomic_add(grad_bias_opacity + tl.arange(0, 1), d_bo)\n    tl.atomic_add(grad_weight_color + tl.arange(0, OUT_C)[:, None] * C + tl\n        .arange(0, C)[None, :], d_wc)\n    tl.atomic_add(grad_bias_color + tl.arange(0, OUT_C), d_bc)\n    tl.store(grad_rays_enc + rays_enc_offs, d_rays_enc, mask=rays_enc_mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c24a8e9e-53d0-4aa0-98ad-3cf4b991cac4"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_SUM': block_size_sum},\n    num_warps=num_warps) for block_size_sum, num_warps in itertools.product\n    ([512, 1024], [2, 4, 8, 16])], key=['clean_audio_max_len',\n    'noisy_audio_max_len'])\n@triton.jit\ndef sum_with_snr_kernel(clean_audio, clean_audio_real_lens,\n    clean_audio_max_len, desired_rms, noisy_audio_ptr,\n    noisy_audio_real_lens, noisy_audio_max_len, output_ptr, BLOCK_SIZE_SUM:\n    'tl.constexpr', BLOCK_SIZE_RMS: 'tl.constexpr'):\n    batch_idx = tl.program_id(0)\n    clean_audio_real_lens_val = tl.load(clean_audio_real_lens + batch_idx)\n    clean_audio_rms = rms_kernel(clean_audio, clean_audio_real_lens,\n        clean_audio_max_len, batch_idx, BLOCK_SIZE_RMS)\n    noisy_audio_real_lens_val = tl.load(noisy_audio_real_lens + batch_idx)\n    noisy_audio_rms = rms_kernel(noisy_audio_ptr, noisy_audio_real_lens,\n        noisy_audio_max_len, batch_idx, BLOCK_SIZE_RMS)\n    desired_rms_val = tl.load(desired_rms + batch_idx)\n    relative_rms = clean_audio_rms / tl.math.pow(10.0, desired_rms_val / 20.0)\n    for offset in range(0, clean_audio_max_len, BLOCK_SIZE_SUM):\n        clean_audio_block_ptr = offset + tl.arange(0, BLOCK_SIZE_SUM)\n        clean_audio_mask = clean_audio_block_ptr < clean_audio_real_lens_val\n        clean_audio_vals = tl.load(clean_audio + batch_idx *\n            clean_audio_max_len + clean_audio_block_ptr, mask=clean_audio_mask)\n        \"\"\"\n        Adjusts the block's start position if it extends beyond the noisy audio array, shifting it leftward as needed.\n        This adjustment keeps the data block within the noisy audio array limits, accounting for its circular nature\n\n        Scenario without adjustment:\n           noisy_audio_array: |----|----|----|----|----|----|----|----|\n           block:                      |~~~~~~~~~~~~~~~~|\n           (Block fits within the array, no adjustment needed)\n\n        Scenario with adjustment:\n           noisy_audio_array: |----|----|----|----|----|----|----|----|\n           block:                                           |~~~~~~~~~~~~~~~~|\n           (Block exceeds array bounds, needs to be shifted left)\n           noisy_audio_array: |----|----|----|----|----|----|----|----|\n           block:                                    |~~~~~~~~~~~~~~~~|  <--- Shifted left\n        \"\"\"\n        offset_over_max = offset % noisy_audio_real_lens_val\n        offset_adjusted = offset_over_max - tl.math.min(offset_over_max, tl\n            .math.max(0, offset_over_max + BLOCK_SIZE_SUM -\n            noisy_audio_real_lens_val))\n        noisy_audio_block_ptr = offset_adjusted + tl.arange(0, BLOCK_SIZE_SUM)\n        noisy_audio_val = tl.load(noisy_audio_ptr + batch_idx *\n            noisy_audio_max_len + noisy_audio_block_ptr, mask=\n            noisy_audio_block_ptr < noisy_audio_real_lens_val)\n        tl.store(output_ptr + batch_idx * clean_audio_max_len +\n            clean_audio_block_ptr, clean_audio_vals + noisy_audio_val * (\n            relative_rms / noisy_audio_rms), mask=clean_audio_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "55885a68-e1be-44e1-8f12-bdd9c7bf60cb"
  },
  {
    "input": "@triton.autotune(list(filter(keep, configsOrig)), key=['N_CTX'])\n@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', HEAD_DIM: 'tl.constexpr',\n    STAGE: 'tl.constexpr', ENABLE_TMA: 'tl.constexpr', LOOP_SCHEDULE:\n    'tl.constexpr', ENABLE_WS: 'tl.constexpr'):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    _attn_fwd_compute(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v,\n        desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n        stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk,\n        stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX,\n        BLOCK_M, BLOCK_N, HEAD_DIM, STAGE, ENABLE_TMA, LOOP_SCHEDULE)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "faebc373-143e-4a2f-bebf-91f3ead81a5b"
  },
  {
    "input": "@triton.jit\ndef _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    d_h = tl.math.exp2((BTL - tl.arange(0, BTL)) * b_b)\n    b_kd = b_k * d_h[:, None]\n    d_q = tl.math.exp2(tl.arange(0, BTS) * b_b)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * d_q[None, :]\n        b_dv *= d_b\n        b_s = tl.dot(b_kd, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        b_dk *= d_b\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((-o_k[:, None] + o_q[None, :]) *\n            b_b), 0) * scale\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f979fe67-8e8e-4e88-aa51-091c938fc494"
  },
  {
    "input": "@triton.jit\ndef add_v8_bf16(a_hi, a_lo, b_hi, b_lo):\n    return tl.inline_asm_elementwise(\n        \"\"\"\n        {\n            .reg .v4 .b32 %acc, %tmp;\n            mov.v4.b32  %acc, 0;\n            mov.b64     {%acc.x, %acc.y}, $2;\n            mov.b64     {%acc.z, %acc.w}, $3;\n            mov.b64     {%tmp.x, %tmp.y}, $4;\n            mov.b64     {%tmp.z, %tmp.w}, $5;\n            add.bf16x2  %acc.x, %acc.x, %tmp.x;\n            add.bf16x2  %acc.y, %acc.y, %tmp.y;\n            add.bf16x2  %acc.z, %acc.z, %tmp.z;\n            add.bf16x2  %acc.w, %acc.w, %tmp.w;\n            mov.b64     $0, {%acc.x, %acc.y};\n            mov.b64     $1, {%acc.z, %acc.w};\n        }\n        \"\"\"\n        , '=l,=l,l,l,l,l', args=[a_hi, a_lo, b_hi, b_lo], dtype=(tl.uint64,\n        tl.uint64), is_pure=True, pack=1)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "c6d1dd77-ac32-4274-bab7-06ce7dc284ea"
  },
  {
    "input": "@triton.jit\ndef forward(output_ptr: 'tl.tensor', input_ptr: 'tl.tensor', rstd_ptr:\n    'tl.tensor', mean_ptr: 'tl.tensor', group_size, y_size, x_size,\n    num_groups, weight_ptr: 'tl.tensor', bias_ptr: 'tl.tensor', eps, dtype:\n    'tl.constexpr', group_block_size: 'tl.constexpr', x_block_size:\n    'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    batch = pid // num_groups\n    group = pid % num_groups\n    num_elements = group_size * x_size\n    batch_offset = batch * num_groups * num_elements\n    group_offset = batch_offset + group * num_elements\n    output_block_ptr = tl.make_block_ptr(output_ptr + group_offset, shape=(\n        group_size, x_size), strides=(x_size, 1), offsets=(0, 0),\n        block_shape=(group_block_size, x_block_size), order=(1, 0))\n    input_block_ptr = tl.make_block_ptr(input_ptr + group_offset, shape=(\n        group_size, x_size), strides=(x_size, 1), offsets=(0, 0),\n        block_shape=(group_block_size, x_block_size), order=(1, 0))\n    rstd_block_ptr = tl.make_block_ptr(rstd_ptr + batch * num_groups, shape\n        =(group_size,), strides=(1,), offsets=(group,), block_shape=(1,),\n        order=(0,))\n    mean_block_ptr = tl.make_block_ptr(mean_ptr + batch * num_groups, shape\n        =(group_size,), strides=(1,), offsets=(group,), block_shape=(1,),\n        order=(0,))\n    input = tl.load(input_block_ptr)\n    mean = tl.sum(tl.view(input / num_elements, (1, group_block_size *\n        x_block_size)), 1)\n    centered_mean = input - mean\n    var = tl.sum(tl.view(centered_mean * centered_mean / num_elements, (1, \n        group_block_size * x_block_size)), 1)\n    rstd = tl.math.rsqrt(var + eps)\n    output = centered_mean * rstd\n    if weight_ptr is not None:\n        weight_block_ptr = tl.make_block_ptr(weight_ptr, shape=(y_size, 1),\n            strides=(1, y_size), offsets=(group * group_size, 0),\n            block_shape=(group_block_size, 1), order=(0, 1))\n        weight = tl.load(weight_block_ptr, boundary_check=(0,))\n        output *= weight\n    if bias_ptr is not None:\n        bias_block_ptr = tl.make_block_ptr(bias_ptr, shape=(y_size, 1),\n            strides=(1, y_size), offsets=(group * group_size, 0),\n            block_shape=(group_block_size, 1), order=(0, 1))\n        bias = tl.load(bias_block_ptr, boundary_check=(0,))\n        output += bias\n    if ACTIVATION:\n        output = silu(output)\n    tl.store(output_block_ptr, output)\n    tl.store(rstd_block_ptr, rstd)\n    tl.store(mean_block_ptr, mean)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "e72dd815-a564-4f27-9f54-74a16d26af83"
  },
  {
    "input": "@triton.jit\ndef _inner_paged_attn_unroll_4_kernel(q, k_cache, v_cache, stride_km,\n    block_base_ptrs, base_offs_kv, alibi_slope, block_offs, seq_len, qkv,\n    qk_max, exp_sum, BLOCK_SIZE: 'tl.constexpr', LO: 'tl.constexpr', HI:\n    'tl.constexpr'):\n    for block_idx in range(LO, HI, 4):\n        offs_kv_0 = tl.load(block_base_ptrs + block_idx + 0\n            ) * stride_km + base_offs_kv\n        offs_kv_1 = tl.load(block_base_ptrs + block_idx + 1\n            ) * stride_km + base_offs_kv\n        offs_kv_2 = tl.load(block_base_ptrs + block_idx + 2\n            ) * stride_km + base_offs_kv\n        offs_kv_3 = tl.load(block_base_ptrs + block_idx + 3\n            ) * stride_km + base_offs_kv\n        k_0 = tl.load(k_cache + offs_kv_0)\n        k_1 = tl.load(k_cache + offs_kv_1)\n        k_2 = tl.load(k_cache + offs_kv_2)\n        k_3 = tl.load(k_cache + offs_kv_3)\n        v_0 = tl.load(v_cache + offs_kv_0)\n        v_1 = tl.load(v_cache + offs_kv_1)\n        v_2 = tl.load(v_cache + offs_kv_2)\n        v_3 = tl.load(v_cache + offs_kv_3)\n        _qk_0 = tl.sum(q[None, :] * k_0, axis=1)\n        _qk_1 = tl.sum(q[None, :] * k_1, axis=1)\n        _qk_2 = tl.sum(q[None, :] * k_2, axis=1)\n        _qk_3 = tl.sum(q[None, :] * k_3, axis=1)\n        if alibi_slope is not None:\n            _qk_0 += alibi_slope * ((block_idx + 0) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_1 += alibi_slope * ((block_idx + 1) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_2 += alibi_slope * ((block_idx + 2) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n            _qk_3 += alibi_slope * ((block_idx + 3) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n        _qk_max = tl.maximum(tl.max(_qk_0, axis=0), qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_1, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_2, axis=0), _qk_max)\n        _qk_max = tl.maximum(tl.max(_qk_3, axis=0), _qk_max)\n        exp_tmp = tl.exp(_qk_0 - _qk_max) + tl.exp(_qk_1 - _qk_max) + tl.exp(\n            _qk_2 - _qk_max) + tl.exp(_qk_3 - _qk_max)\n        _exp_sum = exp_sum * tl.exp(qk_max - _qk_max) + tl.sum(exp_tmp, axis=0)\n        qkv_sum_tmp = tl.exp(_qk_0[:, None] - _qk_max) * v_0 + tl.exp(_qk_1\n            [:, None] - _qk_max) * v_1 + tl.exp(_qk_2[:, None] - _qk_max\n            ) * v_2 + tl.exp(_qk_3[:, None] - _qk_max) * v_3\n        qkv = (qkv * (exp_sum * tl.exp(qk_max - _qk_max)) + qkv_sum_tmp\n            ) / _exp_sum\n        qk_max = _qk_max\n        exp_sum = _exp_sum\n    return qkv, qk_max, exp_sum\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "32872ff1-c81c-4f0d-ba4f-c4c93ddf762a"
  },
  {
    "input": "@triton.jit\ndef element_mul_kernel(X_ptr, X_stride, grad_output_ptr, n_cols, BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by X_ptr with the value pointed by grad_output_ptr.\n    The multiplication is performed in-place on the tensor pointed by X_ptr.\n\n    Parameters:\n    X_ptr: Pointer to the input tensor.\n    X_stride (int): The stride of the input tensor.\n    grad_output_ptr: Pointer to the gradient output value.\n    n_cols (int): The number of columns in the input tensor.\n    BLOCK_SIZE (int): The block size for Triton operations.\n    \"\"\"\n    program_id = tl.program_id(0)\n    X_ptr += program_id * X_stride\n    grad_output = tl.load(grad_output_ptr)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols)\n        tl.store(X_ptr + X_offsets, X_block * grad_output, mask=X_offsets <\n            n_cols)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "89f3d114-40b0-4121-851e-6ef8771c80e4"
  },
  {
    "input": "@triton.jit\ndef apply_act_func(input, drop_p, seed, offset, param, act_func:\n    'tl.constexpr', dropout: 'tl.constexpr'):\n    \"\"\"\n    Applies an activation function to the input, optionally fusing dropout.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        offset: Offset to generate the dropout mask for if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n\n    Returns:\n        Input transformed by the desired activation function,\n        potentially with fused dropout.\n    \"\"\"\n    if act_func == 'sigmoid':\n        input = input\n        output = sigmoid(input)\n    elif act_func == 'tanh':\n        input = input\n        output = tanh(input)\n    elif act_func == 'relu':\n        output = relu(input)\n    elif act_func == 'gelu':\n        input = input\n        output = gelu(input)\n    elif act_func == 'silu':\n        input = input\n        output = silu(input)\n    elif act_func == 'relu6':\n        output = relu6(input)\n    elif act_func == 'hardsigmoid':\n        output = hardsigmoid(input)\n    elif act_func == 'hardswish':\n        output = hardswish(input)\n    elif act_func == 'selu':\n        input = input\n        output = selu(input)\n    elif act_func == 'mish':\n        input = input\n        output = mish(input)\n    elif act_func == 'leaky_relu':\n        output = leaky_relu(input, param)\n    if dropout:\n        output = apply_dropout(output, drop_p, seed, offset)\n    return output\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "86a8aaf9-f3f5-41f3-b9b7-7f6ea138d382"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config(\n    {'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config\n    ({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.\n    Config({'BLOCK_SIZE': 2048})], key=['dim'])\n@triton.jit\ndef _state_passing_bwd_kernel(dout_ptr, out_ptr, dA_cs_ptr,\n    dfinal_states_ptr, seq_idx_ptr, dstates_ptr, ddA_cs_ptr,\n    dinitstates_ptr, states_converted_ptr, dim, nchunks, seqlen, chunk_size,\n    stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_dim,\n    stride_out_batch, stride_out_chunk, stride_out_head, stride_out_dim,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dfinal_states_batch, stride_dfinal_states_head,\n    stride_dfinal_states_dim, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    stride_dstates_batch, stride_dstates_chunk, stride_dstates_head,\n    stride_dstates_dim, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_dinitstates_batch, stride_dinitstates_head,\n    stride_dinitstates_dim, CONVERT_STATES: 'tl.constexpr',\n    HAS_DFINAL_STATES: 'tl.constexpr', HAS_DINITSTATES: 'tl.constexpr',\n    HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_h *\n        stride_dstates_head + (nchunks - 1) * stride_dstates_chunk)\n    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head + (\n        nchunks - 1) * stride_dA_cs_chunk\n    ddA_cs_ptr += pid_b * stride_ddA_cs_batch + pid_h * stride_ddA_cs_head + (\n        nchunks - 1) * stride_ddA_cs_chunk + pid_m\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head + (nchunks -\n        1) * stride_out_chunk\n    dout_ptr += pid_b * stride_dout_batch + pid_h * stride_dout_head + (nchunks\n         - 1) * stride_dout_chunk\n    if CONVERT_STATES:\n        states_converted_ptr += (pid_b * stride_out_batch + pid_h *\n            stride_out_head + (nchunks - 1) * stride_out_chunk)\n    if HAS_DFINAL_STATES:\n        dfinal_states_ptr += (pid_b * stride_dfinal_states_batch + pid_h *\n            stride_dfinal_states_head)\n    if HAS_DINITSTATES:\n        dinitstates_ptr += (pid_b * stride_dinitstates_batch + pid_h *\n            stride_dinitstates_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    dout_ptrs = dout_ptr + offs_m * stride_dout_dim\n    if CONVERT_STATES:\n        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim\n    if HAS_DFINAL_STATES:\n        dstates = tl.load(dfinal_states_ptr + offs_m *\n            stride_dfinal_states_dim, mask=offs_m < dim, other=0.0)\n    else:\n        dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n    if HAS_SEQ_IDX:\n        seq_idx = tl.load(seq_idx_ptr + (seqlen - 1) * stride_seq_idx_seqlen)\n    dstates_ptrs -= stride_dstates_chunk\n    for c in range(nchunks - 1):\n        dA_cs = tl.load(dA_cs_ptr)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            seq_idx_new = tl.load(seq_idx_ptr + ((nchunks - c - 1) *\n                chunk_size - 1) * stride_seq_idx_seqlen)\n            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n            seq_idx = seq_idx_new\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0)\n        if CONVERT_STATES:\n            tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n        ddA = tl.sum(out * dstates) * scale\n        tl.store(ddA_cs_ptr, ddA)\n        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0)\n        dstates = scale * dstates + dout\n        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n        dout_ptrs -= stride_dout_chunk\n        dstates_ptrs -= stride_dstates_chunk\n        dA_cs_ptr -= stride_dA_cs_chunk\n        ddA_cs_ptr -= stride_ddA_cs_chunk\n        out_ptrs -= stride_out_chunk\n        if CONVERT_STATES:\n            states_converted_ptrs -= stride_out_chunk\n    if CONVERT_STATES:\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0)\n        tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n    if not HAS_DINITSTATES:\n        tl.store(ddA_cs_ptr, 0.0)\n    else:\n        dA_cs = tl.load(dA_cs_ptr)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            scale = tl.where(seq_idx == 0, scale, 0.0)\n        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0)\n        ddA = tl.sum(out * dstates) * scale\n        tl.store(ddA_cs_ptr, ddA)\n        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0)\n        dstates = scale * dstates + dout\n        tl.store(dinitstates_ptr + offs_m * stride_dinitstates_dim, dstates,\n            mask=offs_m < dim)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f9998b67-3ae3-4f8d-9a51-c0669c8eaa43"
  },
  {
    "input": "@triton.jit\ndef gated_matmul_bwd_ygrad(dout, y1_grad, y2_grad, act_input_1, act_input_2,\n    M, N, stride_dom, dtype: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', USE_GELU: 'tl.constexpr', IS_EVEN_MNK:\n    'tl.constexpr'):\n    \"\"\"\n    Kernel for backward gated MLP\n\n    Ref :\n    y2_grad = torch.mul(gelu(x @ w1), dout)\n    y1_grad = torch.mul(gelu_grad(x @ w1) * (x @ w2), dout)\n    \"\"\"\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    actin_1_block_ptr = tl.make_block_ptr(base=act_input_1, shape=(M, N),\n        strides=(stride_dom, 1), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    actin_2_block_ptr = tl.make_block_ptr(base=act_input_2, shape=(M, N),\n        strides=(stride_dom, 1), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    dout_block_ptr = tl.make_block_ptr(base=dout, shape=(M, N), strides=(\n        stride_dom, 1), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    if IS_EVEN_MNK:\n        dout_blk = tl.load(dout_block_ptr)\n        actin_1_blk = tl.load(actin_1_block_ptr)\n        actin_2_blk = tl.load(actin_2_block_ptr)\n    else:\n        dout_blk = tl.load(dout_block_ptr, boundary_check=(0, 1))\n        actin_1_blk = tl.load(actin_1_block_ptr, boundary_check=(0, 1))\n        actin_2_blk = tl.load(actin_2_block_ptr, boundary_check=(0, 1))\n    if USE_GELU:\n        actin_act = gelu(actin_1_blk)\n        actin_act_grad = gelu_grad(actin_1_blk)\n    else:\n        actin_act = relu(actin_1_blk)\n        actin_act_grad = relu_grad(actin_1_blk)\n    actin_act *= dout_blk\n    actin_act_grad *= actin_2_blk\n    actin_act_grad *= dout_blk\n    y1_grad_ptrs = tl.make_block_ptr(base=y1_grad, shape=(M, N), strides=(\n        stride_dom, 1), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    y2_grad_ptrs = tl.make_block_ptr(base=y2_grad, shape=(M, N), strides=(\n        stride_dom, 1), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    if IS_EVEN_MNK:\n        tl.store(y1_grad_ptrs, actin_act_grad)\n        tl.store(y2_grad_ptrs, actin_act)\n    else:\n        tl.store(y1_grad_ptrs, actin_act_grad, boundary_check=(0, 1))\n        tl.store(y2_grad_ptrs, actin_act, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b7021b2d-8e72-45c7-b1c5-ad528242eb14"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_m, delta)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "75652581-cd48-4049-bea4-22b688424477"
  },
  {
    "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    nheads, rotary_dim, seqlen_ro, CACHE_KEY_SEQLEN, stride_out_batch,\n    stride_out_seqlen, stride_out_nheads, stride_out_headdim,\n    stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim,\n    BLOCK_K: 'tl.constexpr', IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr',\n    IS_VARLEN: 'tl.constexpr', INTERLEAVED: 'tl.constexpr', CONJUGATE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "1423b210-da40-468b-9662-135dd407da04"
  },
  {
    "input": "@triton.jit\ndef mul_kernel(x_ptr, y_ptr, output_ptr, N, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < N\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x * y\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "e434928e-0bda-4ee4-a156-590f41468ed5"
  },
  {
    "input": "@triton.jit\ndef joint_second_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr:\n    'tl.tensor', sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr',\n    coord_numel: 'tl.constexpr', output_numel: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_stride = 9\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = output_striding + block_size * output_stride * block_id\n    CONST_00 = 3.87298334620742\n    CONST_01 = 2.23606797749979\n    CONST_02 = 4.47213595499958\n    CONST_03 = tl.sqrt(3.0)\n    g_Y10 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_Y11 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_Y12 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_Y20 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_Y21 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_Y22 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_Y23 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_Y24 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    g_x = (CONST_00 * g_Y20 * z + CONST_00 * g_Y21 * y - CONST_01 * g_Y22 *\n        x - CONST_00 * g_Y24 * x + CONST_03 * g_Y10)\n    g_y = (CONST_00 * g_Y21 * x + CONST_02 * g_Y22 * y + CONST_00 * g_Y23 *\n        z + CONST_03 * g_Y11)\n    g_z = (CONST_00 * g_Y20 * x - CONST_01 * g_Y22 * z + CONST_00 * g_Y23 *\n        y + CONST_00 * g_Y24 * z + CONST_03 * g_Y12)\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b70f49f3-edb5-473f-85fe-db05ad5ecbc3"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_gla_fwd_kernel(q, k, v, g, o, h0, ht, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_db = g + i_bh * s_k_h + (BT - 1) * s_k_t + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0)\n        if CHECK and i == 0:\n            b_o = tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[:, None] + tl.dot(b_k, b_v, allow_tf32=\n                False)\n        else:\n            b_o = tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h * tl.exp(d_b)[:, None] + tl.dot(b_k, b_v, allow_tf32=\n                False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_db += BT * K\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "6e25958b-6afc-4d7e-9b77-726ed74375c5"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_int4_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_g, stride_k_d, stride_o_bs,\n    stride_o_h, stride_o_g, stride_o_d, stride_os_bs, stride_os_h,\n    stride_os_g, group_size, BLOCK_GROUP_NUM: 'tl.constexpr',\n    BLOCK_GROUP_DIM: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_g = tl.arange(0, BLOCK_GROUP_NUM)\n    offs_d = tl.arange(0, BLOCK_GROUP_DIM // 2)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data_0 = tl.load(K + cur_index * stride_k_bs + cur_head *\n        stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :] * 2,\n        mask=offs_g[:, None] < group_size, other=0.0)\n    src_data_1 = tl.load(K + cur_index * stride_k_bs + cur_head *\n        stride_k_h + offs_g[:, None] * stride_k_g + offs_d[None, :] * 2 + 1,\n        mask=offs_g[:, None] < group_size, other=0.0)\n    abs_data_0 = tl.abs(src_data_0)\n    abs_data_1 = tl.abs(src_data_1)\n    data_scale = tl.maximum(tl.max(abs_data_0, axis=1), tl.max(abs_data_1,\n        axis=1)) / 7.0\n    q_src_data_0 = src_data_0 / data_scale[:, None]\n    q_src_data_0 = tl.where(q_src_data_0 > 7, 7, q_src_data_0)\n    q_src_data_0 = tl.where(q_src_data_0 < -7, -7, q_src_data_0)\n    q_src_data_1 = src_data_1 / data_scale[:, None]\n    q_src_data_1 = tl.where(q_src_data_1 > 7, 7, q_src_data_1)\n    q_src_data_1 = tl.where(q_src_data_1 < -7, -7, q_src_data_1)\n    low_4 = (q_src_data_0 & 128) >> 4 | q_src_data_0 & 15\n    high_4 = ((q_src_data_1 & 128) >> 4 | q_src_data_1 & 15) << 4\n    out_data = low_4 | high_4\n    o_ptrs = Out + dest_index * stride_o_bs + cur_head * stride_o_h + offs_g[\n        :, None] * stride_o_g + offs_d[None, :]\n    os_ptrs = (Out_scale + dest_index * stride_os_bs + cur_head *\n        stride_os_h + offs_g)\n    tl.store(o_ptrs, out_data, mask=offs_g[:, None] < group_size)\n    tl.store(os_ptrs, data_scale, mask=offs_g < group_size)\n    return\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "8b9d8be1-16b6-472e-b514-83e57f03fd54"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d13c3ace-dc30-4a3f-ba06-39b54d750e3b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "269b89d5-b512-430e-964f-8178ef934a77"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.\n    Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8),\n    triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32},\n    num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({\n    'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton\n    .Config({'BS': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_rwkv6_fwd_kernel_cum(s, o, o_minus_s, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o_minus_s = tl.make_block_ptr(o_minus_s + i_bh * s_s_h, (T, S), (\n        s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_o_minus_s, b_o - b_s, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "423af02d-1846-4f8c-862a-e51d7484b2e6"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel(x_ptr, z_ptr, N0, N1, T, B0: 'tl.constexpr', B1:\n    'tl.constexpr'):\n    \"\"\"2 loops ver.\"\"\"\n    block_id_i = tl.program_id(0)\n    log2_e = 1.44269504\n    off_i = block_id_i * B0 + tl.arange(0, B0)\n    mask_i = off_i < N0\n    exp_sum = tl.zeros([B0], dtype=tl.float32)\n    x_max = tl.full([B0], -float('inf'), dtype=tl.float32)\n    new_x_max = tl.full((B0,), -float('inf'), dtype=tl.float32)\n    for id_j in tl.range(0, T, B1):\n        off_j = id_j + tl.arange(0, B1)\n        off_ij = off_i[:, None] * T + off_j[None, :]\n        mask_j = off_j < T\n        mask_ij = mask_i[:, None] & mask_j[None, :]\n        x = tl.load(x_ptr + off_ij, mask=mask_ij)\n        new_x_max = tl.maximum(x_max, tl.max(x, axis=1))\n        new_exp_x = tl.exp2(log2_e * (x - new_x_max[:, None]))\n        factor = tl.exp2(log2_e * (x_max - new_x_max))\n        exp_sum = exp_sum * factor + tl.sum(new_exp_x, axis=1)\n        x_max = new_x_max\n    for id_j in tl.range(0, T, B1):\n        off_j = id_j + tl.arange(0, B1)\n        off_ij = off_i[:, None] * T + off_j[None, :]\n        mask_j = off_j < T\n        mask_ij = mask_i[:, None] & mask_j[None, :]\n        x = tl.load(x_ptr + off_ij, mask=mask_ij)\n        exp_x = tl.exp2(log2_e * (x - x_max[:, None]))\n        z = exp_x / exp_sum[:, None]\n        tl.store(z_ptr + off_ij, z, mask=mask_ij)\n    return\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "b56fe3dc-78a0-4e60-a63d-b170bdd3c7e7"
  },
  {
    "input": "@triton.jit\ndef softmax_mask_bias_kernel_two_rows(output_ptr, input_ptr, mask_ptr,\n    bias_ptr, input_row_stride, output_row_stride, n_cols, n_heads,\n    BLOCK_SIZE: 'tl.constexpr', use_mask: 'tl.constexpr', use_bias:\n    'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_row_ptr = input_ptr + 2 * row_idx * input_row_stride\n    output_row_ptr = output_ptr + 2 * row_idx * output_row_stride\n    input_ptrs = input_row_ptr + col_offsets\n    output_ptrs = output_row_ptr + col_offsets\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + 2 * row_idx // (n_heads * n_cols) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + 2 * row_idx % (n_heads * n_cols) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n        col_offsets, n_cols, use_mask, use_bias)\n    mask_ptrs = input_ptrs\n    if use_mask:\n        mask_row_ptr = mask_ptr + (2 * row_idx + 1) // (n_heads * n_cols\n            ) * n_cols\n        mask_ptrs = mask_row_ptr + col_offsets\n    bias_ptrs = input_ptrs\n    if use_bias:\n        bias_row_ptr = bias_ptr + (2 * row_idx + 1) % (n_heads * n_cols\n            ) * n_cols\n        bias_ptrs = bias_row_ptr + col_offsets\n    _softmax_core(input_ptrs + n_cols, output_ptrs + n_cols, mask_ptrs,\n        bias_ptrs, col_offsets, n_cols, use_mask, use_bias)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "23de52d8-0fee-4cc0-8343-bd22804fa801"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "2edc0044-e53b-462d-b313-3f5456f2a4ab"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att2_int8v(Prob, V, V_scale, Out, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph,\n    stride_pbs, stride_vbs, stride_vh, stride_vd, stride_vsbs, stride_vsh,\n    stride_vsd, stride_obs, stride_oh, stride_od, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    vs_offs = cur_head * stride_vsh\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        vs_value = tl.load(V_scale + vs_offs + v_loc[:, None] * stride_vsbs,\n            mask=start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value * vs_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "fcff3ba4-e8ba-488c-b79d-7518aa22e976"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, initial_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + ((i + 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1)) * inv_ln2\n        d_b = tl.load(p_db) * inv_ln2\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_k *= tl.math.exp2(d_b[None, :] - b_g)\n        b_h *= tl.math.exp2(d_b)[None, :]\n        b_h += tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale * tl.math.exp2(b_g)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_db = g + i_bh * s_qk_h + (T - (i - 1) * BT - 1\n            ) * s_qk_t + i_k * BK + tl.arange(0, BK)\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1)) * inv_ln2\n        b_db = tl.load(p_db) * inv_ln2\n        g_k = tl.math.exp2(b_db[None, :] - b_g)\n        b_k *= g_k\n        b_q *= tl.math.exp2(tl.trans(b_g))\n        b_dk = tl.trans(tl.dot(b_dh, tl.trans(b_v), allow_tf32=False)\n            ) * scale * g_k\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * scale\n        b_dh *= tl.math.exp2(b_db)[:, None]\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "ee9b5ef6-7dfd-4b0a-9f33-4cd428e3cc1c"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_compute(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v,\n    desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk,\n    stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', STAGE: 'tl.constexpr', ENABLE_TMA: 'tl.constexpr',\n    LOOP_SCHEDULE: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    K_block_ptr = None\n    V_block_ptr = None\n    Q_block_ptr = None\n    O_block_ptr = None\n    if not ENABLE_TMA:\n        Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n            HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m *\n            BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n        v_order: 'tl.constexpr' = (0, 1\n            ) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n        V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n            HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0),\n            block_shape=(BLOCK_N, HEAD_DIM), order=v_order)\n        K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(\n            HEAD_DIM, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n            ), block_shape=(HEAD_DIM, BLOCK_N), order=(0, 1))\n        O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n            HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m *\n            BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    if ENABLE_TMA:\n        q = tl._experimental_descriptor_load(desc_q, [qvk_offset //\n            stride_qm + start_m * BLOCK_M, 0], [BLOCK_M, HEAD_DIM], Q.dtype\n            .element_ty)\n    else:\n        q = tl.load(Q_block_ptr)\n    if STAGE & 1:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, desc_k, desc_v, Q, qvk_offset, stride_kn,\n            stride_vn, stride_vk, start_m, qk_scale, BLOCK_M, HEAD_DIM,\n            BLOCK_N, 4 - STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty ==\n            tl.float8e5, ENABLE_TMA, LOOP_SCHEDULE)\n    if STAGE & 2:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, desc_k, desc_v, Q, qvk_offset, stride_kn,\n            stride_vn, stride_vk, start_m, qk_scale, BLOCK_M, HEAD_DIM,\n            BLOCK_N, 2, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.\n            float8e5, ENABLE_TMA, LOOP_SCHEDULE)\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    if ENABLE_TMA:\n        tl._experimental_descriptor_store(desc_o, acc, [qvk_offset //\n            stride_om + start_m * BLOCK_M, 0])\n    else:\n        tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "dc6f9be1-c266-4fe6-8697-2a51ca91ad82"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_V(k, v, h, g, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t *\n        BT), (BT, BT), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gn = tl.exp(tl.load(p_gn, boundary_check=(0,))[None, :] - b_gk)\n    b_k = b_k * b_gn\n    if i_k == 0:\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n    else:\n        b_A = tl.zeros([BT, BT], dtype=b_k.dtype)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) + tl.dot(b_A, b_do,\n            allow_tf32=False)\n        b_do = b_do * scale\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * b_gn\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_dA = tl.where(m_s, b_dA, 0.0)\n    if i_k == 0:\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "d7fbc610-9329-4df2-8884-efd89297217b"
  },
  {
    "input": "@triton.jit\ndef flash_attention_v1_kernel(q_ptr, k_ptr, v_ptr, o_ptr, q_batch_stride,\n    q_heads_stride, q_seq_stride, q_dim_stride, k_batch_stride,\n    k_heads_stride, k_seq_stride, k_dim_stride, v_batch_stride,\n    v_heads_stride, v_seq_stride, v_dim_stride, out_batch_stride,\n    out_heads_stride, out_seq_stride, out_dim_stride, num_kv_groups,\n    n_heads, m_size, n_size, BLOCK_DHEAD_SIZE: 'tl.constexpr', BLOCK_M_SIZE:\n    'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr', sm_scale, causal_mask):\n    \"\"\"\n    flashattention \u5185\u6838\u5b9e\u73b0\n    \"\"\"\n    block_m_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    cur_batch_idx = head_idx // n_heads\n    cur_head_idx = head_idx % n_heads\n    cur_kv_head_idx = cur_head_idx // num_kv_groups\n    m_range_offs = tl.arange(0, BLOCK_M_SIZE)\n    n_range_offs = tl.arange(0, BLOCK_N_SIZE)\n    dhead_range_offs = tl.arange(0, BLOCK_DHEAD_SIZE)\n    m_offs = block_m_idx * BLOCK_M_SIZE + m_range_offs\n    q_offs = cur_batch_idx * q_batch_stride + cur_head_idx * q_heads_stride + (\n        m_offs[:, None] * q_seq_stride + dhead_range_offs[None, :] *\n        q_dim_stride)\n    k_offs = (cur_batch_idx * k_batch_stride + cur_kv_head_idx *\n        k_heads_stride + (n_range_offs[:, None] * k_seq_stride + \n        dhead_range_offs[None, :] * k_dim_stride))\n    v_offs = (cur_batch_idx * v_batch_stride + cur_kv_head_idx *\n        v_heads_stride + (n_range_offs[:, None] * v_seq_stride + \n        dhead_range_offs[None, :] * v_dim_stride))\n    o_offs = (cur_batch_idx * out_batch_stride + cur_head_idx *\n        out_heads_stride + (m_offs[:, None] * out_seq_stride + \n        dhead_range_offs[None, :] * out_dim_stride))\n    q_ptrs = q_ptr + q_offs\n    k_ptrs = k_ptr + k_offs\n    v_ptrs = v_ptr + v_offs\n    out_ptrs = o_ptr + o_offs\n    l_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32) - float('inf')\n    d_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32)\n    acc = tl.zeros((BLOCK_M_SIZE, BLOCK_DHEAD_SIZE), dtype=tl.float32)\n    q_mask = m_offs[:, None] < m_size\n    q = tl.load(q_ptrs, mask=q_mask, other=0.0)\n    for block_n_start_idx in range(0, n_size, BLOCK_N_SIZE):\n        block_n_offs = block_n_start_idx + n_range_offs\n        k_mask = block_n_offs[:, None] < n_size\n        k = tl.load(k_ptrs + block_n_start_idx * k_seq_stride, mask=k_mask,\n            other=0.0)\n        qk = tl.zeros((BLOCK_M_SIZE, BLOCK_N_SIZE), dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        if causal_mask:\n            offs_k = block_n_offs\n            offs_m = m_offs\n            mask = offs_m[:, None] >= offs_k[None, :]\n            qk = tl.where(mask, qk * sm_scale, -100000000.0)\n        else:\n            qk = qk * sm_scale\n        l_j = tl.max(qk, 1)\n        numerators = tl.exp(qk - l_j[:, None])\n        d_j = tl.sum(numerators, 1)\n        l_new = tl.maximum(l_i, l_j)\n        alpha = tl.exp(l_i - l_new)\n        beta = tl.exp(l_j - l_new)\n        d_new = alpha * d_i + beta * d_j\n        p_scale = beta / d_new\n        p = numerators * p_scale[:, None]\n        sigma = d_i / d_new * alpha\n        acc = acc * sigma[:, None]\n        v = tl.load(v_ptrs + block_n_start_idx * v_seq_stride, mask=k_mask,\n            other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_new\n        d_i = d_new\n    out_mask = m_offs[:, None] < m_size\n    tl.store(out_ptrs, acc, mask=out_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "930e8006-fd5e-427f-a770-20d8e25c377a"
  },
  {
    "input": "@triton.jit\ndef cos_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.cos(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "93fe2800-82e5-4d2f-a6a5-a956fb21aad2"
  },
  {
    "input": "@triton.jit\ndef quant_dot_kernel(scale_ptr, offset_ptr, weight_ptr, activation_ptr,\n    z_ptr, N0, N1, MID, B0: 'tl.constexpr', B1: 'tl.constexpr', B_MID:\n    'tl.constexpr'):\n    block_id_j = tl.program_id(0)\n    block_id_k = tl.program_id(1)\n    off_j = block_id_j * B0 + tl.arange(0, B0)\n    off_k = block_id_k * B1 + tl.arange(0, B1)\n    mask_j = off_j < N0\n    mask_k = off_k < N1\n    z = tl.zeros((B0, B1), dtype=tl.float32)\n    off_z = off_j[:, None] * N1 + off_k[None, :]\n    mask_z = mask_j[:, None] & mask_k[None, :]\n    for l in tl.range(0, MID, B_MID):\n        off_l_div_g = tl.arange(0, B_MID // GROUP) + l // GROUP\n        mask_l_div_g = off_l_div_g < MID // GROUP\n        off_scale = off_j[:, None] * (MID // GROUP) + off_l_div_g[None, :]\n        mask_scale = mask_j[:, None] & mask_l_div_g[None, :]\n        scale = tl.load(scale_ptr + off_scale, mask=mask_scale)\n        shift = tl.load(offset_ptr + off_j, mask=mask_j)\n        off_weight_l = l + tl.arange(0, B_MID // FPINT)\n        mask_weight_l = off_weight_l < MID // FPINT\n        off_weight = off_j[:, None] * (MID // FPINT) + off_weight_l[None, :]\n        mask_weight = mask_j[:, None] & mask_weight_l[None, :]\n        weight = tl.load(weight_ptr + off_weight, mask=mask_weight)\n        off_l = l + tl.arange(0, B_MID)\n        mask_l = off_l < MID\n        off_activation = off_l[:, None] * N1 + off_k[None, :]\n        mask_activation = mask_l[:, None] & mask_k[None, :]\n        activation = tl.load(activation_ptr + off_activation, mask=\n            mask_activation)\n        BITS = 32 // FPINT\n        unpack_offs = tl.arange(0, FPINT) * BITS\n        unpack_upperbound_mask = (1 << BITS) - 1\n        unpacked_shift = shift[:, None] >> unpack_offs & unpack_upperbound_mask\n        unpacked_weight = weight[:, :, None\n            ] >> unpack_offs & unpack_upperbound_mask\n        transformed_weight = scale[:, :, None] * (unpacked_weight -\n            unpacked_shift[:, :, None])\n        transformed_weight = transformed_weight.reshape(unpacked_shift.\n            shape[0], unpacked_shift.shape[-1] * FPINT)\n        z += tl.dot(transformed_weight, activation)\n    tl.store(z_ptr + off_z, z, mask=mask_z)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "88eb9556-3253-4f11-8eaf-fd6a251e8d72"
  },
  {
    "input": "@triton.jit\ndef attn_fwd_inner(acc, l_i, m_i, qk_scale, bias_scale, q, k_ptrs, v_ptrs,\n    bias_ptrs, stride_kn, stride_vk, stride_bn, seqlen_q, seqlen_k,\n    head_dim, start_m, block_min, block_max, dropout_p, philox_seed,\n    batch_philox_offset, max_seqlen_k, encoded_sm_base, offs_n_causal,\n    masked_blocks, n_extra_tokens, alibi_slope, CAUSAL: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', PRE_LOAD_V: 'tl.constexpr', MASK_STEPS: 'tl.constexpr',\n    ENABLE_DROPOUT: 'tl.constexpr', RETURN_ENCODED_SOFTMAX: 'tl.constexpr',\n    PADDED_HEAD: 'tl.constexpr'):\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    for start_n in range(block_min, block_max, BLOCK_N):\n        if MASK_STEPS:\n            k_offs_n = start_n + tl.arange(0, BLOCK_N)\n        else:\n            k_offs_n = None\n        k_offs_d = None if not PADDED_HEAD else tl.arange(0, BLOCK_DMODEL)\n        k = load_fn(k_ptrs, k_offs_d, k_offs_n, head_dim, seqlen_k)\n        if PRE_LOAD_V:\n            v = load_fn(v_ptrs, k_offs_n, k_offs_d, seqlen_k, head_dim)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if MASK_STEPS:\n            if start_n + BLOCK_N == block_max and n_extra_tokens != 0:\n                boundary_m = tl.full([BLOCK_M], seqlen_k, dtype=tl.int32)\n                size_n = start_n + offs_n[None, :]\n                mask = size_n < boundary_m[:, None]\n                qk = tl.where(mask, qk, float('-inf'))\n        if CAUSAL:\n            causal_boundary = start_n + offs_n_causal\n            causal_mask = offs_m[:, None] >= causal_boundary[None, :]\n            qk = tl.where(causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        if bias_ptrs is not None:\n            bias_offs_n = start_n + tl.arange(0, BLOCK_N\n                ) if MASK_STEPS else None\n            bias = load_fn(bias_ptrs, offs_m, bias_offs_n, seqlen_q, seqlen_k)\n            qk += bias * bias_scale\n        if alibi_slope is not None:\n            global_m_positions = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n            global_n_positions = start_n + tl.arange(0, BLOCK_N)\n            alibi_block = compute_alibi_block(alibi_slope, seqlen_q,\n                seqlen_k, global_m_positions, global_n_positions)\n            qk += alibi_block * bias_scale\n        m_ij = tl.maximum(m_i, qk_scale * tl.max(qk, 1))\n        p = tl.math.exp2(qk * qk_scale - m_ij[:, None])\n        if MASK_STEPS or CAUSAL:\n            if qk_scale == 0.0:\n                p = tl.where(libdevice.isnan(p), 0.0, p)\n        l_ij = tl.sum(p, 1)\n        if ENABLE_DROPOUT:\n            philox_offset = (batch_philox_offset + start_m * BLOCK_M *\n                max_seqlen_k + start_n)\n            keep = dropout_mask(philox_seed, philox_offset, dropout_p,\n                BLOCK_M, BLOCK_N, max_seqlen_k)\n            if RETURN_ENCODED_SOFTMAX:\n                mstore2d(tl.where(keep, p, -p), BLOCK_M, BLOCK_N, o_base=\n                    encoded_sm_base, o_start_row=start_m * BLOCK_M,\n                    o_start_col=start_n, o_rows=seqlen_q, o_cols=seqlen_k,\n                    stride_row=max_seqlen_k, stride_col=1)\n            p = tl.where(keep, p, 0.0)\n        elif RETURN_ENCODED_SOFTMAX:\n            mstore2d(p, BLOCK_M, BLOCK_N, o_base=encoded_sm_base,\n                o_start_row=start_m * BLOCK_M, o_start_col=start_n, o_rows=\n                seqlen_q, o_cols=seqlen_k, stride_row=max_seqlen_k,\n                stride_col=1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        acc = acc * alpha[:, None]\n        if not PRE_LOAD_V:\n            v = load_fn(v_ptrs, k_offs_n, k_offs_d, seqlen_k, head_dim)\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n        acc += tl.dot(p, v)\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vk\n        if bias_ptrs is not None:\n            bias_ptrs += BLOCK_N * stride_bn\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "7df4e1c9-8277-4117-b909-ad90f2f4370e"
  },
  {
    "input": "@triton.jit\ndef coor_descent_kernel_forward(a_ptr, b_ptr, input_ptr, mask_ptr, k_ptr,\n    a_iter_stride, b_row_stride, b_iter_stride, input_row_stride,\n    mask_row_stride, n_iters, current_eps, eps_decay, eps, n_cols,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    col_mask = col_offsets < n_cols\n    mask_start_ptr = mask_ptr + row_idx * mask_row_stride\n    mask_ptrs = mask_start_ptr + col_offsets\n    mask_ints = tl.load(mask_ptrs, mask=col_mask, other=0)\n    mask = mask_ints == 1\n    a_ptr = a_ptr + row_idx\n    a = tl.load(a_ptr)\n    b_start_ptr = b_ptr + row_idx * b_row_stride\n    b_ptrs = b_start_ptr + col_offsets\n    b = tl.load(b_ptrs, mask=col_mask, other=0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    input_ptrs = row_start_ptr + col_offsets\n    s = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    k_ptr = k_ptr + row_idx\n    k = tl.load(k_ptr)\n    logk = tl.log(k)\n    for _ in range(n_iters):\n        a = (s + b) / current_eps\n        a = tl.where(mask, a, -float('inf'))\n        a_max = tl.max(a, axis=0)\n        a_minus_max = tl.where(mask, a - a_max, -float('inf'))\n        exp = tl.exp(a_minus_max)\n        sum_exp = tl.sum(exp, axis=0)\n        log_sum_exp = tl.log(sum_exp) + a_max\n        a = current_eps * (logk - log_sum_exp)\n        b = s + a\n        b = tl.where(b >= 0.0, -b, 0.0)\n        current_eps *= eps_decay\n        if current_eps < eps:\n            current_eps = eps\n    next_a_ptrs = a_ptr + a_iter_stride\n    next_b_ptrs = b_ptrs + b_iter_stride\n    tl.store(next_a_ptrs, a)\n    tl.store(next_b_ptrs, b, mask=col_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "28b291b8-6979-4796-bf06-13a7a2b4bb03"
  },
  {
    "input": "@triton.jit\ndef _color_activation(x):\n    return tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "8a0d0c32-6e17-4fc4-8e78-43bb90090976"
  },
  {
    "input": "@triton.jit\ndef gelu_grad(x):\n    cdf = 0.5 * (1.0 + tl.libdevice.erf(x * _sqrt1_2))\n    pdf = tl.exp(-0.5 * x * x) * _gaussian_pdf_normalization\n    return cdf + x * pdf\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "c6ea322c-c23f-479a-82f3-8d765033a5cb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_dh(q, gi, ge, do, dh, dht, dh0, s_k_h, s_k_t,\n    s_v_h, s_v_t, s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NG: 'tl.constexpr',\n    STORE_INITIAL_STATE_GRADIENT: 'tl.constexpr', USE_FINAL_STATE_GRADIENT:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k *\n            BK, i_t * BT), (BK, BT), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        p_gk = tl.make_block_ptr(ge + i_bg * s_k_h, (K, T), (1, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_q = b_q * tl.exp(b_gk) * scale\n        p_gk_last = gi + i_bg * s_k_h + last_idx * K + i_k * BK + tl.arange(\n            0, BK)\n        p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K,\n            other=0.0)\n        b_dh *= tl.exp(b_gk_last)[:, None]\n        b_dh += tl.dot(b_q, b_do)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "99268c98-4cba-4e57-bfb3-5c1afaa8d194"
  },
  {
    "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "7309464e-bf87-4a40-80a7-f918278f509e"
  },
  {
    "input": "@triton.jit\ndef _kernel_bwd_merge_continuous(alpha_c, alpha_d, tmp_merge,\n    tmp_merge_normalized, tmp_merge_grad, stride_alpha_c1, stride_alpha_c2,\n    stride_alpha_c3, stride_alpha_d1, stride_alpha_d2, stride_alpha_d3,\n    stride_alpha_d4, stride_alpha_d5, stride_tmp_merge1, stride_tmp_merge2,\n    r1, r2, r3, r4, b, n, w, L, BLOCK_R1: 'tl.constexpr', BLOCK_R2:\n    'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    start = tl.program_id(1)\n    end = start + w\n    if b_idx >= b:\n        return\n    offset_r = tl.arange(0, BLOCK_R1)\n    l_ptr = alpha_c + b_idx * stride_alpha_c1 + start * stride_alpha_c2 + (\n        start + 1) * stride_alpha_c3 + offset_r\n    l_bwd_ptr = alpha_c + b_idx * stride_alpha_c1 + (start + 1\n        ) * stride_alpha_c2 + start * stride_alpha_c3 + offset_r\n    r_ptr = alpha_c + b_idx * stride_alpha_c1 + (start + 1\n        ) * stride_alpha_c2 + end * stride_alpha_c3 + r1 + offset_r\n    r_bwd_ptr = alpha_c + b_idx * stride_alpha_c1 + end * stride_alpha_c2 + (\n        start + 1) * stride_alpha_c3 + r1 + offset_r\n    mask = tl.arange(0, BLOCK_R1) < r1\n    parent_score = tl.load(tmp_merge + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + tl.arange(0, BLOCK_R1), mask=mask, other=0)\n    do = tl.load(tmp_merge_grad + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + tl.arange(0, BLOCK_R1), mask=mask, other=0)\n    do *= tl.load(tmp_merge_normalized + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + tl.arange(0, BLOCK_R1), mask=mask, other=0)\n    for _ in range(0, w - 1):\n        left_score = tl.load(l_ptr, mask=mask, other=0)\n        right_score = tl.load(r_ptr, mask=mask, other=0)\n        new_grad = tl.exp(left_score + right_score - parent_score) * do\n        tl.atomic_add(l_bwd_ptr, new_grad, mask=mask)\n        tl.atomic_add(r_bwd_ptr, new_grad, mask=mask)\n        l_ptr += stride_alpha_c3\n        r_ptr += stride_alpha_c2\n        l_bwd_ptr += stride_alpha_c2\n        r_bwd_ptr += stride_alpha_c3\n    mask2 = tl.arange(0, BLOCK_R2) < r2\n    parent_score = tl.load(tmp_merge + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + r1 + tl.arange(0, BLOCK_R2), mask=mask2, other=0)\n    do = tl.load(tmp_merge_grad + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + r1 + tl.arange(0, BLOCK_R2), mask=mask2, other=0)\n    do *= tl.load(tmp_merge_normalized + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + r1 + tl.arange(0, BLOCK_R2), mask=mask2, other=0)\n    for gap_start in range(start + 1, end - 1):\n        for gap_end in range(gap_start + 1, end):\n            ptr_c = (alpha_c + b_idx * stride_alpha_c1 + gap_start *\n                stride_alpha_c2 + gap_end * stride_alpha_c3 + 2 * r1 + tl.\n                arange(0, BLOCK_R2))\n            ptr_d = (alpha_d + b_idx * stride_alpha_d1 + start *\n                stride_alpha_d2 + gap_start * stride_alpha_d3 + gap_end *\n                stride_alpha_d4 + end * stride_alpha_d5 + tl.arange(0,\n                BLOCK_R2))\n            cont = tl.load(ptr_c, mask=mask2, other=0)\n            disco = tl.load(ptr_d, mask=mask2, other=0)\n            new_grad = tl.exp(cont + disco - parent_score) * do\n            ptr_bwd_c = (alpha_c + b_idx * stride_alpha_c1 + gap_end *\n                stride_alpha_c2 + gap_start * stride_alpha_c3 + 2 * r1 + tl\n                .arange(0, BLOCK_R2))\n            ptr_bwd_d = (alpha_d + b_idx * stride_alpha_d1 + gap_start *\n                stride_alpha_d2 + start * stride_alpha_d3 + gap_end *\n                stride_alpha_d4 + end * stride_alpha_d5 + tl.arange(0,\n                BLOCK_R2))\n            tl.atomic_add(ptr_bwd_c, new_grad, mask=mask2)\n            tl.atomic_add(ptr_bwd_d, new_grad, mask=mask2)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "81a7b0aa-bff2-475f-bcab-948c61f2c902"
  },
  {
    "input": "@triton.jit\ndef contract_pi(x, y, z):\n    n = tl.maximum(tl.maximum(tl.abs(x), tl.abs(y)), tl.abs(z))\n    x_c = _contract_pi_one(x, n)\n    y_c = _contract_pi_one(y, n)\n    z_c = _contract_pi_one(z, n)\n    return x_c, y_c, z_c\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "0c22e7ec-8104-4598-be9d-efd25ebcb5eb"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_gla_fwd_kernel(q, k, v, g, o, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_db = g + i_bh * s_qk_h + (BT - 1) * s_qk_t + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_g *= inv_ln2\n        d_b = tl.load(p_db) * inv_ln2\n        b_q = b_q * scale * tl.math.exp2(b_g)\n        b_k = b_k * tl.trans(tl.math.exp2(-b_g + d_b[None, :]))\n        b_o = tl.dot(b_q, b_h, allow_tf32=False)\n        b_h *= tl.math.exp2(d_b)[:, None]\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_g = tl.advance(p_g, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_db += BT * DK\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "f5e8344f-aad4-4392-a7de-0eb4d9aa1b6d"
  },
  {
    "input": "@triton.jit\ndef hash(x):\n    x = (x >> 16 ^ x) * 73244475\n    x = (x >> 16 ^ x) * 73244475\n    x = x >> 16 ^ x\n    return x\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "93cd1f3c-27ba-43ff-84b5-fd132cad3915"
  },
  {
    "input": "@triton.jit\ndef post_process_grad(q, k, v, u, do, dk, dq, du, scale, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, H, T: 'tl.constexpr', BT: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_h = i_bh % H\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, 0), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, 0), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, 0), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, 0), (BT, BK), (1, 0))\n    p_du = tl.make_block_ptr(du + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, 0), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, 0), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_t * BT, 0), (BT, BV), (1, 0))\n    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (0,), (BK,), (0,))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    b_vdo = tl.sum(b_v * b_do, axis=1)\n    b_du = b_vdo[:, None] * b_k * b_q * scale\n    b_dq = b_vdo[:, None] * b_k * b_u[None, :] * scale\n    b_dk = b_vdo[:, None] * b_q * b_u[None, :] * scale\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_du, b_du, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ae017b45-7a9c-45b3-a5d0-08495b6ca9fe"
  },
  {
    "input": "@triton.jit\ndef _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds + b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f2743ac1-2f43-474a-9600-b328cb1834f9"
  },
  {
    "input": "@triton.heuristics({'GEMMA': lambda args: bool(args['GEMMA'])})\n@triton.jit\ndef _rms_layernorm_backward(dY, dY_row_stride, dX, dX_row_stride, X,\n    X_row_stride, W, W_row_stride, r, r_row_stride, n_cols, eps, GEMMA:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Fast RMS Layernorm kernel for the backward pass\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dY += row_idx * dY_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    if GEMMA:\n        dX += row_idx * dY_row_stride\n    else:\n        dX = dY\n    dY_row = tl.load(dY + col_offsets, mask=mask, other=0)\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    inv_var = tl.load(r)\n    normed = X_row * inv_var\n    if GEMMA:\n        dY_W = dY_row * (W_row + 1.0)\n    else:\n        dY_W = dY_row * W_row\n    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)\n    output = inv_var / n_cols * (n_cols * dY_W - normed * rowsum_dY_normed)\n    tl.store(dX + col_offsets, output, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0c2f2cc9-16d8-4ce1-b158-6a1de815197c"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_K(q, k, v, h, g, A, do, dh, dq, dk, dv, dA, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT,\n        1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A = tl.dot(b_q * scale, tl.trans(b_k), allow_tf32=False)\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_v = b_v * tl.exp(b_gn[None, :] - b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g) * scale\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        if i_t > 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.exp(b_gn[None, :] - b_g) * tl.dot(b_k, b_dh, allow_tf32=False\n            )\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_k, allow_tf32=False)\n    b_dk += tl.dot(tl.trans(b_dA), b_q, allow_tf32=False)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b8a5b95e-a2be-4fcc-8114-1fbf72b13237"
  },
  {
    "input": "@triton.jit\ndef _flash_decoding_stage2_kernel(Mid_O, Mid_O_LogExpSum, Ouput,\n    mido_batch_stride, mido_heads_stride, mido_partitions_stride,\n    mido_dim_stride, mido_les_batch_stride, mido_les_heads_stride,\n    mido_les_partitions_stride, o_bs_stride, o_heads_stride, o_dim_stride,\n    actual_seq_len, BLOCK_DMODEL: 'tl.constexpr', BLOCK_SEQ: 'tl.constexpr'):\n    \"\"\"Reduction (online softmax)\n\t\"\"\"\n    batch_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_part_v = (batch_idx * mido_batch_stride + head_idx *\n        mido_heads_stride + offs_d * mido_dim_stride)\n    offs_part_max = (batch_idx * mido_les_batch_stride + head_idx *\n        mido_les_heads_stride)\n    part_v_ptrs = Mid_O + offs_part_v\n    part_max_ptrs = Mid_O_LogExpSum + offs_part_max\n    d_i = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    m_i = -float('inf')\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    num_partitions = (actual_seq_len + BLOCK_SEQ - 1) // BLOCK_SEQ\n    for _ in range(0, num_partitions, 1):\n        part_v = tl.load(part_v_ptrs)\n        part_max = tl.load(part_max_ptrs)\n        m_ij = tl.maximum(part_max, m_i)\n        p = tl.exp(part_v - m_ij)\n        alpha = tl.exp(m_i - m_ij)\n        d_i = d_i * alpha + p\n        acc *= alpha\n        acc += p * part_v\n        m_i = m_ij\n        part_v_ptrs += mido_partitions_stride\n        part_max_ptrs += mido_les_partitions_stride\n    offs_out = (batch_idx * o_bs_stride + head_idx * o_heads_stride + \n        offs_d * o_dim_stride)\n    tl.store(Ouput + offs_out, acc / d_i)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "ce9b94ed-e3b9-4e6b-92b3-6245397780bd"
  },
  {
    "input": "@triton.jit\ndef load_fn(block_ptr, first, second, pad):\n    if first and second:\n        tensor = tl.load(block_ptr, boundary_check=(0, 1), padding_option=pad)\n    elif first:\n        tensor = tl.load(block_ptr, boundary_check=(0,), padding_option=pad)\n    elif second:\n        tensor = tl.load(block_ptr, boundary_check=(1,), padding_option=pad)\n    else:\n        tensor = tl.load(block_ptr)\n    return tensor\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "f6ce6352-ce91-4137-8436-3307a39ea7ff"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "cfb709c6-a030-4548-9829-06094810eb02"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, cu_seqlens_q, mid_batch, mid_start,\n    stride_oz, stride_oh, stride_ok, stride_doz, stride_doh, stride_dok,\n    stride_dz, stride_dh, BLOCK_M: 'tl.constexpr', D_HEAD: 'tl.constexpr'):\n    off_z = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_b = tl.load(mid_batch + off_z)\n    off_m = tl.load(mid_start + off_z)\n    q_start = tl.load(cu_seqlens_q + off_b)\n    q_end = tl.load(cu_seqlens_q + off_b + 1)\n    lM = q_end - q_start\n    offs_m = tl.arange(0, BLOCK_M) + off_m\n    offs_k = tl.arange(0, D_HEAD)\n    o_ptrs = Out + (offs_m[:, None] * stride_oz + off_h * stride_oh + \n        offs_k[None, :] * stride_ok)\n    do_ptrs = DO + (offs_m[:, None] * stride_doz + off_h * stride_doh + \n        offs_k[None, :] * stride_dok)\n    mask_m = offs_m < q_end\n    o = tl.load(o_ptrs, mask=mask_m[:, None])\n    do = tl.load(do_ptrs, mask=mask_m[:, None])\n    delta = tl.sum(o * do, axis=1)\n    d_ptrs = Delta + offs_m * stride_dz + off_h * stride_dh\n    tl.store(d_ptrs, delta, mask=mask_m)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0106100e-7689-4ea0-a355-d6196c7dab34"
  },
  {
    "input": "@triton.jit\ndef tl_log1p(a: 'tl.tensor') ->tl.tensor:\n    return tl_libdevice.log1p(a)\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "3eb2a13c-f459-40b6-be51-fededec11bf8"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    P_SEQ, num_block_q, num_block_kv, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qk_scale = sm_scale * 1.44269504\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_h * stride_kh\n    V += off_z * stride_vz + off_h * stride_vh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_kz + off_h * stride_kh\n    DV += off_z * stride_vz + off_h * stride_vh\n    for start_n in range(0, num_block_kv):\n        if CAUSAL:\n            lo = tl.math.max(start_n * BLOCK_M - P_SEQ, 0)\n        else:\n            lo = 0\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        D_ptrs = D + off_hz * N_CTX\n        l_ptrs = L + off_hz * N_CTX\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        for start_m in range(lo, num_block_q * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            if CAUSAL:\n                qk = tl.where(P_SEQ + offs_m_curr[:, None] >= offs_n[None,\n                    :], float(0.0), float('-inf'))\n            else:\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, tl.trans(k))\n            qk *= qk_scale\n            l_i = tl.load(l_ptrs + offs_m_curr)\n            p = tl.math.exp2(qk - l_i[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(tl.trans(p), do)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, tl.trans(v))\n            ds = p * dp * sm_scale\n            dk += tl.dot(tl.trans(ds), q)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k)\n            tl.store(dq_ptrs, dq)\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        tl.store(dk_ptrs, dk)\n        tl.store(dv_ptrs, dv)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ca661115-ac6d-4eff-be6b-38b38657188d"
  },
  {
    "input": "@triton.jit\ndef triton_cross_merge_unidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr',\n    BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp2\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp2\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _y1 = tl.load(p_y1 + _idx, mask=_mask_hw)\n        _y2 = tl.load(p_y2 + _idx, mask=_mask_hw)\n        _y3 = tl.load(p_y3 + _idx, mask=_mask_hw)\n        _y4 = tl.load(p_y4 + _idx, mask=_mask_hw)\n        tl.store(p_x + _idx, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "c4ef3e9c-68b8-4a60-bc6d-8c9e87d91c0a"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignored_index,\n    total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: 'tl.constexpr',\n    HAS_SMOOTHING: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignored_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 - smoothing),\n            probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "09fe09aa-2a47-4018-9153-f19083e5c9ff"
  },
  {
    "input": "@triton.jit\ndef _mm_backward(do, da_ptrs, partial_mask_a, da_lock_ptr, n_locks, b_ptrs,\n    partial_mask_b, stride_ad, stride_bd, D, BLOCK_D: 'tl.constexpr',\n    EVEN_D: 'tl.constexpr'):\n    d_inds = tl.arange(0, BLOCK_D)[None, :]\n    da_ptrs = da_ptrs + d_inds * stride_ad\n    b_ptrs = b_ptrs + d_inds * stride_bd\n    for d in range(0, tl.cdiv(D, BLOCK_D)):\n        if EVEN_D:\n            mask = partial_mask_b\n        else:\n            mask = partial_mask_b & (d_inds < D - d * BLOCK_D)\n        b = tl.load(b_ptrs, mask=mask, other=0.0)\n        da_i = tl.dot(do, b)\n        if EVEN_D:\n            mask = partial_mask_a\n        else:\n            mask = partial_mask_a & (d_inds < D - d * BLOCK_D)\n        lock_offset = d // tl.cdiv(D, BLOCK_D * n_locks)\n        this_da_lock_ptr = da_lock_ptr + lock_offset\n        tl_lock_add(da_ptrs, da_i, mask, this_da_lock_ptr)\n        b_ptrs += BLOCK_D * stride_bd\n        da_ptrs += BLOCK_D * stride_ad\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "34588d9c-d6e0-4d01-9512-b7ca84639b47"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4, pre_hook=init_to_zero([\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages\n    =3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.\n    Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size',\n    'dstate', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_prev_kernel(dout_ptr, prev_states_ptr, C_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, dstate, hdim,\n    batch, seqlen, nchunks, nheads_ngroups_ratio, stride_dout_batch,\n    stride_dout_seqlen, stride_dout_head, stride_dout_hdim,\n    stride_prev_states_batch, stride_prev_states_chunk,\n    stride_prev_states_head, stride_prev_states_hdim,\n    stride_prev_states_dstate, stride_C_batch, stride_C_seqlen,\n    stride_C_head, stride_C_dstate, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    prev_states_ptr += (pid_b * stride_prev_states_batch + pid_c *\n        stride_prev_states_chunk + pid_h * stride_prev_states_head)\n    C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_C_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n        stride_prev_states_dstate + offs_k[:, None] * stride_prev_states_hdim)\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None, :] *\n        stride_C_dstate)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    prev_states = tl.load(prev_states_ptrs, mask=(offs_k[:, None] < hdim) &\n        (offs_n[None, :] < dstate), other=0.0)\n    prev_states = prev_states\n    acc = tl.dot(dout, prev_states)\n    c = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < dstate), other=0.0)\n    ddA_cs = tl.sum(acc * c, axis=1)\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0\n        )\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_m)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    ddA_cs *= scale\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "2fafa754-1bf6-4e5b-ae06-b1e80665410b"
  },
  {
    "input": "@triton.jit\ndef load_full_1d(ptr, sz: 'const', stride=1):\n    \"\"\"Load 1d block [0,...,sz-1]\"\"\"\n    offs = offset_1d(sz)\n    mask = mask_1d(offs, sz)\n    return tl.load(ptr + offs, mask)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "44dad296-7e66-4fb5-bde0-4d4c9787657a"
  },
  {
    "input": "@triton.jit\ndef _sample_3d(image, w, batch_index, ix, iy, iz, ID, IH, IW, C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    image_offs = (image + batch_index * ID * IW * IH * C + iz_ * IW * IH *\n        C + iy_ * IW * C + ix_ * C)\n    mask_w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW) * (iz < ID) *\n        (iz >= 0))\n    if C == 1:\n        val = tl.view(tl.load(image_offs), (BLOCK_SIZE,))\n        out = tl.view(val * mask_w, (BLOCK_SIZE,))\n        return out\n    else:\n        val = tl.view(tl.load(image_offs[:, None] + Coffs[None, :]), (\n            BLOCK_SIZE, C))\n        mask_w_bcast = tl.view(mask_w[:, None], (BLOCK_SIZE, 1))\n        return val * mask_w_bcast\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "394e87f9-e242-4cc1-9131-fe66e119fc1f"
  },
  {
    "input": "@triton.jit\ndef matmul_kernel(x_ptr, y_ptr, z_ptr, m_size, k_size, n_size, m_block_size:\n    'tl.constexpr', k_block_size: 'tl.constexpr', n_block_size: 'tl.constexpr'\n    ):\n    pid = tl.program_id(0)\n    num_n_blocks = tl.cdiv(n_size, n_block_size)\n    m_block = pid // num_n_blocks\n    n_block = pid % num_n_blocks\n    m_offsets = tl.arange(0, m_block_size) + m_block * m_block_size\n    n_offsets = tl.arange(0, n_block_size) + n_block * n_block_size\n    k_offsets = tl.arange(0, k_block_size)\n    x_ptrs = x_ptr + m_offsets[:, None] * k_size + k_offsets[None, :]\n    y_ptrs = y_ptr + k_offsets[:, None] * n_size + n_offsets[None, :]\n    z_ptrs = z_ptr + m_offsets[:, None] * n_size + n_offsets[None, :]\n    z = tl.zeros((m_block_size, n_block_size), tl.float32)\n    for _ in range(0, k_size, k_block_size):\n        x = tl.load(x_ptrs)\n        y = tl.load(y_ptrs)\n        z += tl.dot(x, y, allow_tf32=False)\n        x_ptrs += k_block_size\n        y_ptrs += k_block_size * n_size\n    tl.store(z_ptrs, z)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "41856484-1e07-407a-9ba0-fefa1420244a"
  },
  {
    "input": "@triton.jit\ndef _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\n    h = f * up\n\n    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))\n    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +\n            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] *                            ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )\n\n    Notice sech^2(x) = 1 - tanh^2(x)\n    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )\n\n    See https://www.desmos.com/calculator/nqprfoni6x\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    s = 0.7978845608028654\n    a = s * e_row\n    b = a * 0.044715 * e_row * e_row\n    T = 1.0 + triton_tanh(a + b)\n    T2 = 0.5 * T\n    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b)\n    df_de = T2 + Q2\n    f_row = T2 * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row * df_de\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "66f65bf1-596c-462d-bdb7-2b9aa145b9b2"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_splitK(Q, K, V, sm_scale, Out_splitK, LSE_splitk,\n    block_tables, Seq_len, Seq_starts_k, Seq_starts_q,\n    Seq_starts_q_multiplier, additive_bias, K_fp8_scale_shift,\n    V_fp8_scale_shift, stride_qz, stride_qm, stride_qg, stride_qh,\n    stride_qk, stride_kz, stride_kn, stride_kg, stride_kh, stride_kk,\n    stride_vz, stride_vn, stride_vg, stride_vh, stride_vk, stride_osk_z,\n    stride_osk_g, stride_osk_h, stride_osk_s, stride_osk_m, stride_osk_k,\n    stride_lsek_z, stride_lsek_g, stride_lsek_h, stride_lsek_s,\n    stride_lsek_m, stride_blocktablesz, stride_blocktablesl, stride_bias_b,\n    stride_bias_g, stride_bias_h, stride_bias_qm, stride_bias_km,\n    stride_k_fp8_scale_shift_z: 'tl.constexpr', stride_k_fp8_scale_shift_n:\n    'tl.constexpr', stride_k_fp8_scale_shift_g: 'tl.constexpr',\n    stride_k_fp8_scale_shift_h: 'tl.constexpr', stride_v_fp8_scale_shift_z:\n    'tl.constexpr', stride_v_fp8_scale_shift_n: 'tl.constexpr',\n    stride_v_fp8_scale_shift_g: 'tl.constexpr', stride_v_fp8_scale_shift_h:\n    'tl.constexpr', kv_cache_blocks_per_row: 'tl.constexpr', Z:\n    'tl.constexpr', N_CTX_Q: 'tl.constexpr', N_CTX_K: 'tl.constexpr',\n    BLOCK_N_PER_SPLIT: 'tl.constexpr', H: 'tl.constexpr', G: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', USE_SEQ_LEN: 'tl.constexpr',\n    PACKED_PER_VAL: 'tl.constexpr', N_GROUPS: 'tl.constexpr',\n    BOUNDS_CHECKS_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', IS_SPLITK: 'tl.constexpr', SPLIT_K_EARLY_EXIT:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', NUM_QUERIES_CAUSAL:\n    'tl.constexpr', USE_PAGED_ATTENTION: 'tl.constexpr', PAGE_SIZE:\n    'tl.constexpr', WRITE_LSE: 'tl.constexpr', HAS_ADDITIVE_BIAS:\n    'tl.constexpr'):\n    \"\"\"This kernel can accept non-quantized or int4-quantized keys/values.\n    PACKED_PER_VAL determines the quantization type:\n        - PACKED_PER_VAL == 1 means no quantization\n        - PACKED_PER_VAL == 8 means 4-bit quantization (8 packed quantized values inside one int32)\n    For the quantized case K/V should be int32 tensors.\n    Quantization can be row-wise (when N_GROUPS = 1) or group-wise with N_GROUPS = 2, 4, or 8.\n    Quantization coefficients are stored at the beginning of the row along the last dimension of K/V\n    So K[B, H, M, :] has a form\n    [   quant_coef0, quant_coef1, ...|\n        group0_quant_value0, group0_quant_value1,... |\n        group1_quant_value0, group1_quant_value1,...]\n    where each quant_coef is an int32 which should be interpreted as 2 packed float16: scale and offset.\n\n    Note: this kernel needs to be processed by xformers.triton.vararg_kernel.unroll_varargs\n    before compilation. That will unroll variables marked with \"VAR_ARGS_ARRAY\" into lists.\n    See how FwOp.apply does it below.\n\n    Set IS_SPLITK=False to indicate the MHA result should be written directly.\n    No metadata will be written.\n    \"\"\"\n    internal_dtype = (tl.float64 if Out_splitK.dtype.element_ty is tl.\n        float64 else tl.float32)\n    tl.static_assert(PACKED_PER_VAL == 1 and tl.constexpr(K.dtype.\n        element_ty != tl.int32) or (PACKED_PER_VAL == 4 or PACKED_PER_VAL ==\n        8) and tl.constexpr(K.dtype.element_ty == tl.int32),\n        f'Only int4 and fp8 quantization is supported, K/V should have dtype int32 in the quantized case: PACKED_PER_VAL={PACKED_PER_VAL!r} tl.constexpr(K.dtype)={tl.constexpr(K.dtype)!r} tl.constexpr(K.dtype.element_ty)={tl.constexpr(K.dtype.element_ty)!r}'\n        )\n    tl.static_assert(((N_GROUPS == 1 or N_GROUPS == 2) or N_GROUPS == 4) or\n        N_GROUPS == 8,\n        'Number of quantization groups can be 1 (row-wise quantization), 2, 4, or 8.'\n        )\n    tl.static_assert(N_GROUPS == 1 or K_fp8_scale_shift is None,\n        f'Only row-wise fp8 quantization is supported, but got N_GROUPS={N_GROUPS!r} > 1.'\n        )\n    FP8_QUANTIZED: 'tl.constexpr' = K_fp8_scale_shift is not None\n    INT4_QUANTIZED: 'tl.constexpr' = PACKED_PER_VAL > 1 and not FP8_QUANTIZED\n    PACKED_D_PER_GROUP: 'tl.constexpr' = (BLOCK_DMODEL // PACKED_PER_VAL //\n        N_GROUPS)\n    D_PER_GROUP: 'tl.constexpr' = BLOCK_DMODEL // N_GROUPS\n    start_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_hg = off_zhg % (H * G)\n    off_h = off_hg // G\n    off_g = off_hg % G\n    splitk_idx = tl.program_id(2)\n    if USE_SEQ_LEN:\n        kv_len = tl.load(Seq_len + off_z)\n        if SPLIT_K_EARLY_EXIT and kv_len == 0:\n            return\n    else:\n        kv_len = N_CTX_K\n    if Seq_starts_k is None:\n        start_kv_idx = 0\n    else:\n        start_kv_idx = tl.load(Seq_starts_k + off_z)\n    if Seq_starts_q is None:\n        q_len = N_CTX_Q\n        queries_use_batch_dim = 1\n        off_m = 0\n    else:\n        queries_use_batch_dim = 0\n        off_m = tl.load(Seq_starts_q + off_z) * Seq_starts_q_multiplier\n        q_len = tl.load(Seq_starts_q + off_z + 1\n            ) * Seq_starts_q_multiplier - off_m\n        if q_len == 0:\n            return\n    k_base = K + off_h * stride_kh + off_g * stride_kg\n    v_base = V + off_h * stride_vh + off_g * stride_vg\n    if FP8_QUANTIZED:\n        k_fp8_scale_shift_base = (K_fp8_scale_shift + off_h *\n            stride_k_fp8_scale_shift_h + off_g * stride_k_fp8_scale_shift_g)\n        v_fp8_scale_shift_base = (V_fp8_scale_shift + off_h *\n            stride_v_fp8_scale_shift_h + off_g * stride_v_fp8_scale_shift_g)\n    else:\n        k_fp8_scale_shift_base = None\n        v_fp8_scale_shift_base = None\n    chunk_hi = (splitk_idx + 1) * BLOCK_N_PER_SPLIT\n    chunk_lo = splitk_idx * BLOCK_N_PER_SPLIT\n    ignore_in_first_block = 0\n    if PAGE_SIZE > 0:\n        BLOCKS_IN_PAGE: 'tl.constexpr' = PAGE_SIZE // BLOCK_N\n        is_last_chunk = splitk_idx == tl.num_programs(2) - 1\n        shift = BLOCK_N - 1 if is_last_chunk else 0\n        lo = tl.maximum(chunk_lo, start_kv_idx) // BLOCK_N * BLOCK_N\n        ignore_in_first_block = tl.maximum(0, start_kv_idx - lo)\n        hi = (chunk_hi + shift) // BLOCK_N * BLOCK_N\n        hi = tl.minimum(hi, kv_len + start_kv_idx)\n        block_table = block_tables + stride_blocktablesz * off_z\n        logical_block_idx = lo // BLOCK_N\n    else:\n        lo = chunk_lo\n        hi = tl.minimum(chunk_hi, kv_len)\n        if Seq_starts_k is not None:\n            k_base += start_kv_idx * stride_kn\n            v_base += start_kv_idx * stride_vn\n        else:\n            k_base += off_z * stride_kz\n            v_base += off_z * stride_vz\n        K_block_ptr = tl.make_block_ptr(base=k_base + stride_kk *\n            INT4_QUANTIZED * N_GROUPS, shape=(PACKED_D_PER_GROUP, hi),\n            strides=(stride_kk, stride_kn), offsets=(0, lo), block_shape=(\n            PACKED_D_PER_GROUP, BLOCK_N), order=(0, 1))\n        V_block_ptr = tl.make_block_ptr(base=v_base + stride_vk *\n            INT4_QUANTIZED * N_GROUPS, shape=(hi, PACKED_D_PER_GROUP),\n            strides=(stride_vn, stride_vk), offsets=(lo, 0), block_shape=(\n            BLOCK_N, PACKED_D_PER_GROUP), order=(1, 0))\n        if INT4_QUANTIZED:\n            K_scale_shift_block_ptr = tl.make_block_ptr(base=k_base, shape=\n                (1, hi), strides=(stride_kk, stride_kn), offsets=(0, lo),\n                block_shape=(1, BLOCK_N), order=(0, 1))\n            V_scale_shift_block_ptr = tl.make_block_ptr(base=v_base, shape=\n                (hi, 1), strides=(stride_vn, stride_vk), offsets=(lo, 0),\n                block_shape=(BLOCK_N, 1), order=(1, 0))\n        elif FP8_QUANTIZED:\n            if Seq_starts_k is not None:\n                k_fp8_scale_shift_base += (start_kv_idx *\n                    stride_k_fp8_scale_shift_n)\n                v_fp8_scale_shift_base += (start_kv_idx *\n                    stride_v_fp8_scale_shift_n)\n            else:\n                k_fp8_scale_shift_base += off_z * stride_k_fp8_scale_shift_z\n                v_fp8_scale_shift_base += off_z * stride_v_fp8_scale_shift_z\n            K_scale_shift_block_ptr = tl.make_block_ptr(base=\n                k_fp8_scale_shift_base, shape=(1, hi), strides=(1,\n                stride_k_fp8_scale_shift_n), offsets=(0, lo), block_shape=(\n                1, BLOCK_N), order=(0, 1))\n            V_scale_shift_block_ptr = tl.make_block_ptr(base=\n                v_fp8_scale_shift_base, shape=(hi, 1), strides=(\n                stride_v_fp8_scale_shift_n, 1), offsets=(lo, 0),\n                block_shape=(BLOCK_N, 1), order=(1, 0))\n        else:\n            K_scale_shift_block_ptr = None\n            V_scale_shift_block_ptr = None\n        if HAS_ADDITIVE_BIAS:\n            additive_bias_block_ptr = tl.make_block_ptr(base=additive_bias +\n                off_z * stride_bias_b + off_g * stride_bias_g + off_h *\n                stride_bias_h, shape=(N_CTX_Q, hi), strides=(stride_bias_qm,\n                stride_bias_km), offsets=(start_m * BLOCK_M, lo),\n                block_shape=(BLOCK_M, BLOCK_N), order=(0, 1))\n    if SPLIT_K_EARLY_EXIT and lo >= hi:\n        return\n    Q_block_ptr = tl.make_block_ptr(base=Q + off_m * stride_qm + off_h *\n        stride_qh + off_z * stride_qz * queries_use_batch_dim + off_g *\n        stride_qg, shape=(q_len, BLOCK_DMODEL), strides=(stride_qm,\n        stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M,\n        D_PER_GROUP), order=(1, 0))\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(acc)):\n        acc[i] = tl.zeros([BLOCK_M, D_PER_GROUP], dtype=internal_dtype)\n    qk_scale = sm_scale * 1.44269504\n    q: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(acc)):\n        q[i] = tl.load(tl.advance(Q_block_ptr, (0, i * D_PER_GROUP)),\n            boundary_check=(0,))\n    if IS_CAUSAL:\n        q_offset = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        diag_idx = q_offset[:, None] % NUM_QUERIES_CAUSAL - tl.arange(0,\n            BLOCK_N)[None, :]\n        diag_idx_shifted = tl.constexpr(diag_idx - NUM_QUERIES_CAUSAL + kv_len)\n    for start_n in range(lo, hi, BLOCK_N):\n        if PAGE_SIZE > 0:\n            block_offset_in_page = logical_block_idx % BLOCKS_IN_PAGE\n            logical_page_idx = logical_block_idx // BLOCKS_IN_PAGE\n            physical_page_idx = tl.load(block_table + stride_blocktablesl *\n                logical_page_idx)\n            offset = (physical_page_idx * PAGE_SIZE + block_offset_in_page *\n                BLOCK_N)\n            current_block_size = min(hi - start_n, BLOCK_N)\n            K_block_ptr = tl.make_block_ptr(base=k_base + stride_kk *\n                INT4_QUANTIZED * N_GROUPS, shape=(PACKED_D_PER_GROUP, \n                offset + current_block_size), strides=(stride_kk, stride_kn\n                ), offsets=(0, offset), block_shape=(PACKED_D_PER_GROUP,\n                BLOCK_N), order=(0, 1))\n            V_block_ptr = tl.make_block_ptr(base=v_base + stride_vk *\n                INT4_QUANTIZED * N_GROUPS, shape=(offset +\n                current_block_size, PACKED_D_PER_GROUP), strides=(stride_vn,\n                stride_vk), offsets=(offset, 0), block_shape=(BLOCK_N,\n                PACKED_D_PER_GROUP), order=(1, 0))\n            if INT4_QUANTIZED:\n                K_scale_shift_block_ptr = tl.make_block_ptr(base=k_base,\n                    shape=(1, offset + current_block_size), strides=(\n                    stride_kk, stride_kn), offsets=(0, offset), block_shape\n                    =(1, BLOCK_N), order=(0, 1))\n                V_scale_shift_block_ptr = tl.make_block_ptr(base=v_base,\n                    shape=(offset + current_block_size, 1), strides=(\n                    stride_vn, stride_vk), offsets=(offset, 0), block_shape\n                    =(BLOCK_N, 1), order=(1, 0))\n            elif FP8_QUANTIZED:\n                K_scale_shift_block_ptr = tl.make_block_ptr(base=\n                    k_fp8_scale_shift_base, shape=(1, offset +\n                    current_block_size), strides=(1,\n                    stride_k_fp8_scale_shift_n), offsets=(0, offset),\n                    block_shape=(1, BLOCK_N), order=(0, 1))\n                V_scale_shift_block_ptr = tl.make_block_ptr(base=\n                    v_fp8_scale_shift_base, shape=(offset +\n                    current_block_size, 1), strides=(\n                    stride_v_fp8_scale_shift_n, 1), offsets=(offset, 0),\n                    block_shape=(BLOCK_N, 1), order=(1, 0))\n            else:\n                K_scale_shift_block_ptr = None\n                V_scale_shift_block_ptr = None\n            logical_block_idx += 1\n        k: \"'VAR_ARGS_ARRAY'\"\n        v: \"'VAR_ARGS_ARRAY'\"\n        for i in range(len(acc)):\n            k[i], v[i] = load_dequantize_k_v_group(K_block_ptr, V_block_ptr,\n                K_scale_shift_block_ptr, V_scale_shift_block_ptr,\n                BOUNDS_CHECKS_N, PACKED_PER_VAL, PACKED_D_PER_GROUP,\n                FP8_QUANTIZED, Q.dtype.element_ty, i)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        for i in range(len(acc)):\n            qk += tl.dot(q[i], k[i])\n        qk *= qk_scale\n        if start_n == lo and ignore_in_first_block > 0:\n            qk = tl.where(tl.arange(0, BLOCK_N) < ignore_in_first_block,\n                float('-inf'), qk)\n        if HAS_ADDITIVE_BIAS:\n            loaded_bias = tl.load(additive_bias_block_ptr, boundary_check=(\n                0, 1) if BOUNDS_CHECKS_N else (0,))\n            qk += loaded_bias * 1.44269504\n            additive_bias_block_ptr = tl.advance(additive_bias_block_ptr, (\n                0, BLOCK_N))\n        if BOUNDS_CHECKS_N:\n            qk = tl.where(tl.arange(0, BLOCK_N) < hi - start_n, qk, float(\n                '-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(diag_idx_shifted >= start_n, qk, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        if HAS_ADDITIVE_BIAS or IS_CAUSAL:\n            alpha = tl.where(m_i_new == float('-inf'), 0, alpha)\n            p = tl.where(m_i_new[:, None] == float('-inf'), 0, p)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        p = p\n        for i in range(len(acc)):\n            acc[i] *= alpha[:, None]\n            acc[i] += tl.dot(p, v[i])\n        if not PAGE_SIZE:\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n            if PACKED_PER_VAL > 1:\n                K_scale_shift_block_ptr = tl.advance(K_scale_shift_block_ptr,\n                    (0, BLOCK_N))\n                V_scale_shift_block_ptr = tl.advance(V_scale_shift_block_ptr,\n                    (BLOCK_N, 0))\n    O_block_ptr = tl.make_block_ptr(base=Out_splitK + off_z * stride_osk_z *\n        queries_use_batch_dim + off_m * stride_osk_m + off_g * stride_osk_g +\n        off_h * stride_osk_h + splitk_idx * stride_osk_s, shape=(q_len,\n        D_PER_GROUP), strides=(stride_osk_m, 1), offsets=(start_m * BLOCK_M,\n        0), block_shape=(BLOCK_M, D_PER_GROUP), order=(1, 0))\n    for i in range(len(acc)):\n        attn_out = tl.where(l_i[:, None] == 0, 0.0, acc[i] / l_i[:, None])\n        tl.store(tl.advance(O_block_ptr, (0, i * D_PER_GROUP)), attn_out,\n            boundary_check=(0,))\n    if WRITE_LSE:\n        LSE_splitk_ptr = (LSE_splitk + off_z * stride_lsek_z *\n            queries_use_batch_dim + off_m * stride_lsek_m + off_g *\n            stride_lsek_g + off_h * stride_lsek_h + splitk_idx *\n            stride_lsek_s + (start_m * BLOCK_M + tl.arange(0, BLOCK_M)) *\n            stride_lsek_m)\n        mask = start_m * BLOCK_M + tl.arange(0, BLOCK_M) < q_len\n        lse_dtype = LSE_splitk.dtype.element_ty\n        tl.store(LSE_splitk_ptr, (tl.math.log2(l_i) + m_i) / 1.44269504,\n            mask=mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "c575ae35-6c3b-4fa7-bf46-ff7a3c86c2df"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_gla_fwd_kernel(q, k, v, gk, gv, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * DK if\n        REVERSE else 0)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * DV if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * DV if REVERSE else 0)\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + ((T - 1) *\n            DK if REVERSE else 0)\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if REVERSE else 0)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            h = h * _gk[None, :]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            h = h * _gv[:, None]\n        h += _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += -DK if REVERSE else DK\n        p_k += -DK if REVERSE else DK\n        p_o += -DV if REVERSE else DV\n        p_v += -DV if REVERSE else DV\n        if USE_GK:\n            p_gk += -DK if REVERSE else DK\n        if USE_GV:\n            p_gv += -DV if REVERSE else DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "2aca1541-40fc-4127-8397-26eb837c30fe"
  },
  {
    "input": "@triton.jit\ndef element_mul_kernel(X_ptr, X_stride, grad_output_ptr, n_cols, BLOCK_SIZE:\n    'tl.constexpr'):\n    \"\"\"\n    This function multiplies each element of the tensor pointed by X_ptr with the value pointed by grad_output_ptr.\n    The multiplication is performed in-place on the tensor pointed by X_ptr.\n\n    Parameters:\n    X_ptr: Pointer to the input tensor.\n    X_stride (int): The stride of the input tensor.\n    grad_output_ptr: Pointer to the gradient output value.\n    n_cols (int): The number of columns in the input tensor.\n    BLOCK_SIZE (int): The block size for Triton operations.\n    \"\"\"\n    program_id = tl.program_id(0)\n    X_ptr += program_id * X_stride\n    grad_output = tl.load(grad_output_ptr)\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols)\n        tl.store(X_ptr + X_offsets, X_block * grad_output, mask=X_offsets <\n            n_cols)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8f427430-0cee-4411-b7a0-ed6790e6ddc9"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'NC', 'BT'])\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_intra(q, k, gi, ge, dA, dq, dk, s_k_h, s_k_t,\n    s_k_d, scale, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr',\n    BC: 'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    if i_t * BT + i_i * BC >= T:\n        return\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_q = i_t * BT + i_i * BC\n    m_k = o_k < K\n    p_ge = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_ge = tl.load(p_ge, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    o_i = tl.arange(0, BC)\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    if i_i > 0:\n        b_gn = tl.load(gi + i_bh * T * K + (o_q - 1) * K + o_k, mask=m_k &\n            (i_i > 0) & (o_q <= T), other=0)\n        for i_j in range(0, i_i):\n            p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n                ), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t,\n                s_k_d), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1),\n                (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n            b_k = tl.load(p_k, boundary_check=(0, 1))\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dq += tl.dot(b_dA, b_kg)\n        b_dq *= tl.exp(b_ge - b_gn[None, :])\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gkj = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,))\n        b_gkj = tl.load(p_gkj, boundary_check=(0,))\n        m_i = o_i[:, None] > j\n        tmp = tl.exp(b_ge - b_gkj[None, :])\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tmp, 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    max_block_idx = min(NC, tl.cdiv(T - i_t * BT, BC))\n    if i_i < max_block_idx - 1:\n        p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        for i_j in range(i_i + 1, NC):\n            p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n                ), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_ge = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t,\n                s_k_d), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n            p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1),\n                (i_t * BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_ge = tl.load(p_ge, boundary_check=(0, 1))\n            b_qg = b_q * tl.exp(b_ge - b_gn[None, :])\n            b_dA = tl.load(p_dA, boundary_check=(0, 1))\n            b_dk += tl.dot(tl.trans(b_dA), b_qg, allow_tf32=False)\n        b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gqj = tl.make_block_ptr(ge + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        b_gqj = tl.load(p_gqj, boundary_check=(0,))\n        m_i = o_i[:, None] < j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "38be9cb2-d6a6-4e09-b0a8-26fa40b7c256"
  },
  {
    "input": "@triton.jit\ndef sixth_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST002 = 3.26558761940328\n    CONST003 = 3.26558761940328\n    CONST004 = 6.53117523880657\n    CONST006 = 8.38944649544891\n    CONST007 = 9.79676285820985\n    CONST008 = 10.3266947761614\n    CONST009 = 3.60555127546399\n    CONST010 = -1.78863600265677\n    CONST011 = 14.5309475774982\n    CONST012 = 8.94318001328386\n    CONST013 = 16.5227116418583\n    CONST014 = 16.5227116418583\n    CONST015 = 17.8863600265677\n    CONST017 = 20.6533895523229\n    CONST018 = 20.2812259244849\n    CONST019 = -107.318160159406\n    CONST020 = 17.8863600265677\n    CONST022 = 29.3902885746295\n    CONST024 = 40.5624518489699\n    CONST025 = 41.9472324772445\n    CONST026 = -1.63279380970164\n    CONST027 = -83.8944649544891\n    CONST028 = -78.3741028656788\n    CONST030 = -71.5454401062709\n    CONST032 = -52.2494019104525\n    CONST033 = -52.2494019104525\n    CONST035 = -48.4364919249939\n    CONST036 = -41.3067791046458\n    CONST037 = -36.3273689437454\n    CONST038 = -29.3902885746295\n    CONST039 = -27.0416345659799\n    CONST040 = -26.1247009552263\n    CONST041 = -26.1247009552263\n    CONST042 = -19.5935257164197\n    CONST043 = -2.4218245962497\n    CONST044 = -9.79676285820985\n    CONST045 = -7.15454401062709\n    CONST046 = -3.38020432074749\n    CONST047 = -1.1267347735825\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR04 = VAR07 * VAR07\n    VAR05 = VAR07 * VAR08\n    VAR06 = VAR08 * VAR08\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR13 = VAR16 * VAR16\n    VAR14 = VAR16 * VAR17\n    VAR15 = VAR17 * VAR17\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR22 = VAR25 * VAR25\n    VAR23 = VAR25 * VAR26\n    VAR24 = VAR26 * VAR26\n    Y00 = (CONST011 * VAR05 * z + CONST011 * VAR23 * x + CONST035 * VAR07 *\n        VAR25)\n    Y01 = y * (CONST006 * VAR05 + CONST025 * VAR24 * x + CONST027 * VAR07 *\n        VAR26)\n    Y02 = -CONST045 * VAR05 * z + CONST045 * VAR23 * x + VAR17 * (CONST030 *\n        VAR07 * z - CONST030 * VAR25 * x)\n    Y03 = VAR16 * (-CONST028 * VAR26 * x + CONST040 * VAR07) + y * (\n        CONST007 * VAR05 + CONST038 * VAR24 * x + CONST042 * VAR07 * VAR26)\n    Y04 = CONST003 * VAR05 * z + VAR07 * (CONST004 * VAR25 + CONST033 *\n        VAR17 * z) + x * (CONST002 * VAR23 - CONST032 * VAR15 * z + \n        CONST032 * VAR17 * VAR25)\n    Y05 = CONST008 * VAR05 * y + VAR07 * (CONST017 * VAR26 * y + CONST036 *\n        VAR16) + x * (CONST008 * VAR24 * y + CONST013 * VAR14 + CONST036 *\n        VAR16 * VAR26)\n    Y06 = (CONST009 * VAR13 + CONST018 * VAR17 * VAR24 + CONST039 * VAR15 *\n        VAR26 + CONST047 * VAR04 + CONST047 * VAR22 + VAR06 * (CONST018 *\n        VAR17 + CONST046 * VAR26) + VAR08 * (CONST024 * VAR17 * VAR26 + \n        CONST039 * VAR15 + CONST046 * VAR24))\n    Y07 = CONST008 * VAR23 * y + VAR25 * (CONST017 * VAR08 * y + CONST036 *\n        VAR16) + z * (CONST008 * VAR06 * y + CONST014 * VAR14 + CONST036 *\n        VAR08 * VAR16)\n    Y08 = (CONST026 * VAR04 - CONST026 * VAR22 + CONST040 * VAR17 * VAR24 -\n        CONST041 * VAR15 * VAR26 + VAR06 * (CONST026 * VAR26 - CONST041 *\n        VAR17) + VAR08 * (-CONST026 * VAR24 + CONST041 * VAR15))\n    Y09 = VAR16 * (CONST028 * VAR08 * z - CONST041 * VAR25) + y * (CONST022 *\n        VAR06 * z - CONST042 * VAR08 * VAR25 + CONST044 * VAR23)\n    Y10 = (CONST010 * VAR04 + CONST010 * VAR22 + CONST020 * VAR17 * VAR24 +\n        VAR06 * (CONST012 * VAR26 + CONST015 * VAR17) + VAR08 * (CONST012 *\n        VAR24 + CONST019 * VAR17 * VAR26))\n    Y11 = y * (CONST006 * VAR23 + CONST025 * VAR06 * z + CONST027 * VAR08 *\n        VAR25)\n    Y12 = (-CONST037 * VAR06 * VAR26 + CONST037 * VAR08 * VAR24 + CONST043 *\n        VAR04 - CONST043 * VAR22)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y07, mask=\n        output_row_offset + 7 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y08, mask=\n        output_row_offset + 8 < output_numel)\n    tl.store(output_ptr + output_row_offset + 9, Y09, mask=\n        output_row_offset + 9 < output_numel)\n    tl.store(output_ptr + output_row_offset + 10, Y10, mask=\n        output_row_offset + 10 < output_numel)\n    tl.store(output_ptr + output_row_offset + 11, Y11, mask=\n        output_row_offset + 11 < output_numel)\n    tl.store(output_ptr + output_row_offset + 12, Y12, mask=\n        output_row_offset + 12 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "ef683b0b-5382-4e87-9ff1-b505a71bf5be"
  },
  {
    "input": "@triton.jit\ndef var_len_copy_kernel_triton(old_a_start, old_a_len, old_a_location,\n    new_a_start, new_a_location, BLOCK_SIZE: 'tl.constexpr'):\n    a_id = tl.program_id(0)\n    length = tl.load(old_a_len + a_id)\n    old_start = tl.load(old_a_start + a_id)\n    new_start = tl.load(new_a_start + a_id)\n    old_offset = tl.arange(0, BLOCK_SIZE)\n    new_offset = tl.arange(0, BLOCK_SIZE)\n    for i in range(0, length, BLOCK_SIZE):\n        v = tl.load(old_a_location + old_start + i + old_offset, mask=\n            old_offset < length)\n        tl.store(new_a_location + new_start + i + new_offset, v, mask=\n            new_offset < length)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "c38a3394-de9d-45b4-992e-4aedd54b1d74"
  },
  {
    "input": "@triton.jit\ndef _kernel_matmul_fp8_row_tma_persistent(A_ptr, B_ptr, C_ptr, M, N, K,\n    A_scale, B_scale, stride_am, stride_ak, stride_bn, stride_bk, stride_cm,\n    stride_cn, dot_out_dtype: 'tl.constexpr', allow_tf32: 'tl.constexpr',\n    fp8_fast_accum: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr',\n    AB_DTYPE: 'tl.constexpr', NUM_SMS: 'tl.constexpr') ->None:\n    \"\"\"Matmul kernel of [M, K] @ [N, K] with row-wise scales\n\n    performs swizzled matmul in [BLOCK_M, BLOCK_K] with [BLOCK_K, BLOCK_N] tiles.\n\n    Args:\n        A (TensorWrapper): [M, K] input tensor.\n        B (TensorWrapper): [N, K] input tensor.\n        C (TensorWrapper): [M, N] output tensor.\n        M (int): M dimension of input tensor.\n        N (int): N dimension of input tensor.\n        K (int): K dimension of input tensor.\n        A_scale (TensorWrapper): [M] reciprocal scale tensor per row. A * A_scale = original A\n        B_scale (TensorWrapper): [N] reciprocal scale tensor per row. B * B_scale = original B\n        stride_am (int): Stride of M dimension of A.\n        stride_ak (int): Stride of K dimension of A.\n        stride_bn (int): Stride of N dimension of B.\n        stride_bk (int): Stride of K dimension of B.\n        stride_cm (int): Stride of M dimension of C.\n        stride_cn (int): Stride of N dimension of C.\n        dot_out_dtype (torch.dtype): Output type of tensor core.\n        allow_tf32 (bool): Whether to use TF32 for tensor core.\n        fp8_fast_accum (bool): Whether to use fast accumulation for tensor core.\n        BLOCK_M (int): Block size for M dimension.\n        BLOCK_N (int): Block size for N dimension.\n        BLOCK_K (int): Block size for K dimension.\n        GROUP_M (int): Number of groups for M dimension swizzle.\n        SPLIT_K (int): Number of SM's to launch per row.\n        EVEN_K (bool): Whether K is evenly divisible by BLOCK_K * SPLIT_K.\n        AB_DTYPE (bool): Wether to cast A and B to C.dtype before tensor core.\n    \"\"\"\n    start_pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    k_tiles = tl.cdiv(K, BLOCK_K)\n    num_tiles = num_pid_m * num_pid_n\n    tiles_per_SM = num_tiles // NUM_SMS\n    if start_pid < num_tiles % NUM_SMS:\n        tiles_per_SM += 1\n    tile_id = start_pid - NUM_SMS\n    ki = -1\n    pid_m = 0\n    pid_n = 0\n    offs_am = 0\n    offs_bn = 0\n    num_pid_in_group = GROUP_M * num_pid_n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n    dtype_fp8 = tl.float8e4nv\n    scale_dtype = tl.float32\n    for _ in range(0, k_tiles * tiles_per_SM):\n        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n        if ki == 0:\n            tile_id += NUM_SMS\n            group_id = tile_id // num_pid_in_group\n            first_pid_m = group_id * GROUP_M\n            group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n            pid_m = first_pid_m + tile_id % group_size_m\n            pid_n = tile_id % num_pid_in_group // group_size_m\n            offs_am = pid_m * BLOCK_M\n            offs_bn = pid_n * BLOCK_N\n            offs_am = tl.multiple_of(offs_am, BLOCK_M)\n            offs_bn = tl.multiple_of(offs_bn, BLOCK_N)\n        offs_k = ki * BLOCK_K\n        a = tl._experimental_descriptor_load(A_ptr, [offs_am, offs_k], [\n            BLOCK_M, BLOCK_K], dtype_fp8)\n        b = tl._experimental_descriptor_load(B_ptr, [offs_bn, offs_k], [\n            BLOCK_N, BLOCK_K], dtype_fp8)\n        acc = tl.dot(a, b.T, acc, out_dtype=dot_out_dtype, allow_tf32=\n            allow_tf32)\n        if ki == k_tiles - 1:\n            rm = pid_m * BLOCK_M\n            rn = pid_n * BLOCK_N\n            a_scale = tl._experimental_descriptor_load(A_scale, [rm], [\n                BLOCK_M], scale_dtype)\n            b_scale = tl._experimental_descriptor_load(B_scale, [rn], [\n                BLOCK_N], scale_dtype)\n            scale = a_scale[:, None] * b_scale[None, :]\n            acc *= scale\n            acc = acc\n            tl._experimental_descriptor_store(C_ptr, acc, [rm, rn])\n            acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=dot_out_dtype)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "50af88ce-65fd-40be-9488-1a5e92d2593c"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, M, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    num_block, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_qz + off_h * stride_qh\n    V += off_z * stride_qz + off_h * stride_qh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_qz + off_h * stride_qh\n    DV += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        lo = start_n * BLOCK_M\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        D_ptrs = D + off_hz * N_CTX\n        m_ptrs = M + off_hz * N_CTX\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, k, trans_b=True)\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n            m = tl.load(m_ptrs + offs_m_curr)\n            p = tl.exp(qk * sm_scale - m[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(p, do, trans_a=True)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, v, trans_b=True)\n            ds = p * dp * sm_scale\n            dk += tl.dot(ds, q, trans_a=True)\n            dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n            dq += tl.dot(ds, k)\n            tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "433d6c07-ad1f-43ac-b42b-a3151f469960"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_int8kv(Q, K, V, sm_scale, Out, B_Start_Loc, B_Seqlen,\n    b_prompt_cache_len, stride_qbs, stride_qh, stride_qd, stride_kb,\n    stride_kh, stride_ks, stride_kd, stride_vb, stride_vh, stride_vs,\n    stride_vd, stride_obs, stride_oh, stride_od, kv_group_num, H:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    cur_bh = tl.program_id(1)\n    cur_batch = cur_bh // H\n    cur_head = cur_bh % H\n    cur_kv_head = cur_head // kv_group_num\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = block_start_loc + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    q = tl.load(Q + off_q, mask=offs_m[:, None] < cur_batch_seq_len, other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum(block_start_loc + BLOCK_M + prompt_cache_len,\n        cur_batch_seq_len + prompt_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        off_k = cur_batch * stride_kb + (start_n + offs_n[None, :]\n            ) * stride_ks + cur_kv_head * stride_kh + offs_d[:, None\n            ] * stride_kd\n        k = tl.load(K + off_k, mask=start_n + offs_n[None, :] <\n            block_end_loc, other=0.0)\n        qk = tl.dot(q, k)\n        mask = offs_m[:, None] + prompt_cache_len >= start_n + offs_n[None, :]\n        qk = tl.where(mask, qk * sm_scale, -100000000.0)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        off_v = cur_batch * stride_vb + (start_n + offs_n[:, None]\n            ) * stride_vs + cur_kv_head * stride_vh + offs_d[None, :\n            ] * stride_vd\n        v = tl.load(V + off_v, mask=start_n + offs_n[:, None] <\n            block_end_loc, other=0.0)\n        p = p\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n    acc = acc / l_i[:, None]\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "8504a234-510b-4949-b34f-43555a0bc7f4"
  },
  {
    "input": "@triton.jit\ndef relu(x):\n    \"\"\"\n    ReLU_ activation function\n\n    .. _ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n    \"\"\"\n    zero = 0.0\n    return tl.where(x >= 0, x, zero)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "7a233855-2baf-42e3-a95d-40634895c659"
  },
  {
    "input": "@triton.jit\ndef _row_indices_kernel(offsets, out):\n    pid = tl.program_id(0)\n    row_offset = tl.load(offsets + pid)\n    nnz_blocks = tl.load(offsets + pid + 1) - row_offset\n    for nnz_block in range(nnz_blocks):\n        tl.store(out + row_offset + nnz_block, pid)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "a69700eb-33f8-4101-a048-ce2a1c7c9461"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, TMP, L, M, Out, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh,\n    stride_om, stride_on, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    off_k = off_hz * stride_qh + offs_n[:, None] * stride_kn + offs_d[None, :\n        ] * stride_kk\n    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    q_ptrs = Q + off_q\n    k_ptrs = K + off_k\n    v_ptrs = V + off_v\n    t_ptrs = TMP + off_hz * N_CTX + offs_m\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    q = tl.load(q_ptrs)\n    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(k_ptrs + start_n * stride_kn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        qk *= sm_scale\n        qk += tl.where(offs_m[:, None] >= start_n + offs_n[None, :], 0,\n            float('-inf'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        tl.store(t_ptrs, acc_scale)\n        acc_scale = tl.load(t_ptrs)\n        acc = acc * acc_scale[:, None]\n        v = tl.load(v_ptrs + start_n * stride_vk)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, l_i)\n    tl.store(m_ptrs, m_i)\n    offs_n = tl.arange(0, BLOCK_DMODEL)\n    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :\n        ] * stride_on\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "ed404271-a945-47f3-81b5-2e63ff2e15fc"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BS': 32}, num_warps=2), triton.\n    Config({'BS': 32}, num_warps=4), triton.Config({'BS': 32}, num_warps=8),\n    triton.Config({'BS': 64}, num_warps=2), triton.Config({'BS': 64},\n    num_warps=4), triton.Config({'BS': 64}, num_warps=8), triton.Config({\n    'BS': 128}, num_warps=2), triton.Config({'BS': 128}, num_warps=4),\n    triton.Config({'BS': 128}, num_warps=8)], key=['S'])\n@triton.jit\ndef recurrent_cumsum_bwd_kernel(ds, dz, s_s_h, s_s_t, T: 'tl.constexpr', S:\n    'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_s = i_s * BS + tl.arange(0, BS)\n    mask = o_s < S\n    b_ds = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(T - 1, -1, -1):\n        b_dz = tl.load(dz + i_bh * s_s_h + i_t * s_s_t + o_s, mask=mask,\n            other=0)\n        b_ds = b_ds + b_dz\n        tl.store(ds + i_bh * s_s_h + i_t * s_s_t + o_s, b_ds, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "49754c07-9a5c-4234-8b2f-9971c58d7b82"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_K(v, z, do, dA, s_v_h, s_v_t, s_v_d, scale,\n    T: 'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (i_bh + i_v * n_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_zn = tl.load(p_zn, boundary_check=(0,))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_zn[None, :] - b_z) * scale\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_v = tl.exp(b_v - b_zn[:, None])\n        b_dA = tl.dot(b_do, b_v, allow_tf32=False)\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n            BT + i_j * BC) * V + i_v * BV,), (BV,), (0,))\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_v * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_v = tl.load(p_v, boundary_check=(0,))\n            b_dA = tl.sum(b_do * tl.exp(b_v[None, :] - b_z), 1)\n            b_dA = tl.where(o_i >= j, b_dA, 0)\n            tl.store(dA + o_A + j, b_dA, mask=m_A)\n            p_v = tl.advance(p_v, (V,))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "32cddcaf-20a0-4746-8e1b-fa469815bf87"
  },
  {
    "input": "@triton.jit\ndef _triton_second_order_bwd(x_ptr: 'tl.tensor', y_ptr: 'tl.tensor', z_ptr:\n    'tl.tensor', g_x_ptr: 'tl.tensor', g_y_ptr: 'tl.tensor', g_z_ptr:\n    'tl.tensor', g_1_0_ptr: 'tl.tensor', g_1_1_ptr: 'tl.tensor', g_1_2_ptr:\n    'tl.tensor', g_2_0_ptr: 'tl.tensor', g_2_1_ptr: 'tl.tensor', g_2_2_ptr:\n    'tl.tensor', g_2_3_ptr: 'tl.tensor', g_2_4_ptr: 'tl.tensor', BLOCK_SIZE:\n    'tl.constexpr', vector_length: 'tl.constexpr'):\n    sqrt_3 = 3 ** 0.5\n    sqrt_5 = 5 ** 0.5\n    sqrt_15 = 15 ** 0.5\n    block_id = tl.program_id(0)\n    offset = tl.arange(0, BLOCK_SIZE) + BLOCK_SIZE * block_id\n    x_row_start = x_ptr + offset\n    y_row_start = y_ptr + offset\n    z_row_start = z_ptr + offset\n    x = tl.load(x_row_start, mask=offset < vector_length)\n    y = tl.load(y_row_start, mask=offset < vector_length)\n    z = tl.load(z_row_start, mask=offset < vector_length)\n    g_1_0 = tl.load(g_1_0_ptr + offset, mask=offset < vector_length)\n    g_1_1 = tl.load(g_1_1_ptr + offset, mask=offset < vector_length)\n    g_1_2 = tl.load(g_1_2_ptr + offset, mask=offset < vector_length)\n    g_x = sqrt_3 * g_1_0\n    g_y = sqrt_3 * g_1_1\n    g_z = sqrt_3 * g_1_2\n    g_2_0 = tl.load(g_2_0_ptr + offset, mask=offset < vector_length)\n    g_2_1 = tl.load(g_2_1_ptr + offset, mask=offset < vector_length)\n    g_2_2 = tl.load(g_2_2_ptr + offset, mask=offset < vector_length)\n    g_2_3 = tl.load(g_2_3_ptr + offset, mask=offset < vector_length)\n    g_2_4 = tl.load(g_2_4_ptr + offset, mask=offset < vector_length)\n    g_x += sqrt_15 * z * g_2_0\n    g_z += sqrt_15 * x * g_2_0\n    g_x += sqrt_15 * y * g_2_1\n    g_y += sqrt_15 * x * g_2_1\n    g_y += sqrt_15 * z * g_2_2\n    g_z += sqrt_15 * y * g_2_2\n    g_x += -1.0 * sqrt_5 * x * g_2_3\n    g_y += 2.0 * sqrt_5 * y * g_2_3\n    g_z += -1.0 * sqrt_5 * z * g_2_3\n    g_x += -1.0 * sqrt_15 * x * g_2_4\n    g_z += sqrt_15 * z * g_2_4\n    tl.store(g_x_ptr + offset, g_x, mask=offset < vector_length)\n    tl.store(g_y_ptr + offset, g_y, mask=offset < vector_length)\n    tl.store(g_z_ptr + offset, g_z, mask=offset < vector_length)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5d4097a0-48a7-4ff2-a06a-65240be34924"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_fwd_kernel_intra_V(q, k, g, A, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', NC: 'tl.constexpr', NK: 'tl.constexpr', NG: 'tl.constexpr'\n    ):\n    i_c, i_bh = tl.program_id(0), tl.program_id(1)\n    for i_k in range(0, NK):\n        chunk_gsa_fwd_kernel_intra_Vk(q, k, g, A, i_k, i_c, i_bh, scale, T=\n            T, K=K, BT=BT, BC=BC, BK=BK, NC=NC, NG=NG)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "2dd07be3-00c2-4372-82de-32d55e3a34f1"
  },
  {
    "input": "@triton.jit\ndef _rmsnorm_fwd_kernel(X, Y, W, Rstd, stride_x_row, stride_y_row, N, eps,\n    BLOCK_N: 'tl.constexpr', IS_EVEN_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    if IS_EVEN_N:\n        w = tl.load(W + cols)\n    else:\n        w = tl.load(W + cols, mask=mask)\n    x_hat = x * rstd\n    y = x_hat * w\n    if IS_EVEN_N:\n        tl.store(Y + cols, y)\n    else:\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "eea6a172-c56d-4be1-9143-19682d1ace8c"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "542af0c2-3e1b-4ab3-872d-8529b4b7ce6d"
  },
  {
    "input": "@conv_heuristics()\n@triton.jit\ndef _kernel_delta_x_hwc(x, w, y, stride_xn, stride_xc, stride_xh, stride_xw,\n    stride_wn, stride_wc, stride_wh, stride_ww, stride_yn, stride_yc,\n    stride_yh, stride_yw, stride_biasn, delta_xh_ptr, delta_xw_ptr,\n    delta_xc_ptr, BATCH, IN_C, IN_H, IN_W, KERNEL_N, KERNEL_H, KERNEL_W,\n    OUT_H, OUT_W, stride_h, stride_w, padding_h, padding_w, dilation_h,\n    dilation_w, output_padding_h, output_padding_w, groups, ACC_TYPE:\n    'tl.constexpr', CONV1X1_NHWC: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', BLOCK_K: 'tl.constexpr', GROUP_H: 'tl.constexpr'):\n    \"\"\"\n    each program instance computes a [BLOCK_BATCH, BLOCK_N, BLOCK_H, BLOCK_W] block of y\n    \"\"\"\n    pid_nhw = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    off_x_n = off_y_n\n    off_x_h = off_y_h * stride_h - padding_h\n    off_x_w = off_y_w * stride_w - padding_w\n    off_x_nhw = off_x_n * stride_xn + off_x_h * stride_xh + off_x_w * stride_xw\n    off_x_crs = tl.arange(0, BLOCK_K)\n    CRS = IN_C * KERNEL_H * KERNEL_W\n    if not CONV1X1_NHWC:\n        delta_xh_ptrs = delta_xh_ptr + off_x_crs\n        delta_xw_ptrs = delta_xw_ptr + off_x_crs\n        delta_xc_ptrs = delta_xc_ptr + off_x_crs\n        delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n        delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n        off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw * stride_xw +\n            delta_xc * stride_xc)\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n    else:\n        x_ptrs = x + off_x_nhw[:, None] + off_x_crs[None, :]\n        delta_xh = 0\n        delta_xw = 0\n    mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n        off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n        delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None, :] >= 0\n        ) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n    off_w_crs = tl.arange(0, BLOCK_K)\n    off_w_k = off_y_k\n    w_ptrs = w + off_w_crs[:, None] + off_w_k[None, :] * stride_wn\n    mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n    matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n    matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for crs in range(0, CRS, BLOCK_K):\n        acc += tl.dot(matrix_x, matrix_w)\n        w_ptrs += BLOCK_K\n        off_x_crs = crs + BLOCK_K + tl.arange(0, BLOCK_K)\n        if not CONV1X1_NHWC:\n            delta_xh_ptrs += BLOCK_K\n            delta_xw_ptrs += BLOCK_K\n            delta_xc_ptrs += BLOCK_K\n            delta_xh = tl.load(delta_xh_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xw = tl.load(delta_xw_ptrs, mask=off_x_crs < CRS, other=0)\n            delta_xc = tl.load(delta_xc_ptrs, mask=off_x_crs < CRS, other=0)\n            off_x_crs_unpacked = (delta_xh * stride_xh + delta_xw *\n                stride_xw + delta_xc * stride_xc)\n            x_ptrs = x + off_x_nhw[:, None] + off_x_crs_unpacked[None, :]\n        else:\n            x_ptrs += BLOCK_K\n        mask_x = (off_x_n < BATCH)[:, None] & (off_x_crs < CRS)[None, :] & (\n            off_x_h[:, None] + delta_xh[None, :] >= 0) & (off_x_h[:, None] +\n            delta_xh[None, :] < IN_H) & (off_x_w[:, None] + delta_xw[None,\n            :] >= 0) & (off_x_w[:, None] + delta_xw[None, :] < IN_W)\n        mask_w = (off_x_crs < CRS)[:, None] & (off_w_k < KERNEL_N)[None, :]\n        matrix_x = tl.load(x_ptrs, mask=mask_x, other=0.0)\n        matrix_w = tl.load(w_ptrs, mask=mask_w, other=0.0)\n    acc = acc\n    off_y_k = pid_k * BLOCK_N + tl.arange(0, BLOCK_N)\n    off_y_nhw = pid_nhw * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_y_n = off_y_nhw // (OUT_H * OUT_W)\n    off_y_hw = off_y_nhw % (OUT_H * OUT_W)\n    off_y_h = off_y_hw // OUT_W + output_padding_h\n    off_y_w = off_y_hw % OUT_W + output_padding_w\n    y_ptrs = y + off_y_n[:, None] * stride_yn + off_y_h[:, None\n        ] * stride_yh + off_y_w[:, None] * stride_yw + off_y_k[None, :\n        ] * stride_yc\n    mask_y = (off_y_n < BATCH)[:, None] & (off_y_h < OUT_H + output_padding_h)[\n        :, None] & (off_y_w < OUT_W + output_padding_w)[:, None] & (off_y_k <\n        KERNEL_N)[None, :]\n    tl.store(y_ptrs, acc, mask=mask_y)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "37f0b069-eda5-46e9-b35c-1cda4ac5f3a3"
  },
  {
    "input": "@triton.jit\ndef eighth_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST000 = 1.12741169450483\n    CONST003 = 4.12310562561766\n    CONST004 = 4.50964677801932\n    CONST006 = 6.76447016702898\n    CONST007 = 1.69594242329302\n    CONST008 = 1.88707052233084\n    CONST010 = 2.58397773170915\n    CONST011 = 13.136713523081\n    CONST012 = 13.136713523081\n    CONST014 = -489.184589393411\n    CONST015 = 24.738633753706\n    CONST017 = 24.738633753706\n    CONST019 = 48.9184589393411\n    CONST020 = 48.5105296237322\n    CONST021 = 51.744564931981\n    CONST024 = 65.6835676154051\n    CONST025 = 67.8376969317208\n    CONST029 = 97.0210592474644\n    CONST030 = -6.78376969317208\n    CONST031 = 103.489129863962\n    CONST032 = -407.026181590325\n    CONST033 = 108.231522672464\n    CONST035 = 110.066532613517\n    CONST036 = 110.066532613517\n    CONST037 = -396.284809689477\n    CONST040 = -361.756882439281\n    CONST041 = -1.88707052233084\n    CONST042 = 158.513923875791\n    CONST045 = 180.87844121964\n    CONST046 = 194.042118494929\n    CONST047 = -12.2296147348353\n    CONST048 = 203.513090795162\n    CONST050 = 216.463045344927\n    CONST051 = 217.054129463568\n    CONST052 = 216.463045344927\n    CONST053 = -6.78376969317208\n    CONST054 = -271.350787726883\n    CONST055 = 244.592294696706\n    CONST056 = 244.592294696706\n    CONST057 = -262.734270461621\n    CONST058 = -258.722824659905\n    CONST061 = -217.054129463568\n    CONST062 = -210.187416369296\n    CONST063 = -175.156180307747\n    CONST064 = -162.81047263613\n    CONST066 = -144.702752975712\n    CONST067 = -129.877827206956\n    CONST068 = -129.361412329953\n    CONST070 = -108.231522672464\n    CONST071 = -108.231522672464\n    CONST072 = -87.5780901538735\n    CONST073 = -3.23403530824881\n    CONST074 = -72.3513764878561\n    CONST075 = -70.0624721230988\n    CONST076 = -65.6835676154052\n    CONST077 = -61.1480736741764\n    CONST078 = -61.1480736741764\n    CONST079 = -57.7234787586472\n    CONST080 = -57.7234787586472\n    CONST081 = -51.744564931981\n    CONST082 = -48.5105296237322\n    CONST083 = -40.5868210021738\n    CONST084 = -39.4101405692431\n    CONST085 = -40.7026181590325\n    CONST086 = -36.0771742241545\n    CONST087 = -36.0771742241545\n    CONST088 = -26.4189873126318\n    CONST089 = -20.6718218536732\n    CONST090 = -528.379746252636\n    CONST091 = -16.9594242329302\n    CONST092 = -13.136713523081\n    CONST093 = -12.2296147348353\n    CONST094 = -11.3224231339851\n    CONST095 = -10.3359109268366\n    CONST096 = -9.70210592474644\n    CONST097 = -11.3224231339851\n    CONST098 = -13.5289403340579\n    CONST099 = -6.78376969317208\n    CONST100 = -13.5289403340579\n    CONST101 = -13.136713523081\n    CONST102 = -3.23403530824881\n    CONST103 = -1.61701765412441\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR02 = VAR06 * VAR06\n    VAR03 = VAR06 * VAR07\n    VAR04 = VAR07 * VAR07\n    VAR05 = VAR07 * VAR08\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR11 = VAR15 * VAR16\n    VAR12 = VAR15 * VAR16\n    VAR13 = VAR16 * VAR16\n    VAR14 = VAR16 * VAR17\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR20 = VAR24 * VAR24\n    VAR21 = VAR24 * VAR25\n    VAR22 = VAR25 * VAR25\n    VAR23 = VAR25 * VAR26\n    Y00 = (-CONST066 * VAR05 * VAR25 + CONST066 * VAR07 * VAR23 + CONST089 *\n        VAR03 * z - CONST089 * VAR21 * x)\n    Y01 = y * (CONST040 * VAR07 * VAR24 + CONST051 * VAR05 * VAR26 - \n        CONST074 * VAR22 * x + CONST095 * VAR03)\n    Y02 = CONST097 * VAR03 * z + VAR05 * (CONST042 * VAR17 * z - CONST088 *\n        VAR25) + VAR07 * (-CONST088 * VAR23 + CONST090 * VAR17 * VAR25) + x * (\n        CONST042 * VAR17 * VAR23 + CONST094 * VAR21)\n    Y03 = VAR16 * (CONST014 * VAR07 * VAR26 + CONST019 * VAR05 + CONST055 *\n        VAR24 * x) + y * (CONST035 * VAR05 * VAR26 + CONST077 * VAR22 * x -\n        CONST078 * VAR07 * VAR24 + CONST093 * VAR03)\n    Y04 = CONST099 * VAR03 * z + VAR05 * (-CONST064 * VAR17 * z + CONST099 *\n        VAR25) + VAR07 * (-CONST053 * VAR23 + CONST054 * VAR15 * z) + x * (\n        -CONST053 * VAR21 - CONST054 * VAR15 * VAR25 + CONST064 * VAR17 * VAR23\n        )\n    Y05 = VAR14 * (-CONST062 * VAR26 * x + CONST075 * VAR07) + VAR16 * (\n        CONST057 * VAR24 * x + CONST063 * VAR07 * VAR26 - CONST072 * VAR05\n        ) + y * (CONST011 * VAR05 * VAR26 + CONST024 * VAR07 * VAR24 - \n        CONST084 * VAR22 * x + CONST092 * VAR03)\n    Y06 = CONST102 * VAR03 * z + VAR05 * (CONST029 * VAR17 * z + CONST096 *\n        VAR25) + VAR07 * (CONST046 * VAR17 * VAR25 + CONST058 * VAR15 * z +\n        CONST096 * VAR23) + x * (CONST029 * VAR17 * VAR23 + CONST031 *\n        VAR13 * z + CONST058 * VAR15 * VAR25 + CONST102 * VAR21)\n    Y07 = CONST098 * VAR03 * y + VAR05 * (CONST033 * VAR16 + CONST083 *\n        VAR26 * y) + VAR07 * (CONST050 * VAR16 * VAR26 + CONST067 * VAR14 +\n        CONST083 * VAR24 * y) + x * (CONST015 * VAR12 + CONST067 * VAR14 *\n        VAR26 - CONST070 * VAR16 * VAR24 + CONST098 * VAR22 * y)\n    Y08 = (CONST000 * VAR02 + CONST000 * VAR20 + CONST003 * VAR11 - \n        CONST070 * VAR15 * VAR24 + CONST080 * VAR13 * VAR26 + CONST087 *\n        VAR17 * VAR22 + VAR04 * (CONST004 * VAR26 + CONST086 * VAR17) + \n        VAR06 * (CONST006 * VAR24 - CONST070 * VAR15 + CONST071 * VAR17 *\n        VAR26) + VAR08 * (CONST004 * VAR22 + CONST050 * VAR15 * VAR26 + \n        CONST070 * VAR17 * VAR24 + CONST079 * VAR13))\n    Y09 = CONST098 * VAR21 * y + VAR23 * (CONST033 * VAR16 + CONST083 *\n        VAR08 * y) + VAR25 * (CONST052 * VAR08 * VAR16 + CONST067 * VAR14 +\n        CONST083 * VAR06 * y) + z * (CONST017 * VAR12 + CONST033 * VAR06 *\n        VAR16 + CONST067 * VAR08 * VAR14 + CONST100 * VAR04 * y)\n    Y10 = (CONST073 * VAR08 * VAR22 - CONST102 * VAR04 * VAR26 - CONST103 *\n        VAR02 + CONST103 * VAR20 + VAR13 * (CONST021 * VAR26 + CONST081 *\n        VAR08) + VAR15 * (-CONST068 * VAR06 + CONST068 * VAR24) + VAR17 * (\n        CONST020 * VAR08 * VAR24 + CONST020 * VAR22 + CONST082 * VAR04 + \n        CONST082 * VAR06 * VAR26))\n    Y11 = VAR14 * (CONST062 * VAR08 * z - CONST075 * VAR25) + VAR16 * (-\n        CONST057 * VAR06 * z - CONST063 * VAR08 * VAR25 + CONST072 * VAR23\n        ) + y * (CONST012 * VAR21 + CONST076 * VAR06 * VAR25 + CONST084 *\n        VAR04 * z + CONST101 * VAR08 * VAR23)\n    Y12 = (CONST007 * VAR02 + CONST007 * VAR20 + CONST030 * VAR04 * VAR26 +\n        CONST053 * VAR08 * VAR22 + CONST091 * VAR06 * VAR24 + VAR15 * (\n        CONST025 * VAR06 + CONST025 * VAR24 + CONST032 * VAR08 * VAR26) + \n        VAR17 * (CONST048 * VAR06 * VAR26 + CONST048 * VAR08 * VAR24 + \n        CONST085 * VAR04 + CONST085 * VAR22))\n    Y13 = VAR16 * (CONST014 * VAR08 * VAR25 + CONST019 * VAR23 + CONST056 *\n        VAR06 * z) + y * (CONST036 * VAR08 * VAR23 + CONST047 * VAR21 - \n        CONST077 * VAR06 * VAR25 + CONST078 * VAR04 * z)\n    Y14 = (CONST008 * VAR02 + CONST041 * VAR20 + CONST088 * VAR04 * VAR26 -\n        CONST088 * VAR08 * VAR22 + VAR17 * (-CONST037 * VAR06 * VAR26 + \n        CONST037 * VAR08 * VAR24 + CONST088 * VAR04 - CONST088 * VAR22))\n    Y15 = y * (-CONST040 * VAR06 * VAR25 + CONST061 * VAR08 * VAR23 + \n        CONST074 * VAR04 * z - CONST095 * VAR21)\n    Y16 = (CONST010 * VAR02 + CONST010 * VAR20 + CONST045 * VAR06 * VAR24 +\n        CONST074 * VAR04 * VAR26 + CONST074 * VAR08 * VAR22)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y07, mask=\n        output_row_offset + 7 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y08, mask=\n        output_row_offset + 8 < output_numel)\n    tl.store(output_ptr + output_row_offset + 9, Y09, mask=\n        output_row_offset + 9 < output_numel)\n    tl.store(output_ptr + output_row_offset + 10, Y10, mask=\n        output_row_offset + 10 < output_numel)\n    tl.store(output_ptr + output_row_offset + 11, Y11, mask=\n        output_row_offset + 11 < output_numel)\n    tl.store(output_ptr + output_row_offset + 12, Y12, mask=\n        output_row_offset + 12 < output_numel)\n    tl.store(output_ptr + output_row_offset + 13, Y13, mask=\n        output_row_offset + 13 < output_numel)\n    tl.store(output_ptr + output_row_offset + 14, Y14, mask=\n        output_row_offset + 14 < output_numel)\n    tl.store(output_ptr + output_row_offset + 15, Y15, mask=\n        output_row_offset + 15 < output_numel)\n    tl.store(output_ptr + output_row_offset + 16, Y16, mask=\n        output_row_offset + 16 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "ef26e731-a2fa-4778-8d2a-b2a4bb994744"
  },
  {
    "input": "@triton.jit\ndef add_vec_block_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: 'tl.constexpr',\n    B1: 'tl.constexpr'):\n    block_id_x = tl.program_id(0)\n    block_id_y = tl.program_id(1)\n    off_x = block_id_x * B0 + tl.arange(0, B0)\n    off_y = block_id_y * B1 + tl.arange(0, B1)\n    off_z = off_y[:, None] * N0 + off_x[None, :]\n    mask_x = off_x < N0\n    mask_y = off_y < N1\n    mask_z = mask_y[:, None] & mask_x[None, :]\n    x = tl.load(x_ptr + off_x, mask=mask_x)\n    y = tl.load(y_ptr + off_y, mask=mask_y)\n    z = y[:, None] + x[None, :]\n    tl.store(z_ptr + off_z, z, mask=mask_z)\n    return\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "7309b900-38c1-452c-9b60-be2dcd9761ce"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4,\n    num_warps=8)], key=['chunk_size', 'hdim', 'dstate'])\n@triton.jit\ndef _chunk_scan_fwd_kernel_wip(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr,\n    dt_ptr, dA_cumsum_ptr, seq_idx_ptr, C_ptr, B_ptr, prev_states_ptr,\n    D_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio,\n    stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m,\n    stride_cb_csize_k, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_z_batch, stride_z_seqlen, stride_z_head,\n    stride_z_hdim, stride_out_batch, stride_out_seqlen, stride_out_head,\n    stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_C_batch, stride_C_seqlen, stride_C_head,\n    stride_C_dstate, stride_B_batch, stride_B_seqlen, stride_B_head,\n    stride_B_dstate, stride_states_batch, stride_states_chunk,\n    stride_states_head, stride_states_hdim, stride_states_dstate,\n    stride_D_head, HAS_D: 'tl.constexpr', D_HAS_HDIM: 'tl.constexpr', HAS_Z:\n    'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_DSTATE:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_n = tl.program_id(axis=0)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_C_head)\n    B_ptr += (pid_b * stride_B_batch + pid_c * chunk_size * stride_B_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_B_head)\n    prev_states_ptr += (pid_b * stride_states_batch + pid_c *\n        stride_states_chunk + pid_h * stride_states_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    offs_m = tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate[\n        None, :] * stride_C_dstate)\n    B_ptrs = B_ptr + (offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:,\n        None] * stride_B_dstate)\n    prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n        stride_states_hdim + offs_k_dstate[:, None] * stride_states_dstate)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_m[None,\n        :] * stride_cb_csize_k)\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    out_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None,\n        :] * stride_out_hdim)\n    prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:, None] <\n        dstate) & (offs_n[None, :] < hdim), other=0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):\n        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)\n        dA_cs_m = tl.load(dA_cumsum_ptr + (start_m + offs_m) *\n            stride_dA_cs_csize, mask=offs_m < chunk_size - start_m, other=0.0)\n        if HAS_SEQ_IDX:\n            seq_idx_prev = tl.load(seq_idx_ptr + start_m -\n                stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n            seq_idx_m = tl.load(seq_idx_ptr + (start_m + offs_m) *\n                stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit -\n                start_m, other=-1)\n        if not HAS_SEQ_IDX:\n            scale_m = tl.exp(dA_cs_m)\n        else:\n            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit -\n            start_m) & (offs_k_dstate[None, :] < dstate), other=0.0)\n        acc = tl.dot(C, prev_states) * scale_m[:, None]\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0)\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit -\n            start_m) & (offs_n[None, :] < hdim), other=0.0)\n        if HAS_D:\n            if D_HAS_HDIM:\n                D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=\n                    offs_n < hdim, other=0.0)\n            else:\n                D = tl.load(D_ptr + pid_h * stride_D_head)\n            acc += x * D\n        tl.store(out_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit -\n            start_m) & (offs_n[None, :] < hdim))\n        if start_m + BLOCK_SIZE_M < chunk_size_limit:\n            B = tl.load(B_ptrs, mask=(offs_m[None, :] < chunk_size_limit -\n                start_m) & (offs_k_dstate[:, None] < dstate), other=0.0)\n            dA_cs_last = tl.load(dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) *\n                stride_dA_cs_csize)\n            scale = tl.exp(dA_cs_last - dA_cs_m) * dt_m\n            B = B\n            tmp = tl.dot(B, x)\n            prev_states += tmp\n        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen\n        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen\n        cb_ptrs += (BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M *\n            stride_cb_csize_k)\n        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize\n        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "17d2b36e-f85a-4dda-a99d-2dd8f7a734d9"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim', 'dstate'])\n@triton.jit\ndef _chunk_scan_chunk_state_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, D_ptr, b_ptr, dstates_ptr, dx_ptr, ddt_ptr,\n    dD_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio,\n    stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim,\n    stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m,\n    stride_cb_csize_k, stride_dout_batch, stride_dout_seqlen,\n    stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk,\n    stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_D_head, stride_b_batch, stride_b_seqlen,\n    stride_b_head, stride_b_dstate, stride_dstates_batch,\n    stride_dstates_chunk, stride_dstates_head, stride_dstates_hdim,\n    stride_dstates_dstate, stride_dx_batch, stride_dx_seqlen,\n    stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk,\n    stride_ddt_head, stride_ddt_csize, stride_dD_batch, stride_dD_chunk,\n    stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D: 'tl.constexpr',\n    D_HAS_HDIM: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr', IS_TRITON_22:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + pid_h * stride_dstates_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size_limit, other=0.0)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize)\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last -\n            dA_cs_m), 0.0)\n    offs_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if IS_TRITON_22 and \n        BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_dstate[None,\n        :] * stride_b_dstate)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_dstates_hdim + \n        offs_dstate[:, None] * stride_dstates_dstate)\n    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_dstate[None, :] < dstate), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_dstate[:, None] < dstate\n            ) & (offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates\n        acc = tl.dot(b, dstates) * scale[:, None]\n    else:\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_dstate[None, :] < dstate - k), other=0.0)\n            dstates = tl.load(dstates_ptrs, mask=(offs_dstate[:, None] < \n                dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n            dstates = dstates\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_dstates_dstate\n        acc *= scale[:, None]\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None,\n        :] * stride_cb_csize_k)\n    dout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    K_MAX = chunk_size_limit\n    K_MIN = pid_m * BLOCK_SIZE_M\n    cb_ptrs += K_MIN * stride_cb_csize_k\n    dout_ptrs += K_MIN * stride_dout_seqlen\n    dA_cumsum_ptrs += K_MIN * stride_dA_cs_csize\n    for k in range(K_MIN, K_MAX, BLOCK_SIZE_K):\n        k = tl.multiple_of(k, BLOCK_SIZE_K)\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k\n            [None, :] < K_MAX - k), other=0.0)\n        dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0)\n        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None,\n            :] < K_MAX)\n        cb = tl.where(mask, cb, 0.0)\n        cb = cb\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (pid_b * stride_dx_batch + pid_c * chunk_size *\n        stride_dx_seqlen + pid_h * stride_dx_head)\n    dx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :\n        ] * stride_dx_hdim)\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + \n            offs_n[None, :] * stride_dout_hdim)\n        dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] <\n            chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        dx += dout_res * D\n    tl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    if HAS_D:\n        dD_ptr += (pid_b * stride_dD_batch + pid_c * stride_dD_chunk + \n            pid_h * stride_dD_head + pid_m * stride_dD_csize)\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n            dD = tl.sum(dout_res * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        else:\n            dD = tl.sum(dout_res * x)\n            tl.store(dD_ptr, dD)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "61ad742f-26cc-431a-91f5-b5831ef6be8f"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef act_func_forward_kernel(input_pointer, output_pointer, size, drop_p,\n    seed, param, act_func: 'tl.constexpr', dropout: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Applies an activation function to the input, optionally fusing dropout.\n\n    Args:\n        input_pointer: Pointer to the input to transform.\n            The input must be of shape [size].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element if dropout is True.\n        seed: Seed for generating the dropout mask if dropout is True.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        dropout: Flag for performing dropout on the activation output.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input = tl.load(input_pointer + offset, mask=mask)\n    tl.store(output_pointer + offset, apply_act_func(input, drop_p, seed,\n        offset, param, act_func, dropout), mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "b1bc6e27-04f2-4ebc-9c13-393bb7768dec"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8,\n    pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=\n    init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=\n    init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=\n    init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages\n    =5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr',\n    'ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])\n@triton.jit\ndef _chunk_state_bwd_dx_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr,\n    dA_cumsum_ptr, dx_ptr, ddt_ptr, ddA_cumsum_ptr, chunk_size, hdim,\n    dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch,\n    stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch,\n    stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_batch,\n    stride_dstates_chunk, stride_states_head, stride_states_hdim,\n    stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_dx_batch,\n    stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch,\n    stride_ddt_chunk, stride_ddt_head, stride_ddt_csize,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr', BLOCK_SIZE_DSTATE:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + pid_h * stride_states_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    offs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else\n        BLOCK_SIZE_K)\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] *\n        stride_b_dstate)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + \n        offs_k[:, None] * stride_states_dstate)\n    if BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < dstate), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates\n        acc = tl.dot(b, dstates)\n    else:\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_k[None, :] < dstate - k), other=0.0)\n            dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate -\n                k) & (offs_n[None, :] < hdim), other=0.0)\n            dstates = dstates\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0)\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0)\n    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n    ddA_cs = -(ddt * dt_m)\n    ddA_cs_last = -tl.sum(ddA_cs)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize,\n        ddA_cs_last)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (pid_b * stride_dx_batch + pid_c * chunk_size *\n        stride_dx_seqlen + pid_h * stride_dx_head)\n    dx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :\n        ] * stride_dx_hdim)\n    tl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "66d47ff9-9155-4610-8626-42b4b4dd28ba"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0e7e2188-0f64-4699-a05c-396d38c0495a"
  },
  {
    "input": "@triton.jit\ndef dequantize_row_q8_triton(Q, S, M, K, stride_am, stride_ak, stride_qm,\n    stride_qk, stride_sm, A, MCache: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_k = tl.program_id(axis=1)\n    A_Block_ptr = tl.make_block_ptr(base=A, shape=(M, K), block_shape=(\n        BLOCK_SIZE_M, BLOCK_SIZE_K), offsets=(pid_m * BLOCK_SIZE_M, pid_k *\n        BLOCK_SIZE_K), strides=(stride_am, stride_ak), order=(0, 1))\n    Q_Block_ptr = tl.make_block_ptr(base=Q, shape=(M, K), block_shape=(\n        BLOCK_SIZE_M, BLOCK_SIZE_K), offsets=(pid_m * BLOCK_SIZE_M, pid_k *\n        BLOCK_SIZE_K), strides=(stride_qm, stride_qk), order=(0, 1))\n    S_Block_ptr = tl.make_block_ptr(base=S, shape=(M,), block_shape=(\n        BLOCK_SIZE_M,), offsets=(pid_m * BLOCK_SIZE_M,), strides=(stride_sm\n        ,), order=(0,))\n    quants = tl.load(Q_Block_ptr)\n    scale = tl.load(S_Block_ptr)\n    out = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_K], tl.float32)\n    out += quants * scale[:, None]\n    tl.store(A_Block_ptr, out)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "39dbdf88-18f8-417b-b3f5-194c8d07308f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['BLOCK_N_ZERO', 'BLOCK_ROW', 'MAX_INTERP'])\n@triton.jit\ndef __scan_col_3_compute(NON_ZERO_ROWS, stride_nzr_n, stride_nzr_d,\n    PIXEL_INDICES, stride_pixel_n, stride_pixel_m, V_STARTS, stride_vs_tdst,\n    stride_vs_tm, COL_INDICES, stride_col_n, stride_col_z, N, M, H, T_M,\n    TARGET_WIDTH_MAX: 'tl.constexpr', MAX_INTERP: 'tl.constexpr', NZR_N,\n    NZR_D, BLOCK_N_ZERO: 'tl.constexpr', NCOL_PER_ROW, BLOCK_ROW:\n    'tl.constexpr'):\n    pid_nzr = tl.program_id(0)\n    pid_col = tl.program_id(1)\n    for _i_nzr in range(BLOCK_N_ZERO):\n        i_nzr = pid_nzr * BLOCK_N_ZERO + _i_nzr\n        mask_nzr = i_nzr < NZR_N\n        i_batch = tl.load(NON_ZERO_ROWS + i_nzr * stride_nzr_n + 0 *\n            stride_nzr_d, mask=mask_nzr)\n        i_row = tl.load(NON_ZERO_ROWS + i_nzr * stride_nzr_n + 1 *\n            stride_nzr_d, mask=mask_nzr)\n        n = i_batch\n        ms = pid_col * BLOCK_ROW + tl.arange(0, BLOCK_ROW\n            ) + i_row * NCOL_PER_ROW\n        ms_mask = pid_col * BLOCK_ROW + tl.arange(0, BLOCK_ROW) < NCOL_PER_ROW\n        idx_tdst = ms // (H * T_M)\n        idx_h = ms % (H * T_M) // T_M\n        idx_tm = ms % T_M\n        v_start = tl.load(V_STARTS + idx_tdst * stride_vs_tdst + idx_tm *\n            stride_vs_tm, mask=ms_mask)\n        col_start = tl.load(PIXEL_INDICES + n * stride_pixel_n + (ms - 1) *\n            stride_pixel_m, mask=(ms - 1 >= 0 and ms < M) and ms_mask)\n        col_end = tl.load(PIXEL_INDICES + n * stride_pixel_n + ms *\n            stride_pixel_m, mask=(ms >= 0 and ms < M) and ms_mask)\n        col_len = col_end - col_start\n        range_start = v_start + idx_h * TARGET_WIDTH_MAX\n        tl.store(COL_INDICES + n * stride_col_n + (tl.arange(0, MAX_INTERP)\n            [None, :] + col_start[:, None]) * stride_col_z, tl.arange(0,\n            MAX_INTERP)[None, :] + range_start[:, None], mask=tl.arange(0,\n            MAX_INTERP)[None, :] < col_len[:, None] and ms_mask[:, None])\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "6f99790e-58c0-4245-bb0c-5ffd2ac46bd8"
  },
  {
    "input": "@triton.jit\ndef _bwd_q_kernel_with_bias_calculation(Q, K, V, BW, sm_scale, DO, DQ, L, D,\n    cu_seqlens_q, cu_seqlens_k, mid_batch, mid_start, stride_qz, stride_qh,\n    stride_qk, stride_kz, stride_kh, stride_kk, stride_vz, stride_vh,\n    stride_vk, stride_doz, stride_doh, stride_dok, stride_dqz, stride_dqh,\n    stride_dqk, stride_bw, Z, H, M, N, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', NUM_BUCKETS: 'tl.constexpr',\n    MAX_DISTANCE: 'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    start_z = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_b = tl.load(mid_batch + start_z)\n    off_m = tl.load(mid_start + start_z)\n    q_start = tl.load(cu_seqlens_q + off_b)\n    q_end = tl.load(cu_seqlens_q + off_b + 1)\n    k_start = tl.load(cu_seqlens_k + off_b)\n    k_end = tl.load(cu_seqlens_k + off_b + 1)\n    lM = q_end - q_start\n    lN = k_end - k_start\n    P_SEQ = lM - lN\n    D += off_m * H + off_h\n    L += off_m * H + off_h\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_m = offs_m_base + off_m\n    offs_m_relative = offs_m - q_start\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    offs_n_init = k_start + offs_n_base\n    q_ptrs = Q + (offs_m[:, None] * stride_qz + offs_k[None, :] * stride_qk +\n        off_h * stride_qh)\n    k_ptrs = K + (offs_n_init[:, None] * stride_kz + offs_k[None, :] *\n        stride_kk + off_h * stride_kh)\n    v_ptrs = V + (offs_n_init[:, None] * stride_vz + offs_k[None, :] *\n        stride_vk + off_h * stride_vh)\n    dq_ptrs = DQ + (offs_m[:, None] * stride_dqz + offs_k[None, :] *\n        stride_dqk + off_h * stride_dqh)\n    do_ptrs = DO + (offs_m[:, None] * stride_doz + offs_k[None, :] *\n        stride_dok + off_h * stride_doh)\n    d_ptrs = D + offs_m_base * H\n    l_ptrs = L + offs_m_base * H\n    mask_m = offs_m < q_end\n    q = tl.load(q_ptrs, mask=mask_m[:, None])\n    do = tl.load(do_ptrs, mask=mask_m[:, None])\n    delta = tl.load(d_ptrs, mask=mask_m)\n    l = tl.load(l_ptrs, mask=mask_m)\n    dq = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if CAUSAL:\n        hi = tl.minimum(lN, P_SEQ + off_m - q_start + BLOCK_M)\n        if lM > lN:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = lN\n    for start_n in range(0, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        mask_n = offs_n < lN\n        v = tl.load(v_ptrs, mask=mask_n[:, None])\n        k = tl.load(k_ptrs, mask=mask_n[:, None])\n        if HAS_BIAS:\n            relative_positions = offs_n[None, :] - offs_m_relative[:, None]\n            relative_buckets = tl.zeros_like(relative_positions)\n            num_buckets = NUM_BUCKETS\n            if not CAUSAL:\n                num_buckets //= 2\n                relative_buckets += (relative_positions > 0) * num_buckets\n                relative_positions = tl.abs(relative_positions)\n            else:\n                relative_positions = tl.maximum(-relative_positions, tl.\n                    zeros_like(relative_positions))\n            max_exact = num_buckets // 2\n            is_small = relative_positions < max_exact\n            relative_position_if_large = max_exact + tl.log(\n                relative_positions.to(tl.float32) / max_exact) / tl.log(\n                MAX_DISTANCE / max_exact) * (num_buckets - max_exact)\n            relative_position_if_large = tl.minimum(relative_position_if_large,\n                num_buckets - 1)\n            relative_buckets += tl.where(is_small, relative_positions,\n                relative_position_if_large)\n            bucket_offs = relative_buckets * stride_bw + off_h\n            bias_ptrs = BW + bucket_offs\n            b = tl.load(bias_ptrs, mask=mask_m[:, None] & mask_n[None, :])\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m_relative[:, None] >= offs_n[None, :]\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k)) * sm_scale\n        if HAS_BIAS:\n            s += b\n        p = tl.math.exp2((s - l[:, None]) * log2e)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        ds = tl.where(mask_n, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        dq += tl.dot(ds, k)\n        k_ptrs += BLOCK_N * stride_kz\n        v_ptrs += BLOCK_N * stride_vz\n    dq *= sm_scale\n    tl.store(dq_ptrs, dq, mask=mask_m[:, None])\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "e617a334-148e-4b19-948d-d9fcb84a0cc1"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, O, stride_q_bs, stride_q_head, stride_q_seqlen,\n    stride_q_dim, stride_k_bs, stride_k_head, stride_k_seqlen, stride_k_dim,\n    stride_v_bs, stride_v_head, stride_v_seqlen, stride_v_dim, stride_o_bs,\n    stride_o_head, stride_o_seqlen, stride_o_dim, BS, HEAD, SEQLEN,\n    sm_scale, BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', DIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_bs_head = tl.program_id(1)\n    qkv_base_offset = off_bs_head * stride_q_head\n    Q_block_ptr = tl.make_block_ptr(base=Q + qkv_base_offset, shape=(SEQLEN,\n        DIM), strides=(stride_q_seqlen, stride_q_dim), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, DIM), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qkv_base_offset, shape=(DIM,\n        SEQLEN), strides=(stride_k_dim, stride_k_seqlen), offsets=(0, 0),\n        block_shape=(DIM, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qkv_base_offset, shape=(SEQLEN,\n        DIM), strides=(stride_k_seqlen, stride_v_dim), offsets=(0, 0),\n        block_shape=(BLOCK_N, DIM), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    out_buffer = tl.zeros([BLOCK_M, DIM], dtype=tl.float32)\n    q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n    for start_n in range(0, SEQLEN, BLOCK_N):\n        k = tl.load(K_block_ptr, boundary_check=(0, 1))\n        v = tl.load(V_block_ptr, boundary_check=(0, 1))\n        v = v * sm_scale\n        kv = tl.dot(k, v, allow_tf32=False)\n        out_buffer += tl.dot(q, kv, allow_tf32=False)\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    O_block_ptr = tl.make_block_ptr(base=O + qkv_base_offset, shape=(SEQLEN,\n        DIM), strides=(stride_o_seqlen, stride_o_dim), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, DIM), order=(1, 0))\n    tl.store(O_block_ptr, out_buffer, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "c4105458-d4f9-45a0-8529-6180a76f04e5"
  },
  {
    "input": "@triton.jit\ndef _bwd_do_attention_kernel(O, Do, De, stride_ob, stride_om, stride_oh,\n    stride_og, stride_dob, stride_dom, stride_doh, stride_dog, stride_deb,\n    stride_deh, stride_deg, num_kv_heads, num_groups, headdim, seqlen_q,\n    BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr'):\n    off_q = tl.program_id(0)\n    off_bh = tl.program_id(1)\n    off_gp = tl.program_id(2)\n    off_b = off_bh // num_kv_heads\n    off_h = off_bh % num_kv_heads\n    offs_m = off_q * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o_ptrs = (O + off_b * stride_ob + off_h * stride_oh + off_gp *\n        stride_og + offs_m[:, None] * stride_om + offs_d[None, :])\n    do_ptrs = (Do + off_b * stride_dob + off_h * stride_doh + off_gp *\n        stride_dog + offs_m[:, None] * stride_dom + offs_d[None, :])\n    o = tl.load(o_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :\n        ] < headdim), other=0.0)\n    do = tl.load(do_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None,\n        :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(De + off_b * stride_deb + off_h * stride_deh + off_gp *\n        stride_deg + offs_m, delta, mask=offs_m < seqlen_q)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "05605b50-3c38-4222-b19d-2e777c48c22d"
  },
  {
    "input": "@triton.jit\ndef sinc_kernel(output_ptr, cutoffs_ptr, indices_ptr, num_taps, window_ptr,\n    half_sample_rate, mode: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    batch_idx = tl.program_id(1)\n    pos = tl.program_id(0) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = pos < num_taps\n    cutoff_val = tl.load(cutoffs_ptr + batch_idx) / half_sample_rate\n    index_val = tl.load(indices_ptr + pos, mask=mask)\n    window_val = tl.load(window_ptr + pos, mask=mask)\n    x = index_val * math.pi * cutoff_val\n    sinc_val = tl.where(index_val == 0, 1.0, tl.sin(x) / x)\n    windowed_sinc = sinc_val * window_val\n    normalized_sinc = windowed_sinc / tl.sum(windowed_sinc, axis=0)\n    if mode == 'high':\n        center_idx = num_taps // 2\n        adjusted_val = tl.where(pos == center_idx, 1.0 - normalized_sinc, -\n            normalized_sinc)\n        tl.store(output_ptr + batch_idx * num_taps + pos, adjusted_val,\n            mask=mask)\n    elif mode == 'low':\n        tl.store(output_ptr + batch_idx * num_taps + pos, normalized_sinc,\n            mask=mask)\n    else:\n        raise ValueError(f'Unknown mode: {mode}')\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "723e5c05-95b4-4a74-82c8-f8da9232157f"
  },
  {
    "input": "@triton.jit\ndef _group_norm_forward_kernel(Y_ptr, Y_row_stride, Y_col_stride, X_ptr,\n    X_row_stride, X_col_stride, Mean_ptr, Mean_row_stride, Mean_col_stride,\n    RSTD_ptr, RSTD_row_stride, RSTD_col_stride, W_ptr, B_ptr, hidden_size,\n    channels_per_group, eps, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    References:\n    https://nn.labml.ai/normalization/group_norm/index.html\n    \"\"\"\n    batch_idx = tl.program_id(0)\n    group_idx = tl.program_id(1)\n    X_ptr += batch_idx * X_row_stride + group_idx * X_col_stride\n    Y_ptr += batch_idx * Y_row_stride + group_idx * Y_col_stride\n    block_range = tl.arange(0, BLOCK_SIZE)\n    s = 0.0\n    squared_sum = 0.0\n    for i in tl.range(0, hidden_size, BLOCK_SIZE):\n        hidden_size_offsets = i + block_range\n        mask = hidden_size_offsets < hidden_size\n        X = tl.load(X_ptr + hidden_size_offsets, mask=mask, other=0.0)\n        s += tl.sum(X)\n        squared_sum += tl.sum(X * X)\n    m = s / hidden_size\n    variance = squared_sum / hidden_size - m * m\n    rstd = rsqrt(variance + eps)\n    hidden_size_per_channel = hidden_size // channels_per_group\n    for channel_idx in tl.range(group_idx * channels_per_group, (group_idx +\n        1) * channels_per_group):\n        W = tl.load(W_ptr + channel_idx)\n        B = tl.load(B_ptr + channel_idx)\n        for i in range(0, hidden_size_per_channel, BLOCK_SIZE):\n            hidden_size_offsets = i + block_range\n            mask = hidden_size_offsets < hidden_size_per_channel\n            X = tl.load(X_ptr + hidden_size_offsets, mask=mask, other=m)\n            Y = (X - m) * rstd * W + B\n            tl.store(Y_ptr + hidden_size_offsets, Y, mask=mask)\n        X_ptr += hidden_size_per_channel\n        Y_ptr += hidden_size_per_channel\n    tl.store(Mean_ptr + batch_idx * Mean_row_stride + group_idx *\n        Mean_col_stride, m)\n    tl.store(RSTD_ptr + batch_idx * RSTD_row_stride + group_idx *\n        RSTD_col_stride, rstd)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "074b9f38-7491-47c6-8ed5-b3ba3fefc3e7"
  },
  {
    "input": "@triton.jit\ndef pack64(a, b):\n    tl.static_assert(a.dtype == tl.float32)\n    tl.static_assert(b.dtype == tl.float32)\n    a = a.to(dtype=tl.uint32, bitcast=True)\n    a = a << 32\n    b = b.to(dtype=tl.uint32, bitcast=True)\n    return a | b\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "540462da-ad8c-4f71-8978-76dea868b3fa"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_retention_bwd_kernel(q, k, v, do, dq, dk, dv, h0, s_k_h,\n    s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT -\n        o_i - 1) * b_b)\n    d_b = tl.math.exp2(BT * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v *\n            BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_dd, b_h, allow_tf32=False)\n            b_h = d_b * b_h + tl.dot(b_v * d_k[None, :], b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    d_s = tl.trans(d_s)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dd = b_do * d_q[:, None]\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = b_ds * d_s\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False) * d_k[:, None\n                ]\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None]\n            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e833410a-d71c-4f45-8daf-90ec91bf7e40"
  },
  {
    "input": "@triton.jit\ndef parallel_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr', H: 'tl.constexpr',\n    T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL=BTL,\n        BTS=BTS, BK=BK, BV=BV, K=K, V=V)\n    tl.debug_barrier()\n    _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale, BTL,\n        BTS, BK, BV, K, V)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "4cce8a2e-fb13-445f-a7f2-2e1fac6b62b7"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8),\n    triton.Config({}, num_stages=2, num_warps=4), triton.Config({},\n    num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1\n    )], key=['K'])\n@triton.jit\ndef quantize_int8_perrow_kernel(fpa_ptr, a_ptr, as_ptr, M, K, stride_fpam,\n    stride_fpak, stride_am, stride_ak, stride_asm, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        inta = fpa / a_scale[:, None]\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "e183d1a5-1371-4489-98f7-a950ab544cad"
  },
  {
    "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=[])\n@triton.jit\ndef _group_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, N, BLOCK_N: 'tl.constexpr'\n    ):\n    col = tl.program_id(0)\n    num_heads = tl.num_programs(0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = rows < N\n        offs = rows * num_heads + col\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + col, sum_dw)\n    tl.store(FINAL_DB + col, sum_db)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f04b25bd-2fc3-4b76-94d6-5a36d27cb596"
  },
  {
    "input": "@triton.jit\ndef triton_linear(a_ptr, b_ptr, c_ptr, out_ptr, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, GROUP_M: 'tl.constexpr', EVEN_K:\n    'tl.constexpr', ALLOW_TF32: 'tl.constexpr', ACC_TYPE: 'tl.constexpr',\n    B_PROLOGUE_CAST_TYPE: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    offset_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offset_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    if stride_am == 1 and stride_ak == M or stride_am == K and stride_ak == 1:\n        offset_am = tl.max_contiguous(tl.multiple_of(offset_m % M, BLOCK_M),\n            BLOCK_M)\n    else:\n        offset_am = offset_m % M\n    if stride_bk == 1 and stride_bn == K or stride_bk == N and stride_bn == 1:\n        offset_bn = tl.max_contiguous(tl.multiple_of(offset_n % N, BLOCK_N),\n            BLOCK_N)\n    else:\n        offset_bn = offset_n % N\n    offset_k = tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + (offset_am[:, None] * stride_am + offset_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offset_k[:, None] * stride_bk + offset_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n        else:\n            a = tl.load(a_ptrs, mask=offset_k[None, :] < k, other=0.0)\n            b = tl.load(b_ptrs, mask=offset_k[:, None] < k, other=0.0)\n        if B_PROLOGUE_CAST_TYPE is not None:\n            b = b\n        acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)\n        a_ptrs += BLOCK_K * stride_ak\n        b_ptrs += BLOCK_K * stride_bk\n    offset_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offset_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    idx_m = offset_m[:, None]\n    idx_n = offset_n[None, :]\n    mask = (idx_m < M) & (idx_n < N)\n    xindex = idx_n + N * idx_m\n    c = tl.load(c_ptr + tl.broadcast_to(idx_n, mask.shape), mask,\n        eviction_policy='evict_last')\n    out = acc + c\n    tl.store(out_ptr + tl.broadcast_to(xindex, mask.shape), out, mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "650c97d2-e897-4ebf-9d84-c6d88d14ac7f"
  },
  {
    "input": "@triton.autotune(configs=_config_XtY(), key=['M', 'N', 'K'])\n@triton.heuristics({'NO_K_MASK': lambda args: args['K'] % args['BLOCK_K'] ==\n    0, 'NO_N_MASK': lambda args: args['N'] % args['BLOCK_N'] == 0})\n@triton.jit\ndef _groupXtY(DY_ptr, stride_dym, stride_dyk, X_ptr, stride_xm, stride_xn,\n    DW_ptr, stride_dwe, stride_dwk, stride_dwn, expert_offsets_ptr, M, K:\n    'tl.constexpr', N: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', ACC_TYPE: 'tl.constexpr',\n    allow_tf32: 'tl.constexpr', NO_K_MASK: 'tl.constexpr', NO_N_MASK:\n    'tl.constexpr'):\n    pid0 = tl.program_id(axis=0)\n    pid1 = tl.program_id(axis=1)\n    num0 = tl.num_programs(0)\n    num1 = tl.num_programs(1)\n    pid0, pid1 = tl.swizzle2d(pid0, pid1, num0, num1, 4)\n    K_BLOCK_COUNT = tl.cdiv(K, BLOCK_K)\n    E_idx = pid0 // K_BLOCK_COUNT\n    K_block_id = pid0 % K_BLOCK_COUNT\n    N_block_id = pid1\n    if E_idx == 0:\n        start_idx = 0\n    else:\n        start_idx = tl.load(expert_offsets_ptr + E_idx - 1)\n    end_idx = tl.load(expert_offsets_ptr + E_idx)\n    if end_idx > start_idx:\n        M_block = tl.max_contiguous(start_idx + tl.arange(0, BLOCK_M), BLOCK_M)\n        K_block = K_block_id * BLOCK_K + tl.arange(0, BLOCK_K)\n        K_mask = K_block < K\n        K_block = tl.max_contiguous(tl.multiple_of(K_block % K, BLOCK_K),\n            BLOCK_K)\n        N_block = N_block_id * BLOCK_N + tl.arange(0, BLOCK_N)\n        N_mask = N_block < N\n        N_block = tl.max_contiguous(tl.multiple_of(N_block % N, BLOCK_N),\n            BLOCK_N)\n        M_idxs = M_block\n        xt_blk_ptrs = X_ptr + K_block[:, None] * stride_xn + M_idxs[None, :\n            ] * stride_xm\n        dy_blk_ptrs = DY_ptr + M_idxs[:, None] * stride_dym + N_block[None, :\n            ] * stride_dyk\n        acc = tl.zeros((BLOCK_K, BLOCK_N), dtype=ACC_TYPE)\n        iters = tl.cdiv(end_idx - start_idx, BLOCK_M)\n        for i in range(0, iters):\n            M_mask = i * BLOCK_M + M_block < end_idx\n            if NO_K_MASK:\n                xt = tl.load(xt_blk_ptrs, mask=M_mask[None, :])\n            else:\n                xt = tl.load(xt_blk_ptrs, mask=K_mask[:, None] & M_mask[\n                    None, :])\n            if NO_N_MASK:\n                dy = tl.load(dy_blk_ptrs, mask=M_mask[:, None])\n            else:\n                dy = tl.load(dy_blk_ptrs, mask=M_mask[:, None] & N_mask[\n                    None, :])\n            xt_blk_ptrs += BLOCK_M * stride_xm\n            dy_blk_ptrs += BLOCK_M * stride_dym\n            acc += tl.dot(xt, dy, out_dtype=ACC_TYPE, allow_tf32=allow_tf32)\n        DW_blk_ptrs = DW_ptr + E_idx * stride_dwe + K_block[:, None\n            ] * stride_dwk + N_block[None, :] * stride_dwn\n        acc = acc\n        tl.store(DW_blk_ptrs, acc, mask=K_mask[:, None] & N_mask[None, :])\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d409b175-9d86-4c66-9de1-676315947d3e"
  },
  {
    "input": "@triton.jit\ndef sigmoid_grad(input):\n    \"\"\"\n    Calculates the gradient of sigmoid.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of sigmoid.\n    \"\"\"\n    output_sigmoid = sigmoid(input)\n    return output_sigmoid * (1 - output_sigmoid)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "e896c74a-3cb0-457a-8705-84527008752a"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dh(q, do, dh, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, scale, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "28743a3d-7bd6-4776-8d01-eae1a36e823a"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128,\n    'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32},\n    num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64,\n    'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])\n@triton.jit\ndef dequantize_kernel(b_ptr, b_scale_ptr, fpb_ptr, K, N, stride_bk,\n    stride_bn, stride_fpbk, stride_fpbn, BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    k_block_idx = tl.program_id(axis=0)\n    n_block_idx = tl.program_id(axis=1)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_bn\n    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (\n        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]) * stride_fpbn\n    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]\n    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N\n    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask\n    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)\n    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)\n    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "97ebc1a3-1bae-41aa-a616-e365569fea63"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_compute_A(Q, K, GK, A, stride_q1, stride_q2, stride_q3,\n    stride_q4, stride_a1, stride_a2, stride_a3, stride_a4, Z, H, N_CTX, D,\n    BLOCK_DMODEL_QK: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_k = tl.program_id(2)\n    qk_offset = off_hz * stride_q2 + off_k * BLOCK_DMODEL_QK\n    a_offset = (off_k * Z * H + off_hz) * stride_a2\n    lo = 0\n    hi = BLOCK_N\n    Q_ptr = Q + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    K_ptr = K + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[:, None] + tl.arange(0, 16)[None, :] * stride_q4\n    GK_K_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[:, None] + tl.arange(0, 16)[None, :] * stride_q4\n    GK_Q_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(16, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        q_gk2 = tl.exp(q_gk - q_normalizer[None, :])\n        q = q * q_gk2\n        for k_high in range(0, q_high, 16):\n            k = tl.load(K_ptr + k_high * stride_q4)\n            k_gk = tl.load(GK_K_ptr + k_high * stride_q4)\n            k_gk = tl.exp(q_normalizer[:, None] - k_gk)\n            k = k * k_gk\n            qk = tl.dot(q, k, allow_tf32=False)\n            tl.store(A_ptr + q_high * stride_a4 + k_high, qk)\n    for q_high in range(lo, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        q_gk2 = tl.exp(q_gk - q_normalizer[None, :])\n        q = q * q_gk2\n        q_gk3 = tl.exp(q_normalizer[None, :] - q_gk)\n        k = tl.load(K_ptr + q_high * stride_q4)\n        k = k * tl.trans(q_gk3)\n        qk = tl.dot(q, k, allow_tf32=False)\n        qk = tl.where(tl.arange(0, 16)[:, None] >= tl.arange(0, 16)[None, :\n            ], qk, 0.0)\n        tl.store(A_ptr + q_high * stride_a4 + q_high, qk)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "a58b2708-450f-412b-bc7f-27a65a0fe6c2"
  },
  {
    "input": "@triton.jit\ndef _softmax_kernel_fwd(input_ptr, stride_input_row, output_ptr,\n    stride_output_row, num_cols, BLOCK_SIZE: 'tl.constexpr'):\n    row_id = tl.program_id(axis=0)\n    row_start_ptr = input_ptr + row_id * stride_input_row\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_pointers = row_start_ptr + col_offsets\n    row_data_mask = col_offsets < num_cols\n    x = tl.load(input_pointers, mask=row_data_mask, other=0.0)\n    safe_row = x - tl.max(x, axis=0)\n    numerator = tl.exp(safe_row)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_out = numerator / denominator\n    output_row_ptr = output_ptr + row_id * stride_input_row\n    output_pointers = output_row_ptr + col_offsets\n    tl.store(output_pointers, softmax_out, mask=row_data_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "c73b4ba4-0bec-4bf0-9a26-bf10c298b39a"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, soz, soh, som, sod, DO, L, slzh, slm, NewDO, Delta,\n    N_CTX_Q, BLOCK_M: 'tl.constexpr', D_HEAD: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_d = tl.arange(0, D_HEAD)\n    off_o = off_hz * soh + off_m[:, None] * som + off_d[None, :] * sod\n    off_l = off_hz * slzh + off_m * slm\n    o = tl.load(Out + off_o)\n    do = tl.load(DO + off_o)\n    denom = tl.load(L + off_l)\n    do = do / denom[:, None]\n    delta = tl.sum(o * do, axis=1)\n    tl.store(NewDO + off_o, do)\n    tl.store(Delta + off_l, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ba26e666-0570-4d3e-a439-e71703fcc019"
  },
  {
    "input": "@triton.jit\ndef depth_inv_sphere(far, disparity_at_inf, n, step):\n    frac_step = (step + 1) / n\n    n_disp = (disparity_at_inf - 1) * frac_step + 1\n    return far * (1 / n_disp)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "c339bd68-a9f1-4cf4-82c9-356dc78965e2"
  },
  {
    "input": "@triton.jit\ndef _sample_2d(image, w, batch_index, ix, iy, IH: 'tl.constexpr', IW:\n    'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    val = tl.view(tl.load((image + batch_index * IW * IH * C + iy_ * IW * C +\n        ix_ * C)[:, None] + Coffs[None, :]), (BLOCK_SIZE, C))\n    return val * tl.view((w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW\n        )))[:, None] * channel_bcast, (BLOCK_SIZE, C))\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "88d6648c-00ee-48c7-a9f4-7e4715b574cd"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "2fa2bf06-7437-437c-b7bd-b7bbed7ca5c1"
  },
  {
    "input": "@triton.jit\ndef bias_kernel(out, weights, stride_om, stride_on, stride_wn, N:\n    'tl.constexpr', M: 'tl.constexpr', NH: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_NH: 'tl.constexpr',\n    BIDIRECTIONAL: 'tl.constexpr', NUM_BUCKETS: 'tl.constexpr',\n    MAX_DISTANCE: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_m = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_n = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    relative_positions = offs_n[None, :] - offs_m[:, None]\n    relative_buckets = tl.zeros_like(relative_positions)\n    num_buckets = NUM_BUCKETS\n    if BIDIRECTIONAL:\n        num_buckets //= 2\n        relative_buckets += (relative_positions > 0) * num_buckets\n        relative_positions = tl.abs(relative_positions)\n    else:\n        relative_positions = tl.maximum(-relative_positions, tl.zeros_like(\n            relative_positions))\n    max_exact = num_buckets // 2\n    is_small = relative_positions < max_exact\n    relative_position_if_large = max_exact + tl.log(relative_positions.to(\n        tl.float32) / max_exact) / tl.log(MAX_DISTANCE / max_exact) * (\n        num_buckets - max_exact)\n    relative_position_if_large = tl.minimum(relative_position_if_large, \n        num_buckets - 1)\n    relative_buckets += tl.where(is_small, relative_positions,\n        relative_position_if_large)\n    for i in range(0, NH, BLOCK_NH):\n        offs_nh = i + tl.arange(0, BLOCK_NH)\n        bucket_offs = relative_buckets[:, :, None] * stride_wn + offs_nh[\n            None, None, :]\n        bias_ptrs = weights + bucket_offs\n        bias_values = tl.load(bias_ptrs)\n        out_offs = (offs_m[:, None] * stride_om + offs_n[None, :] * stride_on)[\n            :, :, None] + offs_nh[None, None, :]\n        out_ptrs = out + out_offs\n        o_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)[:, :, None] & (\n            offs_nh[None, None, :] < NH)\n        tl.store(out_ptrs, bias_values, mask=o_mask)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "6b554549-f6db-4092-927f-05b5f10ac96a"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_kernel(X, Y, W, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "b34e6d70-cfeb-48ab-9251-dd8d0ea05349"
  },
  {
    "input": "@triton.jit\ndef _parallel_retention_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    d_h = tl.math.exp2((BTL - tl.arange(0, BTL)) * b_b)\n    b_kd = b_k * d_h[:, None]\n    d_q = tl.math.exp2(tl.arange(0, BTS) * b_b)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * d_q[None, :]\n        b_dv *= d_b\n        b_s = tl.dot(b_kd, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        b_dk *= d_b\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((-o_k[:, None] + o_q[None, :]) *\n            b_b), 0) * scale\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s, tl.trans(b_do), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "27678667-b842-4390-a89b-1040fd119234"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "0628c719-8ad7-409d-aaa0-2f1da2a7b0a5"
  },
  {
    "input": "@triton.jit\ndef _bwd_inter_kernel(Q, K, V, DO, DQ, DK, DV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    DQ_block_ptr = DQ + qk_offset + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    K_block_ptr = K + qk_offset + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + tl.arange(0, CBLOCK)[None, :\n        ] * e + tl.arange(0, e)[:, None]\n    DO_block_ptr = DO + o_offset + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    off_block1 = tl.arange(0, CBLOCK)\n    off_block2 = tl.arange(0, CBLOCK)\n    kv_trans = tl.zeros([e, d], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        for j in range(NUM_CBLOCK):\n            if i > 0:\n                do = tl.load(DO_block_ptr, mask=off_block1[:, None] < n,\n                    other=0.0)\n                dq_inter = tl.dot(do, kv_trans)\n                dq = dq_inter + tl.load(DQ_block_ptr, mask=off_block1[:,\n                    None] < n, other=0.0)\n                tl.store(DQ_block_ptr, dq, mask=off_block1[:, None] < n)\n            DQ_block_ptr += CBLOCK * d\n            DO_block_ptr += CBLOCK * e\n            off_block1 += CBLOCK\n        kv_trans_current = tl.zeros([e, d], dtype=tl.float32)\n        for j in range(NUM_CBLOCK):\n            v_trans = tl.load(V_trans_block_ptr, mask=off_block2[None, :] <\n                n, other=0.0)\n            k = tl.load(K_block_ptr, mask=off_block2[:, None] < n, other=0.0)\n            kv_trans_current += tl.dot(v_trans, k)\n            K_block_ptr += CBLOCK * d\n            V_trans_block_ptr += CBLOCK * e\n            off_block2 += CBLOCK\n        kv_trans += kv_trans_current\n    m = NUM_BLOCK * BLOCK\n    off_block1 = m + tl.arange(0, CBLOCK)\n    off_block2 = m + tl.arange(0, CBLOCK)\n    Q_trans_block_ptr = Q + qk_offset + m * d + tl.arange(0, CBLOCK)[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    K_block_ptr = K + qk_offset + m * d + tl.arange(0, CBLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + m * e + tl.arange(0, CBLOCK)[None, :\n        ] * e + tl.arange(0, e)[:, None]\n    DK_trans_block_ptr = DK + qk_offset + m * d + tl.arange(0, CBLOCK)[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + m * e + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + m * e + tl.arange(0, CBLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    dkv = tl.zeros([d, e], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            K_block_ptr -= CBLOCK * d\n            V_trans_block_ptr -= CBLOCK * e\n            DK_trans_block_ptr -= CBLOCK * d\n            DV_block_ptr -= CBLOCK * e\n            off_block1 -= CBLOCK\n            if i < NUM_BLOCK - 1:\n                k = tl.load(K_block_ptr, mask=off_block1[:, None] < n,\n                    other=0.0)\n                v_trans = tl.load(V_trans_block_ptr, mask=off_block1[None,\n                    :] < n, other=0.0)\n                dk_inter_trans = tl.dot(dkv, v_trans)\n                dv_inter = tl.dot(k, dkv)\n                dk_trans = dk_inter_trans + tl.load(DK_trans_block_ptr,\n                    mask=off_block1[None, :] < n, other=0.0)\n                dv = dv_inter + tl.load(DV_block_ptr, mask=off_block1[:,\n                    None] < n, other=0.0)\n                tl.store(DK_trans_block_ptr, dk_trans, mask=off_block1[None,\n                    :] < n)\n                tl.store(DV_block_ptr, dv, mask=off_block1[:, None] < n)\n        dkv_current = tl.zeros([d, e], dtype=tl.float32)\n        for j in range(NUM_CBLOCK - 1, -1, -1):\n            DO_block_ptr -= CBLOCK * e\n            Q_trans_block_ptr -= CBLOCK * d\n            off_block2 -= CBLOCK\n            do = tl.load(DO_block_ptr, mask=off_block2[:, None] < n, other=0.0)\n            q_trans = tl.load(Q_trans_block_ptr, mask=off_block2[None, :] <\n                n, other=0.0)\n            dkv_current += tl.dot(q_trans, do)\n        dkv += dkv_current\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "846b6b66-df89-45d6-a917-d236f7adb1cb"
  },
  {
    "input": "@triton.jit\ndef paged_attention_v1(scratchpad_key_ptr, scratchpad_value_ptr, output_ptr,\n    query_ptr, key_cache_ptr, value_cache_ptr, block_tables_ptr,\n    context_lens_ptr, scale, num_seqs, num_heads, cache_block_stride,\n    MAX_SEQ_LEN: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', HEAD_SIZE:\n    'tl.constexpr', MAX_NUM_BLOCKS_PER_SEQ: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    head_idx = tl.program_id(1)\n    query_offset = seq_idx * num_seqs + head_idx * HEAD_SIZE\n    query_head = tl.load(query_ptr + query_offset + tl.arange(0, HEAD_SIZE))\n    block_table_offset = seq_idx * MAX_NUM_BLOCKS_PER_SEQ\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    for tok_idx in range(0, context_len):\n        logical_block_idx = tok_idx // BLOCK_SIZE\n        physical_block_idx = tl.load(block_tables_ptr + block_table_offset +\n            logical_block_idx)\n        start_of_block_offset = (physical_block_idx * cache_block_stride + \n            head_idx * HEAD_SIZE * BLOCK_SIZE)\n        tok_idx_within_block = tok_idx % BLOCK_SIZE\n        tok_offsets = start_of_block_offset + BLOCK_SIZE * tl.arange(0,\n            HEAD_SIZE) + tok_idx_within_block\n        tok_key = tl.load(key_cache_ptr + tok_offsets)\n        tok_value = tl.load(value_cache_ptr + tok_offsets)\n        scratchpad_offset = seq_idx * (MAX_SEQ_LEN * num_heads * HEAD_SIZE\n            ) + tok_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n        tl.store(scratchpad_key_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_key)\n        tl.store(scratchpad_value_ptr + scratchpad_offset + tl.arange(0,\n            HEAD_SIZE), tok_value)\n    tl.debug_barrier()\n    start_seq_offset = MAX_SEQ_LEN * num_heads * HEAD_SIZE * seq_idx\n    start_tok_offset = start_seq_offset + tl.arange(0, MAX_SEQ_LEN) * (\n        num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    mask = tl.arange(0, MAX_SEQ_LEN)[:, None] < context_len\n    kv_offs = start_tok_offset[:, None] + tl.arange(0, HEAD_SIZE)[None, :]\n    print_tensor_dim(kv_offs, 'kv_offs_v1')\n    keys = tl.load(scratchpad_key_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(keys, 'keys_v1')\n    values = tl.load(scratchpad_value_ptr + kv_offs, mask=mask, other=0.0)\n    print_tensor_dim(values, 'values_v1')\n    scores = tl.sum(scale * keys * query_head[None, :], axis=1)\n    mask = tl.full([MAX_SEQ_LEN], -float('inf'), dtype=tl.float32)\n    cond = tl.arange(0, MAX_SEQ_LEN) < context_len\n    scores_masked = tl.where(cond, scores, mask)\n    scores_minus_max = scores_masked - tl.max(scores_masked, axis=0)\n    numerator = tl.exp(scores_minus_max)\n    denominator = tl.sum(numerator, axis=0) + float(1e-06)\n    logits = numerator / denominator\n    print_tensor_dim(logits, 'logits_v1')\n    weighted_values = tl.sum(values * logits[:, None], axis=0)\n    print_tensor_dim(weighted_values, 'weighted_values_v1')\n    output_offset = seq_idx * (num_heads * HEAD_SIZE) + head_idx * HEAD_SIZE\n    tl.store(output_ptr + output_offset + tl.arange(0, HEAD_SIZE),\n        weighted_values)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "750a2324-a452-4857-ae16-3fe6d7a6aff6"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_cum(s, o, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr',\n    S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_o = tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "aec973a5-071e-430d-8935-db918df39de7"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_K(q, k, v, h, g, A, do, dh, dq, dk, dv, dA, scale,\n    T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NG:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    o_t = min(i_t * BT + BT, T)\n    m_s = o_i[:, None] >= o_i[None, :]\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bg * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT,\n        1), (i_t * BT, 0), (BT, BT), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A = tl.dot(b_q * scale, tl.trans(b_k))\n    b_A = tl.where(m_s, b_A, 0.0)\n    tl.store(p_A, b_A, boundary_check=(0, 1))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        o_v = i_v * BV + tl.arange(0, BV)\n        p_v = tl.make_block_ptr(v + i_bg * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bg * NT * K * V + i_t * K * V, (V, K),\n            (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT,\n            i_v * BV), (BT, BV), (1, 0))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (o_t - 1\n            ) * V + o_v, BV), BV)\n        m_v = o_v < V\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * T * V, (T, V),\n            (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_gn = tl.load(p_gn, mask=m_v, other=0)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_v = b_v * tl.exp(b_gn[None, :] - b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_h = b_h\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g) * scale\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dh = b_dh\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, tl.trans(b_dh))\n        b_dv = tl.exp(b_gn[None, :] - b_g) * tl.dot(b_k, b_dh)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n        BT, 0), (BT, BT), (1, 0))\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_k)\n    b_dk += tl.dot(tl.trans(b_dA), b_q)\n    p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "664d0d62-1c83-4b50-bc7f-5498737d392e"
  },
  {
    "input": "@triton.jit\ndef triton_wait(wait_addrs):\n    flat_tid = get_flat_tid()\n    if flat_tid == 0:\n        tl.inline_asm_elementwise(\n            \"\"\"\n            {\n                .reg .u32   %tmp32_<1>;\n                .reg .pred  %p<1>;\n\n                wait_signal:\n                    // No need to acquire here since all threads will\n                    // acquire this location after the barrier.\n                    atom.global.sys.cas.b32 %tmp32_0, [$1], 1, 0;\n                    setp.eq.u32 %p0, %tmp32_0, 1;\n                    @!%p0 bra wait_signal;\n\n                barrier_end:\n            }\n            \"\"\"\n            , '=r, l', [wait_addrs], dtype=tl.int32, is_pure=False, pack=1)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "810ab851-c0e4-416d-9445-df1803315ea9"
  },
  {
    "input": "@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, M, N, K, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, c_ptr, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr', K_EXACTLY_DIVISIBLE_BY_BLOCK: 'tl.constexpr'\n    ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k_remaining in range(K, 0, -BLOCK_SIZE_K):\n        if K_EXACTLY_DIVISIBLE_BY_BLOCK:\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n        else:\n            mask = tl.arange(0, BLOCK_SIZE_K) < k_remaining\n            a = tl.load(a_ptrs, mask=mask[None, :], other=0.0)\n            b = tl.load(b_ptrs, mask=mask[:, None], other=0.0)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "f3c5ee49-8a6a-42db-93e7-fb8e7683cd9a"
  },
  {
    "input": "@triton.jit\ndef _fused_moe_kernel_a16w4_subchannel(A, B, C, scale_b_ptr,\n    zero_points_ptr, topk_weights_ptr, sorted_token_ids_ptr, expert_ids_ptr,\n    num_tokens_post_padded_ptr, N, K, EM, num_valid_tokens, stride_am,\n    stride_ak, stride_be, stride_bn, stride_bk, stride_cm, stride_cn,\n    stride_scale_be, stride_scale_bn, stride_scale_bk, stride_zero_points_e,\n    stride_zero_points_n, stride_zero_points_k, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', MUL_ROUTED_WEIGHT:\n    'tl.constexpr', top_k: 'tl.constexpr', add_zero_points: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(EM, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    num_tokens_post_padded = tl.load(num_tokens_post_padded_ptr)\n    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded:\n        return\n    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\n    token_mask = offs_token < num_valid_tokens\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N * 2) // 2) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = A + (offs_token[:, None] // top_k * stride_am + offs_k[None, :\n        ] * stride_ak)\n    off_experts = tl.load(expert_ids_ptr + pid_m)\n    b_ptrs = B + off_experts * stride_be + (offs_k[:, None] * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    if add_zero_points:\n        offs_zp_n = (pid_n * BLOCK_SIZE_N * 2 + tl.arange(0, 2 * BLOCK_SIZE_N)\n            ) % (2 * N)\n        _ZERO_POINT0 = tl.zeros([1], dtype=zero_points_ptr.dtype.element_ty)\n    _A0 = tl.zeros([1, 1], dtype=A.dtype.element_ty)\n    _B0 = tl.zeros([1, 1], dtype=B.dtype.element_ty)\n    _SCALE0 = tl.zeros([1], dtype=scale_b_ptr.dtype.element_ty)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N * 2), dtype=tl.float32)\n    l_shifter = (1 - tl.arange(0, BLOCK_SIZE_N * 2) % 2) * 4\n    for k in range(tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K -\n            k * BLOCK_SIZE_K), other=_A0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=_B0)\n        b = (b << l_shifter[None, :]).__rshift__(4)\n        if add_zero_points:\n            zp_ptrs = (zero_points_ptr + off_experts * stride_zero_points_e +\n                offs_zp_n * stride_zero_points_n + k)\n            zero_points_vals = tl.load(zp_ptrs)\n            b = b - zero_points_vals[None, :]\n        offs_scale_n = pid_n * BLOCK_SIZE_N * 2 + tl.arange(0, 2 * BLOCK_SIZE_N\n            )\n        scale_b_ptrs = (scale_b_ptr + off_experts * stride_scale_be + \n            offs_scale_n * stride_scale_bn + k)\n        scales_val = tl.load(scale_b_ptrs, mask=offs_scale_n < 2 * N, other\n            =_SCALE0)\n        b = b * scales_val[None, :]\n        accumulator += tl.dot(a, b, out_dtype=tl.float32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    if MUL_ROUTED_WEIGHT:\n        moe_weight = tl.load(topk_weights_ptr + offs_token, mask=token_mask,\n            other=0.0)\n        accumulator = accumulator * moe_weight[:, None]\n    accumulator = accumulator\n    offs_cn = pid_n * BLOCK_SIZE_N * 2 + tl.arange(0, BLOCK_SIZE_N * 2)\n    c_ptrs = C + stride_cm * offs_token[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = token_mask[:, None] & (offs_cn[None, :] < N * 2)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "9e356450-0f53-47d5-afbc-cbcdd045adca"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "18f7c0ab-c198-4f7d-8962-c90f71edbd2f"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] >\n    0.0})\n@triton.jit\ndef cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr,\n    labels_ptr, label_smoothing, logit_scale, lse_square_scale,\n    ignore_index, total_classes, class_start_idx, n_cols, logits_row_stride,\n    dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: 'tl.constexpr',\n    HAS_SMOOTHING: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != ignore_index:\n        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\n    else:\n        dloss = 0.0\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    lse = tl.load(lse_ptr + row_idx)\n    probs = tl.exp(logits - lse)\n    probs += 2.0 * lse_square_scale * lse * probs\n    label_idx -= class_start_idx\n    if HAS_SMOOTHING:\n        smooth_negative = label_smoothing / total_classes\n        probs = tl.where(col_offsets == label_idx, probs - (1 -\n            label_smoothing), probs) - smooth_negative\n    else:\n        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\n    tl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=\n        col_offsets < n_cols)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "829c83e8-2706-4e1b-84eb-94e37f779ed7"
  },
  {
    "input": "@triton.jit\ndef max_fn(x, y):\n    return tl.math.max(x, y)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "587461b6-7e6e-46eb-83f4-25a6dc5f876d"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    num_block, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_qz + off_h * stride_qh\n    V += off_z * stride_qz + off_h * stride_qh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_qz + off_h * stride_qh\n    DV += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        lo = 0\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        v = v * sm_scale\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, tl.trans(k), allow_tf32=False)\n            p = qk\n            do = tl.load(do_ptrs)\n            dv += tl.dot(tl.trans(p), do)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            dp += tl.dot(do, tl.trans(v))\n            ds = dp\n            dk += tl.dot(tl.trans(ds), q)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k)\n            tl.store(dq_ptrs, dq)\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dv *= sm_scale\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "da22b4e1-531a-4ad2-869c-f830a478e7d0"
  },
  {
    "input": "@triton.jit\ndef _bwd_dkv_parallel(Q, DO, S, DKV, b: 'tl.constexpr', h: 'tl.constexpr',\n    n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', D_FBLOCK: 'tl.constexpr',\n    E_FBLOCK: 'tl.constexpr', NUM_FBLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_de = tl.program_id(2)\n    off_h = off_bh % h\n    off_d = off_de // NUM_FBLOCK\n    off_e = off_de % NUM_FBLOCK\n    block_offset = off_block * BLOCK\n    qk_block_offset = block_offset * d\n    o_block_offset = block_offset * e\n    kv_block_offset = off_block * d * e\n    qk_offset = off_bh * n * d\n    o_offset = off_bh * n * e\n    kv_offset = off_bh * (NUM_BLOCK + 1) * d * e\n    d_offset = off_d * D_FBLOCK\n    e_offset = off_e * E_FBLOCK\n    DKV_block_ptr = (DKV + kv_offset + kv_block_offset + d_offset * e +\n        e_offset + tl.arange(0, D_FBLOCK)[:, None] * e + tl.arange(0,\n        E_FBLOCK)[None, :])\n    Q_trans_block_ptr = Q + qk_offset + qk_block_offset + d_offset + tl.arange(\n        0, CBLOCK)[None, :] * d + tl.arange(0, D_FBLOCK)[:, None]\n    DO_block_ptr = DO + o_offset + o_block_offset + e_offset + tl.arange(0,\n        CBLOCK)[:, None] * e + tl.arange(0, E_FBLOCK)[None, :]\n    s_ptrs = S + off_h\n    s = tl.load(s_ptrs)\n    c_array = tl.arange(0, CBLOCK)\n    dkv = tl.zeros([D_FBLOCK, E_FBLOCK], dtype=tl.float32)\n    for j in range(NUM_CBLOCK):\n        do = tl.load(DO_block_ptr)\n        q_trans = tl.load(Q_trans_block_ptr)\n        q_decay_trans = tl.exp(-s * (j * CBLOCK + c_array[None, :]))\n        dkv += tl.dot(q_trans * q_decay_trans, do)\n        DO_block_ptr += CBLOCK * e\n        Q_trans_block_ptr += CBLOCK * d\n    tl.store(DKV_block_ptr, dkv)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c6242fdc-b82c-4f74-b618-ec4461dff7fe"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K',\n    'NO_GROUPS'])\n@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // infearure_per_bits * stride_zeros_n\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = zeros * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d50a6562-4824-4aca-bf8f-22043e5ede3b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS,\n    NUM_STAGES)], key=['M'])\n@triton.jit\ndef triton_jagged_mean_kernel_variable_length_loop_buffer_then_sum(\n    input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_ragged\n        ), tl.load(input_ptr_offsets + (pid_ragged + 1))\n    ragged_len = ragged_end - ragged_start\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED\n        ):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer_sum = tl.sum(buffer, axis=0)\n    buffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n    output_offsets = offsets_m + pid_ragged * M\n    output_mask = output_offsets < M * (pid_ragged + 1)\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "6349edcd-b627-489d-a232-9cabc75de51d"
  },
  {
    "input": "@triton.jit\ndef relu(input):\n    \"\"\"\n    Applies ReLU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by ReLU.\n    \"\"\"\n    return tl.maximum(0, input)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "681ad099-cf1f-4abb-b643-f0a06005072b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef fused_chunk_delta_rule_bwd_kernel(q, k, v, d, dht, dh0, do, dq, dk, dv,\n    dd, initial_state, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T,\n    scale, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    USE_DHT: 'tl.constexpr', USE_DHO: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_DHT:\n        p_dht = tl.make_block_ptr(dht + i_bh * DK * DV, (DK, DV), (DV, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (DK, T), (s_k_d, s_k_t),\n            (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_d = tl.make_block_ptr(d + i_bh * s_k_h, (DK, T), (s_k_d, s_k_t),\n            (i_k * BK, i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, DK),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, DV),\n            (s_v_t, s_v_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        b_dh -= tl.dot(b_d, b_dv, allow_tf32=False)\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    if USE_DHO:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * DK * DV, (DK, DV), (DV, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    NT = tl.cdiv(T, BT)\n    for i in range(0, NT):\n        p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dd = tl.dot(b_dv, b_h, allow_tf32=False)\n        p_dd = tl.make_block_ptr(dd + (i_bh + i_v * B * H) * s_k_h, (T, DK),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        tl.store(p_dd, -b_dd, boundary_check=(0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d),\n            (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (DV, T), (s_v_d, s_v_t),\n            (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, DK),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "226cad0f-c0f9-425e-8010-938a210718cc"
  },
  {
    "input": "@triton.jit\ndef _kernel_argmax_merge_discontinuous(alpha_c, alpha_d, marginal_d, w,\n    batch, L, stride_alpha_c1, stride_alpha_c2, stride_alpha_c3,\n    stride_alpha_d1, stride_alpha_d2, stride_alpha_d3, stride_alpha_d4,\n    stride_alpha_d5):\n    b_idx = tl.program_id(0)\n    if b_idx >= batch:\n        return\n    span_length_left = tl.program_id(1) + 1\n    tid = tl.program_id(2)\n    start = 0\n    while tid >= L - w - start:\n        tid -= L - w - start\n        start += 1\n    gap_start = start + span_length_left\n    gap_end = gap_start + (tid + 1)\n    end = gap_end + (w - span_length_left)\n    alpha_c_ptr = alpha_c + b_idx * stride_alpha_c1\n    alpha_d_ptr = alpha_d + b_idx * stride_alpha_d1\n    max_score = tl.load(alpha_c_ptr + start * stride_alpha_c2 + gap_start\n        ) + tl.load(alpha_c_ptr + gap_end * stride_alpha_c2 + end)\n    max_idx = tl.zeros((1,), dtype=tl.float32) - 1\n    for split in range(start + 1, gap_start):\n        c_ptr = alpha_c_ptr + start * stride_alpha_c2 + split * stride_alpha_c3\n        d_ptr = (alpha_d_ptr + split * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        score = tl.load(c_ptr) + tl.load(d_ptr)\n        max_idx = update_position(max_score, score, max_idx, tl.zeros((1,),\n            dtype=tl.float32) + split)\n        max_score = tl.maximum(score, max_score)\n        c_ptr = (alpha_c_ptr + split * stride_alpha_c2 + gap_start *\n            stride_alpha_c3)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + split *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + end * stride_alpha_d5\n            )\n        score = tl.load(c_ptr) + tl.load(d_ptr)\n        max_idx = update_position(max_score, score, max_idx, tl.zeros((1,),\n            dtype=tl.float32) + split + L + 1)\n        max_score = tl.maximum(score, max_score)\n    for split in range(gap_end + 1, end):\n        c_ptr = (alpha_c_ptr + gap_end * stride_alpha_c2 + split *\n            stride_alpha_c3)\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + split * stride_alpha_d4 + end * stride_alpha_d5)\n        score = tl.load(c_ptr) + tl.load(d_ptr)\n        max_idx = update_position(max_score, score, max_idx, tl.zeros((1,),\n            dtype=tl.float32) + split + 2 * (L + 1))\n        max_score = tl.maximum(score, max_score)\n        c_ptr = alpha_c_ptr + split * stride_alpha_c2 + end * stride_alpha_c3\n        d_ptr = (alpha_d_ptr + start * stride_alpha_d2 + gap_start *\n            stride_alpha_d3 + gap_end * stride_alpha_d4 + split *\n            stride_alpha_d5)\n        score = tl.load(c_ptr) + tl.load(d_ptr)\n        max_idx = update_position(max_score, score, max_idx, tl.zeros((1,),\n            dtype=tl.float32) + split + 3 * (L + 1))\n        max_score = tl.maximum(score, max_score)\n    span_score = tl.load(marginal_d + b_idx * stride_alpha_d1 + start *\n        stride_alpha_d2 + gap_start * stride_alpha_d3 + gap_end *\n        stride_alpha_d4 + end * stride_alpha_d5)\n    tl.store(alpha_d + b_idx * stride_alpha_d1 + start * stride_alpha_d2 + \n        gap_start * stride_alpha_d3 + gap_end * stride_alpha_d4 + end *\n        stride_alpha_d5, max_score + span_score)\n    tl.store(alpha_d + b_idx * stride_alpha_d1 + gap_start *\n        stride_alpha_d2 + start * stride_alpha_d3 + gap_end *\n        stride_alpha_d4 + end * stride_alpha_d5 + tl.arange(0, 1), max_idx)\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "6e40a2be-e0bf-482b-bfbd-4c4cb01f4d4b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS':\n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2)],\n    key=['chunk_size', 'K'])\n@triton.jit\ndef _bmm_chunk_bwd_kernel(a_ptr, dout_ptr, db_ptr, res_ptr, seqlen,\n    chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head,\n    stride_ak, stride_dout_batch, stride_dout_chunk, stride_dout_head,\n    stride_dout_csize_m, stride_dout_csize_n, stride_db_batch,\n    stride_db_seqlen, stride_db_head, stride_db_k, stride_res_batch,\n    stride_res_seqlen, stride_res_head, stride_res_k, dot_dtype:\n    'tl.constexpr', HAS_RESIDUAL: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_CS: 'tl.constexpr'\n    ):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    a_ptr += (pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen +\n        pid_h * stride_a_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * stride_dout_chunk + \n        pid_h * stride_dout_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_cs = tl.arange(0, BLOCK_SIZE_CS)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_csize_n + offs_cs\n        [None, :] * stride_dout_csize_m)\n    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] *\n        stride_ak)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size) & (\n            offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),\n            other=0.0)\n        a = tl.load(a_ptrs, mask=(offs_cs[:, None] < chunk_size_limit - cs *\n            BLOCK_SIZE_CS) & (offs_n[None, :] < K), other=0.0)\n        acc += tl.dot(dout, a)\n        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_RESIDUAL:\n        res_ptr += (pid_b * stride_res_batch + pid_c * chunk_size *\n            stride_res_seqlen + pid_h * stride_res_head)\n        res_ptrs = res_ptr + (offs_m[:, None] * stride_res_seqlen + offs_n[\n            None, :] * stride_res_k)\n        res = tl.load(res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_n[None, :] < K))\n        acc += res\n    db = acc\n    db_ptr += (pid_b * stride_db_batch + pid_c * chunk_size *\n        stride_db_seqlen + pid_h * stride_db_head)\n    db_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :\n        ] * stride_db_k)\n    tl.store(db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < K))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "2f60d296-d6b0-4382-9ac9-7d1d519c0111"
  },
  {
    "input": "@triton.jit\ndef parallel_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "cc073017-7dc0-4c30-96fc-f5dd0b806cef"
  },
  {
    "input": "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb,\n    stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb,\n    stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob,\n    stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded,\n    headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE:\n    'tl.constexpr', IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] *\n        stride_qm + offs_d[None, :])\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] *\n        stride_kn + offs_d[None, :])\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] *\n        stride_vn + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:,\n            None] * stride_bm + offs_n[None, :])\n    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\n    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_M & EVEN_N:\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\n    else:\n        q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[\n            None, :] < headdim), other=0.0)\n    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) *\n        BLOCK_M, seqlen_k)\n    for start_n in range(0, end_n, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                k = tl.load(k_ptrs + start_n * stride_kn)\n            else:\n                k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, tl.trans(k))\n        if not EVEN_N:\n            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float\n                ('-inf'))\n        if IS_CAUSAL:\n            qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], \n                0, float('-inf'))\n        if BIAS_TYPE != 'none':\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n <\n                        seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs + start_n)\n                else:\n                    bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] <\n                        seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k\n                        ), other=0.0)\n            qk = qk * softmax_scale + bias\n            m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n            p = tl.exp(qk - m_ij[:, None])\n        else:\n            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n            p = tl.exp(qk * softmax_scale - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        acc_o_scale = tl.exp(m_i - m_ij)\n        tl.store(t_ptrs, acc_o_scale)\n        acc_o_scale = tl.load(t_ptrs)\n        acc_o = acc_o * acc_o_scale[:, None]\n        if EVEN_N & EVEN_M:\n            if EVEN_HEADDIM:\n                v = tl.load(v_ptrs + start_n * stride_vn)\n            else:\n                v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None,\n                    :] < headdim, other=0.0)\n        elif EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n +\n                offs_n)[:, None] < seqlen_k, other=0.0)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n +\n                offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim),\n                other=0.0)\n        p = p\n        acc_o += tl.dot(p, v)\n        m_i = m_ij\n        l_i_new = tl.exp(lse_i - m_ij) + l_ij\n        lse_i = m_ij + tl.log(l_i_new)\n    o_scale = tl.exp(m_i - lse_i)\n    tl.store(t_ptrs, o_scale)\n    o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * o_scale[:, None]\n    start_m = tl.program_id(0)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\n    tl.store(lse_ptrs, lse_i)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    out_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:,\n        None] * stride_om + offs_d[None, :])\n    if EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(out_ptrs, acc_o)\n        else:\n            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\n    else:\n        tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (\n            offs_d[None, :] < headdim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "2c7f302d-9b95-46a8-9282-9ebb541d6ab5"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "22175fa4-87b8-4469-aab6-705692442758"
  },
  {
    "input": "@triton.jit\ndef gelu(x):\n    c = 0.7978845608028654\n    x_cubed = x * x * x\n    tanh_arg = c * (x + 0.044715 * x_cubed)\n    tanh_result = tanh(tanh_arg)\n    return 0.5 * x * (1 + tanh_result)\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "a8921fde-4ba7-4c31-8b1e-0aaf4ddee196"
  },
  {
    "input": "@triton.jit\ndef _voxel_grid_sample(image, batch_index, ix, iy, iz, N: 'tl.constexpr', C:\n    'tl.constexpr', ID: 'tl.constexpr', IH: 'tl.constexpr', IW:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    ix = (ix + 1) / 2 * IW - 0.5\n    iy = (iy + 1) / 2 * IH - 0.5\n    iz = (iz + 1) / 2 * ID - 0.5\n    ix0 = ix - ix % 1\n    iy0 = iy - iy % 1\n    iz0 = iz - iz % 1\n    V000x = ix0\n    V000y = iy0\n    V000z = iz0\n    V100x = ix0\n    V100y = iy0\n    V100z = iz0 + 1\n    V010x = ix0\n    V010y = iy0 + 1\n    V010z = iz0\n    V001x = ix0 + 1\n    V001y = iy0\n    V001z = iz0\n    V101x = ix0 + 1\n    V101y = iy0\n    V101z = iz0 + 1\n    V011x = ix0 + 1\n    V011y = iy0 + 1\n    V011z = iz0\n    V110x = ix0\n    V110y = iy0 + 1\n    V110z = iz0 + 1\n    V111x = ix0 + 1\n    V111y = iy0 + 1\n    V111z = iz0 + 1\n    x = ix - ix0\n    y = iy - iy0\n    z = iz - iz0\n    out_val = _sample_3d(image, (1 - x) * (1 - y) * (1 - z), batch_index,\n        V000x, V000y, V000z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(image,\n        (1 - x) * (1 - y) * z, batch_index, V100x, V100y, V100z, ID, IH, IW,\n        C, BLOCK_SIZE) + _sample_3d(image, (1 - x) * y * (1 - z),\n        batch_index, V010x, V010y, V010z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(image, x * (1 - y) * (1 - z), batch_index, V001x,\n        V001y, V001z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(image, x * (1 -\n        y) * z, batch_index, V101x, V101y, V101z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(image, x * y * (1 - z), batch_index, V011x, V011y,\n        V011z, ID, IH, IW, C, BLOCK_SIZE) + _sample_3d(image, (1 - x) * y *\n        z, batch_index, V110x, V110y, V110z, ID, IH, IW, C, BLOCK_SIZE\n        ) + _sample_3d(image, x * y * z, batch_index, V111x, V111y, V111z,\n        ID, IH, IW, C, BLOCK_SIZE)\n    return out_val\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "4a314d59-8e1f-4eb7-a45d-149da4f6be3c"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv6_bwd_kernel_dq(k, v, w, u, do, dq, dq_aux, h0,\n    s_k_h, s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_dq = dq + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if REVERSE else 0)\n    p_dq_aux = dq_aux + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK) + ((T - 1) * K if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w)\n        h_q = b_h * b_do[:, None]\n        b_dq = tl.sum(h_q + b_kv * b_u[None, :] * b_do[:, None], axis=0)\n        b_dq *= scale\n        b_dq_aux = tl.sum(h_q, axis=0)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_dq, b_dq, mask=mask_bk)\n        tl.store(p_dq_aux, b_dq_aux, mask=mask_bk)\n        p_k += -K if REVERSE else K\n        p_do += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n        p_dq += -K if REVERSE else K\n        p_dq_aux += -K if REVERSE else K\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "75f7e485-e5ec-480b-8c93-103ce0787cee"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_stages=1, num_warps=4),\n    triton.Config({}, num_stages=1, num_warps=8), triton.Config({},\n    num_stages=2, num_warps=4), triton.Config({}, num_stages=2, num_warps=8\n    ), triton.Config({}, num_stages=3, num_warps=4), triton.Config({},\n    num_stages=3, num_warps=8), triton.Config({}, num_stages=4, num_warps=4\n    ), triton.Config({}, num_stages=4, num_warps=8), triton.Config({},\n    num_stages=5, num_warps=4), triton.Config({}, num_stages=5, num_warps=8\n    )], key=['N_CTX'])\n@triton.jit\ndef triton_sparse_fwd_kernel(Q, K, V, seqlens, sm_scale, col_count,\n    col_index, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vn,\n    stride_vk, stride_oz, stride_oh, stride_om, stride_ok, Z, H, N_CTX,\n    NUM_ROWS, MAX_COLS_PRE_ROW, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', dtype: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    num_cols = tl.load(col_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = col_index + (off_hz * NUM_ROWS + start_m) * MAX_COLS_PRE_ROW\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = q * qk_scale\n    m_mask = offs_m[:, None] < seqlen\n    split = tl.maximum(num_cols - BLOCK_N, 0) & ~(BLOCK_N - 1)\n    for start_n in range(0, split, BLOCK_N):\n        cols = tl.load(cols_ptr + start_n + offs_n)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    for start_n in range(split, num_cols, BLOCK_N):\n        n_mask = start_n + offs_n < num_cols\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=n_mask, other=\n            N_CTX - 1)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc = tl.where(m_mask, acc / l_i[:, None], 0.0)\n    tl.store(o_ptrs, acc, mask=m_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "17dd1354-15aa-41de-bbed-922b2e95f207"
  },
  {
    "input": "@triton.heuristics(values={'USE_MASK': lambda args: args['numels'] % args[\n    'BLOCK_SIZE'] != 0, 'NUM_GROUPS': lambda args: triton.cdiv(args[\n    'numels'], args['BLOCK_SIZE'])})\n@triton.jit\ndef _quantize_blockwise_kernel(t_ptr, cutoffs_ptr, q_ptr, absmax_ptr,\n    norm_ptr, numels, BLOCK_SIZE: 'tl.constexpr', NUM_BUCKETS:\n    'tl.constexpr', USE_MASK: 'tl.constexpr', NUM_GROUPS: 'tl.constexpr',\n    RETURN_NORM: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = None\n    absmax_mask = None\n    if USE_MASK:\n        mask = offsets < numels\n        absmax_mask = pid < NUM_GROUPS\n    t = tl.load(t_ptr + offsets, mask=mask)\n    absmax = tl.max(tl.abs(t), axis=0)\n    normalized = t / absmax\n    cutoffs = tl.load(cutoffs_ptr + tl.arange(0, NUM_BUCKETS))\n    q = tl.reshape(normalized, (BLOCK_SIZE, 1)) > cutoffs\n    q = q\n    q = tl.sum(q, axis=1)\n    tl.store(q_ptr + offsets, q, mask=mask)\n    tl.store(absmax_ptr + pid, absmax, mask=absmax_mask)\n    if RETURN_NORM:\n        tl.store(norm_ptr + offsets, normalized, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "2be2877f-70f1-4d4d-8ad9-3a0801c99300"
  },
  {
    "input": "@triton.jit\ndef _splitK_reduce_varargs(Out_splitK: \"'VAR_ARGS_ARRAY'\", LSE_splitK:\n    \"'VAR_ARGS_ARRAY'\", Out, LSE, stride_osk_z: \"'VAR_ARGS_ARRAY'\",\n    stride_osk_g: \"'VAR_ARGS_ARRAY'\", stride_osk_h: \"'VAR_ARGS_ARRAY'\",\n    stride_osk_m: \"'VAR_ARGS_ARRAY'\", stride_osk_k: \"'VAR_ARGS_ARRAY'\",\n    stride_lsek_z: \"'VAR_ARGS_ARRAY'\", stride_lsek_g: \"'VAR_ARGS_ARRAY'\",\n    stride_lsek_h: \"'VAR_ARGS_ARRAY'\", stride_lsek_m: \"'VAR_ARGS_ARRAY'\",\n    stride_oz, stride_og, stride_oh, stride_om, stride_ok, stride_lse_z,\n    stride_lse_g, stride_lse_h, stride_lse_m, BLOCK_SIZE: 'tl.constexpr', H:\n    'tl.constexpr', G: 'tl.constexpr', WRITE_LSE: 'tl.constexpr'):\n    \"\"\"\n    This version of reduce kernel takes attention and LSE of chunks as lists of tensors,\n    as opposed to _splitK_reduce, which takes each as a stacked tensor.\n    \"\"\"\n    off_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    out_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        out_splitk_offset[i] = stride_osk_z[i] * off_z + stride_osk_g[i\n            ] * off_g + stride_osk_h[i] * off_h + stride_osk_m[i\n            ] * off_m + tl.arange(0, BLOCK_SIZE)\n    lse_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        lse_splitk_offset[i] = stride_lsek_z[i] * off_z + stride_lsek_g[i\n            ] * off_g + stride_lsek_h[i] * off_h + stride_lsek_m[i] * off_m\n    lse_max = float('-inf')\n    for split_k_idx in range(len(Out_splitK)):\n        LSE_splitK_ptr = LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        lse_splitk = tl.load(LSE_splitK_ptr)\n        lse_max = tl.maximum(lse_max, lse_splitk)\n    sumexp_normalized = 0.0\n    numerator_normalized = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for split_k_idx in range(len(Out_splitK)):\n        out_splitk = tl.load(Out_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx])\n        lse_splitk = tl.load(LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx])\n        sumexp_normalized_splitk = tl.math.exp2((lse_splitk - lse_max) * \n            1.44269504)\n        sumexp_normalized += sumexp_normalized_splitk\n        numerator_normalized += out_splitk * sumexp_normalized_splitk\n    acc = numerator_normalized / sumexp_normalized\n    acc = tl.where(lse_max == float('-inf'), 0.0, acc)\n    Out_ptr = (Out + stride_oz * off_z + stride_oh * off_h + stride_og *\n        off_g + stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    if acc.dtype is tl.float64 and Out.dtype.element_ty is not tl.float64:\n        acc = acc\n    tl.store(Out_ptr, acc)\n    if WRITE_LSE:\n        l_ptrs = (LSE + off_z * stride_lse_z + off_g * stride_lse_g + off_h *\n            stride_lse_h + off_m * stride_lse_m)\n        to_store = lse_max + tl.math.log2(sumexp_normalized) / 1.44269504\n        to_store = tl.where(lse_max == float('-inf'), lse_max, to_store)\n        tl.store(l_ptrs, to_store)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "06fcbec3-cfaf-4b67-b208-f670769edae9"
  },
  {
    "input": "@triton.jit\ndef ninth_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_7 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_8 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    g_9 = tl.load(sph_grad_ptr + output_row_offset + 9, mask=\n        output_row_offset + 9 < output_numel)\n    g_10 = tl.load(sph_grad_ptr + output_row_offset + 10, mask=\n        output_row_offset + 10 < output_numel)\n    g_11 = tl.load(sph_grad_ptr + output_row_offset + 11, mask=\n        output_row_offset + 11 < output_numel)\n    g_12 = tl.load(sph_grad_ptr + output_row_offset + 12, mask=\n        output_row_offset + 12 < output_numel)\n    g_13 = tl.load(sph_grad_ptr + output_row_offset + 13, mask=\n        output_row_offset + 13 < output_numel)\n    g_14 = tl.load(sph_grad_ptr + output_row_offset + 14, mask=\n        output_row_offset + 14 < output_numel)\n    g_15 = tl.load(sph_grad_ptr + output_row_offset + 15, mask=\n        output_row_offset + 15 < output_numel)\n    g_16 = tl.load(sph_grad_ptr + output_row_offset + 16, mask=\n        output_row_offset + 16 < output_numel)\n    g_17 = tl.load(sph_grad_ptr + output_row_offset + 17, mask=\n        output_row_offset + 17 < output_numel)\n    g_18 = tl.load(sph_grad_ptr + output_row_offset + 18, mask=\n        output_row_offset + 18 < output_numel)\n    CONST000 = 1.59908344719522\n    CONST001 = 2.0\n    CONST002 = 3.0\n    CONST003 = 4.0\n    CONST004 = 5.0\n    CONST005 = 6.39633378878088\n    CONST006 = 7.0\n    CONST007 = 8.63855507530412\n    CONST008 = 9.59450068317133\n    CONST009 = 6.39633378878088\n    CONST011 = 12.7926675775618\n    CONST012 = 12.7926675775618\n    CONST014 = 15.5493991355474\n    CONST015 = 14.391751024757\n    CONST017 = 15.0007324039945\n    CONST018 = 14.45506743704\n    CONST019 = 14.45506743704\n    CONST020 = 13.3827919767794\n    CONST021 = 23.8930627690618\n    CONST022 = 23.8930627690618\n    CONST023 = 27.0429549260581\n    CONST024 = 29.2403830344269\n    CONST025 = 30.001464807989\n    CONST027 = 29.2403830344269\n    CONST028 = 38.3780027326853\n    CONST031 = 39.2300904918661\n    CONST032 = 42.9079114754785\n    CONST033 = 10.7269778688696\n    CONST034 = 54.0859098521163\n    CONST036 = 58.9217071894985\n    CONST037 = 57.8202697481601\n    CONST038 = 60.0029296159779\n    CONST039 = 62.4530292249704\n    CONST040 = 64.3618672132178\n    CONST042 = 69.1084406024329\n    CONST044 = 78.5622762526647\n    CONST045 = 85.815822950957\n    CONST046 = 85.815822950957\n    CONST050 = 107.062335814235\n    CONST052 = 108.171819704233\n    CONST053 = -1935.03633686812\n    CONST055 = 115.64053949632\n    CONST056 = 117.843414378997\n    CONST057 = 117.843414378997\n    CONST059 = 120.005859231956\n    CONST060 = 2176.91587897664\n    CONST061 = 2176.91587897664\n    CONST064 = 150.007324039945\n    CONST065 = -1892.23403121978\n    CONST066 = -1885.49463006395\n    CONST067 = 173.46080924448\n    CONST068 = -1873.59087674911\n    CONST070 = 10.7269778688696\n    CONST071 = 180.008788847934\n    CONST074 = 13.5214774630291\n    CONST076 = 205.957975082297\n    CONST078 = 216.343639408465\n    CONST079 = 4326.8727881693\n    CONST080 = 233.923064275415\n    CONST081 = 233.923064275415\n    CONST082 = 240.011718463912\n    CONST083 = 241.879542108515\n    CONST085 = 255.853351551235\n    CONST086 = 255.853351551235\n    CONST087 = 257.447468852871\n    CONST088 = 257.447468852871\n    CONST090 = 270.429549260581\n    CONST091 = 289.101348740801\n    CONST093 = 300.01464807989\n    CONST097 = 13.0937127087774\n    CONST099 = -3747.18175349822\n    CONST100 = 6.39633378878088\n    CONST103 = 374.718175349822\n    CONST105 = 404.741888237121\n    CONST106 = 411.915950164594\n    CONST107 = 412.45195032649\n    CONST108 = 432.68727881693\n    CONST109 = 435.383175795328\n    CONST110 = 435.383175795327\n    CONST112 = 462.562157985281\n    CONST113 = -1571.24552505329\n    CONST114 = 483.759084217031\n    CONST115 = 511.706703102471\n    CONST116 = 562.077263024733\n    CONST117 = 578.202697481601\n    CONST119 = -1451.27725265109\n    CONST121 = -1451.27725265109\n    CONST123 = 600.029296159779\n    CONST124 = -1440.07031078347\n    CONST129 = -1387.68647395584\n    CONST130 = -1387.68647395584\n    CONST131 = -1373.05316721531\n    CONST132 = -1338.01151506746\n    CONST133 = 725.638626325546\n    CONST134 = -1298.06183645079\n    CONST137 = 788.430846341574\n    CONST138 = -1249.06058449941\n    CONST139 = -1228.09608744593\n    CONST140 = -1228.09608744593\n    CONST141 = 823.831900329187\n    CONST142 = -3245.15459112698\n    CONST143 = -1178.43414378997\n    CONST144 = 870.766351590655\n    CONST145 = 870.766351590655\n    CONST147 = -1124.15452604947\n    CONST149 = -3153.7233853663\n    CONST150 = 960.046873855647\n    CONST151 = 960.046873855647\n    CONST152 = 967.518168434061\n    CONST153 = -1081.71819704233\n    CONST154 = 967.518168434061\n    CONST155 = -1060.59072941097\n    CONST156 = 1023.41340620494\n    CONST157 = 1023.41340620494\n    CONST159 = -967.518168434061\n    CONST160 = 1081.71819704233\n    CONST161 = -960.046873855647\n    CONST163 = -936.795438374555\n    CONST165 = -900.043944239669\n    CONST166 = 1156.4053949632\n    CONST168 = -2902.55450530218\n    CONST170 = 11.2632978048796\n    CONST171 = -785.622762526647\n    CONST172 = -785.622762526647\n    CONST173 = -767.560054653706\n    CONST175 = 1338.01151506746\n    CONST176 = -693.843236977922\n    CONST177 = -693.843236977921\n    CONST178 = -686.526583607656\n    CONST179 = -669.005757533731\n    CONST180 = -669.005757533731\n    CONST182 = -649.030918225395\n    CONST183 = -630.744677073259\n    CONST184 = -628.498210021318\n    CONST185 = -628.498210021317\n    CONST186 = -600.029296159779\n    CONST187 = -589.217071894985\n    CONST188 = -578.202697481601\n    CONST189 = 15.5493991355474\n    CONST190 = -562.077263024733\n    CONST191 = 1500.07324039945\n    CONST192 = -480.023436927823\n    CONST193 = -480.023436927823\n    CONST195 = -462.562157985281\n    CONST196 = -450.021972119834\n    CONST197 = -412.45195032649\n    CONST198 = -409.365362481977\n    CONST199 = -409.365362481976\n    CONST200 = -404.741888237121\n    CONST201 = -392.811381263323\n    CONST202 = -383.780027326853\n    CONST203 = -383.780027326853\n    CONST204 = 1672.51439383433\n    CONST205 = -374.718175349822\n    CONST206 = -353.530243136991\n    CONST207 = -2400.11718463912\n    CONST209 = -346.921618488961\n    CONST210 = -346.921618488961\n    CONST211 = -343.263291803828\n    CONST212 = -338.631358951921\n    CONST213 = -338.631358951921\n    CONST214 = -324.515459112698\n    CONST215 = -315.37233853663\n    CONST216 = -314.249105010659\n    CONST217 = -2356.86828757994\n    CONST218 = -300.01464807989\n    CONST219 = -294.608535947493\n    CONST220 = -289.101348740801\n    CONST221 = -270.013183271901\n    CONST222 = -2312.81078992641\n    CONST223 = 1800.08788847934\n    CONST224 = -241.879542108515\n    CONST225 = -240.011718463912\n    CONST226 = -241.879542108515\n    CONST227 = -4326.8727881693\n    CONST228 = -216.343639408465\n    CONST229 = -210.010253655923\n    CONST230 = -204.682681240988\n    CONST231 = -204.682681240988\n    CONST232 = -204.682681240988\n    CONST233 = -196.405690631662\n    CONST234 = -191.144502152495\n    CONST235 = -191.890013663426\n    CONST236 = -191.890013663427\n    CONST237 = -187.359087674911\n    CONST238 = -180.008788847934\n    CONST239 = -176.765121568496\n    CONST241 = 1873.59087674911\n    CONST242 = -173.46080924448\n    CONST244 = -162.257729556349\n    CONST245 = -156.920361967464\n    CONST246 = -156.920361967464\n    CONST248 = -150.007324039945\n    CONST249 = -144.5506743704\n    CONST250 = -137.14955340795\n    CONST251 = -135.214774630291\n    CONST252 = -127.926675775618\n    CONST253 = -127.926675775618\n    CONST254 = -120.939771054258\n    CONST255 = -120.005859231956\n    CONST256 = -120.939771054258\n    CONST257 = -117.843414378997\n    CONST258 = -117.843414378997\n    CONST259 = -115.64053949632\n    CONST260 = -115.64053949632\n    CONST261 = 1935.03633686812\n    CONST262 = -2163.43639408465\n    CONST263 = -114.421097267943\n    CONST264 = -108.171819704233\n    CONST265 = -107.062335814235\n    CONST266 = -108.171819704233\n    CONST267 = -104.74970167022\n    CONST268 = -96.7518168434061\n    CONST269 = -96.7518168434061\n    CONST270 = -90.0043944239669\n    CONST271 = -90.106382439037\n    CONST272 = -80.2967518606762\n    CONST273 = -78.4601809837321\n    CONST274 = -78.4601809837321\n    CONST275 = -77.2655855030233\n    CONST276 = -78.5622762526647\n    CONST277 = -68.5747767039748\n    CONST278 = -63.9633378878088\n    CONST279 = -62.4530292249704\n    CONST280 = -61.8124684024186\n    CONST281 = -60.0029296159779\n    CONST282 = -63.9633378878088\n    CONST283 = -58.9217071894985\n    CONST284 = -57.8202697481601\n    CONST285 = -57.8202697481601\n    CONST286 = -48.375908421703\n    CONST287 = -48.3759084217031\n    CONST288 = -39.2811381263323\n    CONST289 = -38.6327927515116\n    CONST290 = -39.2811381263323\n    CONST291 = -30.9062342012093\n    CONST292 = -30.001464807989\n    CONST293 = -30.001464807989\n    CONST294 = -27.6433762409732\n    CONST295 = -17.3847567381802\n    CONST296 = -15.0007324039945\n    CONST297 = -14.7304267973746\n    CONST298 = -13.5214774630291\n    CONST299 = -13.0937127087774\n    CONST300 = -13.3827919767794\n    CONST301 = -9.82028453158308\n    CONST302 = -4.91014226579154\n    CONST303 = 2046.82681240988\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR02 = VAR06 * VAR06\n    VAR03 = VAR06 * VAR07\n    VAR04 = VAR07 * VAR07\n    VAR05 = VAR07 * VAR08\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR11 = VAR15 * VAR15\n    VAR12 = VAR15 * VAR16\n    VAR13 = VAR16 * VAR16\n    VAR14 = VAR16 * VAR17\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR20 = VAR24 * VAR24\n    VAR21 = VAR24 * VAR25\n    VAR22 = VAR25 * VAR25\n    VAR23 = VAR25 * VAR26\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += g_0 * (CONST021 * VAR20 + CONST022 * VAR02 + CONST179 * VAR04 *\n        VAR26 + CONST180 * VAR08 * VAR22 + CONST204 * VAR06 * VAR24\n        ) + g_1 * y * (CONST065 * VAR08 * VAR23 - CONST149 * VAR06 * VAR25 +\n        CONST183 * VAR04 * z - CONST271 * VAR21) + g_10 * (CONST012 * VAR21 *\n        x + VAR23 * (CONST028 * VAR07 + CONST203 * VAR17 * x) + VAR25 * (\n        CONST028 * VAR05 + CONST157 * VAR15 * x + CONST173 * VAR07 * VAR17) +\n        z * (CONST011 * VAR03 + CONST157 * VAR07 * VAR15 + CONST198 * VAR13 *\n        x + CONST202 * VAR05 * VAR17)) + g_11 * (CONST150 * VAR07 * VAR14 +\n        CONST250 * VAR12 * x + VAR16 * (CONST093 * VAR24 * x + CONST165 *\n        VAR05 + CONST186 * VAR07 * VAR26) + y * (CONST059 * VAR03 + \n        CONST071 * VAR05 * VAR26 + CONST281 * VAR22 * x)) + g_12 * (VAR23 *\n        (CONST257 * VAR17 * x - CONST290 * VAR07) + VAR25 * (CONST044 *\n        VAR05 + CONST143 * VAR07 * VAR17 - CONST172 * VAR15 * x) + z * (\n        CONST155 * VAR05 * VAR17 + CONST184 * VAR13 * x - CONST217 * VAR07 *\n        VAR15 - CONST288 * VAR03)) + g_13 * (VAR14 * (CONST129 * VAR26 * x -\n        CONST195 * VAR07) + VAR16 * (CONST166 * VAR24 * x + CONST176 *\n        VAR05 - CONST222 * VAR07 * VAR26) + y * (CONST188 * VAR07 * VAR24 +\n        CONST209 * VAR05 * VAR26 - CONST259 * VAR03 + CONST259 * VAR22 * x)\n        ) + g_14 * (CONST042 * VAR03 * z + CONST268 * VAR07 * VAR23 + \n        CONST294 * VAR21 * x + VAR15 * (CONST053 * VAR25 * x + CONST261 *\n        VAR07 * z) + VAR17 * (CONST119 * VAR05 * z + CONST144 * VAR23 * x +\n        CONST152 * VAR07 * VAR25)) + g_15 * (VAR16 * (CONST068 * VAR24 * x -\n        CONST099 * VAR07 * VAR26 + CONST205 * VAR05) + y * (CONST050 *\n        VAR03 + CONST147 * VAR05 * VAR26 - CONST205 * VAR22 * x)) + g_16 * (\n        CONST214 * VAR05 * VAR25 - CONST264 * VAR03 * z + CONST264 * VAR07 *\n        VAR23 - CONST275 * VAR21 * x + VAR17 * (CONST079 * VAR07 * VAR25 + \n        CONST134 * VAR05 * z + CONST134 * VAR23 * x)) + g_17 * y * (\n        CONST065 * VAR05 * VAR26 - CONST149 * VAR07 * VAR24 + CONST183 *\n        VAR22 * x - CONST271 * VAR03) + g_18 * (CONST132 * VAR05 * VAR25 + \n        CONST175 * VAR07 * VAR23 - CONST234 * VAR03 * z + CONST234 * VAR21 * x\n        ) + g_2 * (CONST002 * VAR08 * (CONST034 * VAR22 + CONST153 * VAR17 *\n        VAR24) + CONST004 * VAR06 * (CONST023 * VAR24 - CONST182 * VAR17 *\n        VAR26) + CONST006 * VAR04 * (CONST289 * VAR26 + CONST291 * VAR17) -\n        CONST228 * VAR17 * VAR22 - CONST295 * VAR02 + CONST298 * VAR20\n        ) + g_3 * (VAR16 * (-CONST068 * VAR06 * z + CONST099 * VAR08 *\n        VAR25 + CONST103 * VAR23) + y * (CONST116 * VAR08 * VAR23 - \n        CONST163 * VAR06 * VAR25 + CONST190 * VAR04 * z + CONST272 * VAR21)\n        ) + g_4 * (CONST007 * VAR20 + CONST014 * VAR02 + CONST254 * VAR06 *\n        VAR24 + CONST269 * VAR04 * VAR26 + VAR15 * (CONST114 * VAR06 + \n        CONST114 * VAR24 + CONST168 * VAR08 * VAR26) + VAR17 * (CONST060 *\n        VAR06 * VAR26 + CONST133 * VAR08 * VAR24 + CONST212 * VAR04 + \n        CONST224 * VAR22)) + g_5 * (VAR14 * (CONST130 * VAR08 * z - \n        CONST195 * VAR25) + VAR16 * (CONST195 * VAR23 - CONST222 * VAR06 *\n        z) + y * (CONST067 * VAR08 * VAR23 + CONST200 * VAR04 * z + \n        CONST220 * VAR06 * VAR25 - CONST284 * VAR21)) + g_6 * (CONST002 *\n        VAR08 * (CONST201 * VAR15 * VAR26 - CONST219 * VAR17 * VAR24 + \n        CONST267 * VAR13 + CONST299 * VAR22) + CONST004 * VAR06 * (CONST036 *\n        VAR17 * VAR26 - CONST233 * VAR15 + CONST301 * VAR24) + CONST187 *\n        VAR15 * VAR24 + CONST197 * VAR04 * VAR17 - CONST216 * VAR13 * VAR26 -\n        CONST239 * VAR17 * VAR22 - CONST297 * VAR02 + CONST302 * VAR20\n        ) + g_7 * (CONST002 * VAR08 * (-CONST186 * VAR16 * VAR25 + CONST192 *\n        VAR14 * z + CONST270 * VAR23 * y) + CONST004 * VAR06 * (-CONST218 *\n        VAR16 * z + CONST270 * VAR25 * y) + CONST193 * VAR14 * VAR25 - \n        CONST218 * VAR16 * VAR23 + CONST229 * VAR04 * y * z - CONST250 *\n        VAR12 * z + CONST292 * VAR21 * y) + g_8 * (CONST000 * VAR20 + \n        CONST002 * VAR08 * (CONST005 * VAR22 + CONST115 * VAR15 * VAR26 + \n        CONST230 * VAR13 + CONST235 * VAR17 * VAR24) + CONST004 * VAR06 * (\n        CONST008 * VAR24 + CONST085 * VAR15 + CONST235 * VAR17 * VAR26) + \n        CONST006 * VAR04 * (CONST009 * VAR26 + CONST278 * VAR17) + CONST015 *\n        VAR02 + CONST024 * VAR11 + CONST085 * VAR15 * VAR24 + CONST231 *\n        VAR13 * VAR26 + CONST278 * VAR17 * VAR22) + g_9 * (CONST245 * VAR12 *\n        x + VAR14 * (CONST141 * VAR07 + CONST141 * VAR26 * x) + VAR16 * (\n        CONST131 * VAR07 * VAR26 + CONST178 * VAR05 + CONST178 * VAR24 * x) +\n        y * (CONST045 * VAR03 + CONST046 * VAR22 * x + CONST087 * VAR05 *\n        VAR26 + CONST088 * VAR07 * VAR24))\n    g_y += CONST001 * g_16 * y * (CONST160 * VAR06 * VAR25 + CONST182 *\n        VAR08 * VAR23 + CONST228 * VAR04 * z - CONST291 * VAR21) + g_1 * (-\n        CONST183 * VAR05 * VAR25 + CONST183 * VAR07 * VAR23 + CONST271 *\n        VAR03 * z - CONST271 * VAR21 * x) + g_10 * (CONST252 * VAR21 * y + \n        VAR23 * (CONST157 * VAR16 + CONST203 * VAR08 * y) + VAR25 * (\n        CONST140 * VAR14 + CONST202 * VAR06 * y + CONST303 * VAR08 * VAR16) +\n        z * (CONST080 * VAR12 + CONST139 * VAR08 * VAR14 + CONST157 * VAR06 *\n        VAR16 + CONST252 * VAR04 * y)) + g_11 * (CONST002 * VAR17 * (\n        CONST064 * VAR08 * VAR24 + CONST248 * VAR04 + CONST248 * VAR06 *\n        VAR26 - CONST248 * VAR22) + CONST004 * VAR15 * (CONST082 * VAR06 + \n        CONST225 * VAR24) + CONST006 * VAR13 * (CONST277 * VAR08 - CONST277 *\n        VAR26) + CONST017 * VAR02 + CONST025 * VAR04 * VAR26 + CONST293 *\n        VAR08 * VAR22 + CONST296 * VAR20) + g_12 * (CONST056 * VAR21 * y + \n        VAR23 * (CONST171 * VAR16 + CONST257 * VAR08 * y) + VAR25 * (-\n        CONST113 * VAR08 * VAR16 - CONST185 * VAR14 + CONST187 * VAR06 * y) +\n        z * (CONST066 * VAR08 * VAR14 + CONST206 * VAR04 * y - CONST217 *\n        VAR06 * VAR16)) + g_13 * (CONST002 * VAR17 * (CONST117 * VAR06 *\n        VAR26 + CONST117 * VAR08 * VAR24 + CONST259 * VAR04 + CONST260 *\n        VAR22) + CONST004 * VAR15 * (CONST055 * VAR06 + CONST055 * VAR24 + \n        CONST176 * VAR08 * VAR26) + CONST018 * VAR20 + CONST019 * VAR02 + \n        CONST249 * VAR06 * VAR24 + CONST284 * VAR04 * VAR26 + CONST285 *\n        VAR08 * VAR22) + g_14 * (CONST001 * y * (CONST083 * VAR06 * VAR25 +\n        CONST109 * VAR08 * VAR23 + CONST226 * VAR04 * z + CONST286 * VAR21) +\n        CONST003 * VAR16 * (CONST114 * VAR06 * z + CONST159 * VAR08 * VAR25 -\n        CONST269 * VAR23)) + g_15 * (CONST002 * VAR17 * (CONST039 * VAR22 -\n        CONST163 * VAR06 * VAR26 + CONST163 * VAR08 * VAR24 + CONST279 *\n        VAR04) + CONST020 * VAR02 + CONST237 * VAR04 * VAR26 - CONST237 *\n        VAR08 * VAR22 + CONST300 * VAR20) + g_17 * (CONST137 * VAR06 *\n        VAR24 + CONST170 * VAR02 + CONST170 * VAR20 + CONST215 * VAR04 *\n        VAR26 + CONST215 * VAR08 * VAR22) + g_2 * (CONST108 * VAR22 * x * y -\n        CONST134 * VAR05 * VAR26 * y + CONST262 * VAR07 * VAR24 * y + \n        CONST280 * VAR03 * y) + g_3 * (CONST002 * VAR17 * (CONST103 * VAR23 *\n        x + CONST138 * VAR07 * VAR25 - CONST205 * VAR05 * z) - CONST237 *\n        VAR05 * VAR25 - CONST237 * VAR07 * VAR23 + CONST272 * VAR03 * z + \n        CONST272 * VAR21 * x) + g_4 * (CONST001 * y * (CONST110 * VAR05 *\n        VAR26 - CONST224 * VAR07 * VAR24 + CONST224 * VAR22 * x + CONST287 *\n        VAR03) + CONST003 * VAR16 * (CONST114 * VAR24 * x + CONST159 *\n        VAR07 * VAR26 - CONST269 * VAR05)) + g_5 * (CONST002 * VAR17 * (\n        CONST112 * VAR05 * z + CONST195 * VAR23 * x) + CONST004 * VAR15 * (\n        CONST195 * VAR07 * z - CONST195 * VAR25 * x) + CONST037 * VAR07 *\n        VAR23 + CONST284 * VAR05 * VAR25 - CONST284 * VAR21 * x + CONST285 *\n        VAR03 * z) + g_6 * (CONST258 * VAR03 * y + VAR05 * (CONST057 *\n        VAR26 * y - CONST171 * VAR16) + VAR07 * (CONST113 * VAR16 * VAR26 +\n        CONST185 * VAR14 - CONST187 * VAR24 * y) + x * (-CONST066 * VAR14 *\n        VAR26 - CONST206 * VAR22 * y + CONST217 * VAR16 * VAR24)) + g_7 * (\n        CONST292 * VAR03 * z + VAR05 * (-CONST165 * VAR17 * z + CONST270 *\n        VAR25) + VAR07 * (CONST207 * VAR15 * z + CONST223 * VAR17 * VAR25 +\n        CONST270 * VAR23) + x * (CONST151 * VAR13 * z - CONST165 * VAR17 *\n        VAR23 + CONST207 * VAR15 * VAR25 + CONST292 * VAR21)) + g_8 * (\n        CONST253 * VAR03 * y + VAR05 * (CONST156 * VAR16 + CONST202 * VAR26 *\n        y) + VAR07 * (CONST139 * VAR14 + CONST202 * VAR24 * y + CONST303 *\n        VAR16 * VAR26) + x * (CONST081 * VAR12 + CONST140 * VAR14 * VAR26 +\n        CONST156 * VAR16 * VAR24 + CONST253 * VAR22 * y)) + g_9 * (CONST002 *\n        VAR17 * (CONST211 * VAR06 * VAR26 + CONST211 * VAR08 * VAR24 + \n        CONST263 * VAR04 + CONST263 * VAR22) + CONST004 * VAR15 * (CONST076 *\n        VAR06 + CONST076 * VAR24 + CONST106 * VAR08 * VAR26) + CONST006 *\n        VAR13 * (CONST273 * VAR26 + CONST274 * VAR08) + CONST031 * VAR11 + \n        CONST032 * VAR04 * VAR26 + CONST032 * VAR08 * VAR22 + CONST033 *\n        VAR20 + CONST040 * VAR06 * VAR24 + CONST070 * VAR02)\n    g_z += g_0 * (CONST132 * VAR07 * VAR23 + CONST175 * VAR05 * VAR25 + \n        CONST234 * VAR03 * z - CONST234 * VAR21 * x) + g_1 * y * (-CONST065 *\n        VAR05 * VAR26 + CONST149 * VAR07 * VAR24 - CONST183 * VAR22 * x + \n        CONST271 * VAR03) + g_10 * (CONST000 * VAR02 + CONST002 * VAR26 * (\n        CONST100 * VAR04 + CONST115 * VAR08 * VAR15 + CONST231 * VAR13 + \n        CONST235 * VAR06 * VAR17) + CONST004 * VAR24 * (CONST008 * VAR06 + \n        CONST086 * VAR15 + CONST236 * VAR08 * VAR17) + CONST006 * VAR22 * (\n        CONST005 * VAR08 + CONST282 * VAR17) + CONST015 * VAR20 + CONST027 *\n        VAR11 + CONST086 * VAR06 * VAR15 + CONST232 * VAR08 * VAR13 + \n        CONST282 * VAR04 * VAR17) + g_11 * (CONST161 * VAR14 * VAR25 - \n        CONST250 * VAR12 * z + VAR16 * (CONST123 * VAR08 * VAR25 - CONST165 *\n        VAR23 + CONST218 * VAR06 * z) + y * (CONST038 * VAR04 * z + \n        CONST238 * VAR08 * VAR23 + CONST255 * VAR21)) + g_12 * (CONST002 *\n        VAR26 * (CONST097 * VAR04 - CONST201 * VAR08 * VAR15 + CONST219 *\n        VAR06 * VAR17 - CONST267 * VAR13) + CONST004 * VAR24 * (CONST233 *\n        VAR15 + CONST283 * VAR08 * VAR17 - CONST301 * VAR06) + CONST107 *\n        VAR17 * VAR22 - CONST187 * VAR06 * VAR15 + CONST216 * VAR08 * VAR13 +\n        CONST239 * VAR04 * VAR17 + CONST297 * VAR20 - CONST302 * VAR02\n        ) + g_13 * (VAR14 * (CONST129 * VAR08 * z - CONST195 * VAR25) + \n        VAR16 * (CONST166 * VAR06 * z + CONST177 * VAR23 - CONST222 * VAR08 *\n        VAR25) + y * (CONST188 * VAR06 * VAR25 + CONST210 * VAR08 * VAR23 +\n        CONST260 * VAR04 * z - CONST260 * VAR21)) + g_14 * (CONST007 *\n        VAR02 + CONST189 * VAR20 + CONST256 * VAR06 * VAR24 + CONST269 *\n        VAR08 * VAR22 + VAR15 * (CONST114 * VAR06 + CONST114 * VAR24 + \n        CONST168 * VAR08 * VAR26) + VAR17 * (CONST061 * VAR08 * VAR24 + \n        CONST133 * VAR06 * VAR26 + CONST213 * VAR22 + CONST226 * VAR04)\n        ) + g_15 * (VAR16 * (-CONST068 * VAR06 * z + CONST099 * VAR08 *\n        VAR25 + CONST103 * VAR23) + y * (-CONST147 * VAR08 * VAR23 + \n        CONST205 * VAR04 * z + CONST265 * VAR21)) + g_16 * (CONST074 *\n        VAR02 + CONST090 * VAR08 * VAR22 + CONST244 * VAR04 * VAR26 + \n        CONST251 * VAR06 * VAR24 + CONST295 * VAR20 + VAR17 * (CONST078 *\n        VAR22 - CONST142 * VAR06 * VAR26 + CONST142 * VAR08 * VAR24 + \n        CONST228 * VAR04)) + g_17 * y * (CONST065 * VAR08 * VAR23 - \n        CONST149 * VAR06 * VAR25 + CONST183 * VAR04 * z - CONST271 * VAR21\n        ) + g_18 * (CONST021 * VAR02 + CONST022 * VAR20 + CONST179 * VAR08 *\n        VAR22 + CONST180 * VAR04 * VAR26 + CONST204 * VAR06 * VAR24) + g_2 * (\n        CONST275 * VAR03 * z + VAR05 * (CONST052 * VAR25 - CONST134 * VAR17 *\n        z) + VAR07 * (-CONST214 * VAR23 + CONST227 * VAR17 * VAR25) + x * (\n        -CONST134 * VAR17 * VAR23 + CONST266 * VAR21)) + g_3 * (VAR16 * (\n        CONST099 * VAR07 * VAR26 - CONST205 * VAR05 + CONST241 * VAR24 * x) +\n        y * (CONST116 * VAR05 * VAR26 - CONST163 * VAR07 * VAR24 + CONST190 *\n        VAR22 * x + CONST272 * VAR03)) + g_4 * (CONST042 * VAR21 * x + \n        CONST269 * VAR05 * VAR25 + CONST294 * VAR03 * z + VAR15 * (CONST053 *\n        VAR07 * z + CONST261 * VAR25 * x) + VAR17 * (CONST121 * VAR23 * x +\n        CONST145 * VAR05 * z + CONST154 * VAR07 * VAR25)) + g_5 * (VAR14 *\n        (-CONST130 * VAR26 * x + CONST195 * VAR07) + VAR16 * (CONST112 *\n        VAR05 + CONST222 * VAR24 * x) + y * (CONST091 * VAR07 * VAR24 + \n        CONST105 * VAR22 * x + CONST242 * VAR05 * VAR26 + CONST285 * VAR03)\n        ) + g_6 * (VAR05 * (CONST057 * VAR17 * z + CONST290 * VAR25) + \n        VAR07 * (-CONST143 * VAR17 * VAR25 + CONST172 * VAR15 * z + \n        CONST276 * VAR23) + x * (-CONST155 * VAR17 * VAR23 - CONST184 *\n        VAR13 * z + CONST217 * VAR15 * VAR25 + CONST288 * VAR21)) + g_7 * (\n        CONST292 * VAR03 * y + VAR05 * (-CONST218 * VAR16 + CONST221 *\n        VAR26 * y) + VAR07 * (CONST192 * VAR14 + CONST196 * VAR24 * y + \n        CONST223 * VAR16 * VAR26) + x * (CONST124 * VAR14 * VAR26 + \n        CONST191 * VAR16 * VAR24 + CONST229 * VAR22 * y - CONST250 * VAR12)\n        ) + g_8 * (CONST011 * VAR03 * z + VAR05 * (CONST028 * VAR25 + \n        CONST202 * VAR17 * z) + VAR07 * (CONST028 * VAR23 + CONST157 *\n        VAR15 * z + CONST173 * VAR17 * VAR25) + x * (CONST011 * VAR21 + \n        CONST156 * VAR15 * VAR25 + CONST199 * VAR13 * z + CONST202 * VAR17 *\n        VAR23)) + g_9 * (CONST246 * VAR12 * z + VAR14 * (CONST141 * VAR08 *\n        z + CONST141 * VAR25) + VAR16 * (CONST131 * VAR08 * VAR25 + \n        CONST178 * VAR06 * z + CONST178 * VAR23) + y * (CONST046 * VAR04 *\n        z + CONST046 * VAR21 + CONST087 * VAR08 * VAR23 + CONST088 * VAR06 *\n        VAR25))\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "81ade71d-f94b-41a5-a96c-53ec4bce06d8"
  },
  {
    "input": "@triton.jit\ndef gated_matmul_fwd(out, input, w1, w2, act_input_1, act_input_2, M, N, K,\n    stride_om, stride_im, stride_wn, dtype: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', USE_GELU: 'tl.constexpr',\n    SAVE_ACTIVATION_INPUTS: 'tl.constexpr', IS_EVEN_MNK: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n\n    - Input has shape (M, K)\n    - Weight 1 has shape (K, N)\n    - Weight 2 has shape (K, N)\n    - Output has shape (M, N)\n\n    \"\"\"\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    GROUP_M = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % GROUP_M\n    pid_n = pid % num_pid_in_group // GROUP_M\n    input_block_ptr = tl.make_block_ptr(base=input, shape=(M, K), strides=(\n        stride_im, 1), offsets=(pid_m * BLOCK_M, 0), block_shape=(BLOCK_M,\n        BLOCK_K), order=(1, 0))\n    w1_block_ptr = tl.make_block_ptr(base=w1, shape=(K, N), strides=(1,\n        stride_wn), offsets=(0, pid_n * BLOCK_N), block_shape=(BLOCK_K,\n        BLOCK_N), order=(0, 1))\n    w2_block_ptr = tl.make_block_ptr(base=w2, shape=(K, N), strides=(1,\n        stride_wn), offsets=(0, pid_n * BLOCK_N), block_shape=(BLOCK_K,\n        BLOCK_N), order=(0, 1))\n    acc1 = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    acc2 = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for i in range(0, K, BLOCK_K):\n        if IS_EVEN_MNK:\n            x = tl.load(input_block_ptr)\n            w1_blk = tl.load(w1_block_ptr)\n            w2_blk = tl.load(w2_block_ptr)\n        else:\n            x = tl.load(input_block_ptr, boundary_check=(0, 1))\n            w1_blk = tl.load(w1_block_ptr, boundary_check=(0, 1))\n            w2_blk = tl.load(w2_block_ptr, boundary_check=(0, 1))\n        acc1 += tl.dot(x, w1_blk)\n        acc2 += tl.dot(x, w2_blk)\n        input_block_ptr = tl.advance(input_block_ptr, (0, BLOCK_K))\n        w1_block_ptr = tl.advance(w1_block_ptr, (BLOCK_K, 0))\n        w2_block_ptr = tl.advance(w2_block_ptr, (BLOCK_K, 0))\n    if SAVE_ACTIVATION_INPUTS:\n        act_in_1_ptrs = tl.make_block_ptr(base=act_input_1, shape=(M, N),\n            strides=(stride_om, 1), offsets=(pid_m * BLOCK_M, pid_n *\n            BLOCK_N), block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n        act_in_2_ptrs = tl.make_block_ptr(base=act_input_2, shape=(M, N),\n            strides=(stride_om, 1), offsets=(pid_m * BLOCK_M, pid_n *\n            BLOCK_N), block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n        if IS_EVEN_MNK:\n            tl.store(act_in_1_ptrs, acc1)\n            tl.store(act_in_2_ptrs, acc2)\n        else:\n            tl.store(act_in_1_ptrs, acc1, boundary_check=(0, 1))\n            tl.store(act_in_2_ptrs, acc2, boundary_check=(0, 1))\n    if USE_GELU:\n        acc1 = gelu(acc1)\n    else:\n        acc1 = relu(acc1)\n    acc = acc1 * acc2\n    out_ptrs = tl.make_block_ptr(base=out, shape=(M, N), strides=(stride_om,\n        1), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N), block_shape=(\n        BLOCK_M, BLOCK_N), order=(1, 0))\n    if IS_EVEN_MNK:\n        tl.store(out_ptrs, acc)\n    else:\n        tl.store(out_ptrs, acc, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d564a664-998f-42d9-a40f-590bd6f643ed"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=warps) for warps in [\n    4, 8, 16]], key=['HEAD_SIZE', 'PADDED_NUM_SPLITS', 'PARTITION_SIZE'])\n@triton.jit\ndef _paged_attn_wo_mma_v2_reduce_kernel(out, exp_sums, max_logits, tmp_out,\n    context_lens, stride_exp_m, stride_exp_n, stride_out_m, stride_out_n,\n    stride_tmp_m, stride_tmp_n, stride_tmp_k, HEAD_SIZE: 'tl.constexpr',\n    PADDED_NUM_SPLITS: 'tl.constexpr', PARTITION_SIZE: 'tl.constexpr'):\n    seq_idx = tl.program_id(axis=1)\n    head_idx = tl.program_id(axis=0)\n    context_len = tl.load(context_lens + seq_idx)\n    num_partitions = tl.cdiv(context_len, PARTITION_SIZE)\n    max_logit = float('-inf')\n    offs_logit = seq_idx * stride_exp_m + head_idx * stride_exp_n\n    head_size_offs = tl.arange(0, HEAD_SIZE)\n    tmp_out_ptr = seq_idx * stride_tmp_m + head_idx * stride_tmp_n\n    out_ptr = seq_idx * stride_out_m + head_idx * stride_out_n + head_size_offs\n    acc = tl.zeros([HEAD_SIZE], dtype=tl.float32)\n    global_exp_sum = tl.zeros([1], dtype=tl.float32)\n    logits = tl.load(max_logits + offs_logit + tl.arange(0,\n        PADDED_NUM_SPLITS), mask=tl.arange(0, PADDED_NUM_SPLITS) <\n        num_partitions, other=float('-inf'))\n    max_logit = tl.max(logits, axis=0)\n    exp_sum = tl.load(exp_sums + offs_logit + tl.arange(0,\n        PADDED_NUM_SPLITS), mask=tl.arange(0, PADDED_NUM_SPLITS) <\n        num_partitions, other=0.0)\n    rescaled_exp_sum = exp_sum * tl.exp(logits - max_logit)\n    global_exp_sum += tl.sum(rescaled_exp_sum, axis=0)\n    tmp = tl.load(tmp_out + tmp_out_ptr + tl.arange(0, PADDED_NUM_SPLITS)[:,\n        None] * stride_tmp_k + head_size_offs)\n    acc += tl.sum(tmp * rescaled_exp_sum[:, None], axis=0)\n    inv_sum = 1.0 / (global_exp_sum + 1e-06)\n    tl.store(out + out_ptr, acc * inv_sum)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "45fe7e7e-b4b6-453d-b3f4-6bf8bbde12ab"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, dO, D, SEQ_LEN, BLOCK_SIZE_Q: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr'):\n    block_index_q = tl.program_id(0)\n    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n    index_batch_head = tl.program_id(1)\n    offs_dim = tl.arange(0, HEAD_DIM)\n    O_block = tl.load(O + index_batch_head * HEAD_DIM * SEQ_LEN + offs_q[:,\n        None] * HEAD_DIM + offs_dim[None, :])\n    dO_block = tl.load(dO + index_batch_head * HEAD_DIM * SEQ_LEN + offs_q[\n        :, None] * HEAD_DIM + offs_dim[None, :])\n    D_block = tl.sum(dO_block * O_block, axis=1)\n    D_block_ptrs = D + index_batch_head * SEQ_LEN + offs_q\n    tl.store(D_block_ptrs, D_block)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "fc7ec979-1baa-441e-b3c7-7d41c764aa06"
  },
  {
    "input": "@triton.jit\ndef parallel_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "d3c37763-ae3d-48d2-9996-87f2d3165327"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr',\n    offs_n: 'tl.constexpr', N_CTX: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if STAGE != 1:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1))\n        else:\n            k = tl.load(K_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if STAGE != 1:\n            n_ctx_mask = tl.where((offs_m[:, None] < N_CTX) & (start_n +\n                offs_n[None, :] < N_CTX), 0, float('-inf'))\n            qk += n_ctx_mask\n        qk += tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, float('-inf'))\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        if STAGE != 1:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1))\n        else:\n            v = tl.load(V_block_ptr)\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "12dec9f6-c8ab-467f-a4b3-a8d2739cd756"
  },
  {
    "input": "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args[\n    'BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args[\n    'BLOCK_N'] == 0})\n@triton.jit\ndef _score_kernel(Q, K, M, sm_scale, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_oz,\n    stride_oh, stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX,\n    sliding_window_offset, sliding_window_size, SLIDING_WINDOW:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M:\n    'tl.constexpr', IS_EVEN_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    off_hkv = off_h // (H // H_KV)\n    q_offset = off_z * stride_qz + off_h * stride_qh\n    k_offset = off_z * stride_kz + off_hkv * stride_kh\n    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\n    o = tl.zeros([BLOCK_M], dtype=tl.float32)\n    Q_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL,\n        NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, start_n *\n        BLOCK_N), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    if IS_EVEN_N:\n        k = tl.load(K_block_ptr)\n    else:\n        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    lo = 0\n    hi = ROUND_CTX\n    qk_scale = sm_scale\n    qk_scale *= 1.4426950408889634\n    for start_m in range(lo, hi, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if IS_EVEN_M:\n            q = tl.load(Q_block_ptr)\n        else:\n            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        m = tl.load(m_ptrs)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m - start_n * BLOCK_N + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n        qk = qk - m[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_M) + start_m < N_CTX)[:, None],\n                p, 0)\n        o += tl.sum(p, axis=0)\n        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n        m_ptrs = m_ptrs + BLOCK_M\n    o_offset = off_z * stride_oz + off_h * stride_oh\n    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\n    o_ptrs = Out + o_offset + o_range\n    tl.store(o_ptrs, o, mask=o_range < NKV_CTX)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "fa4034d7-05c6-4b13-b0a8-0e69cb40870e"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=warps) for warps in [\n    4, 8, 16]], key=['QUERY_GROUP_SIZE', 'HEAD_SIZE', 'NUM_PARTITIONS',\n    'PARTITION_SIZE'])\n@triton.jit\ndef _paged_attn_w_mma_v2_reduce_kernel(out_ptr, m_i_ptr, l_i_ptr,\n    tmp_out_ptr, context_lens_ptr, max_num_partitions, stride_o0, stride_o1,\n    stride_o2, HEAD_SIZE: 'tl.constexpr', QUERY_GROUP_SIZE: 'tl.constexpr',\n    PADDED_QUERY_GROUP_SIZE: 'tl.constexpr', NUM_KV_HEADS: 'tl.constexpr',\n    PARTITION_SIZE: 'tl.constexpr', NUM_PARTITIONS: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    num_partitions = tl.cdiv(context_len, PARTITION_SIZE)\n    group_head_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)[:, None\n        ] * HEAD_SIZE + tl.arange(0, HEAD_SIZE)[None, :]\n    group_mask = tl.arange(0, PADDED_QUERY_GROUP_SIZE)[:, None\n        ] < QUERY_GROUP_SIZE\n    if num_partitions == 1:\n        tmp_out_offset = ((seq_idx * NUM_KV_HEADS + kv_head_idx) *\n            max_num_partitions * QUERY_GROUP_SIZE * HEAD_SIZE +\n            group_head_offset)\n        tmp_out = tl.load(tmp_out_ptr + tmp_out_offset, mask=group_mask,\n            other=0.0)\n        out_offset = (seq_idx * stride_o0 + kv_head_idx * QUERY_GROUP_SIZE *\n            stride_o1 + group_head_offset * stride_o2)\n        tl.store(out_ptr + out_offset, tmp_out, mask=group_mask)\n        return\n    ml_offset = (seq_idx * NUM_KV_HEADS + kv_head_idx\n        ) * max_num_partitions * QUERY_GROUP_SIZE + tl.arange(0, NUM_PARTITIONS\n        )[:, None] * QUERY_GROUP_SIZE + tl.arange(0, PADDED_QUERY_GROUP_SIZE)[\n        None, :]\n    mask = (tl.arange(0, NUM_PARTITIONS)[:, None] < num_partitions) & (tl.\n        arange(0, PADDED_QUERY_GROUP_SIZE)[None, :] < QUERY_GROUP_SIZE)\n    m_i = tl.load(m_i_ptr + ml_offset, mask=mask, other=float('-inf'))\n    m = tl.max(m_i, axis=0)\n    l_i = tl.load(l_i_ptr + ml_offset, mask=mask, other=0.0)\n    l_i *= tl.exp(m_i - m[None, :])\n    l = tl.sum(l_i, axis=0)\n    r = l_i / l[None, :]\n    r = tl.reshape(r, (NUM_PARTITIONS, PADDED_QUERY_GROUP_SIZE, 1))\n    tmp_out_offset = (seq_idx * NUM_KV_HEADS + kv_head_idx\n        ) * max_num_partitions * QUERY_GROUP_SIZE * HEAD_SIZE + tl.arange(0,\n        NUM_PARTITIONS)[:, None, None\n        ] * QUERY_GROUP_SIZE * HEAD_SIZE + tl.arange(0, PADDED_QUERY_GROUP_SIZE\n        )[None, :, None] * HEAD_SIZE + tl.arange(0, HEAD_SIZE)[None, None, :]\n    tmp_out = tl.load(tmp_out_ptr + tmp_out_offset, mask=mask[:, :, None],\n        other=0.0)\n    out = tl.sum(tmp_out * r, axis=0)\n    out_offset = (seq_idx * stride_o0 + kv_head_idx * QUERY_GROUP_SIZE *\n        stride_o1 + group_head_offset * stride_o2)\n    tl.store(out_ptr + out_offset, out, mask=group_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "36319d3f-c54f-4be3-b4d8-9acc445d363d"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['hdim', 'dstate', 'chunk_size'])\n@triton.jit\ndef _chunk_state_fwd_kernel(x_ptr, b_ptr, states_ptr, dt_ptr, dA_cumsum_ptr,\n    seq_idx_ptr, hdim, dstate, chunk_size, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head,\n    stride_b_dstate, stride_states_batch, stride_states_chunk,\n    stride_states_head, stride_states_hdim, stride_states_dstate,\n    stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim + offs_k[None, :] *\n        stride_x_seqlen)\n    b_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate + offs_k[:, None] *\n        stride_b_seqlen)\n    dt_ptrs = dt_ptr + offs_k * stride_dt_csize\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    if HAS_SEQ_IDX:\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :\n            ] < chunk_size_limit - k), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) &\n            (offs_n[None, :] < dstate), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size_limit -\n            k, other=0.0)\n        if HAS_SEQ_IDX:\n            seq_idx_k = tl.load(seq_idx_ptrs, mask=offs_k < \n                chunk_size_limit - k, other=-1)\n        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0)\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_last - dA_cs_k) * dt_k\n        else:\n            scale = tl.where(seq_idx_k == seq_idx_last, tl.exp(dA_cs_last -\n                dA_cs_k) * dt_k, 0.0)\n        b *= scale[:, None]\n        b = b\n        acc += tl.dot(x, b)\n        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n        if HAS_SEQ_IDX:\n            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\n    states = acc\n    states_ptr += (pid_b * stride_states_batch + pid_c *\n        stride_states_chunk + pid_h * stride_states_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    states_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim + \n        offs_n[None, :] * stride_states_dstate)\n    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\n    tl.store(states_ptrs, states, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "7548778c-a189-4309-a2d4-f8cffa902a5d"
  },
  {
    "input": "@triton.jit\ndef hardsigmoid(input):\n    \"\"\"\n    Applies hard sigmoid to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by hard sigmoid.\n    \"\"\"\n    return tl.maximum(0, tl.minimum(1, input / 6 + 0.5))\n",
    "category": "Activation Functions",
    "subcategory": "",
    "uuid": "173e53f6-9f54-4698-b491-17c31cac02e1"
  },
  {
    "input": "@triton.jit\ndef patching_kernel(image_ptr, out_ptr, batch_size, batch_stride,\n    image_stride0, image_stride1, N, H, W, C, P, block_x: 'tl.constexpr',\n    block_y: 'tl.constexpr'):\n    batch_idx = tl.program_id(axis=0)\n    row_idx = tl.program_id(axis=1)\n    col_idx = tl.program_id(axis=2)\n    batch_offset = batch_idx * batch_stride\n    row_offset = row_idx * block_y + tl.arange(0, block_y)\n    col_offset = col_idx * block_x + tl.arange(0, block_x)\n    data_offset = row_offset[None, :] * image_stride1 + col_offset[:, None]\n    row_mask = row_offset < H\n    col_mask = col_offset < W\n    data_mask = row_mask[:, None] & col_mask[None, :]\n    img_r = tl.load(image_ptr + batch_offset + data_offset, mask=data_mask)\n    img_g = tl.load(image_ptr + batch_offset + data_offset + image_stride0,\n        mask=data_mask)\n    img_b = tl.load(image_ptr + batch_offset + data_offset + image_stride0 *\n        2, mask=data_mask)\n    P_single_row = P * P * C\n    num_patches_x = (W + P - 1) // P\n    P_offset = (row_idx * num_patches_x + col_idx) * P_single_row\n    out_offset = P_offset + tl.arange(0, block_x * block_y)\n    out_mask = out_offset < N * P * P * C\n    tl.store(out_ptr + batch_offset + out_offset, tl.ravel(img_r), mask=\n        out_mask)\n    tl.store(out_ptr + batch_offset + out_offset + P * P, tl.ravel(img_g),\n        mask=out_mask)\n    tl.store(out_ptr + batch_offset + out_offset + P * P * 2, tl.ravel(\n        img_b), mask=out_mask)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "3aae971b-51f2-4280-8271-7201fe681167"
  },
  {
    "input": "@triton.jit\ndef _fg_kernel(e, g, h, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    f_row = e_row * tl.sigmoid(e_row)\n    f_row = f_row\n    h_row = f_row * g_row\n    tl.store(h + offsets, h_row, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "e6bfecbb-d45e-4486-8c1c-22a6ca62da3a"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, stride_oz, stride_oh, stride_om,\n    stride_ok, stride_doz, stride_doh, stride_dom, stride_dok, stride_dz,\n    stride_dh, stride_dm, M, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr'):\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    Out += off_z * stride_oz + off_h * stride_oh\n    DO += off_z * stride_doz + off_h * stride_doh\n    Delta += off_z * stride_dz + off_h * stride_dh\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o_ptrs = Out + off_m[:, None] * stride_om + off_n[None, :] * stride_ok\n    do_ptrs = DO + off_m[:, None] * stride_dom + off_n[None, :] * stride_dok\n    if DIVISIBLE_M:\n        o = tl.load(o_ptrs)\n        do = tl.load(do_ptrs)\n    else:\n        mask_m = off_m < M\n        o = tl.load(o_ptrs, mask=mask_m[:, None])\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n    delta = tl.sum(o * do, axis=1)\n    d_ptrs = Delta + off_m * stride_dm\n    if DIVISIBLE_M:\n        tl.store(d_ptrs, delta)\n    else:\n        tl.store(d_ptrs, delta, mask=mask_m)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "78ab6942-fd6f-4246-b0cc-67729ac79639"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'], restore_value=['running_mean_pointer',\n    'running_var_pointer'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args[\n    'batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})\n@triton.jit\ndef batch_norm_forward_kernel(input_pointer, weight_pointer, bias_pointer,\n    mean_pointer, inv_std_pointer, pre_act_add_pointer, pre_act_pointer,\n    output_pointer, running_mean_pointer, running_var_pointer, batch_dim,\n    spatial_dim, input_batch_stride, input_feat_stride,\n    input_spatial_stride, pre_act_add_batch_stride, pre_act_add_feat_stride,\n    pre_act_add_spatial_stride, pre_act_batch_stride, pre_act_feat_stride,\n    pre_act_spatial_stride, output_batch_stride, output_feat_stride,\n    output_spatial_stride, momentum, eps, param, affine: 'tl.constexpr',\n    save_stats: 'tl.constexpr', track_running_stats: 'tl.constexpr',\n    is_train: 'tl.constexpr', add_pre_act: 'tl.constexpr', act_func:\n    'tl.constexpr', save_pre_act: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_SPATIAL: 'tl.constexpr'):\n    \"\"\"\n    Batch-normalizes the input, optionally adding a residual and fusing an activation function.\n\n    Args:\n        input_pointer: Pointer to the input to layer-normalize.\n            The input must be of shape [batch_dim, feat_dim, spatial_dim].\n        weight_pointer: Pointer to optional weights for affine transform.\n            The weights, if provided, must be of shape [feat_dim].\n        bias_pointer: Pointer to an optional bias vector for affine transform.\n            The bias vector, if provided, must be of shape [feat_dim].\n        mean_pointer: Pointer to an optional container the input's mean\n            is written to if save_stats is True.\n            The container, if provided, must be of shape [feat_dim].\n        inv_std_pointer: Pointer to an optional container the input's inverse\n            standard deviation is written to if save_stats is True.\n            The container, if provided, must be of shape [feat_dim].\n        pre_act_add_pointer: Pointer to an optional residual added to the pre-activation result.\n            The residual, if provided, must be of shape [batch_dim, feat_dim, spatial_dim].\n        pre_act_pointer: Pointer to an optional container the pre-activation input\n            is written to if act_func is not None and save_pre_act is True.\n            The container, if provided, must be of shape [batch_dim, feat_dim, spatial_dim].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [batch_dim, feat_dim, spatial_dim].\n        running_mean_pointer: Pointer to an optional container the input's running\n            mean is written to if track_running_stats and is_train are True.\n            The container, if provided, must be of shape [feat_dim].\n        running_var_pointer: Pointer to an optional container the input's running\n            variance is written to if track_running_stats and is_train are True.\n            The container, if provided, must be of shape [feat_dim].\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        pre_act_add_batch_stride: Stride necessary to jump one element along the\n            residual's batch dimension.\n        pre_act_add_out_feat_stride: Stride necessary to jump one element along the\n            residual's feature dimension.\n        pre_act_add_spatial_stride: Stride necessary to jump one element along the\n            residual's spatial dimension.\n        pre_act_batch_stride: Stride necessary to jump one element along the\n            pre-activation input container's batch dimension.\n        pre_act_out_feat_stride: Stride necessary to jump one element along the\n            pre-activation input container's feature dimension.\n        pre_act_spatial_stride: Stride necessary to jump one element along the\n            pre-activation input container's spatial dimension.\n        output_batch_stride: Stride necessary to jump one element along the\n            output container's batch dimension.\n        output_feat_stride: Stride necessary to jump one element along the\n            output container's feature dimension.\n        output_spatial_stride: Stride necessary to jump one element along the\n            output container's spatial dimension.\n        momentum: Momentum for the running mean and variance.\n        eps: Epsilon added in the square root in the denominator\n            to avoid division by zero.\n        param: Parameter in the case of parameterized activation functions.\n        affine: Flag for performing an affine transformation on the normalized output.\n        save_stats: Flag for saving the mean and standard deviation.\n        track_running_stats: Flag for tracking running mean and variance if\n            is_train is also True.\n        is_train: Flag indicating if the model is in training mode.\n        add_pre_act: Flag for adding the residual to the pre-activation result.\n        act_func: Name of activation function to apply, with None for identity.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        save_pre_act: Flag for saving the pre-activation input.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    feat_pid = tl.program_id(axis=0)\n    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\n    batch_mask = batch_offset < batch_dim\n    if is_train or not track_running_stats:\n        count = 0\n        mean = 0.0\n        var = 0.0\n        for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n            spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n                BLOCK_SIZE_SPATIAL)\n            spatial_mask = spatial_offset < spatial_dim\n            curr_input_pointer = (input_pointer + input_feat_stride *\n                feat_pid + input_batch_stride * batch_offset[:, None] + \n                input_spatial_stride * spatial_offset[None, :])\n            curr_input = tl.load(curr_input_pointer, mask=batch_mask[:,\n                None] & spatial_mask[None, :])\n            spatial_count = min(BLOCK_SIZE_SPATIAL, spatial_dim - block_ind *\n                BLOCK_SIZE_SPATIAL)\n            curr_count = spatial_count * batch_dim\n            count += curr_count\n            prev_mean = mean\n            mean += (tl.sum(curr_input) - curr_count * mean) / count\n            deltas = tl.where(batch_mask[:, None] & spatial_mask[None, :], \n                (curr_input - mean) * (curr_input - prev_mean), 0.0)\n            var += tl.sum(deltas)\n        var /= count\n        inv_std = tl.rsqrt(var + eps)\n        if save_stats:\n            tl.store(feat_pid + mean_pointer, mean)\n            tl.store(feat_pid + inv_std_pointer, inv_std)\n        if track_running_stats:\n            running_mean_pointer += feat_pid\n            running_var_pointer += feat_pid\n            running_mean = tl.load(running_mean_pointer)\n            running_var = tl.load(running_var_pointer)\n            n = batch_dim * spatial_dim\n            tl.store(running_mean_pointer, (1 - momentum) * running_mean + \n                momentum * mean)\n            tl.store(running_var_pointer, (1 - momentum) * running_var + \n                momentum * var * n / (n - 1))\n    else:\n        mean = tl.load(feat_pid + running_mean_pointer)\n        inv_std = tl.rsqrt(tl.load(feat_pid + running_var_pointer) + eps)\n    if affine:\n        weight = tl.load(feat_pid + weight_pointer)\n        bias = tl.load(feat_pid + bias_pointer)\n    else:\n        weight = 1.0\n        bias = 0.0\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n            BLOCK_SIZE_SPATIAL)\n        spatial_mask = spatial_offset < spatial_dim\n        curr_input_pointer = (input_pointer + input_feat_stride * feat_pid +\n            input_batch_stride * batch_offset[:, None] + \n            input_spatial_stride * spatial_offset[None, :])\n        curr_output_pointer = (output_pointer + output_feat_stride *\n            feat_pid + output_batch_stride * batch_offset[:, None] + \n            output_spatial_stride * spatial_offset[None, :])\n        curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        output = weight * (curr_input - mean) * inv_std + bias\n        if add_pre_act:\n            curr_pre_act_add_pointer = (pre_act_add_pointer + \n                pre_act_add_feat_stride * feat_pid + \n                pre_act_add_batch_stride * batch_offset[:, None] + \n                pre_act_add_spatial_stride * spatial_offset[None, :])\n            curr_pre_act_add = tl.load(curr_pre_act_add_pointer, mask=\n                batch_mask[:, None] & spatial_mask[None, :])\n            output += curr_pre_act_add\n        if act_func is not None:\n            if save_pre_act:\n                curr_pre_act_pointer = (pre_act_pointer + \n                    pre_act_feat_stride * feat_pid + pre_act_batch_stride *\n                    batch_offset[:, None] + pre_act_spatial_stride *\n                    spatial_offset[None, :])\n                tl.store(curr_pre_act_pointer, output, mask=batch_mask[:,\n                    None] & spatial_mask[None, :])\n            output = apply_act_func(output, None, None, None, param,\n                act_func, False)\n        tl.store(curr_output_pointer, output, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n",
    "category": "Normalization",
    "subcategory": "batch norm",
    "uuid": "e1132a25-3b9b-477b-a98b-4809781065fd"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef fused_recurrent_hgrn_bwd_kernel(g, o, dx, dg, do, h0, T: 'tl.constexpr',\n    D: 'tl.constexpr', BD: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_g = g + (i_b * T + T - 1) * D + o_d\n    p_o = o + (i_b * T + T - 2) * D + o_d\n    p_dx = dx + (i_b * T + T - 1) * D + o_d\n    p_dg = dg + (i_b * T + T - 1) * D + o_d\n    p_do = do + (i_b * T + T - 1) * D + o_d\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for i in range(T - 1, -1, -1):\n        b_g = tl.load(p_g, mask=mask, other=0)\n        b_do = tl.load(p_do, mask=mask, other=0)\n        if i > 0:\n            b_o = tl.load(p_o, mask=mask, other=0)\n        elif USE_INITIAL_STATE:\n            b_o = tl.load(h0 + i_b * D + o_d, mask=mask, other=0)\n        else:\n            b_o = tl.zeros([BD], dtype=tl.float32)\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * tl.exp(b_g)\n        b_dg = b_dh * b_o\n        tl.store(p_dx, b_dx, mask=mask)\n        tl.store(p_dg, b_dg, mask=mask)\n        p_g -= D\n        p_o -= D\n        p_dx -= D\n        p_dg -= D\n        p_do -= D\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "269c774d-a876-4944-a78d-6aeb6ca9ad76"
  },
  {
    "input": "@triton.jit\ndef _softmax_fwd_kernel(output_ptr, stride_output_row, input_ptr,\n    stride_input_row, num_cols, block_size: 'tl.constexpr'):\n    row_index = tl.program_id(0)\n    row_start_ptr = input_ptr + row_index * stride_input_row\n    col_offsets = tl.arange(0, block_size)\n    input_pointers = row_start_ptr + col_offsets\n    row_mask = col_offsets < num_cols\n    row = tl.load(input_pointers, mask=row_mask, other=float('-inf'))\n    safe_row = row - tl.max(row, axis=0)\n    numerator = tl.exp(safe_row)\n    denominator = tl.sum(numerator, axis=0)\n    sm_out = numerator / denominator\n    output_row_ptr = output_ptr + row_index * stride_output_row\n    output_pointers = output_row_ptr + col_offsets\n    tl.store(output_pointers, sm_out, mask=row_mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "55251b16-76d4-4115-857f-c1a31130fd16"
  },
  {
    "input": "@triton.jit\ndef triton_qkv_concat(txt_qkv, img_qkv, out_q_ptr, out_k_ptr, out_v_ptr,\n    seq_len, num_heads, head_dim, hidden_dim, seq_txt_len, stride_txt_a,\n    stride_txt_b, stride_img_a, stride_img_b, stride_output_a,\n    stride_output_b, stride_output_c, XBLOCK: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    xoffset = pid * XBLOCK + tl.arange(0, XBLOCK)[:]\n    seq_idx = xoffset // hidden_dim % seq_len\n    batch_idx = xoffset // stride_output_a\n    hidden_dim_idx = xoffset % hidden_dim\n    headdim_idx = xoffset % head_dim\n    head_idx = xoffset // head_dim % num_heads\n    txt_seq_end = tl.full([1], seq_txt_len, tl.int64)\n    txt_mask = seq_idx < txt_seq_end\n    img_mask = seq_idx >= txt_seq_end\n    txt_q_data = tl.load(txt_qkv + (hidden_dim * 0 + hidden_dim_idx + \n        stride_txt_b * seq_idx + stride_txt_a * batch_idx), txt_mask, other=0.0\n        )\n    zero_mask = tl.full(txt_q_data.shape, 0.0, txt_q_data.dtype)\n    masked_txt_q = tl.where(txt_mask, txt_q_data, zero_mask)\n    img_q_data = tl.load(img_qkv + (-stride_txt_a + hidden_dim * 0 +\n        hidden_dim_idx + stride_img_b * seq_idx + stride_img_a * batch_idx),\n        img_mask, other=0.0)\n    zero_mask = tl.full(img_q_data.shape, 0.0, img_q_data.dtype)\n    masked_img_q = tl.where(img_mask, img_q_data, zero_mask)\n    out_q = tl.where(txt_mask, masked_txt_q, masked_img_q)\n    tl.store(out_q_ptr + (headdim_idx + stride_output_c * seq_idx + \n        stride_output_b * head_idx + stride_output_a * batch_idx), out_q, None)\n    txt_k_data = tl.load(txt_qkv + (hidden_dim * 1 + hidden_dim_idx + \n        stride_txt_b * seq_idx + stride_txt_a * batch_idx), txt_mask, other=0.0\n        )\n    zero_mask = tl.full(txt_k_data.shape, 0.0, txt_k_data.dtype)\n    masked_txt_q = tl.where(txt_mask, txt_k_data, zero_mask)\n    img_k_data = tl.load(img_qkv + (-stride_txt_a + hidden_dim * 1 +\n        hidden_dim_idx + stride_img_b * seq_idx + stride_img_a * batch_idx),\n        img_mask, other=0.0)\n    zero_mask = tl.full(img_k_data.shape, 0.0, img_k_data.dtype)\n    masked_img_k = tl.where(img_mask, img_k_data, zero_mask)\n    out_k = tl.where(txt_mask, masked_txt_q, masked_img_k)\n    tl.store(out_k_ptr + (headdim_idx + stride_output_c * seq_idx + \n        stride_output_b * head_idx + stride_output_a * batch_idx), out_k, None)\n    txt_v_data = tl.load(txt_qkv + (hidden_dim * 2 + hidden_dim_idx + \n        stride_txt_b * seq_idx + stride_txt_a * batch_idx), txt_mask, other=0.0\n        )\n    zero_mask = tl.full(txt_v_data.shape, 0.0, txt_v_data.dtype)\n    masked_txt_v = tl.where(txt_mask, txt_v_data, zero_mask)\n    img_v_data = tl.load(img_qkv + (-stride_txt_a + hidden_dim * 2 +\n        hidden_dim_idx + stride_img_b * seq_idx + stride_img_a * batch_idx),\n        img_mask, other=0.0)\n    zero_mask = tl.full(img_v_data.shape, 0.0, img_v_data.dtype)\n    masked_img_q = tl.where(img_mask, img_v_data, zero_mask)\n    output_v = tl.where(txt_mask, masked_txt_v, masked_img_q)\n    tl.store(out_v_ptr + (headdim_idx + stride_output_c * seq_idx + \n        stride_output_b * head_idx + stride_output_a * batch_idx), output_v,\n        None)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "429a0e4c-9e2e-4e3b-b543-f11acdb19d6a"
  },
  {
    "input": "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD_SIZED, key=['BLOCK_DMODEL',\n    'max_seqlen_q', 'max_seqlen_k'])\n@triton.jit\ndef sized_tuned_bwd_kernel_dk_dv(Q, K, V, B, sm_scale, Out, DO, DK, DV, L,\n    D, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n    stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn, stride_dkk,\n    stride_dvz, stride_dvh, stride_dvk, stride_dvn, cu_seqlens_q,\n    cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim,\n    dropout_p, philox_seed, philox_offset_base, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    bare_bwd_kernel_dk_dv(Q, K, V, B, sm_scale, Out, DO, DK, DV, L, D,\n        stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n        stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n        stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n        stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn,\n        stride_dkk, stride_dvz, stride_dvh, stride_dvk, stride_dvn,\n        cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k,\n        head_dim, dropout_p, philox_seed, philox_offset_base, BLOCK_M,\n        BLOCK_DMODEL, BLOCK_N, CAUSAL, ENABLE_DROPOUT, PADDED_HEAD=\n        PADDED_HEAD, BIAS_TYPE=BIAS_TYPE)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "7af97ed6-0cd2-4b5f-83a2-e69962498310"
  },
  {
    "input": "@triton.jit\ndef concat_2D_jagged_jagged_w_prefix(OffsetsA, ValuesA, OffsetsB, ValuesB,\n    Out, D, stride_ad, stride_bd, stride_od, n_prefix_from_B, BLOCK_D:\n    'tl.constexpr'):\n    concat_2D_jagged_w_prefix(OffsetsA, ValuesA, OffsetsB, ValuesB, 0, Out,\n        D, stride_ad, stride_bd, 0, stride_od, n_prefix_from_B, IS_DENSE_A=\n        False, IS_DENSE_B=False, BLOCK_D=BLOCK_D, IS_REPLACE=False)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "d7118221-a385-46b6-a121-1e9161a07d01"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dwdb_kernel(DW, DB, FINAL_DW, FINAL_DB, M, N,\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "6006c125-17b8-468d-9249-a6fc93d95c23"
  },
  {
    "input": "@triton.jit\ndef fw_kernel_wMLP(feature_grid, feature_grid_sizes, input_feature_grid,\n    input_feature_grid_sizes, directions, origins, grid_idx, near, far,\n    splatting_feature, mask, mlp_params, DIM_HIDDEN: 'tl.constexpr', DIM_IN:\n    'tl.constexpr', DIM_OUT: 'tl.constexpr', num_samples: 'tl.constexpr',\n    num_samples_inf: 'tl.constexpr', num_rays: 'tl.constexpr', grid_channel:\n    'tl.constexpr', NUM_GRIDS: 'tl.constexpr', feature_channel:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', mask_out_of_bounds_samples:\n    'tl.constexpr', contract_coords: 'tl.constexpr', disparity_at_inf:\n    'tl.constexpr'):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        prev_vec = sample_grid_rep(input_feature_grid,\n            input_feature_grid_sizes, grid_idx_buffer, sample_x, sample_y,\n            sample_z, feature_channel, NUM_GRIDS, BLOCK_SIZE,\n            mask_out_of_bounds_samples)\n        fused_feature = feature_buffer + prev_vec\n        fused_feature = fused_feature * mask_buffer\n        splat_grid_rep(fused_feature, feature_grid, feature_grid_sizes,\n            grid_idx_buffer, sample_x, sample_y, sample_z, grid_channel,\n            NUM_GRIDS, BLOCK_SIZE, mask_out_of_bounds_samples)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "696ac251-4235-488e-8ac6-fc7ffc929f08"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'K', 'V'])\n@triton.jit\ndef chunk_transform_qk_fwd_kernel(q, k, v, beta, o, A, q_new, k_new,\n    A_local, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', BT: 'tl.constexpr',\n    OUTPUT_ATTENTIONS: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, 0), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, 0), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, 0), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1)) * scale\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    p_T = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_T = tl.load(p_T, boundary_check=(0, 1))\n    o_i = tl.arange(0, BT)\n    m_t = o_i[:, None] >= o_i[None, :]\n    b_qk = tl.where(m_t, tl.dot(b_q, tl.trans(b_k), allow_tf32=False), 0)\n    m_t = o_i[:, None] > o_i[None, :]\n    b_kk = tl.where(m_t, tl.dot(b_k, tl.trans(b_k), allow_tf32=False), 0)\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (\n        BT,), (0,))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_k_beta = b_k * b_beta[:, None]\n    b_qkT = tl.dot(b_qk, b_T, allow_tf32=False)\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(A_local + i_bh * T * BT, (T, BT), (BT, 1),\n            (i_t * BT, 0), (BT, BT), (1, 0))\n        tl.store(p_a, b_qkT, boundary_check=(0, 1))\n    b_kkT = tl.dot(b_kk, b_T, allow_tf32=False)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, 0), (BT, BV), (1, 0))\n    tl.store(p_o, tl.dot(b_qkT, b_v), boundary_check=(0, 1))\n    p_q_new = tl.make_block_ptr(q_new + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n        ), (i_t * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_q_new, b_q - tl.dot(b_qkT, b_k_beta, allow_tf32=False),\n        boundary_check=(0, 1))\n    p_k_new = tl.make_block_ptr(k_new + i_bh * s_k_h, (T, K), (s_k_t, s_k_d\n        ), (i_t * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_k_new, b_k - tl.dot(tl.trans(b_kkT), b_k_beta, allow_tf32=\n        False), boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "1cfe4b34-c1cd-4568-9e8f-56baf79185ab"
  },
  {
    "input": "@triton.jit\ndef _rms_kernel_bwd_dw(input_ptr, dout_ptr, dweight_ptr, rstdev_ptr, nrows,\n    ncols, block_size_row: 'tl.constexpr', block_size_col: 'tl.constexpr'):\n    row_index = tl.program_id(0)\n    cols = row_index * block_size_col + tl.arange(0, block_size_col)\n    dw = tl.zeros((block_size_row, block_size_col), dtype=tl.float32)\n    unroll: 'tl.constexpr' = 4\n    for outer in range(0, nrows, block_size_row * unroll):\n        for inner in range(unroll):\n            rows = outer + inner * block_size_row + tl.arange(0, block_size_row\n                )\n            mask = rows[:, None] < block_size_row & (cols[None, :] <\n                block_size_col)\n            offsets = rows[:, None] * block_size_col + cols[None, :]\n            input = tl.load(input_ptr + offsets, mask=mask, other=0.0)\n            dout = tl.load(dout_ptr + offsets, mask=mask, other=0.0)\n            rstdev = tl.load(rstdev_ptr + rows, mask=rows < block_size_row,\n                other=0.0)\n            input_pred = input * rstdev[:, None]\n            dw += dout * input_pred\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(dweight_ptr, sum_dw, mask=cols < block_size_col)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "c074b09e-0ea6-412c-9a7a-15f58acbbc3c"
  },
  {
    "input": "@triton.jit\ndef cross_entropy_fwd_bwd_kernel(output_loss_ptr, output_logit_grad_ptr,\n    input_logit_ptr, input_targ_ptr, input_divisor_ptr, output_loss_stride,\n    output_logit_grad_stride, input_logit_stride, input_targ_stride, n_cols,\n    ignore_index, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    logit_grad_row_start_ptr = (output_logit_grad_ptr + row_idx *\n        output_logit_grad_stride)\n    logit_row_start_ptr = input_logit_ptr + row_idx * input_logit_stride\n    targ_ptr = input_targ_ptr + row_idx * input_targ_stride\n    loss_ptr = output_loss_ptr + row_idx * output_loss_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    logit_row_ptrs = logit_row_start_ptr + col_offsets\n    logit_grad_row_ptrs = logit_grad_row_start_ptr + col_offsets\n    logit_row = tl.load(logit_row_ptrs, mask=col_offsets < n_cols, other=\n        float('-Inf'))\n    targ = tl.load(targ_ptr)\n    divisor = tl.load(input_divisor_ptr)\n    logit_row = logit_row - tl.max(logit_row, axis=0)\n    exp_logit_row = tl.exp(logit_row)\n    sum_exp_logit_row = tl.sum(exp_logit_row, axis=0)\n    log_sum_exp_logit_row = tl.log(sum_exp_logit_row)\n    logit_gt_logit = tl.sum(tl.where(targ == col_offsets, logit_row, 0.0))\n    loss = log_sum_exp_logit_row - logit_gt_logit\n    loss = loss / divisor\n    loss = tl.where(targ == ignore_index, 0.0, loss)\n    tl.store(loss_ptr, loss)\n    targ_one_hot = tl.where(targ == col_offsets, 1.0, 0.0)\n    grad = exp_logit_row / sum_exp_logit_row - targ_one_hot\n    grad = grad / divisor\n    grad = tl.where(targ == ignore_index, 0.0, grad)\n    tl.store(logit_grad_row_ptrs, grad, mask=col_offsets < n_cols)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "5da08ed0-28d4-4e33-9b3b-baeb7263e035"
  },
  {
    "input": "@triton.jit\ndef _splitK_reduce(Out_splitK, LSE_splitK, Out, LSE, split_k:\n    'tl.constexpr', splitK_pow2: 'tl.constexpr', stride_osk_z:\n    'tl.constexpr', stride_osk_g: 'tl.constexpr', stride_osk_h:\n    'tl.constexpr', stride_osk_s: 'tl.constexpr', stride_osk_m:\n    'tl.constexpr', stride_osk_k: 'tl.constexpr', stride_lsek_z:\n    'tl.constexpr', stride_lsek_g: 'tl.constexpr', stride_lsek_h:\n    'tl.constexpr', stride_lsek_s: 'tl.constexpr', stride_lsek_m:\n    'tl.constexpr', stride_oz: 'tl.constexpr', stride_og: 'tl.constexpr',\n    stride_oh: 'tl.constexpr', stride_om: 'tl.constexpr', stride_ok:\n    'tl.constexpr', stride_lse_z: 'tl.constexpr', stride_lse_g:\n    'tl.constexpr', stride_lse_h: 'tl.constexpr', stride_lse_m:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', H: 'tl.constexpr', G:\n    'tl.constexpr', WRITE_LSE: 'tl.constexpr'):\n    off_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    Out_splitK_ptr = (Out_splitK + stride_osk_z * off_z + stride_osk_g *\n        off_g + stride_osk_h * off_h + stride_osk_m * off_m + tl.arange(0,\n        BLOCK_SIZE)[None, :] + stride_osk_s * tl.arange(0, splitK_pow2)[:,\n        None])\n    LSE_splitK_ptr0 = (LSE_splitK + stride_lsek_z * off_z + stride_lsek_g *\n        off_g + stride_lsek_h * off_h + stride_lsek_m * off_m + \n        stride_lsek_s * tl.arange(0, splitK_pow2))\n    if splitK_pow2 > split_k:\n        mask_1d = tl.arange(0, splitK_pow2) < split_k\n        mask_2d = mask_1d[:, None]\n        lse_splitk = tl.load(LSE_splitK_ptr0, mask=mask_1d, other=float('-inf')\n            )\n        lse_max = tl.max(lse_splitk)\n        out_splitk = tl.load(Out_splitK_ptr, mask=mask_2d, other=0)\n        lse_splitk = tl.load(LSE_splitK_ptr0, mask=mask_1d, other=float('-inf')\n            )\n    else:\n        lse_splitk = tl.load(LSE_splitK_ptr0)\n        lse_max = tl.max(lse_splitk)\n        out_splitk = tl.load(Out_splitK_ptr)\n        lse_splitk = tl.load(LSE_splitK_ptr0)\n    sumexp_normalized_splitk = tl.math.exp2((lse_splitk - lse_max) * 1.44269504\n        )\n    sumexp_normalized = tl.sum(sumexp_normalized_splitk, axis=0)\n    numerator_normalized = tl.sum(out_splitk * sumexp_normalized_splitk[:,\n        None], axis=0)\n    acc = numerator_normalized / sumexp_normalized\n    acc = tl.where(lse_max == float('-inf'), 0.0, acc)\n    Out_ptr = (Out + stride_oz * off_z + stride_oh * off_h + stride_og *\n        off_g + stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    if acc.dtype is tl.float64 and Out.dtype.element_ty is not tl.float64:\n        acc = acc\n    tl.store(Out_ptr, acc)\n    if WRITE_LSE:\n        l_ptrs = (LSE + off_z * stride_lse_z + off_g * stride_lse_g + off_h *\n            stride_lse_h + off_m * stride_lse_m)\n        to_store = lse_max + tl.math.log2(sumexp_normalized) / 1.44269504\n        to_store = tl.where(lse_max == float('-inf'), lse_max, to_store)\n        tl.store(l_ptrs, to_store)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "6f289be0-2fe8-4cd1-9552-a2d5e945c064"
  },
  {
    "input": "@triton.jit\ndef max_fn(x, y):\n    return tl.math.max(x, y)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "c08f52dd-e309-4e10-8acf-499b758616d6"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n    BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', start_m, start_n, num_steps, MASK: 'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    offs_n = start_n + tl.arange(0, BLOCK_N2)\n    offs_k = tl.arange(0, HEAD_DIM)\n    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    Di = tl.load(D + offs_m)\n    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n    curr_n = start_n\n    step_n = BLOCK_N2\n    for blk_idx in range(num_steps):\n        kT = tl.load(kT_ptrs)\n        vT = tl.load(vT_ptrs)\n        qk = tl.dot(q, kT)\n        p = tl.math.exp2(qk - m)\n        if MASK:\n            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(mask, p, 0.0)\n        dp = tl.dot(do, vT)\n        ds = p * (dp - Di[:, None])\n        ds = ds\n        dq += tl.dot(ds, tl.trans(kT))\n        curr_n += step_n\n        kT_ptrs += step_n * stride_tok\n        vT_ptrs += step_n * stride_tok\n    return dq\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "3161bf67-6f7f-4ae6-8223-bad2b2836985"
  },
  {
    "input": "@triton.jit\ndef first_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_Y10 = tl.load(sph_grad_ptr + output_row_offset, mask=\n        output_row_offset < output_numel)\n    g_Y11 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_Y12 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    CONST_00 = tl.sqrt(3.0)\n    g_x += CONST_00 * g_Y10\n    g_y += CONST_00 * g_Y11\n    g_z += CONST_00 * g_Y12\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "8e374896-a5c1-4c73-ab32-fcc43adb6a5d"
  },
  {
    "input": "@triton.jit\ndef _DWf_DW_dfg_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    e = e.float()\n    se = 1.0 / (1.0 + torch.exp(-e))\n    f = (se * e).to(dtype)\n    h = f * g\n    df = DW * f\n    dg = DW * g\n    de = (dg.float() * se * (1.0 + e * (1.0 - se))).to(dtype)\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    se_row = tl.sigmoid(e_row)\n    f_row = se_row * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row * se_row * (1.0 + e_row * (1.0 - se_row))\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "652f5412-e1a3-42f1-8afc-7ac0162ced71"
  },
  {
    "input": "@triton.jit\ndef _fwd_kv_parallel(K, V, S, KV, b: 'tl.constexpr', h: 'tl.constexpr', n:\n    'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', D_FBLOCK: 'tl.constexpr',\n    E_FBLOCK: 'tl.constexpr', NUM_FBLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_de = tl.program_id(2)\n    off_h = off_bh % h\n    off_d = off_de // NUM_FBLOCK\n    off_e = off_de % NUM_FBLOCK\n    block_offset = off_block * BLOCK\n    k_block_offset = block_offset * d\n    v_block_offset = block_offset * e\n    kv_block_offset = off_block * d * e\n    k_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    kv_offset = off_bh * (NUM_BLOCK + 1) * d * e\n    d_offset = off_d * D_FBLOCK\n    e_offset = off_e * E_FBLOCK\n    K_trans_block_ptr = K + k_offset + k_block_offset + d_offset + tl.arange(\n        0, CBLOCK)[None, :] * d + tl.arange(0, D_FBLOCK)[:, None]\n    V_block_ptr = V + v_offset + v_block_offset + e_offset + tl.arange(0,\n        CBLOCK)[:, None] * e + tl.arange(0, E_FBLOCK)[None, :]\n    KV_block_ptr = (KV + kv_offset + kv_block_offset + d_offset * e +\n        e_offset + tl.arange(0, D_FBLOCK)[:, None] * e + tl.arange(0,\n        E_FBLOCK)[None, :])\n    s_ptrs = S + off_h\n    s = tl.load(s_ptrs)\n    c_array = tl.arange(0, CBLOCK)\n    kv = tl.zeros([D_FBLOCK, E_FBLOCK], dtype=tl.float32)\n    for j in range(NUM_CBLOCK):\n        k_trans = tl.load(K_trans_block_ptr)\n        v = tl.load(V_block_ptr)\n        k_decay = tl.exp(-s * (BLOCK - (j * CBLOCK + c_array[None, :])))\n        kv += tl.dot(k_trans * k_decay, v)\n        K_trans_block_ptr += CBLOCK * d\n        V_block_ptr += CBLOCK * e\n    tl.store(KV_block_ptr, kv)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b5909739-64b6-47f4-b3ea-4b7fd00736b6"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Logics, V, Out, B_Loc, B_Start_Loc, B_Seqlen, max_input_len,\n    stride_logic_h, stride_logic_bs, stride_vbs, stride_vh, stride_vd,\n    stride_obs, stride_oh, stride_od, stride_b_loc_b, stride_b_loc_s,\n    other_kv_index, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_loc = tl.load(B_Start_Loc + cur_batch)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    off_v = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    off_b_loc = cur_batch * stride_b_loc_b + (max_input_len - cur_batch_seq_len\n        ) * stride_b_loc_s\n    v_ptrs = V + off_v\n    e_max = float('-inf')\n    e_sum = 0.0\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        v_index = tl.load(B_Loc + off_b_loc + (start_n + offs_n) *\n            stride_b_loc_s, mask=start_n + offs_n < cur_batch_seq_len,\n            other=other_kv_index)\n        qk = tl.load(Logics + cur_head * stride_logic_h + (\n            cur_batch_start_loc + start_n + offs_n) * stride_logic_bs, mask\n            =start_n + offs_n < cur_batch_seq_len, other=float('-inf'))\n        n_e_max = tl.maximum(tl.max(qk, 0), e_max)\n        old_scale = tl.exp(e_max - n_e_max)\n        p = tl.exp(qk - n_e_max)\n        e_sum = e_sum * old_scale + tl.sum(p, 0)\n        v = tl.load(v_ptrs + v_index[:, None] * stride_vbs)\n        acc = acc * old_scale + tl.sum(p[:, None] * v, 0)\n        e_max = n_e_max\n    acc = acc / e_sum\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "6d68bf04-f608-4e03-9c99-cf4e1e96c4ca"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})\n@triton.jit\ndef layer_norm_backward_kernel(output_grad_pointer, input_pointer,\n    mean_pointer, inv_std_pointer, weight_pointer, input_grad_pointer,\n    weight_grad_pointer, bias_grad_pointer, batch_dim, feat_dim,\n    output_grad_batch_stride, output_grad_feat_stride, input_batch_stride,\n    input_feat_stride, input_grad_batch_stride, input_grad_feat_stride,\n    weight_grad_batch_stride, weight_grad_feat_stride,\n    bias_grad_batch_stride, bias_grad_feat_stride, scale_by_weight:\n    'tl.constexpr', add_bias: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_FEAT: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of layer normalization.\n\n    Args:\n        output_grad_pointer: Pointer to layer normalization's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim].\n        mean_pointer: Pointer to the input's mean.\n            The mean should be of shape [batch_dim].\n        inv_std_pointer: Pointer to the input's inverse standard deviation.\n            The inverse standard deviation should be of shape [batch_dim].\n        weight_pointer: Pointer to optional weights if affine transform occurred.\n            The weights, if provided, must be of shape [feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim].\n        weight_grad_pointer: Pointer to an optional container the weights' row-wise gradients\n            are written to if scale_by_weight is True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        bias_grad_pointer: Pointer to an optional container the bias vector's row-wise gradients\n            are written to if scale_by_weight and add_bias are True, which should later be summed.\n            The container, if provided, must be of shape [batch_dim/BLOCK_SIZE_BATCH, feat_dim].\n        batch_dim: Batch dimension.\n        feat_dim: Dimensionality of the features.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        weight_grad_batch_stride: Stride necessary to jump one element along the\n            weight gradient container's batch dimension.\n        weight_grad_feat_stride: Stride necessary to jump one element along the\n            weight gradient container's feature dimension.\n        bias_grad_batch_stride: Stride necessary to jump one element along the\n            weight gradient container's batch dimension.\n        bias_grad_feat_stride: Stride necessary to jump one element along the\n            weight gradient container's feature dimension.\n        scale_by_weight: Flag for scaling the normalized output by weights.\n        add_bias: Flag for adding a bias vector to the normalized output\n            if scale_by_weight is True.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_FEAT: Block size across the feature dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\n    batch_mask = batch_offset < batch_dim\n    feat_mask = feat_offset < feat_dim\n    output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n        ] + output_grad_feat_stride * feat_offset[None, :]\n    input_pointer += input_batch_stride * batch_offset[:, None\n        ] + input_feat_stride * feat_offset[None, :]\n    input_grad_pointer += input_grad_batch_stride * batch_offset[:, None\n        ] + input_grad_feat_stride * feat_offset[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[\n        None, :])\n    mean = tl.load(mean_pointer + batch_offset, mask=batch_mask)\n    inv_std = tl.load(inv_std_pointer + batch_offset, mask=batch_mask)\n    pre_lin = (input - mean[:, None]) * inv_std[:, None]\n    if scale_by_weight:\n        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n        weight_output_grad_prod = weight * output_grad\n    else:\n        weight_output_grad_prod = output_grad\n    term1 = tl.sum(pre_lin * weight_output_grad_prod, axis=1) / feat_dim\n    term1 = pre_lin * term1[:, None]\n    term2 = tl.sum(weight_output_grad_prod, axis=1) / feat_dim\n    input_grad = inv_std[:, None] * (weight_output_grad_prod - (term1 +\n        term2[:, None]))\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        feat_mask[None, :])\n    if scale_by_weight:\n        weight_grad_pointer += (weight_grad_batch_stride * batch_pid + \n            weight_grad_feat_stride * feat_offset)\n        tl.store(weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0),\n            mask=feat_mask)\n        if add_bias:\n            bias_grad_pointer += (bias_grad_batch_stride * batch_pid + \n                bias_grad_feat_stride * feat_offset)\n            tl.store(bias_grad_pointer, tl.sum(output_grad, axis=0), mask=\n                feat_mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "9187d020-8580-45c4-85bb-cc99154cec5e"
  },
  {
    "input": "@triton.jit\ndef parallel_retention_fwd_kernel(q, k, v, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr', BTS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    o_k = tl.arange(0, BTS)\n    d_h = tl.math.exp2((BTS - o_k) * b_b)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_h[None, :]\n        b_o = b_o * tl.math.exp2(b_b * BTS)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    d_q = tl.math.exp2(tl.arange(0, BTL) * b_b)\n    b_o *= d_q[:, None]\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "941fa82e-f538-4a6b-bbd1-d94a9801eb1f"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n    BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', start_m, start_n, num_steps, MASK: 'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    offs_n = start_n + tl.arange(0, BLOCK_N2)\n    offs_k = tl.arange(0, HEAD_DIM)\n    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    Di = tl.load(D + offs_m)\n    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n    curr_n = start_n\n    step_n = BLOCK_N2\n    for blk_idx in range(num_steps):\n        kT = tl.load(kT_ptrs)\n        vT = tl.load(vT_ptrs)\n        qk = tl.dot(q, kT)\n        p = tl.math.exp2(qk - m)\n        if MASK:\n            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(mask, p, 0.0)\n        dp = tl.dot(do, vT)\n        ds = p * (dp - Di[:, None])\n        ds = ds\n        dq += tl.dot(ds, tl.trans(kT))\n        curr_n += step_n\n        kT_ptrs += step_n * stride_tok\n        vT_ptrs += step_n * stride_tok\n    return dq\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b5c1a464-7fe5-4e8a-bd52-aea33c699ddd"
  },
  {
    "input": "@triton.jit\ndef _approx_backward_kernel(DW, e, g, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    f = 1/2 * e * (1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) ))\n    h = f * up\n\n    df/de (with help from https://arxiv.org/pdf/2305.12073.pdf :))\n    df/de = 1/2 * [1 + tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )] +\n            1/2 * sech^2 [   sqrt(2/pi) * x * (1 + 0.044715 * x^2 )  ] *                            ( sqrt(2/pi) * x * (1 + 0.044715 * x^2 * 3 ) )\n\n    Notice sech^2(x) = 1 - tanh^2(x)\n    So reuse tanh( sqrt(2/pi) * x * (1 + 0.044715 * x^2 ) )\n\n    See https://www.desmos.com/calculator/nqprfoni6x\n    \"\"\"\n    block_idx = tl.program_id(0)\n    offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    DW_row = tl.load(DW + offsets, mask=mask, other=0)\n    e_row = tl.load(e + offsets, mask=mask, other=0)\n    g_row = tl.load(g + offsets, mask=mask, other=0)\n    s = 0.7978845608028654\n    a = s * e_row\n    b = a * 0.044715 * e_row * e_row\n    T = 1.0 + tl.math.tanh(a + b)\n    T2 = 0.5 * T\n    Q2 = -T2 * (T - 2.0) * (a + 3.0 * b)\n    df_de = T2 + Q2\n    f_row = T2 * e_row\n    f_row = f_row\n    h_row = f_row * g_row\n    df_row = DW_row * f_row\n    dg_row = DW_row * g_row\n    de_row = dg_row * df_de\n    de_row = de_row\n    tl.store(DW + offsets, h_row, mask=mask)\n    tl.store(e + offsets, df_row, mask=mask)\n    tl.store(g + offsets, de_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a788c34a-92bb-4a72-97a7-77bf48ce44f2"
  },
  {
    "input": "@triton.jit\ndef relu(x):\n    \"\"\"ReLU(Rectified Linear Unit, \u4fee\u6b63\u7ebf\u6027\u5355\u5143), only support inference.\n    max(0, x)\n    \"\"\"\n    return tl.maximum(0, x)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "870bfff1-698c-458c-92bd-af2e6a568b91"
  },
  {
    "input": "@triton.jit\ndef chunk_simple_gla_fwd_kernel_h(k, v, h, g, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * K * V, (K, V), (V, \n            1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_g_last = tl.load(g + i_bh * T + i_t * BT + BT - 1)\n        b_h *= tl.math.exp2(b_g_last)\n        b_g = tl.load(g + i_bh * T + i_t * BT + tl.arange(0, BT))\n        b_h += tl.dot(b_k, b_v * tl.math.exp2(b_g_last - b_g)[:, None],\n            allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * K * V, (K, V), (V, 1),\n            (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "04eee2e9-8f60-4b91-bc22-553b60f4ec91"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_softmax(Logics, B_Start_Loc, B_Seqlen, Prob_Out,\n    stride_logic_h, stride_logic_bs, stride_prob_h, stride_prob_bs,\n    BLOCK_SIZE: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    row = tl.load(Logics + cur_head * stride_logic_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_logic_bs, mask\n        =col_offsets < cur_batch_seq_len, other=-float('inf'))\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(Prob_Out + cur_head * stride_prob_h + (\n        cur_batch_in_all_start_index + col_offsets) * stride_prob_bs,\n        softmax_output, mask=col_offsets < cur_batch_seq_len)\n    return\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "3c0e0a03-bf85-4186-90b3-bbdb77543f15"
  },
  {
    "input": "@triton.jit\ndef _dequant_kernel(q_idx_ptr, absmax_ptr, qmap_ptr, dq_ptr, stride_qm,\n    stride_qn, M, N, GROUP_SIZE: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offsets = rm[:, None] * stride_qm + rn[None, :] * stride_qn\n    mask = (rm[:, None] < M) & (rn[None, :] < N)\n    tl.static_print(offsets)\n    group_offsets = offsets // GROUP_SIZE\n    tl.static_print('group_offsets', group_offsets)\n    q_idx = tl.load(q_idx_ptr + offsets, mask=mask)\n    tl.static_print(q_idx)\n    q_vals = tl.load(qmap_ptr + q_idx)\n    absmax = tl.load(absmax_ptr + group_offsets, mask=group_offsets < M * N //\n        GROUP_SIZE)\n    dq = q_vals * absmax\n    tl.store(dq_ptr + offsets, dq, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "6b1b9ca7-31f3-4af0-a81e-0435e2d553ac"
  },
  {
    "input": "@triton.jit\ndef _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "cc1ff29e-01ce-45b7-93c0-f5d1eeb4c346"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv6_bwd_kernel_dkv(q, k, v, w, u, do, dk, dk_aux, dv,\n    dh0, s_k_h, s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    p_do = do + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        not REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if \n        not REVERSE else 0)\n    p_dk = dk + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(0, BK) + (\n        (T - 1) * K if not REVERSE else 0)\n    p_dk_aux = dk_aux + (i_bh + i_v * B * H) * s_k_h + i_k * BK + tl.arange(\n        0, BK) + ((T - 1) * K if not REVERSE else 0)\n    p_dv = dv + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if not REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if \n        not REVERSE else 0)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bk[:, None] & mask_bv[None, :]\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(T - 1, -1, -1):\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_do = tl.load(p_do, mask=mask_bv, other=0)\n        b_dkv = b_q[:, None] * b_do[None, :]\n        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n        tl.store(p_dk_aux, b_dk, mask=mask_bk)\n        b_dk += tl.sum(b_dkv * b_u[:, None] * b_v[None, :], axis=1)\n        b_dv = tl.sum((b_dh + b_dkv * b_u[:, None]) * b_k[:, None], axis=0)\n        tl.store(p_dk, b_dk, mask=mask_bk)\n        tl.store(p_dv, b_dv, mask=mask_bv)\n        b_dh *= tl.exp(b_w)[:, None]\n        b_dh += b_dkv\n        p_q += K if REVERSE else -K\n        p_k += K if REVERSE else -K\n        p_v += V if REVERSE else -V\n        p_w += K if REVERSE else -K\n        p_do += V if REVERSE else -V\n        p_dk += K if REVERSE else -K\n        p_dk_aux += K if REVERSE else -K\n        p_dv += V if REVERSE else -V\n    if USE_INITIAL_STATE:\n        p_dh0 = dh0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]\n            ) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n        tl.store(p_dh0, b_dh, mask=mask_kv)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d60f71fa-3619-4c7a-aa90-502af5c8d684"
  },
  {
    "input": "@triton.jit\ndef dropout_rng(philox_seed, philox_offset, m, n, stride):\n    rng_offsets = dropout_offsets(philox_seed, philox_offset, m, n, stride)\n    return tl.rand(philox_seed, rng_offsets)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "bbe49b36-391d-4671-974a-7108f0656dcc"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dk_dv(Q, K, V, softmax_scale, dO, dQ, dK, dV, M, D,\n    stride_batch, stride_head, stride_seq, stride_dim, NUM_HEADS, SEQ_LEN,\n    BLOCK_Q: 'tl.constexpr', BLOCK_KV: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', STAGE: 'tl.constexpr'):\n    index_batch_head = tl.program_id(2)\n    index_batch = index_batch_head // NUM_HEADS\n    index_head = index_batch_head % NUM_HEADS\n    offset_batch_head = stride_batch * index_batch + stride_head * index_head\n    offset_batch_head_seq = index_batch_head * SEQ_LEN\n    Q += offset_batch_head\n    K += offset_batch_head\n    V += offset_batch_head\n    dO += offset_batch_head\n    dQ += offset_batch_head\n    dK += offset_batch_head\n    dV += offset_batch_head\n    M += offset_batch_head_seq\n    D += offset_batch_head_seq\n    offs_dim = tl.arange(0, HEAD_DIM)\n    index_block_kv = tl.program_id(0)\n    start_kv = index_block_kv * BLOCK_KV\n    offs_kv = start_kv + tl.arange(0, BLOCK_KV)\n    dV_block = tl.zeros([BLOCK_KV, HEAD_DIM], dtype=tl.float32)\n    dK_block = tl.zeros([BLOCK_KV, HEAD_DIM], dtype=tl.float32)\n    K_block = tl.load(K + offs_kv[:, None] * stride_seq + offs_dim[None, :] *\n        stride_dim)\n    V_block = tl.load(V + offs_kv[:, None] * stride_seq + offs_dim[None, :] *\n        stride_dim)\n    offs_q = tl.arange(0, BLOCK_Q)\n    qT_ptrs = Q + offs_q[None, :] * stride_seq + offs_dim[:, None] * stride_dim\n    dO_ptrs = dO + offs_q[:, None] * stride_seq + offs_dim[None, :\n        ] * stride_dim\n    curr_q = 0\n    num_steps = SEQ_LEN // BLOCK_Q\n    for blk_idx in range(num_steps):\n        qT_block = tl.load(qT_ptrs)\n        offs_q = curr_q + tl.arange(0, BLOCK_Q)\n        m = tl.load(M + offs_q)\n        QK_T_block = softmax_scale * tl.dot(K_block, qT_block)\n        P_T_block = tl.math.exp(QK_T_block - m[None, :])\n        if STAGE == 3:\n            mask_block = offs_q[None, :] >= offs_kv[:, None]\n            P_T_block = tl.where(mask_block, P_T_block, 0.0)\n        dO_block = tl.load(dO_ptrs)\n        dV_block += tl.dot(P_T_block, dO_block)\n        Di = tl.load(D + offs_q)\n        dpT_block = tl.dot(V_block, tl.trans(dO_block))\n        dS_T_block = P_T_block * (dpT_block - Di[None, :])\n        dS_T_block = dS_T_block\n        dK_block += softmax_scale * tl.dot(dS_T_block, tl.trans(qT_block))\n        curr_q += BLOCK_Q\n        qT_ptrs += BLOCK_Q * stride_seq\n        dO_ptrs += BLOCK_Q * stride_seq\n    dV_block_ptrs = dV + offs_kv[:, None] * stride_seq + offs_dim[None, :\n        ] * stride_dim\n    tl.store(dV_block_ptrs, dV_block)\n    dK_block_ptrs = dK + offs_kv[:, None] * stride_seq + offs_dim[None, :\n        ] * stride_dim\n    tl.store(dK_block_ptrs, dK_block)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "4f58ffb1-e9ad-434b-bcda-da6bedffe82f"
  },
  {
    "input": "@triton.jit\ndef kernel_vector_addition(a_ptr, b_ptr, out_ptr, num_elems: 'tl.constexpr',\n    block_size: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * block_size\n    thread_offsets = block_start + tl.arange(0, block_size)\n    mask = thread_offsets < num_elems\n    a_pointers = tl.load(a_ptr + thread_offsets, mask=mask)\n    b_pointers = tl.load(b_ptr + thread_offsets, mask=mask)\n    res = a_pointers + b_pointers\n    tl.store(out_ptr + thread_offsets, res, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "61863148-c1c1-4519-9270-9ee2e1df2355"
  },
  {
    "input": "@triton.jit\ndef _triton_mixed_sparse_attn_fwd_kernel(Q, K, V, seqlens, sm_scale,\n    block_count, block_offset, column_count, column_index, Out, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, stride_oz,\n    stride_oh, stride_om, stride_ok, Z, H, N_CTX, NUM_ROWS, NNZ_S, NNZ_V,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', dtype: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    seqlen = tl.load(seqlens + off_hz // H)\n    if start_m * BLOCK_M >= seqlen:\n        return\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    qo_offset = off_hz // H * stride_qz + off_hz % H * stride_qh\n    kv_offset = off_hz // H * stride_kz + off_hz % H * stride_kh\n    q_ptrs = Q + qo_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n        ] * stride_qk\n    k_ptrs = K + kv_offset + offs_d[:, None] * stride_kk\n    v_ptrs = V + kv_offset + offs_d[None, :] * stride_vk\n    o_ptrs = Out + qo_offset + offs_m[:, None] * stride_om + offs_d[None, :\n        ] * stride_ok\n    num_blks = tl.load(block_count + off_hz * NUM_ROWS + start_m)\n    blks_ptr = block_offset + (off_hz * NUM_ROWS + start_m) * NNZ_S\n    num_cols = tl.load(column_count + off_hz * NUM_ROWS + start_m)\n    cols_ptr = column_index + (off_hz * NUM_ROWS + start_m) * NNZ_V\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(q_ptrs)\n    q = q * qk_scale\n    m_mask = offs_m[:, None] < seqlen\n    for block_index in range(num_blks):\n        start_n = tl.load(blks_ptr + block_index)\n        cols = start_n + offs_n\n        n_mask = cols < seqlen\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :\n            ], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None\n            ], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        causal_mask = cols[None, :] <= offs_m[:, None]\n        qk = tl.where(m_mask & causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    for start_n in range(0, num_cols, BLOCK_N):\n        n_mask = start_n + offs_n < num_cols\n        cols = tl.load(cols_ptr + start_n + offs_n, mask=n_mask, other=0)\n        k = tl.load(k_ptrs + cols[None, :] * stride_kn, mask=n_mask[None, :\n            ], other=0.0)\n        v = tl.load(v_ptrs + cols[:, None] * stride_vn, mask=n_mask[:, None\n            ], other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk = tl.where(m_mask & n_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n    acc /= l_i[:, None]\n    tl.store(o_ptrs, acc, mask=m_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "48f362f8-abd8-4356-a0e0-40861c44ae95"
  },
  {
    "input": "@triton.jit\ndef parallel_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "808526c4-6ba3-484a-aed2-e6eab4403063"
  },
  {
    "input": "@triton.jit\ndef chunk_delta_rule_bwd_kernel_dqkd(q, k, v, d, h, do, dh, dq, dk, dv, dd,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s = tl.where(o_i[:, None] <= o_i[None, :], b_s, 0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dd = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dd += tl.dot(b_dv, b_h, allow_tf32=False)\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale, 0)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dd = tl.make_block_ptr(dd + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dd, -b_dd, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d96abf0f-28f4-4660-84aa-6fa20d4f3545"
  },
  {
    "input": "@triton.jit\ndef _fwd_kv_reduce(K, V, S, KV, b: 'tl.constexpr', h: 'tl.constexpr', n:\n    'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', D_FBLOCK: 'tl.constexpr',\n    E_FBLOCK: 'tl.constexpr', NUM_FBLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_h = off_bh % h\n    off_d = tl.program_id(1)\n    off_e = tl.program_id(2)\n    kv_offset = off_bh * (NUM_BLOCK + 1) * d * e\n    d_offset = off_d * D_FBLOCK\n    e_offset = off_e * E_FBLOCK\n    KV_block_ptr = KV + kv_offset + d_offset * e + e_offset + tl.arange(0,\n        D_FBLOCK)[:, None] * e + tl.arange(0, E_FBLOCK)[None, :]\n    s_ptrs = S + off_h\n    s = tl.load(s_ptrs)\n    block_decay = tl.exp(-s * BLOCK)\n    kv = tl.zeros([D_FBLOCK, E_FBLOCK], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        kv_current = tl.load(KV_block_ptr)\n        tl.store(KV_block_ptr, kv)\n        kv = block_decay * kv + kv_current\n        KV_block_ptr += d * e\n    tl.store(KV_block_ptr, kv)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "b5f46094-cbbf-4e8d-b5db-65f391bdf021"
  },
  {
    "input": "@triton.jit\ndef relu6_grad(input):\n    \"\"\"\n    Calculates the gradient of ReLU6.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of ReLU6.\n    \"\"\"\n    return tl.where((0 < input) & (input < 6), 1, 0)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "56c9b8f8-0565-4ad0-93da-2c5985b03c3a"
  },
  {
    "input": "@triton.jit\ndef triton_cross_merge(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp0 + i_w * BW * DH + tl.arange(\n        0, BW)[None, :] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NW - i_w - 1\n        ) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]) * DH + (NH - i_h - 1\n        ) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (DH - NH * BH) + (DW -\n        NW * BW) * DH\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _y1 = tl.load(p_y1 + _idx, mask=_mask_hw)\n        _y2 = tl.load(p_y2 + _idx, mask=_mask_hw)\n        _y3 = tl.load(p_y3 + _idx, mask=_mask_hw)\n        _y4 = tl.load(p_y4 + _idx, mask=_mask_hw)\n        tl.store(p_x + _idx, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "23827b84-77e2-4603-ac25-fbc7b7994a11"
  },
  {
    "input": "@triton.jit\ndef clamp(x: 'tl.tensor', min_val, max_val) ->tl.tensor:\n    \"\"\"Clamps all elements in `x` into range [min, max].\n\n    Args:\n        x (tl.tensor): the input tensor.\n        min_val (Number): lower bound of the range.\n        max_val (Number): upper bound of the range.\n\n    Returns:\n        tl.tensor: the output tensor.\n    \"\"\"\n    return tl.math.min(tl.math.max(x, min_val), max_val)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "0353f0df-30c3-4256-892a-ce2d1da96f4b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "e5436848-beeb-4d81-9923-42558748710c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS', 'HAS_DROPOUT'])\n@triton.heuristics({'HAS_ROWSCALE': lambda args: args['ROWSCALE'] is not None})\n@triton.heuristics({'HAS_DY1': lambda args: args['DY1'] is not None})\n@triton.heuristics({'HAS_DX1': lambda args: args['DX1'] is not None})\n@triton.heuristics({'HAS_B1': lambda args: args['DB1'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, W1, DY1,\n    DX1, DW1, DB1, DRESIDUAL_IN, ROWSCALE, SEEDS, Mean, Rstd, stride_x_row,\n    stride_y_row, stride_dy_row, stride_dx_row, stride_dres_row,\n    stride_dy1_row, stride_dx1_row, stride_dres_in_row, M, N, eps,\n    dropout_p, rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', HAS_DROPOUT: 'tl.constexpr',\n    HAS_ROWSCALE: 'tl.constexpr', HAS_DY1: 'tl.constexpr', HAS_DX1:\n    'tl.constexpr', HAS_B1: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if HAS_DY1:\n        DY1 += row_start * stride_dy1_row\n    if HAS_DX1:\n        DX1 += row_start * stride_dx1_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    if HAS_DY1:\n        w1 = tl.load(W1 + cols, mask=mask)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_DY1:\n        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n        if HAS_B1:\n            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if HAS_DY1:\n            dy1 = tl.load(DY1 + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_DY1:\n            wdy += w1 * dy1\n            dw1 += dy1 * xhat\n            if HAS_B1:\n                db1 += dy1\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        if HAS_DX1:\n            if HAS_DROPOUT:\n                keep_mask = tl.rand(tl.load(SEEDS + M + row), cols, n_rounds=7\n                    ) > dropout_p\n                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n            else:\n                dx1 = dx\n            tl.store(DX1 + cols, dx1, mask=mask)\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + row), cols, n_rounds=7\n                ) > dropout_p\n            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n        if HAS_ROWSCALE:\n            rowscale = tl.load(ROWSCALE + row)\n            dx *= rowscale\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n        if HAS_DY1:\n            DY1 += stride_dy1_row\n        if HAS_DX1:\n            DX1 += stride_dx1_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n    if HAS_DY1:\n        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)\n        if HAS_B1:\n            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "e993cacf-a35c-4136-9889-e15e62fe69b4"
  },
  {
    "input": "@triton.jit\ndef squared_relu_grad(x):\n    return tl.where(x >= 0, 2.0 * x, 0.0)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "682015d7-ca37-4269-8502-dd6eec7838f4"
  },
  {
    "input": "@triton.jit\ndef _cross_entropy_backward(logits_ptr, logits_row_stride, dloss_ptr,\n    dloss_row_stride, logsumexp_ptr, labels_ptr, VOCAB_SIZE: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        CE_i = -y log(P) = y * (log[sum(exp(x))] - x)\n        dC/dx = d/dx (y * log[sum(exp(x))] - x * y)\n\n        From https://en.wikipedia.org/wiki/LogSumExp\n        d/dx logsumexp = exp(x) / sum(exp(x)) = softmax(x)\n\n        dC/dx = y * exp(x) / sum(exp(x)) - d/dx (x * y)\n        dC/dx = y * exp[ log[exp(x) / sum(exp(x))] ] using x = exp(log(x)) trick\n        dC/dx = y * exp[x - logsumexp] - d/dx (x * y)\n\n        If y == 0: dC/dx = 0\n        If y == 1 and x == label: dC/dlabel = exp[x - logsumexp] - 1\n        If y == 1 and x != label: dC/dx     = exp[x - logsumexp]\n    \"\"\"\n    row_idx = tl.program_id(0)\n    block_idx = tl.program_id(1)\n    logits_ptr += row_idx * logits_row_stride\n    dloss_ptr += row_idx * dloss_row_stride\n    col_offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < VOCAB_SIZE\n    label_idx = tl.load(labels_ptr + row_idx)\n    if label_idx != -100:\n        dloss = tl.load(dloss_ptr)\n    else:\n        dloss = 0.0\n    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float('inf'))\n    logsumexp = tl.load(logsumexp_ptr + row_idx)\n    y = tl.exp(x - logsumexp)\n    y = tl.where(col_offsets == label_idx, y - 1.0, y)\n    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b3bbb611-b16a-4603-86fd-d7a5bb6a2601"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=4)], key=[])\n@triton.jit\ndef conv2d_kernel(input_ptr, input_batch_stride, input_channel_stride,\n    input_row_stride, input_col_stride, height, width, channels, kernel_ptr,\n    kernel_height, kernel_width, kernel_dim_stride, kernel_channel_stride,\n    kernel_row_stride, kernel_col_stride, bias_ptr, output_ptr,\n    output_width, output_batch_stride, output_channel_stride,\n    output_row_stride, output_col_stride, BLOCK_SIZE_ROW: 'tl.constexpr',\n    BLOCK_SIZE_COL: 'tl.constexpr'):\n    batch_idx = tl.program_id(0)\n    kernel_idx = tl.program_id(1)\n    row_idx = tl.program_id(2)\n    bias_offset = kernel_idx\n    bias = tl.load(bias_ptr + bias_offset)\n    batch_offset = batch_idx * input_batch_stride\n    output_batch_offset = batch_idx * output_batch_stride\n    output_channel_offset = kernel_idx * output_channel_stride\n    output_row_offset = row_idx * output_row_stride\n    kernel_row_offset = tl.arange(0, BLOCK_SIZE_ROW)\n    kernel_row_mask = kernel_row_offset[:, None] < kernel_height\n    kernel_row_offset = kernel_row_offset[:, None] * kernel_row_stride\n    kernel_col_offset = tl.arange(0, BLOCK_SIZE_COL)\n    kernel_col_mask = kernel_col_offset[None, :] < kernel_width\n    kernel_col_offset = kernel_col_offset[None, :] * kernel_col_stride\n    kernel_mask = kernel_row_mask & kernel_col_mask\n    for col_idx in range(output_width):\n        elem = 0.0\n        input_row_offset = row_idx * kernel_height + tl.arange(0,\n            BLOCK_SIZE_ROW)\n        input_row_mask = input_row_offset[:, None] < height\n        input_row_offset = input_row_offset[:, None] * input_row_stride\n        input_col_offset = col_idx * kernel_width + tl.arange(0, BLOCK_SIZE_ROW\n            )\n        input_col_mask = input_col_offset[None, :] < width\n        input_col_offset = input_col_offset[None, :] * input_col_stride\n        input_mask = input_row_mask & input_col_mask\n        for c in range(channels):\n            input_offset = (input_ptr + batch_offset + c *\n                input_channel_stride + input_row_offset + input_col_offset)\n            input_data = tl.load(input_offset, input_mask)\n            kernel_offset = (kernel_ptr + kernel_idx * kernel_dim_stride + \n                c * kernel_channel_stride + kernel_row_offset +\n                kernel_col_offset)\n            kernel_data = tl.load(kernel_offset, kernel_mask)\n            dot_prdct = input_data * kernel_data\n            elem += tl.sum(dot_prdct)\n        output_offset = (output_ptr + output_batch_offset +\n            output_channel_offset + output_row_offset + col_idx)\n        tl.store(output_offset, elem + bias)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "315b2fc4-ab7c-4fa5-9a60-ea22fda35bd4"
  },
  {
    "input": "@triton.jit\ndef exp_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.exp(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "exponential",
    "uuid": "524c311b-ab05-4f9b-8a76-ff4e924ddffd"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=offs_h[:, None] < head_num, other=0.0)\n    tl.store(o_ptrs, k, mask=offs_h[:, None] < head_num)\n    return\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "58698305-1bfd-4fcf-8ade-4d80fdf14f3a"
  },
  {
    "input": "@triton.jit\ndef _kldiv_kernel_backward(target_ptr, target_stride, new_grads_ptr,\n    new_grads_stride, n_cols, BLOCK_SIZE: 'tl.constexpr', log_target:\n    'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    target_ptr += pid * target_stride\n    new_grads_ptr += pid * new_grads_stride\n    offsets = tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        offsets = i + tl.arange(0, BLOCK_SIZE)\n        mask = offsets < n_cols\n        target = tl.load(target_ptr + offsets, mask=mask, other=0.0)\n        if not log_target:\n            res = target * -1\n        else:\n            res = -tl.exp(target)\n        tl.store(new_grads_ptr + offsets, res, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "bfb2fea6-6585-4356-bf02-e03c24f31e26"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_flex(x, y, x_layout: 'tl.constexpr', y_layout:\n    'tl.constexpr', operation: 'tl.constexpr', onebyone: 'tl.constexpr',\n    scans: 'tl.constexpr', BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    HWRoute0 = i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    HWRoute1 = i_w * BW * DH + tl.arange(0, BW)[None, :\n        ] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    HWRoute2 = (NH - i_h - 1) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]\n        ) * DW + (NW - i_w - 1) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (\n        DH - NH * BH) * DW + (DW - NW * BW)\n    HWRoute3 = (NW - i_w - 1) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]\n        ) * DH + (NH - i_h - 1) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (\n        DH - NH * BH) + (DW - NW * BW) * DH\n    if scans == 1:\n        HWRoute1 = HWRoute0\n        HWRoute2 = HWRoute0\n        HWRoute3 = HWRoute0\n    elif scans == 2:\n        HWRoute1 = HWRoute0\n        HWRoute3 = HWRoute2\n    _tmp1 = DC * DH * DW\n    y_ptr_base = y + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if y_layout == 0\n         else i_c * BC)\n    if y_layout == 0:\n        p_y1 = y_ptr_base + HWRoute0\n        p_y2 = y_ptr_base + _tmp1 + HWRoute1\n        p_y3 = y_ptr_base + 2 * _tmp1 + HWRoute2\n        p_y4 = y_ptr_base + 3 * _tmp1 + HWRoute3\n    else:\n        p_y1 = y_ptr_base + HWRoute0 * 4 * DC\n        p_y2 = y_ptr_base + DC + HWRoute1 * 4 * DC\n        p_y3 = y_ptr_base + 2 * DC + HWRoute2 * 4 * DC\n        p_y4 = y_ptr_base + 3 * DC + HWRoute3 * 4 * DC\n    if onebyone == 0:\n        x_ptr_base = x + i_b * _tmp1 + (i_c * BC * DH * DW if x_layout == 0\n             else i_c * BC)\n        if x_layout == 0:\n            p_x = x_ptr_base + HWRoute0\n        else:\n            p_x = x_ptr_base + HWRoute0 * DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _x = tl.load(p_x + _idx_x, mask=_mask_hw)\n                tl.store(p_y1 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, _x, mask=_mask_hw)\n        elif operation == 1:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _y1 = tl.load(p_y1 + _idx_y, mask=_mask_hw)\n                _y2 = tl.load(p_y2 + _idx_y, mask=_mask_hw)\n                _y3 = tl.load(p_y3 + _idx_y, mask=_mask_hw)\n                _y4 = tl.load(p_y4 + _idx_y, mask=_mask_hw)\n                tl.store(p_x + _idx_x, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n    else:\n        x_ptr_base = x + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if x_layout ==\n            0 else i_c * BC)\n        if x_layout == 0:\n            p_x1 = x_ptr_base + HWRoute0\n            p_x2 = p_x1 + _tmp1\n            p_x3 = p_x2 + _tmp1\n            p_x4 = p_x3 + _tmp1\n        else:\n            p_x1 = x_ptr_base + HWRoute0 * 4 * DC\n            p_x2 = p_x1 + DC\n            p_x3 = p_x2 + DC\n            p_x4 = p_x3 + DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_y1 + _idx_y, tl.load(p_x1 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, tl.load(p_x2 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, tl.load(p_x3 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, tl.load(p_x4 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n        else:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_x1 + _idx_x, tl.load(p_y1 + _idx_y), mask=_mask_hw)\n                tl.store(p_x2 + _idx_x, tl.load(p_y2 + _idx_y), mask=_mask_hw)\n                tl.store(p_x3 + _idx_x, tl.load(p_y3 + _idx_y), mask=_mask_hw)\n                tl.store(p_x4 + _idx_x, tl.load(p_y4 + _idx_y), mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "d94a2848-ae92-4302-8ba1-ce6d44527e5a"
  },
  {
    "input": "@triton.jit\ndef _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(2 * b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "94e21885-94e7-43b2-a3b5-ee49338ed8ec"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    actual_seqlen_k, actual_seqlen_q, dropout_p, philox_seed,\n    batch_philox_offset, encoded_softmax_block_ptr, block_min, block_max,\n    offs_n_causal, masked_blocks, n_extra_tokens, bias_ptr, alibi_slope,\n    IS_CAUSAL: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', OFFS_M: 'tl.constexpr', OFFS_N:\n    'tl.constexpr', PRE_LOAD_V: 'tl.constexpr', MASK_STEPS: 'tl.constexpr',\n    ENABLE_DROPOUT: 'tl.constexpr', RETURN_ENCODED_SOFTMAX: 'tl.constexpr',\n    PADDED_HEAD: 'tl.constexpr'):\n    for start_n in range(block_min, block_max, BLOCK_N):\n        k = load_fn(K_block_ptr, PADDED_HEAD, MASK_STEPS and n_extra_tokens !=\n            0, 'zero')\n        if PRE_LOAD_V:\n            v = load_fn(V_block_ptr, MASK_STEPS and n_extra_tokens != 0,\n                PADDED_HEAD, 'zero')\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if MASK_STEPS:\n            if start_n + BLOCK_N == block_max and n_extra_tokens != 0:\n                boundary_m = tl.full([BLOCK_M], actual_seqlen_k, dtype=tl.int32\n                    )\n                size_n = start_n + OFFS_N[None, :]\n                mask = size_n < boundary_m[:, None]\n                qk = tl.where(mask, qk, float('-inf'))\n        if IS_CAUSAL:\n            causal_boundary = start_n + offs_n_causal\n            causal_mask = OFFS_M[:, None] >= causal_boundary[None, :]\n            qk = tl.where(causal_mask, qk, float('-inf'))\n        qk += tl.dot(q, k)\n        if bias_ptr is not None:\n            bias = load_fn(bias_ptr, False, MASK_STEPS and n_extra_tokens !=\n                0, 'zero')\n            qk += bias * 1.44269504089\n        if alibi_slope is not None:\n            global_m_positions = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n            global_n_positions = start_n + tl.arange(0, BLOCK_N)\n            relative_pos_block = global_m_positions[:, None\n                ] + actual_seqlen_k - global_n_positions[None, :\n                ] - actual_seqlen_q\n            relative_pos_block = tl.abs(relative_pos_block)\n            alibi_block = -1 * alibi_slope * relative_pos_block\n            qk += alibi_block * 1.44269504089\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        if ENABLE_DROPOUT:\n            philox_offset = (batch_philox_offset + start_m * BLOCK_M *\n                actual_seqlen_k + start_n - BLOCK_N)\n            keep = dropout_mask(philox_seed, philox_offset, dropout_p,\n                BLOCK_M, BLOCK_N, actual_seqlen_k)\n            if RETURN_ENCODED_SOFTMAX:\n                tl.store(encoded_softmax_block_ptr, tl.where(keep, p, -p))\n            p = tl.where(keep, p, 0.0)\n        elif RETURN_ENCODED_SOFTMAX:\n            tl.store(encoded_softmax_block_ptr, p)\n        alpha = tl.math.exp2(m_i - m_ij)\n        acc = acc * alpha[:, None]\n        if not PRE_LOAD_V:\n            v = load_fn(V_block_ptr, MASK_STEPS and n_extra_tokens != 0,\n                PADDED_HEAD, 'zero')\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n        acc += tl.dot(p, v)\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        if bias_ptr is not None:\n            bias_ptr = tl.advance(bias_ptr, (0, BLOCK_N))\n        if RETURN_ENCODED_SOFTMAX:\n            encoded_softmax_block_ptr = tl.advance(encoded_softmax_block_ptr,\n                (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "688425f8-27ec-4073-b094-6fa196d2403c"
  },
  {
    "input": "@triton.jit\ndef MSABwdFused(b_ij_ptr, logsumexp_ptr, N_head, RES_LEN: 'tl.constexpr',\n    BLOCK_SIZE_ROW: 'tl.constexpr', BLOCK_SIZE_COL: 'tl.constexpr'):\n    pid_zh = tl.program_id(0)\n    pid_i = tl.program_id(1)\n    pid_z = pid_zh // N_head\n    pid_h = pid_zh % N_head\n    log2_e = 1.44269504089\n    z_off = pid_z\n    h_off = pid_h\n    i_off = pid_i * BLOCK_SIZE_ROW\n    offs_i = i_off + tl.arange(0, BLOCK_SIZE_ROW)\n    lse_off = z_off * RES_LEN * N_head + offs_i[:, None] * N_head + h_off\n    lse_mask = (offs_i < RES_LEN)[:, None]\n    logsumexp = tl.load(logsumexp_ptr + lse_off, lse_mask, 0)\n    for j in range(0, RES_LEN, BLOCK_SIZE_COL):\n        offs_j = j + tl.arange(0, BLOCK_SIZE_COL)\n        b_offs = z_off * RES_LEN * RES_LEN * N_head + offs_i[:, None\n            ] * RES_LEN * N_head + offs_j[None, :] * N_head + h_off\n        ij_mask = (offs_i < RES_LEN)[:, None] & (offs_j < RES_LEN)[None, :]\n        b = tl.load(b_ij_ptr + b_offs, ij_mask, -INF)\n        b = tl.exp2(log2_e * (b - logsumexp))\n        tl.store(b_ij_ptr + b_offs, b, ij_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "multi-head attention",
    "uuid": "d914872a-96c3-49fb-9873-aeaca5087729"
  },
  {
    "input": "@triton.jit\ndef _kernel_inside_merge_continuous(alpha_c, alpha_d, tmp_merge,\n    tmp_merge_normalized, tmp_normalizer, stride_alpha_c1, stride_alpha_c2,\n    stride_alpha_c3, stride_alpha_d1, stride_alpha_d2, stride_alpha_d3,\n    stride_alpha_d4, stride_alpha_d5, stride_tmp_merge1, stride_tmp_merge2,\n    stride_tmp_merge_normalized1, r1, r2, r3, r4, b, n, w, L, BLOCK_R1:\n    'tl.constexpr', BLOCK_R2: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    start = tl.program_id(1)\n    end = start + w\n    if b_idx >= b:\n        return\n    offset_r = tl.arange(0, BLOCK_R1)\n    l_ptr = alpha_c + b_idx * stride_alpha_c1 + start * stride_alpha_c2 + (\n        start + 1) * stride_alpha_c3 + offset_r\n    r_ptr = alpha_c + b_idx * stride_alpha_c1 + (start + 1\n        ) * stride_alpha_c2 + end * stride_alpha_c3 + r1 + offset_r\n    acc1 = tl.zeros((BLOCK_R1,), dtype=tl.float32) - 1000000000.0\n    mask = tl.arange(0, BLOCK_R1) < r1\n    mask2 = tl.arange(0, BLOCK_R2) < r2\n    for _ in range(0, w - 1):\n        left = tl.load(l_ptr, mask=mask, other=-1000000000.0)\n        right = tl.load(r_ptr, mask=mask, other=-1000000000.0)\n        merge = left + right\n        acc1 = logaddexp(acc1, merge)\n        l_ptr += stride_alpha_c3\n        r_ptr += stride_alpha_c2\n    acc2 = tl.zeros((BLOCK_R2,), dtype=tl.float32) - 1000000000.0\n    for gap_start in range(start + 1, end - 1):\n        for gap_end in range(gap_start + 1, end):\n            ptr_c = (alpha_c + b_idx * stride_alpha_c1 + gap_start *\n                stride_alpha_c2 + gap_end * stride_alpha_c3 + 2 * r1 + tl.\n                arange(0, BLOCK_R2))\n            ptr_d = (alpha_d + b_idx * stride_alpha_d1 + start *\n                stride_alpha_d2 + gap_start * stride_alpha_d3 + gap_end *\n                stride_alpha_d4 + end * stride_alpha_d5 + tl.arange(0,\n                BLOCK_R2))\n            cont = tl.load(ptr_c, mask=mask2, other=-1000000000.0)\n            disco = tl.load(ptr_d, mask=mask2, other=-1000000000.0)\n            merge = cont + disco\n            acc2 = logaddexp(acc2, merge)\n    tl.store(tmp_merge + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + tl.arange(0, BLOCK_R1), acc1, mask=mask)\n    tl.store(tmp_merge + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + r1 + tl.arange(0, BLOCK_R2), acc2, mask=mask2)\n    acc1_max = tl.max(acc1, 0)\n    acc2_max = tl.max(acc2, 0)\n    acc_max = tl.maximum(acc1_max, acc2_max)\n    tl.store(tmp_normalizer + b_idx * stride_tmp_merge_normalized1 + start,\n        acc_max)\n    out1 = tl.exp(acc1 - acc_max)\n    tl.store(tmp_merge_normalized + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + tl.arange(0, BLOCK_R1), out1, mask=mask)\n    out2 = tl.exp(acc2 - acc_max)\n    tl.store(tmp_merge_normalized + b_idx * stride_tmp_merge1 + start *\n        stride_tmp_merge2 + r1 + tl.arange(0, BLOCK_R2), out2, mask=mask2)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "2e1a32df-d606-48f8-ac84-d44f4fb5759d"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_gla_fwd_kernel(q, k, v, g, o, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_db = g + i_bh * s_qk_h + (BT - 1) * s_qk_t + i_k * BK + tl.arange(0, BK)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < DK\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        d_b = tl.load(p_db, mask=mask, other=0)\n        if CHECK and i == 0:\n            b_o = tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h * tl.math.exp2(d_b)[:, None] + tl.dot(b_k, b_v,\n                allow_tf32=False)\n        else:\n            b_o = tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h * tl.math.exp2(d_b)[:, None] + tl.dot(b_k, b_v,\n                allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_db += BT * DK\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0303f7cc-2a64-44d7-a691-7956e5447d11"
  },
  {
    "input": "@triton.autotune(configs=_get_autotune_config(), key=['M', 'N', 'K'])\n@triton.jit\ndef _triton_gemm_kernel(a_ptr, b_ptr, c_ptr, stride_am: 'tl.constexpr',\n    stride_ak: 'tl.constexpr', stride_bk: 'tl.constexpr', stride_bn:\n    'tl.constexpr', stride_cm: 'tl.constexpr', stride_cn: 'tl.constexpr', M:\n    'tl.constexpr', N: 'tl.constexpr', K: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a_data = tl.load(a_ptrs, mask=offs_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        b_data = tl.load(b_ptrs, mask=offs_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a_data, b_data)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    acc = acc\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] *\n        stride_cn)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "05a18f14-6682-4826-b12d-f38ddeab3259"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024},\n    num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048},\n    num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])\n@triton.jit\ndef triton_red_fused_native_layer_norm_0(in_out_ptr0, in_ptr0, in_ptr1,\n    in_ptr2, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK: 'tl.constexpr',\n    RBLOCK: 'tl.constexpr'):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]\n    xmask = xindex < xnumel\n    rbase = tl.arange(0, RBLOCK)[None, :]\n    x0 = xindex\n    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp0 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy\n            ='evict_last')\n        tmp1 = tmp0\n        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = (triton_helpers.\n            welford_reduce(tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0)\n            )\n        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)\n        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)\n        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)\n    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(tmp3_mean,\n        tmp3_m2, tmp3_weight, 1)\n    tmp3 = tmp3_tmp[:, None]\n    tmp4 = tmp4_tmp[:, None]\n    tmp5 = tmp5_tmp[:, None]\n    tl.store(out_ptr0 + x0, tmp3, None)\n    tmp6 = rnumel\n    tmp7 = tmp4 / tmp6\n    tmp8 = 1e-05\n    tmp9 = tmp7 + tmp8\n    tmp10 = libdevice.rsqrt(tmp9)\n    tl.debug_barrier()\n    tl.store(in_out_ptr0 + x0, tmp10, None)\n    for roffset in range(0, rnumel, RBLOCK):\n        rindex = roffset + rbase\n        rmask = rindex < rnumel\n        r1 = rindex\n        tmp11 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask,\n            eviction_policy='evict_first')\n        tmp15 = tl.load(in_ptr1 + r1, rmask, eviction_policy='evict_last')\n        tmp18 = tl.load(in_ptr2 + r1, rmask, eviction_policy='evict_last')\n        tmp12 = tmp11\n        tmp13 = tmp12 - tmp3\n        tmp14 = tmp13 * tmp10\n        tmp16 = tmp15\n        tmp17 = tmp14 * tmp16\n        tmp19 = tmp18\n        tmp20 = tmp17 + tmp19\n        tmp21 = tmp20\n        tl.store(out_ptr1 + (r1 + rnumel * x0), tmp21, rmask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "daa609f9-2d4f-441c-8e66-af161fb36447"
  },
  {
    "input": "@triton.jit\ndef hardsigmoid_grad(input):\n    \"\"\"\n    Calculates the gradient of hard sigmoid.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of hard sigmoid.\n    \"\"\"\n    return tl.where((-3 < input) & (input < 3), 1 / 6, 0)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "f8df206b-2cb7-4395-b174-07248d9cd85b"
  },
  {
    "input": "@eval(\n    \"\"\"triton.heuristics({\n    'ROW_SIZE':\n    lambda kwargs: triton.next_power_of_2(kwargs['C'] // kwargs['groups']),\n    'BLOCK_SIZE':\n    lambda kwargs: max(\n        1, min(triton.next_power_of_2(kwargs['cluster_size']),\n               4096 // (triton.next_power_of_2(kwargs['C'] // kwargs['groups']))\n               )),\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps':\n    lambda kwargs: max(1, min(16, kwargs['ROW_SIZE'] * kwargs['BLOCK_SIZE'] // 128)),\n    'C_G': lambda kwargs: kwargs['C'] // kwargs['groups'],\n})\"\"\"\n    )\n@triton.jit\ndef group_norm_4d_channels_last_forward_collect_stats_kernel_stage_1(input_ptr,\n    N, C, HxW, groups, cluster_size, cluster_num, cluster_mean_ptr,\n    cluster_m2_ptr, cluster_weight_ptr, C_G, ROW_SIZE: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    group = tl.program_id(0)\n    cluster = tl.program_id(1)\n    pid_batch = tl.program_id(2)\n    offset = pid_batch * C * HxW + group * C_G\n    X = input_ptr + offset\n    _mean = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _m2 = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    _weight = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n    row = tl.arange(0, ROW_SIZE)\n    start = cluster * cluster_size\n    end = start + cluster_size\n    end = min(end, HxW)\n    for off in range(start, end, BLOCK_SIZE):\n        r = off + tl.arange(0, BLOCK_SIZE)\n        m2_ = tl.zeros((BLOCK_SIZE, ROW_SIZE), dtype=tl.float32)\n        mask = (r < end)[:, None] & (row[None, :] < C_G)\n        weight_ = mask\n        x = tl.load(X + (r * C)[:, None] + row[None, :], mask=mask)\n        _mean, _m2, _weight = welford_combine(_mean, _m2, _weight, x, m2_,\n            weight_)\n    _mean = tl.view(_mean, (BLOCK_SIZE * ROW_SIZE,))\n    _m2 = tl.view(_m2, (BLOCK_SIZE * ROW_SIZE,))\n    _weight = tl.view(_weight, (BLOCK_SIZE * ROW_SIZE,))\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    offset = pid_batch * groups * cluster_num + group * cluster_num + cluster\n    tl.store(cluster_mean_ptr + offset, mean)\n    tl.store(cluster_m2_ptr + offset, m2)\n    tl.store(cluster_weight_ptr + offset, weight)\n",
    "category": "Normalization",
    "subcategory": "",
    "uuid": "25198f28-b913-4249-80ab-d14a18dbeac7"
  },
  {
    "input": "@triton.jit\ndef triton_modulation_gate_proj(img_ptr, mod_ptr, proj_ptr, output_ptr,\n    batch_size, head_size, modulation_size, XBLOCK: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    xoffset = pid * XBLOCK + tl.arange(0, XBLOCK)[:]\n    batch_idx = xoffset // batch_size\n    head_dim_idx = xoffset % head_size\n    modulation_offset = head_dim_idx + modulation_size * batch_idx\n    img = tl.load(img_ptr + xoffset, None)\n    mod_gate = tl.load(mod_ptr + (modulation_offset + head_size * 2), None,\n        eviction_policy='evict_last')\n    proj = tl.load(proj_ptr + xoffset, None)\n    output = img + mod_gate * proj\n    tl.store(output_ptr + xoffset, output, None)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "1ed0bbad-0f76-4658-a458-17d5ccbf660a"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef p_loss_backward_kernel(output_grad_pointer, input_pointer,\n    target_pointer, input_grad_pointer, target_grad_pointer, size, p_loss:\n    'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of the mean absolute error or\n    mean squared error.\n\n    Args:\n        output_grad_pointer: Pointer to the error's output gradients.\n            The output gradients must be a scalar or of shape [size].\n        input_pointer: Pointer to the input.\n            The input must be of shape [size].\n        target_pointer: Pointer to the target.\n            The target must be of shape [size].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [size].\n        target_grad_pointer: Pointer to a container the target's gradients are written to.\n            The container must be of shape [size].\n        size: Number of elements in the input and target.\n        p_loss: p-norm used to compute the error whose gradient is calculated.\n            Options are 1 for MAE and 2 for MSE.\n        reduction: Reduction strategy for the output whose gradient is calculated.\n            Options are 'none' for no reduction, 'mean' for averaging the error\n            across all entries, and 'sum' for summing the error across all entries.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad_mask = None\n    if reduction == 'none':\n        output_grad_pointer += offset\n        output_grad_mask = mask\n    input = tl.load(input_pointer + offset, mask=mask)\n    target = tl.load(target_pointer + offset, mask=mask)\n    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask)\n    if p_loss == 1:\n        input_grad = tl.where(target <= input, 1, -1)\n    elif p_loss == 2:\n        input_grad = 2 * (input - target)\n    if reduction == 'mean':\n        input_grad /= size\n    input_grad *= output_grad\n    tl.store(input_grad_pointer + offset, input_grad, mask=mask)\n    tl.store(target_grad_pointer + offset, -input_grad, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f5392bb6-2710-4e44-8855-3ace9ba81caa"
  },
  {
    "input": "@triton.jit\ndef _parallel_retention_bwd_dq(i_bh, i_c, i_k, i_v, i_h, k, v, do, dq,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d),\n        (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, 0), (BV, BTS), (0, 1))\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BTS)\n    d_h = tl.math.exp2((BTS - tl.arange(0, BTS)) * b_b)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_h[None, :]\n        b_dq *= d_b\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= tl.math.exp2(tl.arange(0, BTL) * b_b)[:, None] * scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t), (\n        i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((o_q[:, None] - o_k[None, :]) *\n            b_b), 0)\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False) * d_s * scale\n        b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "75621701-0d22-4541-8af0-c2c022597182"
  },
  {
    "input": "@triton.jit\ndef fused_cross_entropy_fwd_bwd_kernel(output_loss_ptr,\n    output_logit_grad_ptr, input_logit_ptr, input_targ_ptr,\n    input_divisor_ptr, output_loss_stride, output_logit_grad_stride,\n    input_logit_stride, input_targ_stride, n_cols, ignore_index:\n    'tl.constexpr', requires_grad: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    logit_grad_row_start_ptr = (output_logit_grad_ptr + row_idx *\n        output_logit_grad_stride)\n    logit_row_start_ptr = input_logit_ptr + row_idx * input_logit_stride\n    targ_ptr = input_targ_ptr + row_idx * input_targ_stride\n    loss_ptr = output_loss_ptr + row_idx * output_loss_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    logit_row_ptrs = logit_row_start_ptr + col_offsets\n    logit_grad_row_ptrs = logit_grad_row_start_ptr + col_offsets\n    logit_row_unnormalized = tl.load(logit_row_ptrs, mask=col_offsets <\n        n_cols, other=float('-Inf'))\n    targ = tl.load(targ_ptr)\n    divisor = tl.load(input_divisor_ptr)\n    logit_row = logit_row_unnormalized - tl.max(logit_row_unnormalized, axis=0)\n    exp_logit_row = tl.exp(logit_row)\n    sum_exp_logit_row = tl.sum(exp_logit_row, axis=0)\n    log_sum_exp_logit_row = tl.log(sum_exp_logit_row)\n    logit_gt_logit = tl.sum(tl.where(targ == col_offsets, logit_row, 0.0))\n    loss = log_sum_exp_logit_row - logit_gt_logit\n    loss = loss / divisor\n    loss = tl.where(targ == ignore_index, 0.0, loss)\n    tl.store(loss_ptr, loss)\n    if requires_grad:\n        targ_one_hot = tl.where(targ == col_offsets, 1.0, 0.0)\n        grad = exp_logit_row / sum_exp_logit_row - targ_one_hot\n        grad = grad / divisor\n        grad = tl.where(targ == ignore_index, 0.0, grad)\n        tl.store(logit_grad_row_ptrs, grad, mask=col_offsets < n_cols)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "261e351d-a388-442e-893e-ab3fb35dcad3"
  },
  {
    "input": "@triton.jit\ndef _triton_rope(q_ptr, q_row_stride, k_ptr, k_row_stride, cos,\n    cos_row_stride, sin, sin_row_stride, sl, bs: 'tl.constexpr', n_qh:\n    'tl.constexpr', n_kh: 'tl.constexpr', hd: 'tl.constexpr', pad_n_qh:\n    'tl.constexpr', pad_n_kh: 'tl.constexpr', pad_hd: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr', BACKWARD_PASS: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * q_row_stride\n    k_ptr = k_ptr + pid * k_row_stride\n    cos_row_idx = pid % sl\n    cos = cos + cos_row_idx * cos_row_stride\n    sin = sin + cos_row_idx * sin_row_stride\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    cos_mask = cos_offsets < hd // 2\n    cos_row = tl.load(cos + cos_offsets, mask=cos_mask, other=0)\n    sin_row = tl.load(sin + cos_offsets, mask=cos_mask, other=0)\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        )\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        )\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "0a3d8d47-e2b6-4e6a-83d5-7474f2d2b5f5"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': \n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)],\n    key=['chunk_size', 'K', 'IS_CAUSAL'])\n@triton.jit\ndef _bmm_chunk_fwd_kernel(a_ptr, b_ptr, out_ptr, seq_idx_ptr, seqlen,\n    chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head,\n    stride_ak, stride_b_batch, stride_b_seqlen, stride_b_head, stride_bk,\n    stride_out_batch, stride_out_chunk, stride_out_head, stride_outm,\n    stride_outn, stride_seq_idx_batch, stride_seq_idx_seqlen, IS_CAUSAL:\n    'tl.constexpr', dot_dtype: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    if IS_CAUSAL:\n        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n            return\n    a_ptr += (pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen +\n        pid_h * stride_a_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h * stride_b_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] *\n        stride_b_seqlen)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_k[None, :] < K - k * BLOCK_SIZE_K), other=0.0)\n        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) &\n            (offs_n[None, :] < chunk_size_limit), other=0.0)\n        acc += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit, other=-2)\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    out = acc\n    out_ptr += (pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h *\n        stride_out_head)\n    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] *\n        stride_outn)\n    tl.store(out_ptrs, out, mask=(offs_m[:, None] < chunk_size) & (offs_n[\n        None, :] < chunk_size))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4835f091-edac-4b9c-a735-652e823350b8"
  },
  {
    "input": "@triton.jit\ndef causal_product_kernel(q_ptr, k_ptr, v_ptr, output_ptr, batch, length,\n    dim, vdim, **meta):\n    BLOCK_SIZE = meta['BLOCK_SIZE']\n    pid = tl.program_id(axis=0)\n    state = tl.zeros((BLOCK_SIZE, BLOCK_SIZE), dtype=tl.float32)\n    cur_qk_pos = pid * length * dim\n    cur_v_pos = pid * length * vdim\n    dim_ptrs = tl.arange(0, BLOCK_SIZE)\n    qk_mask = dim_ptrs < dim\n    v_mask = dim_ptrs < vdim\n    for _ in range(0, length, 1):\n        qk_row_offsets = cur_qk_pos + dim_ptrs\n        v_row_offsets = cur_v_pos + dim_ptrs\n        k = tl.load(k_ptr + qk_row_offsets, mask=qk_mask, other=0)\n        v = tl.load(v_ptr + v_row_offsets, mask=v_mask, other=0)\n        context = tl.dot(k[:, None], v[None, :])\n        state += context\n        q = tl.load(q_ptr + qk_row_offsets, mask=qk_mask, other=0)\n        output = tl.dot(q[None, :], state)\n        tl.store(output_ptr + v_row_offsets[None, :], output, mask=v_mask[\n            None, :])\n        cur_qk_pos += dim\n        cur_v_pos += vdim\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "8be26b14-89e8-417f-ad16-1928affe951f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_reversed_cumsum_fwd_kernel(s, z, s_s_h, s_s_t, s_s_d, T:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "440180f9-e566-4e83-8ee7-9eae35c921b9"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "4392da7c-5781-449a-9a4b-62d9cb25027d"
  },
  {
    "input": "@triton.jit\ndef triton_silu(x_ptr, b_ptr, xnumel, XBLOCK: 'tl.constexpr'):\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    x = tl.load(x_ptr + x0, mask=xmask)\n    output = x * tl.sigmoid(x)\n    tl.store(b_ptr + x0, output, xmask)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "cc78f38f-acad-4d66-9ab3-77828f85d11a"
  },
  {
    "input": "@triton.jit\ndef sum_kernel(x_ptr, output_ptr, M, N, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    m_mask = m_offset < M\n    out = tl.zeros((BLOCK_M,), dtype=tl.float32)\n    for start in range(0, N, BLOCK_N):\n        n_offset = start + tl.arange(0, BLOCK_N)\n        offset = m_offset[:, None] * N + n_offset[None, :]\n        n_mask = n_offset < N\n        mask = m_mask[:, None] & n_mask[None, :]\n        inp = tl.load(x_ptr + offset, mask=mask, other=0)\n        out += tl.sum(inp, axis=1)\n    tl.store(output_ptr + m_offset, out, mask=m_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "37c1d1dc-7e3c-48b6-bb70-0e237ae1cc35"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_rwkv6_bwd_kernel_inter(q, k, v, h, gi, ge, u, do, dh, dA, dq, dk,\n    dq2, dk2, dg, du, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, s_h_h,\n    s_h_t, s_h_d, scale, H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    last_idx = min(T, i_t * BT + BT) - 1\n    p_gn = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), (\n        last_idx * K + i_k * BK,), (BK,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dgk = tl.zeros([BK], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * V * K, (V, K), (\n            s_h_d, s_h_t), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dgk += tl.sum(b_h * b_dh, axis=0)\n        b_dq += tl.dot(b_do, b_h)\n        b_dk += tl.dot(b_v, b_dh)\n    p_gk = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dgk *= tl.exp(b_gn)\n    b_dq *= scale\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    p_gi = tl.make_block_ptr(gi + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_gi = tl.load(p_gi, boundary_check=(0, 1))\n    b_dq = b_dq * tl.exp(b_gk)\n    b_dk = b_dk * tl.exp(b_gn[None, :] - b_gi)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_dk * b_k, axis=0)\n    b_dq += tl.load(p_dq, boundary_check=(0, 1))\n    b_dk += tl.load(p_dk, boundary_check=(0, 1))\n    b_dg = b_q * b_dq - b_k * b_dk\n    b_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :\n        ] + b_dgk[None, :] - b_q * b_dq\n    o_i = tl.arange(0, BT)\n    p_dA_dig = dA + i_bh * T * BT + (i_t * BT + o_i) * BT + o_i\n    b_dA_dig = tl.load(p_dA_dig, mask=i_t * BT + o_i < T, other=0)\n    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK,), (BK,), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    b_dq += b_dA_dig[:, None] * b_u[None, :] * b_k\n    b_dk += b_dA_dig[:, None] * b_u[None, :] * b_q\n    b_du = tl.sum(b_dA_dig[:, None] * b_q * b_k, axis=0)\n    p_du = tl.make_block_ptr(du + (i_h + i_t * n_bh) * K, (K,), (1,), (i_k *\n        BK,), (BK,), (0,))\n    tl.store(p_du, b_du, boundary_check=(0,))\n    p_dg = tl.make_block_ptr(dg + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq2 + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk2 + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dg, b_dg, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "44e92393-8858-46d2-9324-63e6d9664458"
  },
  {
    "input": "@triton.jit\ndef max_fn(x, y):\n    return tl.math.max(x, y)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "5e362512-f9ba-46fc-b7b7-b9a83ef0ff30"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, m_i, d_i, q, k_ptrs, v_ptrs, k_seq_stride,\n    v_seq_stride, offs_m, qk_scale, n_size, causal_mask, BLOCK_M_SIZE:\n    'tl.constexpr', BLOCK_N_SIZE: 'tl.constexpr', fp8_v: 'tl.constexpr'):\n    n_range_offs = tl.arange(0, BLOCK_N_SIZE)\n    for block_n_start_idx in range(0, n_size, BLOCK_N_SIZE):\n        block_n_start_idx = tl.multiple_of(block_n_start_idx, BLOCK_N_SIZE)\n        block_n_offs = block_n_start_idx + n_range_offs\n        k_mask = block_n_offs[:, None] < n_size\n        k = tl.load(k_ptrs + block_n_start_idx * k_seq_stride, mask=k_mask,\n            other=0.0)\n        qk = tl.dot(q, tl.trans(k))\n        if causal_mask:\n            offs_k = block_n_offs\n            mask = offs_m[:, None] >= offs_k[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -100000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        d_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        d_i = d_i * alpha + d_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(v_ptrs + block_n_start_idx * v_seq_stride, mask=k_mask,\n            other=0.0)\n        p = p\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n    return acc, d_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5392fe57-6ef0-41bf-98cc-c3e0405e501b"
  },
  {
    "input": "@triton.jit\ndef matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % num_pid_in_group % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        accumulator = tl.dot(a, b, accumulator, allow_tf32=False)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, c, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b6751fbc-cd2f-4a29-ab55-77abc3cfddf9"
  },
  {
    "input": "@triton.jit\ndef _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c5dff599-16fa-4f7a-8b4e-c96d8149a8d0"
  },
  {
    "input": "@triton.jit\ndef _geglu_tanh_forward_kernel(a, b, c, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    a += program_id * stride\n    b += program_id * stride\n    c += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    c_row = geglu_a * b_row\n    tl.store(c + col_offsets, c_row, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "1e09c13e-023d-43c8-88e4-89f253219333"
  },
  {
    "input": "@triton.jit\ndef grad_rotary_embedded_vector(grad_vec_rope, vec_origin, vec_rot, cos,\n    sin, HID, BLOCK_HID):\n    grad_vec_origin = grad_vec_rope * cos\n    idx_vec_origin_hid = tl.arange(0, BLOCK_HID)\n    grad_vec_rot = grad_vec_rope * sin\n    grad_vec_rot = tl.where(idx_vec_origin_hid < HID // 2, -grad_vec_rot,\n        grad_vec_rot)\n    idx_vec_rot_hid = (idx_vec_origin_hid + HID // 2) % HID\n    return grad_vec_origin, idx_vec_origin_hid, grad_vec_rot, idx_vec_rot_hid\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "e8c824f2-6a72-4718-bfb1-009ff1bb19c3"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_fwd_kernel_h(k, v, h, h0, ht, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "",
    "uuid": "5dd57b40-cb0a-4057-8fef-d5485627abdb"
  },
  {
    "input": "@triton.jit\ndef fwd_sequential_scan_complex(v_real, v_imag, decay_real, decay_imag,\n    hidden_real, hidden_imag, hidden_real_input, hidden_imag_input, B, L, C,\n    BLOCK_M: 'tl.constexpr'):\n    offset_b = tl.program_id(0)\n    if offset_b >= B:\n        return\n    offset_n = tl.program_id(1)\n    ptr = tl.arange(0, BLOCK_M) + offset_b * L * C + offset_n * BLOCK_M\n    ptr_input_hidden = tl.arange(0, BLOCK_M\n        ) + offset_b * C + offset_n * BLOCK_M\n    h_real = tl.load(hidden_real_input + ptr_input_hidden)\n    h_imag = tl.load(hidden_imag_input + ptr_input_hidden)\n    for _ in range(L):\n        x_real = tl.load(v_real + ptr)\n        x_imag = tl.load(v_imag + ptr)\n        f_real = tl.load(decay_real + ptr)\n        f_imag = tl.load(decay_imag + ptr)\n        h_real_new = h_real * f_real - h_imag * f_imag + x_real\n        h_imag_new = h_real * f_imag + h_imag * f_real + x_imag\n        tl.store(hidden_real + ptr, h_real_new)\n        tl.store(hidden_imag + ptr, h_imag_new)\n        h_real = h_real_new\n        h_imag = h_imag_new\n        ptr += C\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "4ea28a78-2e8c-430a-b786-41dc4931b20a"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256,\n    'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8)],\n    key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'sumexp_ptr', 'z_nv_ptr']\n    )\n@triton.jit\ndef linear_xent_fwd_prep_bwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr,\n    z_nv_ptr, losses_ptr, sumexp_ptr, stride_x_N, stride_x_H, stride_A_H,\n    stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group:\n    'tl.constexpr', V: 'tl.constexpr', N: 'tl.constexpr', H: 'tl.constexpr',\n    V_BLOCK_SIZE: 'tl.constexpr', N_BLOCK_SIZE: 'tl.constexpr',\n    H_BLOCK_SIZE: 'tl.constexpr', V_TILES: 'tl.constexpr'=4):\n    idx_N = tl.program_id(axis=0)\n    idx_V_group = tl.program_id(axis=1)\n    V_GROUP_SIZE: 'tl.constexpr' = V_TILES * V_BLOCK_SIZE\n    x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(\n        stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N *\n        N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(\n        1, 0))\n    A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(\n        stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE),\n        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n    z_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V),\n        strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, \n        idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE,\n        V_BLOCK_SIZE), order=(1, 0))\n    sumexp_row_ptr = sumexp_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0,\n        N_BLOCK_SIZE)\n    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0,\n        N_BLOCK_SIZE)\n    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    y = tl.load(y_ptr + N_range)\n    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10000000.0)\n    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\n    loss = 0.0\n    for _ in range(V_TILES):\n        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n        for _ in range(H // H_BLOCK_SIZE):\n            x_chunk = tl.load(x_block_ptr)\n            A_v = tl.load(A_block_ptr)\n            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n        s = s * tl.exp(m - m_new) + s_update\n        mask = y[:, None] == V_range[None, :]\n        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n        tl.store(z_block_ptr, z_j_to_k)\n        m = m_new\n        x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n        V_range = V_range + V_BLOCK_SIZE\n    lse = m + tl.log(s)\n    sum_exp = tl.exp(lse)\n    tl.atomic_add(losses_ptr + idx_N, loss)\n    tl.atomic_add(sumexp_row_ptr, sum_exp)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "f82daa88-fbfd-4cf7-9179-ddd6c5b86ba1"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, B, Mean, Rstd, stride: 'tl.constexpr', N:\n    'tl.constexpr', eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    if BLOCK_SIZE >= N:\n        cols = tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N)\n        m2_ = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        weight_ = cols < N\n        _mean, _m2, _weight = x, m2_, weight_\n    else:\n        _mean = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        _m2 = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        _weight = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n        for off in range(0, N, BLOCK_SIZE):\n            cols = off + tl.arange(0, BLOCK_SIZE)\n            x = tl.load(X + cols, mask=cols < N)\n            m2_ = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n            weight_ = cols < N\n            if off == 0:\n                _mean, _m2, _weight = x, m2_, weight_\n            else:\n                _mean, _m2, _weight = welford_combine(_mean, _m2, _weight,\n                    x, m2_, weight_)\n    mean, m2, weight = tl.reduce((_mean, _m2, _weight), 0, welford_combine)\n    var = m2 / weight\n    rstd = 1 / tl.sqrt(var + eps)\n    mean = mean\n    rstd = rstd\n    if Mean is not None:\n        tl.store(Mean + row, mean)\n    if Rstd is not None:\n        tl.store(Rstd + row, rstd)\n    if BLOCK_SIZE >= N:\n        cols = tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        if W is None:\n            w = tl.full((BLOCK_SIZE,), 1.0, dtype=x.dtype)\n        else:\n            w = tl.load(W + cols, mask=mask)\n        if B is None:\n            b = tl.zeros((BLOCK_SIZE,), dtype=x.dtype)\n        else:\n            b = tl.load(B + cols, mask=mask)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n    else:\n        for off in range(0, N, BLOCK_SIZE):\n            cols = off + tl.arange(0, BLOCK_SIZE)\n            mask = cols < N\n            if W is None:\n                w = tl.full((BLOCK_SIZE,), 1.0, dtype=x.dtype)\n            else:\n                w = tl.load(W + cols, mask=mask)\n            if B is None:\n                b = tl.zeros((BLOCK_SIZE,), dtype=x.dtype)\n            else:\n                b = tl.load(B + cols, mask=mask)\n            x = tl.load(X + cols, mask=mask)\n            x_hat = (x - mean) * rstd\n            y = x_hat * w + b\n            tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "7d78a691-f8e8-407e-b56c-4b233e8047d5"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args[\n    'batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})\n@triton.jit\ndef batch_norm_backward_kernel(output_grad_pointer, input_pointer,\n    mean_pointer, inv_std_pointer, weight_pointer, input_grad_pointer,\n    weight_grad_pointer, bias_grad_pointer, batch_dim, spatial_dim,\n    output_grad_batch_stride, output_grad_feat_stride,\n    output_grad_spatial_stride, input_batch_stride, input_feat_stride,\n    input_spatial_stride, input_grad_batch_stride, input_grad_feat_stride,\n    input_grad_spatial_stride, affine: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_SPATIAL: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of batch normalization.\n\n    Args:\n        output_grad_pointer: Pointer to layer normalization's output gradients.\n            The output gradients must be of shape [batch_dim, feat_dim, spatial_dim].\n        input_pointer: Pointer to the input.\n            The input must be of shape [batch_dim, feat_dim, spatial_dim].\n        mean_pointer: Pointer to the input's mean.\n            The mean should be of shape [feat_dim].\n        inv_std_pointer: Pointer to the input's inverse standard deviation.\n            The inverse standard deviation should be of shape [feat_dim].\n        weight_pointer: Pointer to optional weights if affine transform occurred.\n            The weights, if provided, must be of shape [feat_dim].\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim, spatial_dim].\n        weight_grad_pointer: Pointer to an optional container the weights' gradients\n            are written to if scale_by_weight is True.\n            The container, if provided, must be of shape [feat_dim].\n        bias_grad_pointer: Pointer to an optional container the bias vector's gradients\n            are written to if scale_by_weight is True.\n            The container, if provided, must be of shape [feat_dim].\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        output_grad_spatial_stride: Stride necessary to jump one element along the\n            output gradients' spatial dimension.\n        input_batch_stride: Stride necessary to jump one element along the\n            input's batch dimension.\n        input_feat_stride: Stride necessary to jump one element along the\n            input's feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        input_grad_spatial_stride: Stride necessary to jump one element along the\n            input gradient container's spatial dimension.\n        affine: Flag for performing an affine transformation on the normalized output.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    feat_pid = tl.program_id(axis=0)\n    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\n    batch_mask = batch_offset < batch_dim\n    mean = tl.load(feat_pid + mean_pointer)\n    inv_std = tl.load(feat_pid + inv_std_pointer)\n    term1 = 0.0\n    term2 = 0.0\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n            BLOCK_SIZE_SPATIAL)\n        spatial_mask = spatial_offset < spatial_dim\n        curr_output_grad_pointer = (output_grad_pointer + \n            output_grad_feat_stride * feat_pid + output_grad_batch_stride *\n            batch_offset[:, None] + output_grad_spatial_stride *\n            spatial_offset[None, :])\n        curr_input_pointer = (input_pointer + input_feat_stride * feat_pid +\n            input_batch_stride * batch_offset[:, None] + \n            input_spatial_stride * spatial_offset[None, :])\n        curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        curr_pre_lin = (curr_input - mean) * inv_std\n        curr_output_grad = tl.load(curr_output_grad_pointer, mask=\n            batch_mask[:, None] & spatial_mask[None, :])\n        term1 += tl.sum(curr_pre_lin * curr_output_grad)\n        term2 += tl.sum(curr_output_grad)\n    if affine:\n        weight = tl.load(feat_pid + weight_pointer)\n        weight_grad = 0.0\n        bias_grad = 0.0\n    else:\n        weight = 1.0\n    count = batch_dim * spatial_dim\n    term1 *= weight / count\n    term2 *= weight / count\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0,\n            BLOCK_SIZE_SPATIAL)\n        spatial_mask = spatial_offset < spatial_dim\n        curr_output_grad_pointer = (output_grad_pointer + \n            output_grad_feat_stride * feat_pid + output_grad_batch_stride *\n            batch_offset[:, None] + output_grad_spatial_stride *\n            spatial_offset[None, :])\n        curr_input_pointer = (input_pointer + input_feat_stride * feat_pid +\n            input_batch_stride * batch_offset[:, None] + \n            input_spatial_stride * spatial_offset[None, :])\n        curr_input_grad_pointer = (input_grad_pointer + \n            input_grad_feat_stride * feat_pid + input_grad_batch_stride *\n            batch_offset[:, None] + input_grad_spatial_stride *\n            spatial_offset[None, :])\n        curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        curr_pre_lin = (curr_input - mean) * inv_std\n        curr_output_grad = tl.load(curr_output_grad_pointer, mask=\n            batch_mask[:, None] & spatial_mask[None, :])\n        curr_input_grad = inv_std * (weight * curr_output_grad - (term1 *\n            curr_pre_lin + term2))\n        tl.store(curr_input_grad_pointer, curr_input_grad, mask=batch_mask[\n            :, None] & spatial_mask[None, :])\n        if affine:\n            weight_grad += tl.sum(curr_pre_lin * curr_output_grad)\n            bias_grad += tl.sum(curr_output_grad)\n    if affine:\n        tl.store(feat_pid + weight_grad_pointer, weight_grad)\n        tl.store(feat_pid + bias_grad_pointer, bias_grad)\n",
    "category": "Normalization",
    "subcategory": "batch norm",
    "uuid": "2fee73f7-c8c6-46dd-baad-00f53030fcd3"
  },
  {
    "input": "@triton.jit\ndef dropout_mask(philox_seed, philox_offset, dropout_p, m, n, stride):\n    rng_output = dropout_rng(philox_seed, philox_offset, m, n, stride)\n    rng_keep = rng_output > dropout_p\n    return rng_keep\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "39e16c36-fc21-482a-837d-bcb0e4eca8f9"
  },
  {
    "input": "@triton.jit\ndef tenth_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_7 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_8 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    g_9 = tl.load(sph_grad_ptr + output_row_offset + 9, mask=\n        output_row_offset + 9 < output_numel)\n    g_10 = tl.load(sph_grad_ptr + output_row_offset + 10, mask=\n        output_row_offset + 10 < output_numel)\n    g_11 = tl.load(sph_grad_ptr + output_row_offset + 11, mask=\n        output_row_offset + 11 < output_numel)\n    g_12 = tl.load(sph_grad_ptr + output_row_offset + 12, mask=\n        output_row_offset + 12 < output_numel)\n    g_13 = tl.load(sph_grad_ptr + output_row_offset + 13, mask=\n        output_row_offset + 13 < output_numel)\n    g_14 = tl.load(sph_grad_ptr + output_row_offset + 14, mask=\n        output_row_offset + 14 < output_numel)\n    g_15 = tl.load(sph_grad_ptr + output_row_offset + 15, mask=\n        output_row_offset + 15 < output_numel)\n    g_16 = tl.load(sph_grad_ptr + output_row_offset + 16, mask=\n        output_row_offset + 16 < output_numel)\n    g_17 = tl.load(sph_grad_ptr + output_row_offset + 17, mask=\n        output_row_offset + 17 < output_numel)\n    g_18 = tl.load(sph_grad_ptr + output_row_offset + 18, mask=\n        output_row_offset + 18 < output_numel)\n    g_19 = tl.load(sph_grad_ptr + output_row_offset + 19, mask=\n        output_row_offset + 19 < output_numel)\n    g_20 = tl.load(sph_grad_ptr + output_row_offset + 20, mask=\n        output_row_offset + 20 < output_numel)\n    CONST000 = 2.0\n    CONST002 = 4.0\n    CONST003 = 4.82870805793735\n    CONST004 = 6.0\n    CONST005 = 4.9743298563255\n    CONST006 = 8.0\n    CONST007 = 4.9743298563255\n    CONST008 = 10.5521471197994\n    CONST009 = 3.0\n    CONST010 = 5.0\n    CONST011 = 7.0\n    CONST012 = 13.264879616868\n    CONST014 = 12.1657520803952\n    CONST015 = 16.7271353825295\n    CONST016 = -2030.35546709287\n    CONST017 = 19.3148322317494\n    CONST018 = -6131.53904851919\n    CONST019 = 22.862985426232\n    CONST020 = 23.213539329519\n    CONST021 = 24.6216766128653\n    CONST022 = 17.5869118663323\n    CONST024 = 28.9722483476241\n    CONST025 = 33.9852909359329\n    CONST026 = 33.9852909359329\n    CONST027 = 35.5238206489124\n    CONST028 = 6180.7463141598\n    CONST029 = 38.6296644634988\n    CONST030 = 39.794638850604\n    CONST031 = 38.6296644634988\n    CONST032 = -2007.25624590353\n    CONST033 = -2007.25624590353\n    CONST034 = 45.8257569495584\n    CONST035 = 45.725970852464\n    CONST037 = 56.3871618715269\n    CONST038 = 56.2781179722634\n    CONST039 = -1989.33395633909\n    CONST040 = -1989.33395633909\n    CONST041 = 59.691958275906\n    CONST042 = 66.9085415301178\n    CONST043 = 69.640617988557\n    CONST044 = -8121.42186837148\n    CONST045 = 77.2593289269976\n    CONST046 = 78.6510608948335\n    CONST047 = -1969.73412902922\n    CONST048 = 77.3468749368712\n    CONST049 = -1969.73412902922\n    CONST050 = -9.65741611587469\n    CONST051 = 90.1358837481638\n    CONST053 = 94.9693240781945\n    CONST055 = 96.5741611587469\n    CONST057 = 98.486706451461\n    CONST058 = 100.362812295177\n    CONST059 = 101.517773354644\n    CONST060 = 106.571461946737\n    CONST061 = 106.571461946737\n    CONST062 = 109.491768723557\n    CONST063 = 109.491768723557\n    CONST064 = 112.774323743054\n    CONST065 = 112.774323743054\n    CONST067 = 2165.26701586663\n    CONST070 = 133.817083060236\n    CONST071 = 139.281235977114\n    CONST072 = 139.281235977114\n    CONST073 = 141.5719096107\n    CONST074 = 142.09528259565\n    CONST075 = 147.730059677192\n    CONST076 = 150.544218442765\n    CONST077 = 150.074981259369\n    CONST079 = 2202.22970505534\n    CONST080 = -3939.46825805844\n    CONST081 = -5968.00186901728\n    CONST082 = 176.592751833137\n    CONST083 = 176.178376404427\n    CONST085 = 185.708314636152\n    CONST087 = 196.973412902922\n    CONST089 = 225.548647486108\n    CONST090 = 225.548647486108\n    CONST091 = 4330.53403173327\n    CONST093 = 244.831037842559\n    CONST094 = -1804.38917988886\n    CONST095 = -1804.38917988886\n    CONST097 = 2317.77986780993\n    CONST098 = 278.562471954228\n    CONST100 = 284.190565191299\n    CONST101 = -1761.78376404427\n    CONST103 = -9946.66978169547\n    CONST104 = 9.948659712651\n    CONST108 = -7878.93651611688\n    CONST111 = 338.322971229162\n    CONST112 = 360.877835977772\n    CONST114 = -1671.37483172537\n    CONST116 = 2436.42656051144\n    CONST119 = 393.946825805844\n    CONST120 = -1648.19901710928\n    CONST121 = 401.451249180707\n    CONST122 = 406.071093418574\n    CONST123 = 412.04975427732\n    CONST125 = -1624.2843736743\n    CONST126 = 426.285847786949\n    CONST127 = 426.285847786948\n    CONST128 = 2486.66744542387\n    CONST130 = 451.097294972216\n    CONST131 = 451.097294972216\n    CONST132 = 451.097294972215\n    CONST133 = 6606.68911516602\n    CONST134 = 6606.68911516602\n    CONST135 = -1575.78730322338\n    CONST136 = -1575.78730322338\n    CONST137 = -3608.77835977772\n    CONST139 = -1545.18657853995\n    CONST140 = -1545.18657853995\n    CONST142 = 535.268332240943\n    CONST143 = 4635.55973561985\n    CONST144 = 541.428124558099\n    CONST145 = -3545.5214322526\n    CONST146 = 557.124943908456\n    CONST147 = -3523.56752808854\n    CONST148 = -5571.24943908456\n    CONST151 = 15.7883647328499\n    CONST153 = 2642.67564606641\n    CONST154 = 2642.67564606641\n    CONST155 = 2676.34166120471\n    CONST156 = 629.208487158668\n    CONST158 = 4727.36190967013\n    CONST159 = -1392.81235977114\n    CONST160 = -1390.66792068596\n    CONST162 = 663.111318779698\n    CONST163 = -3427.63452979582\n    CONST164 = -1378.81389032045\n    CONST165 = 676.645942458323\n    CONST167 = -1338.17083060236\n    CONST168 = -1338.17083060236\n    CONST169 = 721.755671955545\n    CONST171 = 2785.62471954228\n    CONST173 = 772.593289269975\n    CONST175 = 787.893651611688\n    CONST176 = 787.893651611688\n    CONST177 = 6.632439808434\n    CONST178 = 812.142186837148\n    CONST180 = -1218.21328025572\n    CONST181 = -1202.92611992591\n    CONST182 = -1202.92611992591\n    CONST183 = -3248.56874734859\n    CONST184 = -3248.56874734859\n    CONST185 = -5285.35129213281\n    CONST186 = -1181.84047741753\n    CONST190 = 2936.30627340712\n    CONST192 = 2954.60119354383\n    CONST193 = -1114.24988781691\n    CONST194 = -16.581099521085\n    CONST195 = -1101.11485252767\n    CONST196 = -1081.63060497797\n    CONST197 = 15.7302121789667\n    CONST199 = 984.86706451461\n    CONST202 = -1027.70719569249\n    CONST203 = -1021.9231747532\n    CONST204 = -3065.7695242596\n    CONST205 = -1015.17773354644\n    CONST206 = 3090.3731570799\n    CONST207 = -994.666978169547\n    CONST208 = -984.86706451461\n    CONST209 = -984.86706451461\n    CONST210 = -979.324151370235\n    CONST211 = 1070.53666448189\n    CONST212 = -979.324151370235\n    CONST213 = 3151.57460644675\n    CONST216 = -927.111947123971\n    CONST217 = -927.11194712397\n    CONST218 = -5.63871618715269\n    CONST219 = -2954.60119354383\n    CONST220 = -902.194589944431\n    CONST221 = -900.449887556215\n    CONST222 = -880.891882022136\n    CONST223 = -880.891882022136\n    CONST224 = -875.934149788456\n    CONST226 = -4944.59705132784\n    CONST228 = 3248.56874734859\n    CONST229 = -835.687415862684\n    CONST230 = 1218.21328025572\n    CONST231 = -824.099508554641\n    CONST232 = -824.863625092051\n    CONST233 = -824.863625092051\n    CONST234 = -812.142186837148\n    CONST235 = 5352.68332240943\n    CONST236 = -787.893651611688\n    CONST237 = -787.893651611688\n    CONST238 = -772.593289269976\n    CONST239 = -742.833258544608\n    CONST240 = -2785.62471954228\n    CONST241 = -734.07656835178\n    CONST242 = 1321.3378230332\n    CONST243 = 1321.3378230332\n    CONST244 = -706.371007332549\n    CONST245 = -696.40617988557\n    CONST246 = 1353.29188491665\n    CONST247 = -675.337415667161\n    CONST248 = -675.337415667161\n    CONST250 = 3427.63452979582\n    CONST251 = -669.085415301178\n    CONST252 = -669.085415301178\n    CONST253 = -669.085415301178\n    CONST255 = -663.111318779698\n    CONST256 = -2707.14062279049\n    CONST258 = 1392.81235977114\n    CONST259 = 1412.7420146651\n    CONST260 = -4727.36190967013\n    CONST261 = -2676.34166120471\n    CONST262 = -618.07463141598\n    CONST263 = -611.735236846792\n    CONST264 = -611.735236846792\n    CONST265 = 1443.51134391109\n    CONST266 = -590.920238708766\n    CONST267 = -10828.562491162\n    CONST268 = -580.101562026534\n    CONST269 = -2626.31217203896\n    CONST272 = 5571.24943908456\n    CONST273 = -12.8765548211663\n    CONST274 = -557.124943908456\n    CONST275 = -557.124943908456\n    CONST277 = -541.428124558099\n    CONST278 = -6685.49932690147\n    CONST279 = 7664.42381064899\n    CONST280 = -525.262434407792\n    CONST281 = 1532.8847621298\n    CONST283 = -497.333489084773\n    CONST284 = -497.333489084773\n    CONST285 = -492.433532257305\n    CONST286 = 1575.78730322338\n    CONST287 = 1575.78730322338\n    CONST288 = -463.555973561985\n    CONST289 = -450.224943778107\n    CONST290 = -450.224943778107\n    CONST291 = -450.224943778108\n    CONST292 = -437.967074894228\n    CONST293 = -2472.29852566392\n    CONST294 = 1624.2843736743\n    CONST295 = -2472.29852566392\n    CONST296 = -406.071093418574\n    CONST297 = -393.946825805844\n    CONST298 = -393.946825805844\n    CONST299 = -2436.42656051144\n    CONST300 = -386.296644634988\n    CONST301 = -386.296644634988\n    CONST302 = -4456.99955126765\n    CONST303 = -337.668707833581\n    CONST304 = -337.668707833581\n    CONST305 = -331.555659389849\n    CONST306 = -331.555659389849\n    CONST307 = -2363.68095483506\n    CONST309 = -309.03731570799\n    CONST310 = -4404.45941011068\n    CONST311 = -309.03731570799\n    CONST312 = -305.867618423396\n    CONST313 = -305.867618423396\n    CONST314 = -305.867618423396\n    CONST315 = -300.731529981477\n    CONST316 = 9946.66978169547\n    CONST318 = -290.050781013267\n    CONST319 = -284.190565191299\n    CONST320 = -278.562471954228\n    CONST321 = -278.562471954228\n    CONST322 = -2317.77986780993\n    CONST323 = -10505.2486881558\n    CONST324 = -251.683394863467\n    CONST325 = -251.683394863467\n    CONST326 = -246.216766128653\n    CONST327 = -244.831037842559\n    CONST328 = -2285.08968653055\n    CONST329 = -2285.08968653055\n    CONST330 = 3862.96644634988\n    CONST331 = -223.028471767059\n    CONST332 = -220.222970505534\n    CONST333 = -206.215906273013\n    CONST334 = -203.035546709287\n    CONST335 = -196.973412902922\n    CONST336 = -196.973412902922\n    CONST337 = -182.903883409856\n    CONST338 = -2228.49977563382\n    CONST340 = 16.4144510752435\n    CONST341 = 3939.46825805844\n    CONST342 = 3939.46825805844\n    CONST343 = -154.518657853995\n    CONST344 = -154.518657853995\n    CONST345 = -150.074981259369\n    CONST346 = -147.730059677191\n    CONST347 = -146.815313670356\n    CONST348 = -142.09528259565\n    CONST349 = -131.315608601948\n    CONST350 = -131.315608601948\n    CONST351 = -130.52285145597\n    CONST352 = -125.841697431734\n    CONST353 = -125.841697431734\n    CONST354 = -112.556235944527\n    CONST355 = -103.107953136506\n    CONST356 = -101.517773354644\n    CONST357 = 1949.9373036796\n    CONST358 = -98.486706451461\n    CONST359 = -98.486706451461\n    CONST360 = -2141.07332896377\n    CONST361 = -2141.07332896377\n    CONST362 = -92.854157318076\n    CONST363 = -88.2963759165686\n    CONST366 = -77.3468749368713\n    CONST367 = 8121.42186837148\n    CONST369 = -67.6645942458323\n    CONST372 = -59.691958275906\n    CONST373 = -49.2433532257305\n    CONST374 = -49.2433532257305\n    CONST375 = -45.1097294972216\n    CONST376 = -45.1097294972216\n    CONST377 = -42.2085884791976\n    CONST378 = -27.2034486491732\n    CONST379 = -24.6216766128653\n    CONST380 = -22.862985426232\n    CONST381 = -19.7354559160624\n    CONST383 = -17.5869118663323\n    CONST384 = -16.4144510752435\n    CONST385 = -16.0956935264578\n    CONST386 = -14.5025390506634\n    CONST388 = -16.581099521085\n    CONST389 = -15.7883647328499\n    CONST390 = -14.0695294930659\n    CONST391 = -11.2774323743054\n    CONST392 = -11.2774323743054\n    CONST393 = -13.264879616868\n    CONST394 = -6.632439808434\n    CONST395 = -5.63871618715269\n    CONST396 = -4.82870805793735\n    CONST397 = -3.21913870529156\n    CONST398 = -11.2774323743054\n    VAR05 = x * x * x * x * x\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR01 = VAR05 * VAR06\n    VAR02 = VAR06 * VAR06\n    VAR03 = VAR06 * VAR07\n    VAR04 = VAR07 * VAR07\n    VAR14 = y * y * y * y * y\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR10 = VAR14 * VAR15\n    VAR11 = VAR15 * VAR15\n    VAR12 = VAR15 * VAR16\n    VAR13 = VAR16 * VAR16\n    VAR23 = z * z * z * z * z\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR19 = VAR23 * VAR24\n    VAR20 = VAR24 * VAR24\n    VAR21 = VAR24 * VAR25\n    VAR22 = VAR25 * VAR25\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += g_0 * (CONST093 * VAR02 * z + CONST210 * VAR08 * VAR21 + \n        CONST250 * VAR06 * VAR23 + CONST328 * VAR04 * VAR25 - CONST378 * VAR19\n        ) + g_1 * y * (CONST062 * VAR20 + CONST063 * VAR02 + CONST204 *\n        VAR04 * VAR26 + CONST204 * VAR08 * VAR22 + CONST279 * VAR06 * VAR24\n        ) + g_10 * (CONST000 * x * (CONST089 * VAR17 * VAR22 + CONST169 *\n        VAR13 * VAR26 + CONST220 * VAR15 * VAR24 + CONST355 * VAR11 + \n        CONST395 * VAR20) + CONST002 * VAR07 * (CONST111 * VAR17 * VAR24 + \n        CONST112 * VAR13 + CONST220 * VAR15 * VAR26 + CONST392 * VAR22) + \n        CONST004 * VAR05 * (CONST090 * VAR17 * VAR26 + CONST315 * VAR15 + \n        CONST392 * VAR24) + CONST006 * VAR03 * (CONST037 * VAR17 + CONST218 *\n        VAR26) + CONST391 * VAR01) + g_11 * (CONST070 * VAR21 * x * y + \n        VAR23 * (CONST121 * VAR07 * y + CONST168 * VAR16 * x) + VAR25 * (\n        CONST121 * VAR05 * y + CONST261 * VAR07 * VAR16 - CONST361 * VAR14 *\n        x) + z * (CONST070 * VAR03 * y + CONST167 * VAR05 * VAR16 + \n        CONST263 * VAR12 * x - CONST361 * VAR07 * VAR14)) + g_12 * (\n        CONST000 * x * (CONST003 * VAR20 - CONST301 * VAR15 * VAR24 + \n        CONST343 * VAR17 * VAR22 + CONST363 * VAR11) + CONST002 * VAR07 * (\n        CONST123 * VAR13 + CONST300 * VAR15 * VAR26 - CONST397 * VAR22) + \n        CONST004 * VAR05 * (CONST301 * VAR15 - CONST344 * VAR17 * VAR26 + \n        CONST397 * VAR24) + CONST006 * VAR03 * (CONST045 * VAR17 + CONST396 *\n        VAR26) + CONST385 * VAR01) + g_13 * (CONST221 * VAR12 * x * z + \n        VAR14 * (-CONST260 * VAR07 * z + CONST286 * VAR25 * x) + VAR16 * (\n        CONST080 * VAR07 * VAR25 + CONST145 * VAR05 * z + CONST297 * VAR23 *\n        x) + y * (-CONST237 * VAR05 * VAR25 - CONST297 * VAR07 * VAR23 - \n        CONST298 * VAR03 * z)) + g_14 * (CONST000 * x * (CONST005 * VAR20 -\n        CONST159 * VAR15 * VAR24 + CONST193 * VAR13 * VAR26 + CONST320 *\n        VAR17 * VAR22) + CONST002 * VAR07 * (CONST020 * VAR22 + CONST085 *\n        VAR13 + CONST245 * VAR17 * VAR24 + CONST258 * VAR15 * VAR26) + \n        CONST004 * VAR05 * (CONST020 * VAR24 + CONST320 * VAR15 + CONST320 *\n        VAR17 * VAR26) + CONST006 * VAR03 * (CONST007 * VAR26 + CONST043 *\n        VAR17) + CONST388 * VAR01) + g_15 * (VAR14 * (-CONST147 * VAR07 * z +\n        CONST147 * VAR25 * x) + VAR16 * (CONST153 * VAR23 * x + CONST190 *\n        VAR07 * VAR25 + CONST310 * VAR05 * z) + y * (CONST156 * VAR03 * z +\n        CONST222 * VAR07 * VAR23 + CONST324 * VAR21 * x)) + g_16 * (\n        CONST000 * x * (CONST047 * VAR15 * VAR24 + CONST175 * VAR17 * VAR22 +\n        CONST380 * VAR20) + CONST002 * VAR07 * (-CONST047 * VAR15 * VAR26 +\n        CONST379 * VAR22) + CONST004 * VAR05 * (CONST021 * VAR24 + CONST236 *\n        VAR17 * VAR26 + CONST349 * VAR15) + CONST006 * VAR03 * (CONST019 *\n        VAR26 + CONST038 * VAR17) + CONST383 * VAR01) + g_17 * (VAR16 * (\n        CONST183 * VAR23 * x + CONST184 * VAR05 * z - CONST267 * VAR07 *\n        VAR25) + y * (CONST178 * VAR03 * z + CONST234 * VAR07 * VAR23 - \n        CONST268 * VAR21 * x + CONST299 * VAR05 * VAR25)) + g_18 * (\n        CONST060 * VAR20 * x + CONST126 * VAR03 * VAR26 + CONST283 * VAR05 *\n        VAR24 + CONST305 * VAR07 * VAR22 + CONST381 * VAR01 + VAR17 * (\n        CONST039 * VAR22 * x + CONST081 * VAR05 * VAR26 + CONST316 * VAR07 *\n        VAR24 - CONST319 * VAR03)) + g_19 * y * (CONST018 * VAR05 * VAR25 -\n        CONST018 * VAR07 * VAR23 - CONST224 * VAR03 * z + CONST224 * VAR21 * x\n        ) + g_2 * (CONST074 * VAR02 * z + CONST100 * VAR08 * VAR21 + \n        CONST255 * VAR04 * VAR25 + CONST389 * VAR19 + VAR17 * (CONST040 *\n        VAR04 * z + CONST081 * VAR08 * VAR23 - CONST103 * VAR06 * VAR25 - \n        CONST319 * VAR21)) + g_20 * (CONST163 * VAR05 * VAR24 - CONST212 *\n        VAR03 * VAR26 + CONST327 * VAR20 * x - CONST329 * VAR07 * VAR22 + \n        CONST378 * VAR01) + g_3 * (VAR16 * (CONST044 * VAR08 * VAR24 + \n        CONST144 * VAR22 + CONST277 * VAR04 + CONST367 * VAR06 * VAR26) + y *\n        (CONST016 * VAR04 * VAR26 - CONST205 * VAR06 * VAR24 + CONST230 *\n        VAR08 * VAR22 - CONST351 * VAR02 + CONST356 * VAR20)) + g_4 * (\n        CONST008 * VAR19 + CONST009 * VAR08 * (CONST175 * VAR17 * VAR23 + \n        CONST269 * VAR15 * VAR25 + CONST390 * VAR21) + CONST010 * VAR06 * (\n        CONST175 * VAR15 * z + CONST176 * VAR17 * VAR25 + CONST373 * VAR23) +\n        CONST011 * VAR04 * (CONST303 * VAR17 * z + CONST390 * VAR25) + \n        CONST053 * VAR02 * z + CONST175 * VAR15 * VAR23 + CONST304 * VAR17 *\n        VAR21) + g_5 * (VAR14 * (CONST185 * VAR08 * VAR26 - CONST222 *\n        VAR06 - CONST223 * VAR24) + VAR16 * (CONST079 * VAR08 * VAR24 + \n        CONST133 * VAR06 * VAR26 + CONST202 * VAR04 + CONST241 * VAR22) + y *\n        (CONST046 * VAR20 + CONST073 * VAR02 + CONST195 * VAR06 * VAR24 + \n        CONST222 * VAR04 * VAR26)) + g_6 * (CONST009 * VAR08 * (CONST098 *\n        VAR17 * VAR23 + CONST239 * VAR13 * z + CONST393 * VAR21) + CONST010 *\n        VAR06 * (-CONST193 * VAR15 * z + CONST320 * VAR17 * VAR25) + \n        CONST011 * VAR04 * (CONST012 * VAR25 + CONST321 * VAR17 * z) + \n        CONST041 * VAR02 * z + CONST098 * VAR17 * VAR21 + CONST193 * VAR15 *\n        VAR23 - CONST239 * VAR13 * VAR25 + CONST394 * VAR19) + g_7 * (VAR12 *\n        (CONST289 * VAR08 - CONST290 * VAR26) + VAR14 * (-CONST049 * VAR06 +\n        CONST186 * VAR24 + CONST307 * VAR08 * VAR26) + VAR16 * (CONST164 *\n        VAR04 + CONST192 * VAR08 * VAR24 + CONST199 * VAR06 * VAR26 - \n        CONST266 * VAR22) + y * (CONST075 * VAR02 + CONST285 * VAR06 *\n        VAR24 + CONST297 * VAR08 * VAR22 + CONST374 * VAR20)) + g_8 * (\n        CONST009 * VAR08 * (-CONST140 * VAR15 * VAR25 + CONST231 * VAR13 *\n        z - CONST273 * VAR21 + CONST288 * VAR17 * VAR23) + CONST010 * VAR06 *\n        (CONST017 * VAR23 + CONST173 * VAR15 * z + CONST288 * VAR17 * VAR25\n        ) + CONST011 * VAR04 * (-CONST273 * VAR25 + CONST344 * VAR17 * z) +\n        CONST024 * VAR02 * z + CONST082 * VAR11 * z + CONST173 * VAR15 *\n        VAR23 + CONST231 * VAR13 * VAR25 + CONST344 * VAR17 * VAR21 - \n        CONST397 * VAR19) + g_9 * (CONST009 * VAR08 * (CONST042 * VAR22 * y +\n        CONST211 * VAR14 * VAR26 + CONST251 * VAR16 * VAR24 + CONST312 *\n        VAR12) + CONST010 * VAR06 * (CONST058 * VAR24 * y + CONST142 *\n        VAR14 + CONST252 * VAR16 * VAR26) + CONST011 * VAR04 * (CONST042 *\n        VAR26 * y + CONST331 * VAR16) + CONST015 * VAR20 * y + CONST025 *\n        VAR10 + CONST076 * VAR02 * y + CONST142 * VAR14 * VAR24 + CONST312 *\n        VAR12 * VAR26 + CONST331 * VAR16 * VAR22)\n    g_y += CONST000 * g_18 * y * (CONST027 * VAR02 + CONST027 * VAR20 + \n        CONST128 * VAR06 * VAR24 + CONST207 * VAR04 * VAR26 + CONST207 *\n        VAR08 * VAR22) + CONST000 * g_2 * y * (-CONST039 * VAR05 * VAR25 + \n        CONST039 * VAR07 * VAR23 + CONST319 * VAR03 * z - CONST319 * VAR21 * x\n        ) + g_1 * (CONST014 * VAR01 + CONST062 * VAR20 * x + CONST203 *\n        VAR07 * VAR22 + CONST281 * VAR05 * VAR24 + CONST292 * VAR03 * VAR26\n        ) + g_10 * (CONST034 * VAR10 + CONST064 * VAR20 * y + CONST065 *\n        VAR02 * y + CONST067 * VAR14 * VAR24 + CONST182 * VAR16 * VAR22 + \n        CONST233 * VAR12 * VAR26 + VAR04 * (CONST131 * VAR26 * y + CONST181 *\n        VAR16) + VAR06 * (CONST067 * VAR14 + CONST137 * VAR16 * VAR26 + \n        CONST165 * VAR24 * y) + VAR08 * (CONST091 * VAR14 * VAR26 + \n        CONST130 * VAR22 * y + CONST137 * VAR16 * VAR24 + CONST232 * VAR12)\n        ) + g_11 * (CONST015 * VAR19 + VAR21 * (CONST042 * VAR08 + CONST253 *\n        VAR17) + VAR23 * (CONST033 * VAR08 * VAR17 + CONST058 * VAR06 + \n        CONST155 * VAR15) + VAR25 * (CONST032 * VAR06 * VAR17 + CONST042 *\n        VAR04 + CONST235 * VAR08 * VAR15 + CONST361 * VAR13) + z * (\n        CONST015 * VAR02 + CONST155 * VAR06 * VAR15 + CONST253 * VAR04 *\n        VAR17 - CONST312 * VAR11 + CONST360 * VAR08 * VAR13)) + g_12 * (-\n        CONST140 * VAR16 * VAR22 - CONST244 * VAR12 * VAR26 + CONST293 *\n        VAR14 * VAR24 + CONST343 * VAR20 * y - CONST344 * VAR02 * y + VAR04 *\n        (CONST140 * VAR16 - CONST311 * VAR26 * y) + VAR06 * (CONST139 *\n        VAR16 * VAR26 - CONST295 * VAR14) + VAR08 * (-CONST140 * VAR16 *\n        VAR24 + CONST244 * VAR12 + CONST309 * VAR22 * y)) + g_13 * (\n        CONST009 * VAR17 * (CONST208 * VAR06 * VAR25 + CONST266 * VAR04 * z +\n        CONST335 * VAR08 * VAR23 - CONST336 * VAR21) + CONST010 * VAR15 * (\n        CONST176 * VAR08 * VAR25 - CONST186 * VAR06 * z + CONST298 * VAR23) +\n        CONST011 * VAR13 * (CONST077 * VAR25 + CONST290 * VAR08 * z) - \n        CONST350 * VAR04 * VAR25 - CONST358 * VAR06 * VAR23 - CONST374 *\n        VAR02 * z + CONST384 * VAR19) + g_14 * (CONST071 * VAR02 * y + \n        CONST072 * VAR20 * y - CONST193 * VAR14 * VAR24 + CONST193 * VAR16 *\n        VAR22 + VAR04 * (CONST193 * VAR16 + CONST274 * VAR26 * y) + VAR06 *\n        (CONST159 * VAR24 * y - CONST193 * VAR14 + CONST272 * VAR16 * VAR26\n        ) + VAR08 * (-CONST148 * VAR16 * VAR24 + CONST274 * VAR22 * y + \n        CONST278 * VAR14 * VAR26)) + g_15 * (CONST009 * VAR17 * (CONST241 *\n        VAR04 * z - CONST241 * VAR06 * VAR25 + CONST242 * VAR08 * VAR23 + \n        CONST347 * VAR21) + CONST010 * VAR15 * (CONST083 * VAR23 + CONST101 *\n        VAR08 * VAR25 - CONST223 * VAR06 * z) + CONST046 * VAR02 * z + \n        CONST197 * VAR19 + CONST332 * VAR06 * VAR23 + CONST352 * VAR08 * VAR21\n        ) + g_16 * (-CONST108 * VAR06 * VAR16 * VAR26 - CONST280 * VAR16 *\n        VAR22 - CONST354 * VAR02 * y + CONST354 * VAR20 * y + VAR04 * (\n        CONST135 * VAR26 * y + CONST280 * VAR16) + VAR08 * (CONST108 *\n        VAR16 * VAR24 + CONST287 * VAR22 * y)) + g_17 * (CONST009 * VAR17 *\n        (CONST048 * VAR21 + CONST125 * VAR08 * VAR23 - CONST256 * VAR06 *\n        VAR25 + CONST277 * VAR04 * z) + CONST059 * VAR02 * z + CONST296 *\n        VAR04 * VAR25 - CONST318 * VAR08 * VAR21 + CONST334 * VAR06 * VAR23 +\n        CONST386 * VAR19) + g_19 * (CONST014 * VAR19 + CONST062 * VAR02 * z +\n        CONST203 * VAR04 * VAR25 + CONST281 * VAR06 * VAR23 + CONST292 *\n        VAR08 * VAR21) + g_3 * (CONST009 * VAR17 * (CONST144 * VAR22 * x + \n        CONST256 * VAR07 * VAR24 + CONST294 * VAR05 * VAR26 + CONST366 *\n        VAR03) + CONST122 * VAR07 * VAR22 + CONST318 * VAR03 * VAR26 - \n        CONST334 * VAR05 * VAR24 + CONST356 * VAR20 * x - CONST386 * VAR01\n        ) + g_4 * (CONST248 * VAR03 * y * z + VAR05 * (CONST213 * VAR16 * z +\n        CONST286 * VAR25 * y) + VAR07 * (CONST287 * VAR23 * y + CONST323 *\n        VAR16 * VAR25) + x * (CONST213 * VAR16 * VAR23 + CONST247 * VAR21 * y)\n        ) + g_5 * (CONST009 * VAR17 * (-CONST241 * VAR07 * VAR24 + CONST241 *\n        VAR22 * x + CONST243 * VAR05 * VAR26 + CONST347 * VAR03) + CONST010 *\n        VAR15 * (CONST083 * VAR05 + CONST101 * VAR07 * VAR26 - CONST223 *\n        VAR24 * x) + CONST046 * VAR20 * x + CONST197 * VAR01 + CONST332 *\n        VAR05 * VAR24 + CONST353 * VAR03 * VAR26) + g_6 * (CONST275 * VAR03 *\n        y * z + VAR05 * (CONST274 * VAR25 * y - CONST302 * VAR16 * z) + \n        VAR07 * (CONST146 * VAR23 * y + CONST302 * VAR14 * z) + x * (\n        CONST146 * VAR21 * y - CONST302 * VAR14 * VAR25 + CONST302 * VAR16 *\n        VAR23)) + g_7 * (CONST009 * VAR17 * (CONST087 * VAR05 * VAR26 - \n        CONST209 * VAR07 * VAR24 - CONST266 * VAR22 * x + CONST336 * VAR03) +\n        CONST010 * VAR15 * (CONST186 * VAR24 * x + CONST237 * VAR07 * VAR26 -\n        CONST298 * VAR05) + CONST011 * VAR13 * (-CONST290 * VAR26 * x + \n        CONST345 * VAR07) + CONST340 * VAR01 + CONST350 * VAR07 * VAR22 + \n        CONST358 * VAR05 * VAR24 + CONST374 * VAR20 * x) + g_8 * (CONST311 *\n        VAR03 * y * z + VAR05 * (CONST206 * VAR16 * z + CONST216 * VAR25 *\n        y) + VAR07 * (CONST028 * VAR16 * VAR25 + CONST216 * VAR23 * y + \n        CONST226 * VAR14 * z) + x * (CONST206 * VAR16 * VAR23 + CONST226 *\n        VAR14 * VAR25 + CONST259 * VAR12 * z + CONST311 * VAR21 * y)) + g_9 * (\n        CONST015 * VAR01 + VAR03 * (CONST042 * VAR26 + CONST253 * VAR17) + \n        VAR05 * (CONST033 * VAR17 * VAR26 + CONST058 * VAR24 + CONST155 *\n        VAR15) + VAR07 * (CONST032 * VAR17 * VAR24 + CONST042 * VAR22 + \n        CONST235 * VAR15 * VAR26 + CONST361 * VAR13) + x * (CONST015 *\n        VAR20 + CONST155 * VAR15 * VAR24 + CONST253 * VAR17 * VAR22 - \n        CONST314 * VAR11 + CONST361 * VAR13 * VAR26))\n    g_z += g_0 * (CONST093 * VAR20 * x + CONST210 * VAR03 * VAR26 + \n        CONST250 * VAR05 * VAR24 + CONST328 * VAR07 * VAR22 - CONST378 * VAR01\n        ) + g_1 * y * (-CONST018 * VAR05 * VAR25 + CONST018 * VAR07 * VAR23 +\n        CONST224 * VAR03 * z - CONST224 * VAR21 * x) + g_10 * (CONST095 *\n        VAR15 * VAR23 + CONST132 * VAR17 * VAR21 + CONST265 * VAR13 * VAR25 +\n        CONST333 * VAR11 * z + CONST391 * VAR19 + CONST398 * VAR02 * z + \n        VAR04 * (CONST131 * VAR17 * z + CONST376 * VAR25) + VAR06 * (\n        CONST094 * VAR15 * z + CONST246 * VAR17 * VAR25 + CONST369 * VAR23) +\n        VAR08 * (CONST137 * VAR15 * VAR25 + CONST246 * VAR17 * VAR23 + \n        CONST265 * VAR13 * z + CONST375 * VAR21)) + g_11 * (CONST009 *\n        VAR26 * (CONST042 * VAR04 * y + CONST211 * VAR08 * VAR14 + CONST251 *\n        VAR06 * VAR16 + CONST313 * VAR12) + CONST010 * VAR24 * (CONST058 *\n        VAR06 * y + CONST142 * VAR14 + CONST252 * VAR08 * VAR16) + CONST011 *\n        VAR22 * (CONST042 * VAR08 * y + CONST331 * VAR16) + CONST015 *\n        VAR02 * y + CONST026 * VAR10 + CONST076 * VAR20 * y + CONST142 *\n        VAR06 * VAR14 + CONST314 * VAR08 * VAR12 + CONST331 * VAR04 * VAR16\n        ) + g_12 * (CONST050 * VAR02 * z + CONST082 * VAR11 * z + CONST097 *\n        VAR15 * VAR23 + CONST120 * VAR13 * VAR25 + CONST262 * VAR17 * VAR21 -\n        CONST385 * VAR19 + VAR04 * (CONST273 * VAR25 - CONST311 * VAR17 * z\n        ) + VAR06 * (CONST017 * VAR23 + CONST238 * VAR15 * z) + VAR08 * (\n        CONST029 * VAR21 - CONST140 * VAR15 * VAR25 + CONST217 * VAR17 * VAR23)\n        ) + g_13 * (VAR12 * (CONST290 * VAR08 - CONST290 * VAR26) + VAR14 *\n        (CONST049 * VAR24 - CONST186 * VAR06 - CONST307 * VAR08 * VAR26) + \n        VAR16 * (-CONST164 * VAR22 + CONST209 * VAR08 * VAR24 + CONST219 *\n        VAR06 * VAR26 + CONST266 * VAR04) + y * (-CONST285 * VAR06 * VAR24 -\n        CONST297 * VAR04 * VAR26 + CONST346 * VAR20 - CONST374 * VAR02)\n        ) + g_14 * (CONST104 * VAR02 * z + CONST114 * VAR15 * VAR23 + \n        CONST146 * VAR17 * VAR21 + CONST194 * VAR19 - CONST239 * VAR13 *\n        VAR25 + VAR04 * (CONST274 * VAR17 * z - CONST362 * VAR25) + VAR06 *\n        (CONST072 * VAR23 + CONST171 * VAR15 * z + CONST240 * VAR17 * VAR25\n        ) + VAR08 * (CONST030 * VAR21 + CONST114 * VAR17 * VAR23 - CONST148 *\n        VAR15 * VAR25 + CONST338 * VAR13 * z)) + g_15 * (VAR14 * (CONST185 *\n        VAR08 * VAR26 - CONST222 * VAR24 - CONST223 * VAR06) + VAR16 * (\n        CONST079 * VAR06 * VAR26 + CONST134 * VAR08 * VAR24 + CONST202 *\n        VAR22 + CONST241 * VAR04) + y * (CONST046 * VAR02 + CONST073 *\n        VAR20 + CONST195 * VAR06 * VAR24 + CONST223 * VAR08 * VAR22)\n        ) + g_16 * (CONST022 * VAR19 + CONST035 * VAR02 * z + CONST175 *\n        VAR15 * VAR23 + CONST291 * VAR17 * VAR21 + VAR04 * (CONST057 *\n        VAR25 + CONST135 * VAR17 * z) + VAR06 * (CONST341 * VAR15 * z + \n        CONST346 * VAR23) + VAR08 * (CONST108 * VAR15 * VAR25 + CONST158 *\n        VAR17 * VAR23 + CONST337 * VAR21)) + g_17 * (VAR16 * (-CONST044 *\n        VAR06 * VAR26 + CONST044 * VAR08 * VAR24 + CONST144 * VAR22 + \n        CONST277 * VAR04) + y * (-CONST016 * VAR08 * VAR22 + CONST059 *\n        VAR02 + CONST180 * VAR04 * VAR26 + CONST205 * VAR06 * VAR24 + \n        CONST351 * VAR20)) + g_18 * (CONST061 * VAR02 * z + CONST127 *\n        VAR08 * VAR21 + CONST284 * VAR06 * VAR23 + CONST306 * VAR04 * VAR25 +\n        CONST381 * VAR19 + VAR17 * (CONST039 * VAR04 * z + CONST081 * VAR08 *\n        VAR23 + CONST316 * VAR06 * VAR25 - CONST319 * VAR21)) + g_19 * y * (\n        CONST062 * VAR02 + CONST063 * VAR20 + CONST204 * VAR04 * VAR26 + \n        CONST204 * VAR08 * VAR22 + CONST279 * VAR06 * VAR24) + g_2 * (\n        CONST151 * VAR01 + CONST162 * VAR07 * VAR22 + CONST319 * VAR03 *\n        VAR26 + CONST348 * VAR20 * x + VAR17 * (-CONST040 * VAR22 * x - \n        CONST081 * VAR05 * VAR26 + CONST103 * VAR07 * VAR24 + CONST319 * VAR03)\n        ) + g_20 * (-CONST163 * VAR06 * VAR23 + CONST212 * VAR08 * VAR21 - \n        CONST327 * VAR02 * z + CONST329 * VAR04 * VAR25 - CONST378 * VAR19\n        ) + g_3 * (VAR16 * (-CONST183 * VAR23 * x + CONST228 * VAR05 * z + \n        CONST267 * VAR07 * VAR25) + y * (CONST116 * VAR07 * VAR23 - \n        CONST234 * VAR05 * VAR25 + CONST234 * VAR21 * x + CONST268 * VAR03 * z)\n        ) + g_4 * (CONST008 * VAR01 + VAR03 * (CONST303 * VAR17 + CONST377 *\n        VAR26) + VAR05 * (CONST175 * VAR15 - CONST307 * VAR17 * VAR26 + \n        CONST326 * VAR24) + VAR07 * (CONST108 * VAR15 * VAR26 + CONST341 *\n        VAR17 * VAR24 + CONST359 * VAR22) + x * (CONST053 * VAR20 + \n        CONST307 * VAR17 * VAR22 + CONST341 * VAR15 * VAR24)) + g_5 * (\n        VAR14 * (CONST147 * VAR07 * z - CONST147 * VAR25 * x) + VAR16 * (\n        CONST154 * VAR05 * z + CONST190 * VAR07 * VAR25 + CONST310 * VAR23 *\n        x) + y * (CONST156 * VAR21 * x + CONST222 * VAR05 * VAR25 + \n        CONST325 * VAR03 * z)) + g_6 * (CONST177 * VAR01 + VAR03 * (\n        CONST030 * VAR26 + CONST321 * VAR17) + VAR05 * (-CONST193 * VAR15 +\n        CONST229 * VAR17 * VAR26) + VAR07 * (CONST239 * VAR13 + CONST258 *\n        VAR17 * VAR24 + CONST362 * VAR22) + x * (CONST148 * VAR15 * VAR24 -\n        CONST338 * VAR13 * VAR26 + CONST357 * VAR17 * VAR22 + CONST372 * VAR20)\n        ) + g_7 * (-CONST221 * VAR12 * x * z + VAR14 * (CONST136 * VAR07 *\n        z + CONST260 * VAR25 * x) + VAR16 * (CONST119 * VAR05 * z - \n        CONST145 * VAR23 * x + CONST342 * VAR07 * VAR25) + y * (CONST237 *\n        VAR07 * VAR23 + CONST297 * VAR05 * VAR25 + CONST298 * VAR21 * x)\n        ) + g_8 * (-CONST397 * VAR01 + VAR03 * (CONST031 * VAR26 + CONST344 *\n        VAR17) + VAR05 * (CONST055 * VAR24 + CONST160 * VAR17 * VAR26 + \n        CONST173 * VAR15) + VAR07 * (CONST051 * VAR22 + CONST143 * VAR15 *\n        VAR26 + CONST231 * VAR13 + CONST322 * VAR17 * VAR24) + x * (\n        CONST024 * VAR20 + CONST082 * VAR11 + CONST196 * VAR17 * VAR22 + \n        CONST295 * VAR13 * VAR26 + CONST330 * VAR15 * VAR24)) + g_9 * (\n        CONST070 * VAR03 * y * z + VAR05 * (CONST121 * VAR25 * y + CONST168 *\n        VAR16 * z) + VAR07 * (CONST121 * VAR23 * y + CONST261 * VAR16 *\n        VAR25 - CONST361 * VAR14 * z) + x * (CONST070 * VAR21 * y + \n        CONST167 * VAR16 * VAR23 + CONST264 * VAR12 * z - CONST361 * VAR14 *\n        VAR25))\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "3774e03d-866f-47bf-9a92-1cacd4a0a45f"
  },
  {
    "input": "@triton.jit\ndef matmul_kernel_with_block_pointers(a_ptr, b_ptr, c_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', BLOCK_K:\n    'tl.constexpr', GROUP_M: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    GROUP_M = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + pid % GROUP_M\n    pid_n = pid % num_pid_in_group // GROUP_M\n    a_block_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(\n        stride_am, stride_ak), offsets=(pid_m * BLOCK_M, 0), block_shape=(\n        BLOCK_M, BLOCK_K), order=(1, 0))\n    b_block_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(\n        stride_bk, stride_bn), offsets=(0, pid_n * BLOCK_N), block_shape=(\n        BLOCK_K, BLOCK_N), order=(1, 0))\n    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)\n    for k in range(0, K, BLOCK_K):\n        a = tl.load(a_block_ptr, boundary_check=(0, 1))\n        b = tl.load(b_block_ptr, boundary_check=(0, 1))\n        accumulator += tl.dot(a, b)\n        a_block_ptr = tl.advance(a_block_ptr, (0, BLOCK_K))\n        b_block_ptr = tl.advance(b_block_ptr, (BLOCK_K, 0))\n    c = accumulator\n    c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(\n        stride_cm, stride_cn), offsets=(pid_m * BLOCK_M, pid_n * BLOCK_N),\n        block_shape=(BLOCK_M, BLOCK_N), order=(1, 0))\n    tl.store(c_block_ptr, c, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "26ab2ad8-de75-4d70-930d-1ab10edda14a"
  },
  {
    "input": "@triton.jit\ndef _fwd_recurrence(S, p2, O, NUM_BLOCK, D_MODEL_K: 'tl.constexpr',\n    D_MODEL_V: 'tl.constexpr', BLOCK_MODEL: 'tl.constexpr'):\n    offset_bh = tl.program_id(0)\n    offset_d = tl.program_id(1)\n    offset_s = tl.program_id(2)\n    S = (S + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None, :]\n        )\n    O = (O + offset_bh * NUM_BLOCK * D_MODEL_K * D_MODEL_V + offset_d *\n        D_MODEL_V * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[:, None] *\n        D_MODEL_V + offset_s * BLOCK_MODEL + tl.arange(0, BLOCK_MODEL)[None,\n        :] + D_MODEL_K * D_MODEL_V)\n    p2 = p2 + offset_bh * NUM_BLOCK * D_MODEL_V + tl.arange(0, BLOCK_MODEL\n        ) + offset_s * BLOCK_MODEL + D_MODEL_V\n    acc = tl.zeros([BLOCK_MODEL, BLOCK_MODEL], dtype=tl.float32)\n    acc += tl.load(S)\n    S += D_MODEL_K * D_MODEL_V\n    tl.store(O, acc)\n    O += D_MODEL_K * D_MODEL_V\n    for i in range(NUM_BLOCK - 2):\n        p_v = tl.load(p2)\n        S_i = tl.load(S)\n        acc = acc * p_v[None, :] + S_i\n        tl.store(O, acc)\n        p2 += D_MODEL_V\n        S += D_MODEL_K * D_MODEL_V\n        O += D_MODEL_K * D_MODEL_V\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "5a086295-c3a8-4144-a030-1dcb51f9df78"
  },
  {
    "input": "@triton.jit\ndef bwd_preprocess(Out, DO, Delta, stride_oz, stride_oh, stride_om,\n    stride_on, stride_doz, stride_doh, stride_dom, stride_don, seqlen_q,\n    head_dim, BLOCK_M: 'tl.constexpr', D_HEAD: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    num_h = tl.num_programs(1)\n    o_offset = off_h * stride_oh + off_z * stride_oz\n    O_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(seqlen_q,\n        head_dim), strides=(stride_om, stride_on), offsets=(off_m, 0),\n        block_shape=(BLOCK_M, D_HEAD), order=(1, 0))\n    do_offset = off_h * stride_doh + off_z * stride_doz\n    DO_block_ptr = tl.make_block_ptr(base=DO + do_offset, shape=(seqlen_q,\n        head_dim), strides=(stride_dom, stride_don), offsets=(off_m, 0),\n        block_shape=(BLOCK_M, D_HEAD), order=(1, 0))\n    o = tl.load(O_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    do = tl.load(DO_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    delta = tl.sum(o * do, axis=1)\n    off_zh = off_z * num_h + off_h * 1\n    delta_ptrs = Delta + off_zh * seqlen_q + off_m + tl.arange(0, BLOCK_M)\n    overflow = off_m + BLOCK_M - seqlen_q\n    if overflow > 0:\n        boundary = tl.full((BLOCK_M,), BLOCK_M - overflow, dtype=tl.int32)\n        mask = boundary > tl.arange(0, BLOCK_M)\n        tl.store(delta_ptrs, delta, mask=mask)\n    else:\n        tl.store(delta_ptrs, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a87798d8-1a1d-44c2-94c6-8025528bd831"
  },
  {
    "input": "@triton.jit\ndef store_1d(vals, ptr, sz: 'const', n, max, stride=1):\n    \"\"\"Store 1d block into nth chunk of vector (defined by ptr), where each chunk has size sz\"\"\"\n    offs = offset_1d(sz, n)\n    mask = mask_1d(offs, max)\n    tl.store(ptr + offs, vals, mask)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "cf9a5c24-8dbd-41ed-89aa-76a651d9c517"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BT'])\n@triton.jit\ndef chunk_rwkv6_fwd_A_kernel_intra_sub_intra(q, k, gi, ge, u, A, s_k_h,\n    s_k_t, s_k_d, scale, H: 'tl.constexpr', T: 'tl.constexpr', K:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BK:\n    'tl.constexpr', NC: 'tl.constexpr'):\n    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    if i_t * BT + i_i * BC >= T:\n        return\n    i_j = i_i\n    i_h = i_bh % H\n    o_i = tl.arange(0, BC)\n    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_j * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    i_k = 0\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(ge + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    p_u = tl.make_block_ptr(u + i_h * s_k_t, (s_k_t,), (1,), i_k * BK, (BK,\n        ), (0,))\n    b_u = tl.load(p_u, boundary_check=(0,))\n    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n        b_A = tl.zeros([BC], dtype=tl.float32)\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gk = tl.make_block_ptr(gi + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)\n        b_A = tl.where(o_i > j, b_A * scale, 0.0)\n        p_qi = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_qi = tl.load(p_qi, boundary_check=(0,))\n        A_jj = tl.sum(b_qi * b_k * b_u * scale)\n        b_A = tl.where(o_i != j, b_A, A_jj)\n        tl.store(A + o_A + j, b_A, mask=m_A)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "d25bea1a-7d15-4f44-b4b0-08588ce1494a"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef logsigmoid_bwd_kernel(x, dx, dy, temperature, T: 'tl.constexpr', D:\n    'tl.constexpr', B: 'tl.constexpr'):\n    i = tl.program_id(0)\n    o_i = i * B + tl.arange(0, B)\n    m_i = o_i < T\n    b_x = tl.load(x + o_i, mask=m_i, other=0.0)\n    b_dy = tl.load(dy + o_i, mask=m_i, other=0.0)\n    b_dx = b_dy * (1.0 - tl.sigmoid(b_x)) / temperature\n    tl.store(dx + o_i, b_dx, mask=m_i)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "3f0b5be7-3f80-46e4-b3ee-753821f76f70"
  },
  {
    "input": "@triton.jit\ndef _splitK_reduce_varargs_backward(Out_splitK: \"'VAR_ARGS_ARRAY'\",\n    LSE_splitK: \"'VAR_ARGS_ARRAY'\", Dout_splitK: \"'VAR_ARGS_ARRAY'\",\n    DLSE_splitK: \"'VAR_ARGS_ARRAY'\", Out, LSE, DOut, DLSE, stride_osk_z:\n    \"'VAR_ARGS_ARRAY'\", stride_osk_g: \"'VAR_ARGS_ARRAY'\", stride_osk_h:\n    \"'VAR_ARGS_ARRAY'\", stride_osk_m: \"'VAR_ARGS_ARRAY'\", stride_osk_k:\n    \"'VAR_ARGS_ARRAY'\", stride_lsek_z: \"'VAR_ARGS_ARRAY'\", stride_lsek_g:\n    \"'VAR_ARGS_ARRAY'\", stride_lsek_h: \"'VAR_ARGS_ARRAY'\", stride_lsek_m:\n    \"'VAR_ARGS_ARRAY'\", stride_oz, stride_og, stride_oh, stride_om,\n    stride_ok, stride_lse_z, stride_lse_g, stride_lse_h, stride_lse_m,\n    stride_doz, stride_dog, stride_doh, stride_dom, stride_dok,\n    stride_dlse_z, stride_dlse_g, stride_dlse_h, stride_dlse_m, BLOCK_SIZE:\n    'tl.constexpr', H: 'tl.constexpr', G: 'tl.constexpr'):\n    \"\"\"\n    Backward for _splitK_reduce_varargs. Similar to forward, it takes\n    attention and LSE of chunks as lists of tensors,\n    and outputs the corresponding gradients in the same format.\n    \"\"\"\n    off_m = tl.program_id(0)\n    off_zhg = tl.program_id(1)\n    off_z = off_zhg // (H * G)\n    off_h = off_zhg // G % H\n    off_g = off_zhg % G\n    out_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        out_splitk_offset[i] = stride_osk_z[i] * off_z + stride_osk_g[i\n            ] * off_g + stride_osk_h[i] * off_h + stride_osk_m[i\n            ] * off_m + tl.arange(0, BLOCK_SIZE)\n    lse_splitk_offset: \"'VAR_ARGS_ARRAY'\"\n    for i in range(len(Out_splitK)):\n        lse_splitk_offset[i] = stride_lsek_z[i] * off_z + stride_lsek_g[i\n            ] * off_g + stride_lsek_h[i] * off_h + stride_lsek_m[i] * off_m\n    lse_max = float('-inf')\n    for split_k_idx in range(len(Out_splitK)):\n        LSE_splitK_ptr = LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        lse_splitk = tl.load(LSE_splitK_ptr)\n        lse_max = tl.maximum(lse_max, lse_splitk)\n    offset_out = (stride_oz * off_z + stride_oh * off_h + stride_og * off_g +\n        stride_om * off_m + tl.arange(0, BLOCK_SIZE))\n    offset_dout = (stride_doz * off_z + stride_doh * off_h + stride_dog *\n        off_g + stride_dom * off_m + tl.arange(0, BLOCK_SIZE))\n    out = tl.load(Out + offset_out)\n    dattn = tl.load(DOut + offset_dout)\n    offset_lse = (stride_lse_z * off_z + stride_lse_h * off_h + \n        stride_lse_g * off_g + stride_lse_m * off_m)\n    offset_dlse = (stride_dlse_z * off_z + stride_dlse_h * off_h + \n        stride_dlse_g * off_g + stride_dlse_m * off_m)\n    lse = tl.load(LSE + offset_lse)\n    dlse = tl.load(DLSE + offset_dlse)\n    for split_k_idx in range(len(Out_splitK)):\n        out_splitk = tl.load(Out_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx])\n        lse_splitk = tl.load(LSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx])\n        dout_splitk_ptr = Dout_splitK[split_k_idx] + out_splitk_offset[\n            split_k_idx]\n        dlse_splitk_ptr = DLSE_splitK[split_k_idx] + lse_splitk_offset[\n            split_k_idx]\n        dattn_dattn_i = tl.exp(lse_splitk - lse_max) / tl.exp(lse - lse_max)\n        dX_dattn_i = dattn_dattn_i * dattn\n        tl.store(dout_splitk_ptr, dX_dattn_i)\n        dattn_dlse_i = (out_splitk - out) * dattn_dattn_i\n        dlse_dlse_i = dattn_dattn_i\n        dX_dlse_i = dlse_dlse_i * dlse + tl.sum(dattn_dlse_i * dattn)\n        tl.store(dlse_splitk_ptr, dX_dlse_i)\n",
    "category": "Gradient Operations",
    "subcategory": "gradient accumulation",
    "uuid": "5f4920e3-6199-479e-9844-989255525b5d"
  },
  {
    "input": "@triton.jit\ndef _triton_dropout(x_ptr, x_keep_ptr, output_ptr, n_elements, p,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n    output = tl.where(x_keep, x / (1 - p), 0.0)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "90fe6eb9-aed2-480e-b611-47347a721738"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N_COLS': 32,\n    'BLOCK_SIZE_N_ROWS': 32}, num_stages=3, num_warps=1), triton.Config({\n    'BLOCK_SIZE_N_COLS': 64, 'BLOCK_SIZE_N_ROWS': 32}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 128,\n    'BLOCK_SIZE_N_ROWS': 32}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 256, 'BLOCK_SIZE_N_ROWS': 32}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 32,\n    'BLOCK_SIZE_N_ROWS': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 64, 'BLOCK_SIZE_N_ROWS': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 128,\n    'BLOCK_SIZE_N_ROWS': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 256, 'BLOCK_SIZE_N_ROWS': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 32,\n    'BLOCK_SIZE_N_ROWS': 128}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 64, 'BLOCK_SIZE_N_ROWS': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 128,\n    'BLOCK_SIZE_N_ROWS': 128}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 256, 'BLOCK_SIZE_N_ROWS': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 32,\n    'BLOCK_SIZE_N_ROWS': 256}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 64, 'BLOCK_SIZE_N_ROWS': 256}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N_COLS': 128,\n    'BLOCK_SIZE_N_ROWS': 256}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_N_COLS': 256, 'BLOCK_SIZE_N_ROWS': 256}, num_stages=3,\n    num_warps=8)], key=['n_cols', 'n_rows'], reset_to_zero=[\n    'per_channel_amax_ptr'])\n@triton.jit\ndef fast_abs_max_kernel(weights_ptr, per_channel_amax_ptr, col_stride,\n    row_stride, n_cols, n_rows, BLOCK_SIZE_N_COLS: 'tl.constexpr',\n    BLOCK_SIZE_N_ROWS: 'tl.constexpr'):\n    \"\"\"\n    Computes the per-channel absolute maximum of the weights.\n    Args:\n        weights_ptr (pointer): pointer to the weights\n        per_channel_amax_ptr (pointer): pointer to the per-channel amax output vector\n        col_stride (int): stride for moving to the next row of the matrix (next column)\n        row_stride (int): stride for moving to the next column of the matrix (next row)\n        n_cols (int): number of columns in the weight matrix (assuming x @ W^T).\n            So n_cols is n_rows of W\n        n_rows (int): number of rows. - same as above -\n        BLOCK_SIZE_N_COLS (tl.constexpr): block size for operating along the columns\n        BLOCK_SIZE_N_ROWS (tl.constexpr): block size for operating along the rows\n    \"\"\"\n    pid = tl.program_id(0)\n    n_pid_rows = tl.cdiv(n_rows, BLOCK_SIZE_N_ROWS)\n    col_block_idx = pid // n_pid_rows\n    row_block_idx = pid % n_pid_rows\n    col_offs = col_block_idx * BLOCK_SIZE_N_COLS + tl.arange(0,\n        BLOCK_SIZE_N_COLS)\n    row_offs = row_block_idx * BLOCK_SIZE_N_ROWS + tl.arange(0,\n        BLOCK_SIZE_N_ROWS)\n    ptrs = weights_ptr + (col_offs[:, None] * col_stride + row_offs[None, :\n        ] * row_stride)\n    block_weights = tl.load(ptrs, mask=(col_offs[:, None] < n_cols) & (\n        row_offs[None, :] < n_rows), other=float('-inf'))\n    block_weights_abs = tl.where((col_offs[:, None] < n_cols) & (row_offs[\n        None, :] < n_rows), tl.abs(block_weights), float('-inf'))\n    abs_max_block_weights = tl.max(block_weights_abs, axis=1)\n    tl.atomic_max(per_channel_amax_ptr + col_offs, abs_max_block_weights,\n        mask=col_offs < n_cols)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d674bb5c-0b40-451c-a025-34514c1542c5"
  },
  {
    "input": "@triton.jit\ndef _triton_qwen2vl_mrope(q_ptr, k_ptr, cos, sin, sl, n_qh: 'tl.constexpr',\n    n_kh: 'tl.constexpr', hd: 'tl.constexpr', pad_n_qh: 'tl.constexpr',\n    pad_n_kh: 'tl.constexpr', pad_hd: 'tl.constexpr', mrope_section_t:\n    'tl.constexpr', mrope_section_h: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', BACKWARD_PASS: 'tl.constexpr'=False):\n    pid = tl.program_id(0)\n    q_ptr = q_ptr + pid * (n_qh * hd)\n    k_ptr = k_ptr + pid * (n_kh * hd)\n    t_end = mrope_section_t\n    h_end = t_end + mrope_section_h\n    cos_row_idx = pid % sl\n    t_cos = cos + cos_row_idx * hd\n    h_cos = t_cos + sl * hd\n    w_cos = h_cos + sl * hd\n    t_sin = sin + cos_row_idx * hd\n    h_sin = t_sin + sl * hd\n    w_sin = h_sin + sl * hd\n    cos_offsets = tl.arange(0, pad_hd // 2)\n    t_mask = cos_offsets < t_end\n    h_mask = (t_end <= cos_offsets) & (cos_offsets < h_end)\n    w_mask = (h_end <= cos_offsets) & (cos_offsets < hd // 2)\n    t_cos_row = tl.load(t_cos + cos_offsets, mask=t_mask, other=0)\n    h_cos_row = tl.load(h_cos + cos_offsets, mask=h_mask, other=0)\n    w_cos_row = tl.load(w_cos + cos_offsets, mask=w_mask, other=0)\n    t_sin_row = tl.load(t_sin + cos_offsets, mask=t_mask, other=0)\n    h_sin_row = tl.load(h_sin + cos_offsets, mask=h_mask, other=0)\n    w_sin_row = tl.load(w_sin + cos_offsets, mask=w_mask, other=0)\n    cos_row = t_cos_row + h_cos_row + w_cos_row\n    sin_row = t_sin_row + h_sin_row + w_sin_row\n    first_half_q_offsets = tl.arange(0, pad_n_qh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_half_k_offsets = tl.arange(0, pad_n_kh)[:, None] * hd + tl.arange(\n        0, pad_hd // 2)[None, :]\n    first_q_mask = (tl.arange(0, pad_n_qh)[:, None] < n_qh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    first_k_mask = (tl.arange(0, pad_n_kh)[:, None] < n_kh) & (tl.arange(0,\n        pad_hd // 2)[None, :] < hd // 2)\n    q_tile_1 = tl.load(q_ptr + first_half_q_offsets, mask=first_q_mask, other=0\n        )\n    k_tile_1 = tl.load(k_ptr + first_half_k_offsets, mask=first_k_mask, other=0\n        )\n    second_half_q_offsets = first_half_q_offsets + hd // 2\n    second_half_k_offsets = first_half_k_offsets + hd // 2\n    second_q_mask = first_q_mask\n    second_k_mask = first_k_mask\n    q_tile_2 = tl.load(q_ptr + second_half_q_offsets, mask=second_q_mask,\n        other=0)\n    k_tile_2 = tl.load(k_ptr + second_half_k_offsets, mask=second_k_mask,\n        other=0)\n    if not BACKWARD_PASS:\n        new_q_tile_1 = q_tile_1 * cos_row - q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row + q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row - k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row + k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n    else:\n        new_q_tile_1 = q_tile_1 * cos_row + q_tile_2 * sin_row\n        tl.store(q_ptr + first_half_q_offsets, new_q_tile_1, mask=first_q_mask)\n        new_q_tile_2 = q_tile_2 * cos_row - q_tile_1 * sin_row\n        tl.store(q_ptr + second_half_q_offsets, new_q_tile_2, mask=\n            second_q_mask)\n        new_k_tile_1 = k_tile_1 * cos_row + k_tile_2 * sin_row\n        tl.store(k_ptr + first_half_k_offsets, new_k_tile_1, mask=first_k_mask)\n        new_k_tile_2 = k_tile_2 * cos_row - k_tile_1 * sin_row\n        tl.store(k_ptr + second_half_k_offsets, new_k_tile_2, mask=\n            second_k_mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "81b029f5-820f-4ace-9ef2-3ed5c9282bb2"
  },
  {
    "input": "@triton.jit\ndef streamk_kernel(a_ptr, b_ptr, c_ptr, M: 'tl.constexpr', N:\n    'tl.constexpr', K: 'tl.constexpr', scratchpad, locks, stride_am:\n    'tl.constexpr', stride_ak: 'tl.constexpr', stride_bk: 'tl.constexpr',\n    stride_bn: 'tl.constexpr', stride_cm: 'tl.constexpr', stride_cn:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr', USE_ATOMICS: 'tl.constexpr'):\n    iters_per_tile = tl.cdiv(K, BLOCK_SIZE_K)\n    total_iters = tl.cdiv(M, BLOCK_SIZE_M) * tl.cdiv(N, BLOCK_SIZE_N\n        ) * iters_per_tile\n    iters_per_cta = tl.cdiv(total_iters, tl.num_programs(0))\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    itr = pid * iters_per_cta\n    iter_end = itr + iters_per_cta\n    GROUP_SIZE_M = 8\n    while itr < iter_end and itr < total_iters:\n        tile_idx = itr // iters_per_tile\n        tile_iter = tile_idx * iters_per_tile\n        tile_iter_end = tile_iter + iters_per_tile\n        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n        group_id = tile_idx // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = tile_idx % num_pid_m\n        pid_n = tile_idx // num_pid_m\n        pid_k = itr - tile_iter\n        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n            stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n            stride_bn)\n        local_iter = pid_k\n        local_iter_end = min(iter_end, tile_iter_end) - tile_iter\n        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for k in range(0, local_iter_end - local_iter):\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n            acc += tl.dot(a, b)\n            a_ptrs += BLOCK_SIZE_K * stride_ak\n            b_ptrs += BLOCK_SIZE_K * stride_bk\n        acc = acc\n        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        c_offs = stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n        tile_started = itr == tile_iter\n        tile_ended = iter_end >= tile_iter_end\n        scratch_off = pid * BLOCK_SIZE_M * BLOCK_SIZE_N\n        offs_scratch = tl.arange(0, BLOCK_SIZE_M)[:, None\n            ] * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)[None, :]\n        if USE_ATOMICS:\n            tl.atomic_add(c_ptr + c_offs, acc, c_mask)\n        elif not tile_started:\n            tl.store(scratchpad + scratch_off + offs_scratch, acc, c_mask)\n            tl.atomic_xchg(locks + pid, 1)\n        else:\n            if not tile_ended:\n                cta_end = tl.cdiv(tile_iter_end, iters_per_cta)\n                cas = pid + 1\n                while cas < cta_end:\n                    while tl.atomic_cas(locks + cas, 1, 2) != 1:\n                        pass\n                    acc += tl.load(scratchpad + cas * BLOCK_SIZE_M *\n                        BLOCK_SIZE_N + offs_scratch, c_mask)\n                    cas += 1\n            tl.store(c_ptr + c_offs, acc, c_mask)\n        itr = tile_iter_end\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "109f0285-2cb3-40ad-9cbf-f1270a4e7112"
  },
  {
    "input": "@triton.jit\ndef zeroslike(x):\n    return tl.zeros(x.shape, tl.float32)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "d05702bf-b4db-4dd4-98dd-64db365e554d"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', STAGE:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(\n        BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n        ), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    q = tl.load(Q_block_ptr)\n    if STAGE & 1:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL, BLOCK_N,\n            4 - STAGE, offs_m, offs_n, N_CTX)\n    if STAGE & 2:\n        tl.debug_barrier()\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL, BLOCK_N,\n            2, offs_m, offs_n, N_CTX)\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "59fa08e1-bd10-40f7-ad36-4e97b7ff4d53"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef dropout_forward_kernel(input_pointer, output_pointer, size, drop_p,\n    seed, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Randomly zeroes elements in the input.\n\n    Args:\n        input_pointer: Pointer to the input to perform dropout on.\n            The input must be of shape [size].\n        output_pointer: Pointer to a container the result is written to.\n            The container must be of shape [size].\n        size: Number of elements in the input.\n        drop_p: Probability of dropping an element.\n        seed: Seed for generating the dropout mask.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    input = tl.load(input_pointer + offset, mask=mask)\n    output = apply_dropout(input, drop_p, seed, offset)\n    tl.store(output_pointer + offset, output, mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "0b928517-76fc-4e58-a142-1120b492974c"
  },
  {
    "input": "@triton.jit\ndef bwd_sequential_scan_fused(grad_output, v, f, h, B, L, C, BLOCK_M:\n    'tl.constexpr'):\n    offset_b = tl.program_id(0)\n    if offset_b >= B:\n        return\n    offset_n = tl.program_id(1)\n    ptr = tl.arange(0, BLOCK_M) + offset_b * L * C + (L - 1\n        ) * C + offset_n * BLOCK_M\n    grad_h = tl.zeros([BLOCK_M], dtype=tl.float32)\n    for time_step in range(L - 1, -1, -1):\n        grad = tl.load(grad_output + ptr)\n        grad_h += grad\n        decay = tl.load(f + ptr)\n        decay = tl.sigmoid(decay)\n        input = tl.load(v + ptr)\n        grad_v = (1 - decay) * grad_h\n        tl.store(v + ptr, grad_v)\n        hidden_state = tl.load(h + ptr - C, mask=ptr >= offset_b * L * C +\n            C, other=0.0)\n        grad_f = grad_h * (hidden_state - input) * decay * (1 - decay)\n        tl.store(f + ptr, grad_f)\n        grad_h *= decay\n        ptr -= C\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d15d1b52-101b-470b-a02a-ceccea29b8a6"
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_bidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y1 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y2 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y3 + _idx, _x, mask=_mask_hw)\n        tl.store(p_y4 + _idx, _x, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "cfb3d3af-6a94-4c2b-b971-be86a34478a5"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.heuristics({'HAS_SCALE': lambda args: args['scale'] is not None})\n@triton.jit\ndef logsumexp_fwd_kernel(x, z, scale, D: 'tl.constexpr', B: 'tl.constexpr',\n    HAS_SCALE: 'tl.constexpr'):\n    i_n, i_d = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * B + tl.arange(0, B)\n    m_d = o_d < D\n    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\n    if HAS_SCALE:\n        b_x = b_x * scale\n    b_m = tl.max(b_x, 0)\n    b_z = tl.log(tl.sum(tl.exp(b_x - b_m), 0)) + b_m\n    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)\n",
    "category": "Math Utils",
    "subcategory": "exponential",
    "uuid": "cb67112d-96d4-414a-a477-73972146803a"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_add_kernel(x_ptr, y_ptr, h1_ptr, w_ptr, eps, stride, N_COLS:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', INCLUDE_WEIGHT: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    x_ptr += row * stride\n    y_ptr += row * stride\n    h1_ptr += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        ax = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_last')\n        ay = tl.load(y_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first')\n        a = ax + ay\n        tl.store(x_ptr + cols, a, mask=mask)\n        _mean += a * a\n    rstd = rsqrt(tl.sum(_mean, axis=0) / N_COLS + eps)\n    for offset in range(0, N_COLS, BLOCK_SIZE):\n        cols = offset + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N_COLS\n        a = tl.load(x_ptr + cols, mask=mask, other=0.0, eviction_policy=\n            'evict_first')\n        if INCLUDE_WEIGHT:\n            w = tl.load(w_ptr + cols, mask=mask)\n            tl.store(h1_ptr + cols, a * rstd * w, mask=mask)\n        else:\n            tl.store(h1_ptr + cols, a * rstd, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "bd76c1b5-de6e-4d40-aadd-6601013aac49"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd(Q, K, V, sm_scale, DO, DQ, DK, DV, M, D, stride_z, stride_h,\n    stride_tok, stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1:\n    'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr',\n    BLK_SLICE_FACTOR: 'tl.constexpr', HEAD_DIM: 'tl.constexpr'):\n    LN2: 'tl.constexpr' = 0.6931471824645996\n    bhid = tl.program_id(2)\n    off_chz = bhid * N_CTX\n    adj = stride_h * (bhid % H) + stride_z * (bhid // H)\n    pid = tl.program_id(0)\n    Q += adj\n    K += adj\n    V += adj\n    DO += adj\n    DQ += adj\n    DK += adj\n    DV += adj\n    M += off_chz\n    D += off_chz\n    offs_k = tl.arange(0, HEAD_DIM)\n    start_n = pid * BLOCK_N1\n    start_m = start_n\n    MASK_BLOCK_M1: 'tl.constexpr' = BLOCK_M1 // BLK_SLICE_FACTOR\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, MASK_BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n,\n        start_m, num_steps, MASK=True)\n    start_m += num_steps * MASK_BLOCK_M1\n    num_steps = (N_CTX - start_m) // BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n, start_m,\n        num_steps, MASK=False)\n    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dv_ptrs, dv)\n    dk *= sm_scale\n    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dk_ptrs, dk)\n    start_m = pid * BLOCK_M2\n    end_n = start_m + BLOCK_M2\n    MASK_BLOCK_N2: 'tl.constexpr' = BLOCK_N2 // BLK_SLICE_FACTOR\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n        )\n    m = tl.load(M + offs_m)\n    m = m[:, None]\n    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, MASK_BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps *\n        MASK_BLOCK_N2, num_steps, MASK=True)\n    end_n -= num_steps * MASK_BLOCK_N2\n    num_steps = end_n // BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps * BLOCK_N2,\n        num_steps, MASK=False)\n    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    dq *= LN2\n    tl.store(dq_ptrs, dq)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "5afaafd2-88ae-49fa-aed7-9685a3add030"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_quantize_kv(K, Dest_loc, Out, Out_scale,\n    stride_k_bs, stride_k_h, stride_k_d, stride_o_bs, stride_o_h,\n    stride_o_d, stride_os_bs, stride_os_h, stride_os_d, head_num, head_dim,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    src_data = tl.load(K + cur_index * stride_k_bs + offs_h[:, None] *\n        stride_k_h + stride_k_d * offs_d[None, :], mask=(offs_h[:, None] <\n        head_num) & (offs_d[None, :] < head_dim), other=0.0)\n    abs_data = tl.abs(src_data)\n    data_scale = (tl.max(abs_data, axis=1) / 127.0)[:, None]\n    q_src_data = src_data / data_scale\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    os_ptrs = Out_scale + dest_index * stride_os_bs + stride_os_h * offs_h[\n        :, None]\n    tl.store(o_ptrs, q_src_data, mask=(offs_h[:, None] < head_num) & (\n        offs_d[None, :] < head_dim))\n    tl.store(os_ptrs, data_scale, mask=offs_h[:, None] < head_num)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "b81e31af-ec6b-4824-98a7-0229b60a1d71"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_kernel(X, Y, W, B, Mean, Rstd, stride, N, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "5af99556-512e-457d-9aa2-fbad9b5f54ad"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "5b1f78af-76ec-406e-b4e9-6cdb6c7e7a28"
  },
  {
    "input": "@triton_autotune(configs=_get_bw_configs(), key=['AUTOTUNE_Z', 'H',\n    'AUTOTUNE_MAX_SEQ_LEN', 'DimQ', 'DimV', 'BUCKET_FN', 'ATTN_BIAS_TYPE'])\n@triton.jit\ndef _ragged_hstu_attn_bwd(Q, K, V, sort_by_length_indices, seq_offsets, TS,\n    TW, PW, Bias, seq2_offsets, num_targets, DOut, DQ, DK, DV, DBias, DTW,\n    DPW, LOCK, stride_qm, stride_qh, stride_kn, stride_kh, stride_vn,\n    stride_vh, stride_ts, stride_dom, stride_doh, stride_dqm, stride_dqh,\n    stride_dkn, stride_dkh, stride_dvn, stride_dvh, alpha, Z, AUTOTUNE_Z, H,\n    MAX_SEQ_LEN, AUTOTUNE_MAX_SEQ_LEN, DimQ, DimV, num_buckets, max_pos_ind,\n    time_bucket_incr, time_bucket_div, time_delta, CONTEXTUAL_SEQ_LEN:\n    'tl.constexpr', MAX_ATTN_LEN: 'tl.constexpr', INVALID_MASK_TYPE:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', BUCKET_FN: 'tl.constexpr',\n    ATTN_BIAS_TYPE: 'tl.constexpr', USE_TIME_BIAS: 'tl.constexpr',\n    USE_POS_BIAS: 'tl.constexpr', FUSED_BIAS_BWD: 'tl.constexpr',\n    HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS: 'tl.constexpr',\n    ALLOW_TF32: 'tl.constexpr', BLOCK_D_Q: 'tl.constexpr', BLOCK_D_V:\n    'tl.constexpr', SEQUENCE_PARALLEL: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', UNROLL: 'tl.constexpr',\n    HAS_SORT_BY_LENGTH_INDICES: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    if HAS_SORT_BY_LENGTH_INDICES:\n        off_z = tl.load(sort_by_length_indices + off_z)\n    off_h = off_hz % H\n    off_h = off_h\n    seq_start = tl.load(seq_offsets + off_z)\n    seq_end = tl.load(seq_offsets + off_z + 1)\n    seq_len = seq_end - seq_start\n    if HAS_MULTIPLE_TARGETS:\n        n_targets = tl.load(num_targets + off_z)\n    else:\n        n_targets = None\n    Q = Q + seq_start * stride_qm + off_h * stride_qh\n    K = K + seq_start * stride_kn + off_h * stride_kh\n    V = V + seq_start * stride_vn + off_h * stride_vh\n    DOut = DOut + seq_start * stride_dom + off_h * stride_doh\n    DQ = DQ + seq_start * stride_dqm + off_h * stride_dqh\n    DK = DK + seq_start * stride_dkn + off_h * stride_dkh\n    DV = DV + seq_start * stride_dvn + off_h * stride_dvh\n    if ATTN_BIAS_TYPE == 'fused':\n        if USE_TIME_BIAS:\n            TS = TS + off_z * stride_ts\n        if FUSED_BIAS_BWD:\n            if USE_TIME_BIAS:\n                DTW = DTW + off_hz * (num_buckets + 1)\n            if USE_POS_BIAS:\n                if HAS_MAX_POS_IND:\n                    DPW = DPW + off_hz * (2 * max_pos_ind - 1)\n                else:\n                    DPW = DPW + off_hz * (2 * MAX_SEQ_LEN - 1)\n    elif ATTN_BIAS_TYPE == 'separate':\n        seq2_start = tl.load(seq2_offsets + off_z)\n        bias_start = seq2_start * H + off_h * seq_len * seq_len\n        Bias = Bias + bias_start\n        DBias = DBias + bias_start\n    if SEQUENCE_PARALLEL:\n        start_n = tl.program_id(1) * BLOCK_N\n        if start_n >= seq_len:\n            return\n        _ragged_hstu_attn_bwd_one_col_block(start_n=start_n, seq_len=\n            seq_len, n_targets=n_targets, Q=Q, K=K, V=V, TS=TS, TW=TW, PW=\n            PW, Bias=Bias, DOut=DOut, DQ=DQ, DK=DK, DV=DV, DBias=DBias, DTW\n            =DTW, DPW=DPW, LOCK=LOCK, stride_qm=stride_qm, stride_kn=\n            stride_kn, stride_vn=stride_vn, stride_dom=stride_dom,\n            stride_dqm=stride_dqm, stride_dkn=stride_dkn, stride_dvn=\n            stride_dvn, alpha=alpha, MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=\n            num_buckets, max_pos_ind=max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN,\n            time_bucket_incr=time_bucket_incr, time_bucket_div=\n            time_bucket_div, time_delta=time_delta, INVALID_MASK_TYPE=\n            INVALID_MASK_TYPE, CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN,\n            ATTN_BIAS_TYPE=ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS,\n            USE_POS_BIAS=USE_POS_BIAS, FUSED_BIAS_BWD=FUSED_BIAS_BWD,\n            HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n            HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n            ALLOW_TF32=ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q, BLOCK_D_V=BLOCK_D_V,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, UNROLL=UNROLL, ATOMIC_ADD=True)\n    else:\n        for start_n in range(0, seq_len, BLOCK_N):\n            _ragged_hstu_attn_bwd_one_col_block(start_n=start_n, seq_len=\n                seq_len, n_targets=n_targets, Q=Q, K=K, V=V, TS=TS, TW=TW,\n                PW=PW, Bias=Bias, DOut=DOut, DQ=DQ, DK=DK, DV=DV, DBias=\n                DBias, DTW=DTW, DPW=DPW, LOCK=LOCK, stride_qm=stride_qm,\n                stride_kn=stride_kn, stride_vn=stride_vn, stride_dom=\n                stride_dom, stride_dqm=stride_dqm, stride_dkn=stride_dkn,\n                stride_dvn=stride_dvn, alpha=alpha, MAX_SEQ_LEN=MAX_SEQ_LEN,\n                num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n                MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n                time_bucket_incr, time_bucket_div=time_bucket_div,\n                time_delta=time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, FUSED_BIAS_BWD=FUSED_BIAS_BWD,\n                HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n                HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n                ALLOW_TF32=ALLOW_TF32, BLOCK_D_Q=BLOCK_D_Q, BLOCK_D_V=\n                BLOCK_D_V, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, UNROLL=UNROLL,\n                ATOMIC_ADD=False)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "8d807cbe-7544-4476-a2df-41cb73b183ee"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'UNUSED': 1}, num_stages=\n    num_stages, num_warps=num_warps) for num_stages in (1, 2, 3, 4, 5) for\n    num_warps in (1, 2, 4, 8)], key=['in_features', 'out_features',\n    'num_codebooks', 'codebook_size', 'out_group_size', 'in_group_size',\n    'num_input_groups', 'num_input_groups_next_power_of_2',\n    'compute_in_fp32', 'has_output_scale', 'has_bias'])\n@triton.jit\ndef _aqlm_gemv_simple(input_vec_ptr, output_vec_ptr, codes_ptr,\n    codebooks_ptr, scales_ptr, bias_ptr, in_features: 'tl.constexpr',\n    out_features: 'tl.constexpr', num_codebooks: 'tl.constexpr',\n    codebook_size: 'tl.constexpr', out_group_size: 'tl.constexpr',\n    in_group_size: 'tl.constexpr', num_input_groups: 'tl.constexpr',\n    num_input_groups_next_power_of_2: 'tl.constexpr', compute_in_fp32:\n    'tl.constexpr', has_output_scale: 'tl.constexpr', has_bias:\n    'tl.constexpr', UNUSED: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    input_vec = tl.load(input_vec_ptr + tl.arange(0,\n        num_input_groups_next_power_of_2)[:, None, None, None] *\n        in_group_size + tl.arange(0, in_group_size)[None, None, None, :],\n        mask=tl.arange(0, num_input_groups_next_power_of_2)[:, None, None,\n        None] < num_input_groups)\n    dtype = input_vec.dtype\n    codes_i_ptrs = (codes_ptr + pid * num_input_groups * num_codebooks + tl\n        .arange(0, num_input_groups_next_power_of_2)[:, None] *\n        num_codebooks + tl.arange(0, num_codebooks)[None, :])\n    codes_i_mask_1d = tl.arange(0, num_input_groups_next_power_of_2\n        ) < num_input_groups\n    codes_i = tl.load(codes_i_ptrs, mask=codes_i_mask_1d[:, None])\n    codes_i = codes_i\n    codes_i = codes_i + (codes_i < 0) * codebook_size\n    codes_i += tl.arange(0, num_codebooks)[None, :] * codebook_size\n    out_group_ix = tl.arange(0, out_group_size)[None, None, :, None]\n    in_group_ix = tl.arange(0, in_group_size)[None, None, None, :]\n    weight_i_ptrs = (codebooks_ptr + codes_i[:, :, None, None] *\n        out_group_size * in_group_size + out_group_ix * in_group_size +\n        in_group_ix)\n    weights_i = tl.load(weight_i_ptrs, mask=codes_i_mask_1d[:, None, None,\n        None], other=0)\n    if compute_in_fp32:\n        weights_i = weights_i\n        input_vec = input_vec\n    output_i = weights_i * input_vec\n    if out_group_size == 1:\n        output_i = tl.sum(output_i)\n    else:\n        output_i = tl.sum(output_i, axis=1)\n        output_i = tl.sum(output_i, axis=2)\n        output_i = tl.sum(output_i, axis=0)\n    if has_output_scale:\n        output_i *= tl.load(scales_ptr + pid)\n    if has_bias:\n        output_i += tl.load(bias_ptr + pid)\n    if out_group_size == 1:\n        tl.store(output_vec_ptr + pid, output_i)\n    else:\n        tl.store(output_vec_ptr + pid * out_group_size + tl.arange(0,\n            out_group_size), output_i)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "cd62b850-407e-4490-8d17-006a5902e92d"
  },
  {
    "input": "@triton.jit\ndef _load_mlp_bias_bcast(biases, C, offs, BLOCK_SIZE):\n    return tl.view(tl.load((biases + offs + tl.arange(0, C))[None, :] + tl.\n        zeros((BLOCK_SIZE, 1), dtype=tl.int32)), (BLOCK_SIZE, C))\n",
    "category": "Linear Operations",
    "subcategory": "bias addition",
    "uuid": "226f34e8-d2df-432f-bbba-74e83d3a01e1"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_rwkv6_fwd_kernel_inter(q, v, g, h, o, A, s_k_h, s_k_t, s_k_d,\n    s_v_h, s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_ge = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_g = tl.load(p_ge, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_A = tl.where(m_s, b_A, 0.0)\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "329c6c5e-bc46-4e07-866e-f696ab865068"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att2_int8v(Prob, V, V_scale, Out, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_ph, stride_pbs, stride_vbs, stride_vh,\n    stride_vd, stride_vsbs, stride_vsh, stride_vsd, stride_obs, stride_oh,\n    stride_od, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    v_loc_off = cur_batch_req_idx * stride_req_to_tokens_b + (\n        cur_batch_start_index + offs_n) * stride_req_to_tokens_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    vs_offs = cur_kv_head * stride_vsh\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(Req_to_tokens + v_loc_off + start_n *\n            stride_req_to_tokens_s, mask=start_n + offs_n <\n            cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        vs_value = tl.load(V_scale + vs_offs + v_loc[:, None] * stride_vsbs,\n            mask=start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value * vs_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "1e6be76b-a659-4a87-becc-04b8f0402712"
  },
  {
    "input": "@triton.jit\ndef _d_linear(d_y, w, b, x):\n    d_x = tl.dot(d_y, tl.trans(w), allow_tf32=ALLOW_TF32)\n    d_w = tl.dot(tl.trans(d_y), x, allow_tf32=ALLOW_TF32)\n    d_b = tl.sum(d_y, axis=0)\n    return d_x, d_w, d_b\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "5b720b62-49e4-4f24-be1b-19ef4137eb42"
  },
  {
    "input": "@triton.jit\ndef _triton_fourth_order_fwd(x_ptr: 'tl.tensor', y_ptr: 'tl.tensor', z_ptr:\n    'tl.tensor', sh_1_0_ptr: 'tl.tensor', sh_1_1_ptr: 'tl.tensor',\n    sh_1_2_ptr: 'tl.tensor', sh_2_0_ptr: 'tl.tensor', sh_2_1_ptr:\n    'tl.tensor', sh_2_2_ptr: 'tl.tensor', sh_2_3_ptr: 'tl.tensor',\n    sh_2_4_ptr: 'tl.tensor', sh_3_0_ptr: 'tl.tensor', sh_3_1_ptr:\n    'tl.tensor', sh_3_2_ptr: 'tl.tensor', sh_3_3_ptr: 'tl.tensor',\n    sh_3_4_ptr: 'tl.tensor', sh_3_5_ptr: 'tl.tensor', sh_3_6_ptr:\n    'tl.tensor', sh_4_0_ptr: 'tl.tensor', sh_4_1_ptr: 'tl.tensor',\n    sh_4_2_ptr: 'tl.tensor', sh_4_3_ptr: 'tl.tensor', sh_4_4_ptr:\n    'tl.tensor', sh_4_5_ptr: 'tl.tensor', sh_4_6_ptr: 'tl.tensor',\n    sh_4_7_ptr: 'tl.tensor', sh_4_8_ptr: 'tl.tensor', BLOCK_SIZE:\n    'tl.constexpr', vector_length: 'tl.constexpr'):\n    sqrt_3 = 3 ** 0.5\n    block_id = tl.program_id(0)\n    offset = tl.arange(0, BLOCK_SIZE) + BLOCK_SIZE * block_id\n    x_row_start = x_ptr + offset\n    y_row_start = y_ptr + offset\n    z_row_start = z_ptr + offset\n    x = tl.load(x_row_start, mask=offset < vector_length)\n    y = tl.load(y_row_start, mask=offset < vector_length)\n    z = tl.load(z_row_start, mask=offset < vector_length)\n    sh_1_0 = x * sqrt_3\n    sh_1_1 = y * sqrt_3\n    sh_1_2 = z * sqrt_3\n    sqrt_15 = 15 ** 0.5\n    sqrt_5 = 5 ** 0.5\n    sq_x = x * x\n    sq_y = y * y\n    sq_z = z * z\n    sh_2_0 = sqrt_15 * x * z\n    sh_2_1 = sqrt_15 * x * y\n    sh_2_2 = sqrt_5 * (sq_y - 0.5 * (sq_x + sq_z))\n    sh_2_3 = sqrt_15 * y * z\n    sh_2_4 = 0.5 * sqrt_15 * (sq_z - sq_x)\n    sqrt_42 = 42 ** 0.5\n    sqrt_168 = 168 ** 0.5\n    sqrt_7 = 7 ** 0.5\n    sh_3_0 = 1 / 6 * sqrt_42 * (sh_2_0 * z + sh_2_4 * x)\n    sh_3_1 = sqrt_7 * sh_2_0 * y\n    sh_3_2 = 1 / 8 * sqrt_168 * (4 * sq_y - (sq_x + sq_z)) * x\n    sh_3_3 = 0.5 * sqrt_7 * y * (2 * sq_y - 3 * (sq_x + sq_z))\n    sh_3_4 = 1 / 8 * sqrt_168 * z * (4 * sq_y - (sq_x + sq_z))\n    sh_3_5 = sqrt_7 * (sh_2_4 * y)\n    sh_3_6 = 1 / 6 * sqrt_42 * (sh_2_4 * z - sh_2_0 * x)\n    sqrt_2 = 2 ** 0.5\n    sqrt_210 = 210 ** 0.5\n    sqrt_14 = 14 ** 0.5\n    sqrt_21 = 21 ** 0.5\n    sqrt_70 = 70 ** 0.5\n    sqrt_105 = 105 ** 0.5\n    sqrt_6 = 6 ** 0.5\n    sh_4_0 = 3 / 4 * sqrt_2 * (sh_3_0 * z + sh_3_6 * x)\n    sh_4_1 = (3 / 4 * sh_3_0 * y + 3 / 8 * sqrt_6 * sh_3_1 * z + 3 / 8 *\n        sqrt_6 * sh_3_5 * x)\n    sh_4_2 = (-3 / 56 * sqrt_14 * sh_3_0 * z + 3 / 14 * sqrt_21 * sh_3_1 *\n        y + 3 / 56 * sqrt_210 * sh_3_2 * z + 3 / 56 * sqrt_210 * sh_3_4 * x +\n        3 / 56 * sqrt_14 * sh_3_6 * x)\n    sh_4_3 = (-3 / 56 * sqrt_42 * sh_3_1 * z + 3 / 28 * sqrt_105 * sh_3_2 *\n        y + 3 / 28 * sqrt_70 * sh_3_3 * x + 3 / 56 * sqrt_42 * sh_3_5 * x)\n    sh_4_4 = (-3 / 28 * sqrt_42 * sh_3_2 * x + 3 / 7 * sqrt_7 * sh_3_3 * y -\n        3 / 28 * sqrt_42 * sh_3_4 * z)\n    sh_4_5 = (-3 / 56 * sqrt_42 * sh_3_1 * x + 3 / 28 * sqrt_70 * sh_3_3 *\n        z + 3 / 28 * sqrt_105 * sh_3_4 * y - 3 / 56 * sqrt_42 * sh_3_5 * z)\n    sh_4_6 = (-3 / 56 * sqrt_14 * sh_3_0 * x - 3 / 56 * sqrt_210 * sh_3_2 *\n        x + 3 / 56 * sqrt_210 * sh_3_4 * z + 3 / 14 * sqrt_21 * sh_3_5 * y -\n        3 / 56 * sqrt_14 * sh_3_6 * z)\n    sh_4_7 = (-3 / 8 * sqrt_6 * sh_3_1 * x + 3 / 8 * sqrt_6 * sh_3_5 * z + \n        3 / 4 * sh_3_6 * y)\n    sh_4_8 = 3 / 4 * sqrt_2 * (-sh_3_0 * x + sh_3_6 * z)\n    sh_1_0_start = sh_1_0_ptr + offset\n    sh_1_1_start = sh_1_1_ptr + offset\n    sh_1_2_start = sh_1_2_ptr + offset\n    sh_2_0_start = sh_2_0_ptr + offset\n    sh_2_1_start = sh_2_1_ptr + offset\n    sh_2_2_start = sh_2_2_ptr + offset\n    sh_2_3_start = sh_2_3_ptr + offset\n    sh_2_4_start = sh_2_4_ptr + offset\n    sh_3_0_start = sh_3_0_ptr + offset\n    sh_3_1_start = sh_3_1_ptr + offset\n    sh_3_2_start = sh_3_2_ptr + offset\n    sh_3_3_start = sh_3_3_ptr + offset\n    sh_3_4_start = sh_3_4_ptr + offset\n    sh_3_5_start = sh_3_5_ptr + offset\n    sh_3_6_start = sh_3_6_ptr + offset\n    sh_4_0_start = sh_4_0_ptr + offset\n    sh_4_1_start = sh_4_1_ptr + offset\n    sh_4_2_start = sh_4_2_ptr + offset\n    sh_4_3_start = sh_4_3_ptr + offset\n    sh_4_4_start = sh_4_4_ptr + offset\n    sh_4_5_start = sh_4_5_ptr + offset\n    sh_4_6_start = sh_4_6_ptr + offset\n    sh_4_7_start = sh_4_7_ptr + offset\n    sh_4_8_start = sh_4_8_ptr + offset\n    tl.store(sh_1_0_start, sh_1_0, mask=offset < vector_length)\n    tl.store(sh_1_1_start, sh_1_1, mask=offset < vector_length)\n    tl.store(sh_1_2_start, sh_1_2, mask=offset < vector_length)\n    tl.store(sh_2_0_start, sh_2_0, mask=offset < vector_length)\n    tl.store(sh_2_1_start, sh_2_1, mask=offset < vector_length)\n    tl.store(sh_2_2_start, sh_2_2, mask=offset < vector_length)\n    tl.store(sh_2_3_start, sh_2_3, mask=offset < vector_length)\n    tl.store(sh_2_4_start, sh_2_4, mask=offset < vector_length)\n    tl.store(sh_3_0_start, sh_3_0, mask=offset < vector_length)\n    tl.store(sh_3_1_start, sh_3_1, mask=offset < vector_length)\n    tl.store(sh_3_2_start, sh_3_2, mask=offset < vector_length)\n    tl.store(sh_3_3_start, sh_3_3, mask=offset < vector_length)\n    tl.store(sh_3_4_start, sh_3_4, mask=offset < vector_length)\n    tl.store(sh_3_5_start, sh_3_5, mask=offset < vector_length)\n    tl.store(sh_3_6_start, sh_3_6, mask=offset < vector_length)\n    tl.store(sh_4_0_start, sh_4_0, mask=offset < vector_length)\n    tl.store(sh_4_1_start, sh_4_1, mask=offset < vector_length)\n    tl.store(sh_4_2_start, sh_4_2, mask=offset < vector_length)\n    tl.store(sh_4_3_start, sh_4_3, mask=offset < vector_length)\n    tl.store(sh_4_4_start, sh_4_4, mask=offset < vector_length)\n    tl.store(sh_4_5_start, sh_4_5, mask=offset < vector_length)\n    tl.store(sh_4_6_start, sh_4_6, mask=offset < vector_length)\n    tl.store(sh_4_7_start, sh_4_7, mask=offset < vector_length)\n    tl.store(sh_4_8_start, sh_4_8, mask=offset < vector_length)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "dee0ce75-2527-4b45-bd79-9cab9969411d"
  },
  {
    "input": "@triton.jit\ndef bwd_inner_chunk(q, k, g, dA, dq, dk, s_k_h, s_k_t, s_k_d, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_dq = dq + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_gq = g + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_dA = dA + i_bh * (tl.cdiv(T, BT) * BT * BT) + i_t * BT * BT + tl.arange(\n        0, BT)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0)\n        gq = tl.load(p_gq, mask=mask, other=0)\n        score = tl.exp(gq[None, :] - b_g)\n        score = tl.where(o_i[:, None] <= i, score, 0)\n        _dA = tl.load(p_dA)\n        _dA = tl.where(o_i <= i, _dA, 0)\n        b_dk += _dA[:, None] * score * _q[None, :]\n        b_dq = tl.sum(_dA[:, None] * score * b_k, axis=0)\n        tl.store(p_dq, b_dq, mask=mask)\n        p_q += K\n        p_dq += K\n        p_gq += K\n        p_dA += BT\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "184c73e2-4996-47a5-8633-a05a3cc7bb36"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_k_h, s_k_t,\n    s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr', H: 'tl.constexpr',\n    T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_1o = tl.zeros([BV, BK], dtype=tl.float32)\n    b_h_2o = tl.zeros([BV, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i * BT\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i * BT < T)\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h_1o, allow_tf32=False)\n        if i_v == 0:\n            b_dq += b_dz[:, None] * k_1o\n        b_dq_2o = tl.dot(b_do, b_h_2o, allow_tf32=False) * 0.5\n        if i_v == 0:\n            b_dq_2o += b_dz[:, None] * k_2o * 0.5\n        b_dq_2o = tl.reshape(b_dq_2o, [BT, BK, BK])\n        b_dq += tl.sum(b_dq_2o * b_q[:, :, None], axis=1)\n        b_dq += tl.sum(b_dq_2o * b_q[:, None, :], axis=2)\n        b_dq *= scale\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_h_2o = b_h_2o + tl.dot(b_v, b_k_2o, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_v, b_k, allow_tf32=False)\n        if i_v == 0:\n            k_1o += tl.sum(b_k, axis=0)[None, :]\n            k_2o += tl.sum(b_k_2o, axis=0)[None, :]\n    tl.debug_barrier()\n    b_h_1o = None\n    b_h_2o = None\n    b_dh_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_dh_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    b_dh_0o = tl.zeros([BV], dtype=tl.float32)\n    m_s = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    dq_1o = tl.zeros([1, BK], dtype=tl.float32)\n    dq_2o = tl.zeros([BK * BK, 1], dtype=tl.float32)\n    for i in range(tl.cdiv(T, BT) * BT - BT, -BT, -BT):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_k_h, (T, K),\n            (s_k_t, s_k_d), (i, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_v_h, (T, V),\n            (s_v_t, s_v_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i < T)\n        b_q = b_q * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds *= 1 + b_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s2, b_do, allow_tf32=False)\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_dv += tl.dot(b_k, b_dh_1o, allow_tf32=False)\n        b_dv += tl.dot(b_k_2o, b_dh_2o, allow_tf32=False)\n        b_dv += b_dh_0o\n        b_dk += tl.dot(b_v, tl.trans(b_dh_1o), allow_tf32=False)\n        if i_v == 0:\n            b_dk += dq_1o\n        b_dk_2o = tl.dot(b_dh_2o, tl.trans(b_v), allow_tf32=False)\n        if i_v == 0:\n            b_dk_2o += dq_2o\n        b_dk_2o = tl.reshape(b_dk_2o, [BK, BK, BT])\n        b_k_fp32 = tl.trans(b_k)\n        b_dk2 = tl.sum(b_dk_2o * b_k_fp32[:, None, :], axis=0)\n        b_dk2 += tl.sum(b_dk_2o * b_k_fp32[None, :, :], axis=1)\n        b_dk += tl.trans(b_dk2)\n        b_dh_0o += tl.sum(b_do, axis=0)\n        b_dh_1o = b_dh_1o + tl.dot(b_q, b_do, allow_tf32=False)\n        b_q_2o = b_q[None, :, :] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BK * BK, BT])\n        b_dh_2o = b_dh_2o + tl.dot(b_q_2o, b_do, allow_tf32=False) * 0.5\n        if i_v == 0:\n            dq_1o += tl.sum(b_dz[None, :] * b_q, axis=1)[None, :]\n            dq_2o += (tl.sum(b_dz[None, :] * b_q_2o, axis=1) * 0.5)[:, None]\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "710cc6f6-3748-4ef4-ad4f-45237cf2a279"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd(X, Y, W, Rstd, N, eps, BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    row_offset = tl.program_id(0) * BLOCK_M\n    row_index = row_offset + tl.arange(0, BLOCK_M)[:, None]\n    col_index = tl.arange(0, BLOCK_N)[None, :]\n    col_mask = col_index < N\n    x = tl.load(X + N * row_index + col_index, col_mask, other=0.0)\n    w = tl.load(W + col_index, col_mask, eviction_policy='evict_last',\n        other=0.0)\n    xx = x * x\n    xx = tl.broadcast_to(xx, [BLOCK_M, BLOCK_N])\n    mean = tl.sum(xx, axis=1)[:, None] / N\n    rstd = tl.rsqrt(mean + eps)\n    y = x * rstd * w\n    tl.store(Rstd + row_index, rstd)\n    tl.store(Y + N * row_index + col_index, y, col_mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "51dbdc75-8d43-49a0-9b90-edc9668d1cda"
  },
  {
    "input": "@triton.jit\ndef triton_local_scan(x, y, K: 'tl.constexpr', flip: 'tl.constexpr', BC:\n    'tl.constexpr', BH: 'tl.constexpr', BW: 'tl.constexpr', DC:\n    'tl.constexpr', DH: 'tl.constexpr', DW: 'tl.constexpr', NH:\n    'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    _i = (tl.arange(0, BH) + BH * i_h)[:, None]\n    _j = (tl.arange(0, BW) + BW * i_w)[None, :]\n    _c_offset = (DW // K * (_i // K) + _j // K) * K * K + _i % K * K + _j % K\n    if flip:\n        _c_offset = DH * DW - _c_offset - 1\n    p_y = y + i_b * _tmp1 + _tmp0 + _c_offset\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _x = tl.load(p_x + _idx, mask=_mask_hw)\n        tl.store(p_y + _idx, _x, mask=_mask_hw)\n    tl.debug_barrier()\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "3e617cb1-5f5d-4faf-ae35-616bcacb85f4"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    o_i = tl.arange(0, BT)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s = tl.where(o_i[:, None] <= o_i[None, :], b_s, 0)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False) * scale\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) + tl.dot(b_s, b_do,\n            allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * scale, 0)\n    b_dq += tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk += tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "cc678a93-ef0d-4344-8430-111f59f3b869"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4,\n    num_warps=8)], key=['chunk_size', 'hdim', 'dstate'])\n@triton.jit\ndef _chunk_scan_fwd_kernel_wip(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr,\n    dt_ptr, dA_cumsum_ptr, seq_idx_ptr, C_ptr, B_ptr, prev_states_ptr,\n    D_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio,\n    stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m,\n    stride_cb_csize_k, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_z_batch, stride_z_seqlen, stride_z_head,\n    stride_z_hdim, stride_out_batch, stride_out_seqlen, stride_out_head,\n    stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_C_batch, stride_C_seqlen, stride_C_head,\n    stride_C_dstate, stride_B_batch, stride_B_seqlen, stride_B_head,\n    stride_B_dstate, stride_states_batch, stride_states_chunk,\n    stride_states_head, stride_states_hdim, stride_states_dstate,\n    stride_D_head, HAS_D: 'tl.constexpr', D_HAS_HDIM: 'tl.constexpr', HAS_Z:\n    'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_DSTATE:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_n = tl.program_id(axis=0)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_C_head)\n    B_ptr += (pid_b * stride_B_batch + pid_c * chunk_size * stride_B_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_B_head)\n    prev_states_ptr += (pid_b * stride_states_batch + pid_c *\n        stride_states_chunk + pid_h * stride_states_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    offs_m = tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate[\n        None, :] * stride_C_dstate)\n    B_ptrs = B_ptr + (offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:,\n        None] * stride_B_dstate)\n    prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n        stride_states_hdim + offs_k_dstate[:, None] * stride_states_dstate)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_m[None,\n        :] * stride_cb_csize_k)\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    out_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None,\n        :] * stride_out_hdim)\n    prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:, None] <\n        dstate) & (offs_n[None, :] < hdim), other=0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):\n        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)\n        dA_cs_m = tl.load(dA_cumsum_ptr + (start_m + offs_m) *\n            stride_dA_cs_csize, mask=offs_m < chunk_size - start_m, other=0.0)\n        if HAS_SEQ_IDX:\n            seq_idx_prev = tl.load(seq_idx_ptr + start_m -\n                stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n            seq_idx_m = tl.load(seq_idx_ptr + (start_m + offs_m) *\n                stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit -\n                start_m, other=-1)\n        if not HAS_SEQ_IDX:\n            scale_m = tl.exp(dA_cs_m)\n        else:\n            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit -\n            start_m) & (offs_k_dstate[None, :] < dstate), other=0.0)\n        acc = tl.dot(C, prev_states) * scale_m[:, None]\n        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0)\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit -\n            start_m) & (offs_n[None, :] < hdim), other=0.0)\n        if HAS_D:\n            if D_HAS_HDIM:\n                D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=\n                    offs_n < hdim, other=0.0)\n            else:\n                D = tl.load(D_ptr + pid_h * stride_D_head)\n            acc += x * D\n        tl.store(out_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit -\n            start_m) & (offs_n[None, :] < hdim))\n        if start_m + BLOCK_SIZE_M < chunk_size_limit:\n            B = tl.load(B_ptrs, mask=(offs_m[None, :] < chunk_size_limit -\n                start_m) & (offs_k_dstate[:, None] < dstate), other=0.0)\n            dA_cs_last = tl.load(dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) *\n                stride_dA_cs_csize)\n            scale = tl.exp(dA_cs_last - dA_cs_m) * dt_m\n            B = B\n            tmp = tl.dot(B, x)\n            prev_states += tmp\n        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen\n        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen\n        cb_ptrs += (BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M *\n            stride_cb_csize_k)\n        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize\n        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "26713e33-569f-4acd-9188-f556d57841fb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K',\n    'NO_GROUPS'])\n@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // infearure_per_bits * stride_zeros_n\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = zeros * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "0e009f5e-4b8c-4119-9b2a-a2306f6a13dc"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'UNUSED': 1}, num_stages=\n    num_stages, num_warps=num_warps) for num_stages in (1, 2, 3, 4, 5) for\n    num_warps in (1, 2, 4, 8)], key=['in_features', 'out_features',\n    'num_codebooks', 'codebook_size', 'out_group_size', 'in_group_size',\n    'num_input_groups', 'num_input_groups_next_power_of_2',\n    'compute_in_fp32', 'has_output_scale', 'has_bias'])\n@triton.jit\ndef _aqlm_gemv_simple(input_vec_ptr, output_vec_ptr, codes_ptr,\n    codebooks_ptr, scales_ptr, bias_ptr, in_features: 'tl.constexpr',\n    out_features: 'tl.constexpr', num_codebooks: 'tl.constexpr',\n    codebook_size: 'tl.constexpr', out_group_size: 'tl.constexpr',\n    in_group_size: 'tl.constexpr', num_input_groups: 'tl.constexpr',\n    num_input_groups_next_power_of_2: 'tl.constexpr', compute_in_fp32:\n    'tl.constexpr', has_output_scale: 'tl.constexpr', has_bias:\n    'tl.constexpr', UNUSED: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    input_vec = tl.load(input_vec_ptr + tl.arange(0,\n        num_input_groups_next_power_of_2)[:, None, None, None] *\n        in_group_size + tl.arange(0, in_group_size)[None, None, None, :],\n        mask=tl.arange(0, num_input_groups_next_power_of_2)[:, None, None,\n        None] < num_input_groups)\n    dtype = input_vec.dtype\n    codes_i_ptrs = (codes_ptr + pid * num_input_groups * num_codebooks + tl\n        .arange(0, num_input_groups_next_power_of_2)[:, None] *\n        num_codebooks + tl.arange(0, num_codebooks)[None, :])\n    codes_i_mask_1d = tl.arange(0, num_input_groups_next_power_of_2\n        ) < num_input_groups\n    codes_i = tl.load(codes_i_ptrs, mask=codes_i_mask_1d[:, None])\n    codes_i = codes_i\n    codes_i = codes_i + (codes_i < 0) * codebook_size\n    codes_i += tl.arange(0, num_codebooks)[None, :] * codebook_size\n    out_group_ix = tl.arange(0, out_group_size)[None, None, :, None]\n    in_group_ix = tl.arange(0, in_group_size)[None, None, None, :]\n    weight_i_ptrs = (codebooks_ptr + codes_i[:, :, None, None] *\n        out_group_size * in_group_size + out_group_ix * in_group_size +\n        in_group_ix)\n    weights_i = tl.load(weight_i_ptrs, mask=codes_i_mask_1d[:, None, None,\n        None], other=0)\n    if compute_in_fp32:\n        weights_i = weights_i\n        input_vec = input_vec\n    output_i = weights_i * input_vec\n    if out_group_size == 1:\n        output_i = tl.sum(output_i)\n    else:\n        output_i = tl.sum(output_i, axis=1)\n        output_i = tl.sum(output_i, axis=2)\n        output_i = tl.sum(output_i, axis=0)\n    if has_output_scale:\n        output_i *= tl.load(scales_ptr + pid)\n    if has_bias:\n        output_i += tl.load(bias_ptr + pid)\n    if out_group_size == 1:\n        tl.store(output_vec_ptr + pid, output_i)\n    else:\n        tl.store(output_vec_ptr + pid * out_group_size + tl.arange(0,\n            out_group_size), output_i)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "3d6ca870-1488-47e9-9eca-f5bc8a95bc7e"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b\n        )\n    d_q = d_q * scale\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t), (\n        i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d),\n            (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h, (V, NT * K), (1, s_h_t),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h, (NT * K, V), (s_h_t, 1),\n            (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * s_vo_h, (T, V),\n            (s_vo_t, s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s,\n            b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_ds = b_ds * d_s\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    p_dq = tl.make_block_ptr(dq + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0ea582b7-8b94-410a-87ef-04a7831c32a1"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_gsa_inference_kernel(q, k, v, s, g, o, hk0, hv0, hkt,\n    hvt, scale, K: 'tl.constexpr', V: 'tl.constexpr', M: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NG: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    i_bg = i_bh // NG\n    b_s = tl.load(s + i_bg * M + tl.arange(0, M))\n    b_g = tl.load(g + i_bg * M + tl.arange(0, M))\n    b_g = tl.exp(b_g)\n    b_ok = tl.zeros([M], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        p_hk0 = hk0 + i_bg * K * M + o_k[None, :] * M + tl.arange(0, M)[:, None\n            ]\n        mask_k = o_k < K\n        mask_hk = (tl.arange(0, M) < M)[:, None] & mask_k[None, :]\n        b_hk = tl.load(p_hk0, mask=mask_hk, other=0.0)\n        b_q = tl.load(q + i_bh * K + o_k, mask=mask_k, other=0.0) * scale\n        b_k = tl.load(k + i_bg * K + o_k, mask=mask_k, other=0.0)\n        b_hk = b_hk * b_g[:, None] + b_k[None, :] * b_s[:, None]\n        b_ok += tl.sum(b_hk * b_q[None, :], axis=1)\n        if i_bh % NG == 0:\n            p_hkt = hkt + i_bg * K * M + o_k[None, :] * M + tl.arange(0, M)[\n                :, None]\n            tl.store(p_hkt, b_hk, mask=mask_hk)\n    b_qv = tl.softmax(b_ok)\n    for i_v in range(tl.cdiv(V, BV)):\n        o_v = i_v * BV + tl.arange(0, BV)\n        p_hv0 = hv0 + i_bg * M * V + tl.arange(0, M)[None, :] * V + o_v[:, None\n            ]\n        mask_v = o_v < V\n        mask_hv = mask_v[:, None] & (tl.arange(0, M) < M)[None, :]\n        b_hv = tl.load(p_hv0, mask=mask_hv, other=0)\n        b_v = tl.load(v + i_bg * V + o_v, mask=mask_v, other=0)\n        b_hv = b_hv * b_g[None, :] + b_s[None, :] * b_v[:, None]\n        b_ov = tl.sum(b_hv * b_qv[None, :], axis=1)\n        tl.store(o + i_bh * V + o_v, b_ov, mask=mask_v)\n        if i_bh % NG == 0:\n            p_hvt = hvt + i_bg * M * V + tl.arange(0, M)[None, :] * V + o_v[\n                :, None]\n            tl.store(p_hvt, b_hv, mask=mask_hv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "05088c20-5174-4fe1-a5a7-778f1997a1f3"
  },
  {
    "input": "@triton.jit\ndef bwd_kernel_dk_dv(Q, K, V, B, sm_scale, Out, DO, DK, DV, L, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_bz,\n    stride_bh, stride_bm, stride_bn, stride_oz, stride_oh, stride_om,\n    stride_ok, stride_dkz, stride_dkh, stride_dkn, stride_dkk, stride_dvz,\n    stride_dvh, stride_dvk, stride_dvn, num_head_q: \"'i32'\", num_head_k:\n    \"'i32'\", cu_seqlens_q, cu_seqlens_k, num_seqlens: \"'i32'\", max_seqlen_q:\n    \"'i32'\", max_seqlen_k: \"'i32'\", head_dim: \"'i32'\", dropout_p,\n    philox_seed_ptr, philox_offset1: \"'*u32'\", philox_offset2: \"'u32'\",\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr',\n    PADDED_HEAD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    philox_seed = 0\n    philox_offset_base = philox_offset2\n    if ENABLE_DROPOUT:\n        philox_seed = tl.load(philox_seed_ptr)\n        philox_offset_base += tl.load(philox_offset1)\n    start_k = tl.program_id(0) * BLOCK_N\n    off_h_k = tl.program_id(1)\n    off_z = tl.program_id(2)\n    num_z = tl.num_programs(2)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_n = start_k + tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    ld_offs_d = None if not PADDED_HEAD else tl.arange(0, BLOCK_DMODEL)\n    cu_seqlens_q_start = 0\n    cu_seqlens_k_start = 0\n    seqlen_q = max_seqlen_q\n    seqlen_k = max_seqlen_k\n    batch_index = off_z\n    if num_seqlens > 0:\n        cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)\n        cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)\n        seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start\n        if start_k >= seqlen_k:\n            return\n        cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n        cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n        seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n        batch_index = 0\n    if num_seqlens < 0:\n        cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)\n        cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)\n        seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start\n        if start_k >= seqlen_k:\n            return\n        cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)\n        cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)\n        seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start\n        cu_seqlens_q_start = 0\n        cu_seqlens_k_start = 0\n        batch_index = off_z\n    k_offset = (off_h_k * stride_kh + batch_index * stride_kz + \n        cu_seqlens_k_start * stride_kn)\n    K += k_offset\n    kt_ptrs = K + offs_d[:, None] * stride_kk + offs_n[None, :] * stride_kn\n    if start_k + BLOCK_N <= seqlen_k:\n        kt = load_fn(kt_ptrs, ld_offs_d, None, head_dim, seqlen_k)\n    else:\n        kt = load_fn(kt_ptrs, ld_offs_d, offs_n, head_dim, seqlen_k)\n    v_offset = (off_h_k * stride_vh + batch_index * stride_vz + \n        cu_seqlens_k_start * stride_vk)\n    V += v_offset\n    vt_ptrs = V + offs_d[:, None] * stride_vn + offs_n[None, :] * stride_vk\n    if start_k + BLOCK_N <= seqlen_k:\n        vt = load_fn(vt_ptrs, ld_offs_d, None, head_dim, seqlen_k)\n    else:\n        vt = load_fn(vt_ptrs, ld_offs_d, offs_n, head_dim, seqlen_k)\n    if BIAS_TYPE == 0:\n        B_block_ptr = 0\n    elif BIAS_TYPE == 1:\n        B_block_ptr = tl.make_block_ptr(base=B + off_h_k * stride_bh + \n            batch_index * stride_bz, shape=(seqlen_q, seqlen_k), strides=(\n            stride_bm, stride_bn), offsets=(0, start_k), block_shape=(\n            BLOCK_M, BLOCK_N), order=(1, 0))\n    else:\n        tl.static_assert(False, f'Unsupported BIAS_TYPE {BIAS_TYPE}')\n    dk_offset = (off_h_k * stride_dkh + batch_index * stride_dkz + \n        cu_seqlens_k_start * stride_dkn)\n    DK += dk_offset\n    dv_offset = (off_h_k * stride_dvh + batch_index * stride_dvz + \n        cu_seqlens_k_start * stride_dvk)\n    DV += dv_offset\n    dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504089\n    bias_scale = 1.0 / sm_scale\n    group_size = num_head_q // num_head_k\n    q_lo = start_k if CAUSAL else 0\n    q_hi = seqlen_q\n    real_seqlen_q = q_hi - q_lo\n    n_blocks = tl.cdiv(q_hi - q_lo, BLOCK_M)\n    n_extra_tokens = 0\n    if real_seqlen_q < BLOCK_M:\n        n_extra_tokens = BLOCK_M - real_seqlen_q\n    elif real_seqlen_q % BLOCK_M:\n        n_extra_tokens = real_seqlen_q % BLOCK_M\n    is_irregular_q = n_extra_tokens != 0\n    leading_masked_blocks = 0\n    trailing_masked_blocks = 0\n    if CAUSAL:\n        leading_masked_blocks = tl.cdiv(BLOCK_N, BLOCK_M)\n        trailing_masked_blocks = 1 if is_irregular_q else 0\n    else:\n        leading_masked_blocks = 0\n        trailing_masked_blocks = 1 if is_irregular_q else 0\n    n_full_blocks = n_blocks - leading_masked_blocks - trailing_masked_blocks\n    for off_h_q in range(off_h_k * group_size, off_h_k * group_size +\n        group_size):\n        off_zh = off_z * num_head_q + off_h_q * 1\n        if ENABLE_DROPOUT:\n            batch_philox_offset = (philox_offset_base + off_zh *\n                max_seqlen_q * max_seqlen_k)\n        else:\n            batch_philox_offset = 0\n        D_ptrs = D + off_zh * max_seqlen_q\n        l_ptrs = L + off_zh * max_seqlen_q\n        q_offset = (off_h_q * stride_qh + batch_index * stride_qz + \n            cu_seqlens_q_start * stride_qm)\n        q_ptrs = Q + q_offset + offs_m[:, None] * stride_qm + offs_d[None, :\n            ] * stride_qk\n        do_offset = (off_h_q * stride_oh + batch_index * stride_oz + \n            cu_seqlens_q_start * stride_om)\n        do_ptrs = DO + do_offset + offs_m[:, None] * stride_om + offs_d[None, :\n            ] * stride_ok\n        lo = 0\n        hi = 0\n        if leading_masked_blocks > 0:\n            lo = q_lo\n            hi = lo + leading_masked_blocks * BLOCK_M\n            overflow_size = 0 if hi < q_hi else hi - q_hi\n            dk, dv = bwd_inner_dk_dv(dk, dv, qk_scale, bias_scale, q_ptrs,\n                stride_qm, kt, vt, B_block_ptr, do_ptrs, stride_om, l_ptrs,\n                D_ptrs, seqlen_q, seqlen_k, head_dim, start_k, lo, hi,\n                overflow_size, dropout_p, philox_seed, batch_philox_offset,\n                max_seqlen_k, BLOCK_M, BLOCK_DMODEL, BLOCK_N, False, CAUSAL,\n                ENABLE_DROPOUT, PADDED_HEAD, BIAS_TYPE)\n            tl.debug_barrier()\n        if n_full_blocks > 0:\n            lo = q_lo + leading_masked_blocks * BLOCK_M\n            hi = lo + n_full_blocks * BLOCK_M\n            dk, dv = bwd_inner_dk_dv(dk, dv, qk_scale, bias_scale, q_ptrs,\n                stride_qm, kt, vt, B_block_ptr, do_ptrs, stride_om, l_ptrs,\n                D_ptrs, seqlen_q, seqlen_k, head_dim, start_k, lo, hi, 0,\n                dropout_p, philox_seed, batch_philox_offset, max_seqlen_k,\n                BLOCK_M, BLOCK_DMODEL, BLOCK_N, True, False, ENABLE_DROPOUT,\n                PADDED_HEAD, BIAS_TYPE)\n        if n_full_blocks >= 0 and trailing_masked_blocks > 0:\n            tl.debug_barrier()\n            lo = (q_lo + leading_masked_blocks * BLOCK_M + n_full_blocks *\n                BLOCK_M)\n            hi = q_hi\n            overflow_size = lo + trailing_masked_blocks * BLOCK_M - q_hi\n            dk, dv = bwd_inner_dk_dv(dk, dv, qk_scale, bias_scale, q_ptrs,\n                stride_qm, kt, vt, B_block_ptr, do_ptrs, stride_om, l_ptrs,\n                D_ptrs, seqlen_q, seqlen_k, head_dim, start_k, lo, hi,\n                overflow_size, dropout_p, philox_seed, batch_philox_offset,\n                max_seqlen_k, BLOCK_M, BLOCK_DMODEL, BLOCK_N, False, CAUSAL,\n                ENABLE_DROPOUT, PADDED_HEAD, BIAS_TYPE)\n    dk = dk * sm_scale\n    dv = dv\n    mstore2d(dk, BLOCK_N, BLOCK_DMODEL, o_base=DK, o_start_row=start_k,\n        o_start_col=0, o_rows=seqlen_k, o_cols=head_dim, stride_row=\n        stride_dkn, stride_col=stride_dkk)\n    mstore2d(dv, BLOCK_N, BLOCK_DMODEL, o_base=DV, o_start_row=start_k,\n        o_start_col=0, o_rows=seqlen_k, o_cols=head_dim, stride_row=\n        stride_dvk, stride_col=stride_dvn)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f4c190b8-91b5-4d65-844b-18d40ce507f6"
  },
  {
    "input": "@triton.jit\ndef store_full_1d(vals, ptr, sz: 'const', stride=1):\n    \"\"\"Store 1d block into vector (defined by ptr)\"\"\"\n    offs = offset_1d(sz)\n    mask = mask_1d(offs, sz)\n    tl.store(ptr + offs, vals, mask)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "96cef924-6120-44ad-9c1d-778c142e046e"
  },
  {
    "input": "@triton.jit\ndef selu_grad(input):\n    \"\"\"\n    Calculates the gradient of SELU.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Gradient of SELU.\n    \"\"\"\n    scale = 1.0507009873554805\n    alpha = 1.6732632423543772\n    return scale * tl.where(input <= 0, alpha * tl.exp(input), 1)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "fe3d806d-5442-4e0b-9823-92c2bae0637d"
  },
  {
    "input": "@triton.jit\ndef mul_scalar_kernel(x_ptr, scalar, output_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = x * scalar\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "58db484b-9de3-4a2c-8f0c-5acda61541d7"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 16}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 16}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=4,\n    num_warps=8), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=4,\n    num_warps=8)], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_kernel_old(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddAcs_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddAcs_batch, stride_ddAcs_chunk, stride_ddAcs_head,\n    stride_ddAcs_csize_m, stride_ddAcs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None,\n        :] * stride_cb_csize_n)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] <\n        chunk_size_limit_n), other=0.0)\n    acc = tl.dot(dout, x)\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[\n        None, :] < chunk_size), other=0.0)\n    acc *= cb\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0)\n    acc *= dt_n\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    dA_cs_n = tl.load(dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=\n        offs_n < chunk_size, other=0.0)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] >= offs_n[None, :] + 1\n    acc = tl.where(mask, acc, 0.0)\n    acc = tl.cumsum(acc, axis=1)\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    ddAcs_ptr += (pid_b * stride_ddAcs_batch + pid_c * stride_ddAcs_chunk +\n        pid_h * stride_ddAcs_head + pid_m * stride_ddAcs_csize_m)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n\n    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < \n        chunk_size - 1)\n    tl.store(ddAcs_ptr, 0.0)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "090c77c4-c887-44d0-bf49-c5a3be21cfc2"
  },
  {
    "input": "@triton.jit\ndef _total_attention_kernel(Q, K, L, TA, sm_scale, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, Z, H,\n    M, N, P_SEQ, num_groups, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL: 'tl.constexpr',\n    DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    start_n = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    off_hk = off_h // num_groups\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_hk * stride_kh\n    L += (off_z * H + off_h) * M\n    TA += (off_z * H + off_h) * N\n    if CAUSAL:\n        lo = tl.maximum(start_n * BLOCK_N - P_SEQ, 0)\n        lo = lo // BLOCK_M * BLOCK_M\n    else:\n        lo = 0\n    offs_m_init = lo + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m_base = tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m_init[:, None] * stride_qm + offs_k[None, :] *\n        stride_qk)\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)\n    ta_ptrs = TA + offs_n\n    if DIVISIBLE_N:\n        k = tl.load(k_ptrs)\n    else:\n        mask_n = offs_n < N\n        k = tl.load(k_ptrs, mask=mask_n[:, None])\n    tot_attn = tl.zeros([BLOCK_N], dtype=tl.float32)\n    for start_m in range(lo, M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m = start_m + offs_m_base\n        causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n        if DIVISIBLE_M:\n            q = tl.load(q_ptrs)\n        else:\n            mask_m = offs_m < M\n            valid_mask = mask_m[:, None]\n            q = tl.load(q_ptrs, mask=mask_m[:, None])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k))\n        if DIVISIBLE_M:\n            l = tl.load(L + offs_m)\n        else:\n            l = tl.load(L + offs_m, mask=mask_m)\n        p = tl.math.exp2(s * qk_scale - l[:, None] * log2e)\n        if not DIVISIBLE_M:\n            p = tl.where(valid_mask, p, 0.0)\n        if CAUSAL:\n            p = tl.where(causal_mask, p, 0.0)\n        tot_attn += tl.sum(p, 0)\n        q_ptrs += BLOCK_M * stride_qm\n    if DIVISIBLE_N:\n        tl.store(ta_ptrs, tot_attn)\n    else:\n        tl.store(ta_ptrs, tot_attn, mask=mask_n)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "8060268c-f4b4-45a9-a07b-8d4b446b0b59"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS,\n    NUM_STAGES)], key=['M'])\n@triton.jit\ndef triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer(\n    input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_b), tl.load(\n        input_ptr_offsets + (pid_b + 1))\n    ragged_len = ragged_end - ragged_start\n    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED\n        ):\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n        buffer += tl.sum(input, axis=0)\n    buffer_view = buffer.reshape((BLOCK_SIZE_M,))\n    buffer_view_mean = buffer_view * (1 / ragged_len)\n    output_offsets = offsets_m + pid_b * M\n    output_mask = output_offsets < M * (pid_b + 1)\n    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "e1a2f8af-adb0-4528-81e6-a37478729db8"
  },
  {
    "input": "@triton.jit\ndef bias_kernel_backward(d_weights, d_out, weights, stride_om, stride_on,\n    stride_wn, N: 'tl.constexpr', M: 'tl.constexpr', NH: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_NH:\n    'tl.constexpr', BIDIRECTIONAL: 'tl.constexpr', NUM_BUCKETS:\n    'tl.constexpr', MAX_DISTANCE: 'tl.constexpr', GROUP_SIZE_M: 'tl.constexpr'\n    ):\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_M)\n    num_pid_n = tl.cdiv(N, BLOCK_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_m = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    offs_n = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    relative_positions = offs_m[:, None] - offs_n[None, :]\n    relative_buckets = tl.zeros_like(relative_positions)\n    num_buckets = NUM_BUCKETS\n    if BIDIRECTIONAL:\n        num_buckets //= 2\n        relative_buckets += (relative_positions > 0) * num_buckets\n        relative_positions = tl.abs(relative_positions)\n    else:\n        relative_positions = tl.maximum(-relative_positions, tl.zeros_like(\n            relative_positions))\n    max_exact = num_buckets // 2\n    is_small = relative_positions < max_exact\n    relative_position_if_large = max_exact + tl.log(relative_positions.to(\n        tl.float32) / max_exact) / tl.log(MAX_DISTANCE / max_exact) * (\n        num_buckets - max_exact)\n    relative_position_if_large = tl.minimum(relative_position_if_large, \n        num_buckets - 1)\n    relative_buckets += tl.where(is_small, relative_positions,\n        relative_position_if_large)\n    for i in range(0, NH, BLOCK_NH):\n        offs_nh = i + tl.arange(0, BLOCK_NH)\n        bucket_offs = relative_buckets[:, :, None] * stride_wn + offs_nh[\n            None, None, :]\n        d_out_ptrs = d_out + (offs_m[:, None] * stride_om + offs_n[None, :] *\n            stride_on)[:, :, None] + offs_nh[None, None, :]\n        o_mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)[:, :, None] & (\n            offs_nh[None, None, :] < NH)\n        d_out_values = tl.load(d_out_ptrs, mask=o_mask, other=0.0)\n        d_weights_ptrs = d_weights + bucket_offs\n        tl.atomic_add(d_weights_ptrs, d_out_values, mask=relative_buckets[:,\n            :, None] < NUM_BUCKETS)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ba3be97a-c35f-4b54-b101-ee944be6f5d0"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_fwd(C, ACT_INPUT, A, B, bias, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bn, stride_bk,\n    BLOCK_M: 'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', A_ROWMAJOR: 'tl.constexpr', B_COLMAJOR:\n    'tl.constexpr', BIAS: 'tl.constexpr', SAVE_ACT_INPUT: 'tl.constexpr',\n    ACTIVATION: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Bias has shape (N,)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    if A_ROWMAJOR:\n        A = A + (ram[:, None] * stride_am + rk[None, :])\n    else:\n        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    if B_COLMAJOR:\n        B = B + (rk[:, None] + rbn[None, :] * stride_bn)\n    else:\n        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        if A_ROWMAJOR:\n            A += BLOCK_K\n        else:\n            A += BLOCK_K * stride_ak\n        if B_COLMAJOR:\n            B += BLOCK_K\n        else:\n            B += BLOCK_K * stride_bk\n    if BIAS:\n        bias = tl.load(bias + rn, mask=rn < N, other=0.0)\n        acc += bias[None, :]\n    if SAVE_ACT_INPUT:\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        tl.store(act_in_ptrs, acc)\n    if ACTIVATION == 'gelu':\n        acc = gelu(acc)\n    elif ACTIVATION == 'gelu_approx':\n        acc = gelu_approx(acc)\n    elif ACTIVATION == 'squared_relu':\n        acc = squared_relu(acc)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "717722ba-1825-4d73-8960-1c67b8354f33"
  },
  {
    "input": "@triton.jit\ndef tenth_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST001 = 1.75869118663323\n    CONST002 = -1021.9231747532\n    CONST004 = 4.58257569495584\n    CONST005 = 6.632439808434\n    CONST006 = 4.82870805793735\n    CONST007 = 4.9743298563255\n    CONST008 = 1545.18657853995\n    CONST009 = 10.5521471197994\n    CONST010 = 12.1657520803952\n    CONST011 = 13.264879616868\n    CONST013 = 15.7883647328499\n    CONST014 = 15.7302121789667\n    CONST015 = 16.4144510752435\n    CONST016 = 12.8765548211663\n    CONST017 = 19.3148322317494\n    CONST018 = 16.7271353825295\n    CONST019 = 22.862985426232\n    CONST020 = 535.268332240943\n    CONST021 = 23.213539329519\n    CONST022 = 24.6216766128653\n    CONST023 = 27.2034486491732\n    CONST024 = 541.428124558099\n    CONST025 = -994.666978169547\n    CONST026 = 33.9852909359329\n    CONST027 = 33.9852909359329\n    CONST028 = 35.5238206489124\n    CONST029 = -984.86706451461\n    CONST030 = -4.82870805793735\n    CONST031 = 1070.53666448189\n    CONST032 = -463.555973561985\n    CONST034 = 53.2857309733686\n    CONST035 = 53.2857309733686\n    CONST036 = 56.3871618715269\n    CONST037 = 56.3871618715269\n    CONST039 = -1989.33395633909\n    CONST041 = -450.224943778107\n    CONST042 = 66.9085415301178\n    CONST043 = 69.640617988557\n    CONST044 = 69.640617988557\n    CONST045 = -437.967074894228\n    CONST046 = 77.2593289269976\n    CONST047 = 78.6510608948335\n    CONST049 = -1969.73412902922\n    CONST050 = 77.3468749368712\n    CONST051 = 1624.2843736743\n    CONST054 = 94.7301883970997\n    CONST056 = 100.362812295177\n    CONST057 = -412.04975427732\n    CONST058 = 101.517773354644\n    CONST059 = -5.63871618715269\n    CONST060 = -406.071093418574\n    CONST061 = 109.491768723557\n    CONST062 = -393.946825805844\n    CONST063 = -902.194589944431\n    CONST065 = -386.296644634988\n    CONST066 = -386.296644634988\n    CONST070 = 4.9743298563255\n    CONST071 = 150.074981259369\n    CONST074 = 685.526905959165\n    CONST075 = -337.668707833581\n    CONST076 = -337.668707833581\n    CONST077 = 176.178376404427\n    CONST078 = 176.592751833137\n    CONST079 = 185.708314636152\n    CONST080 = -326.441383790078\n    CONST081 = -1.60956935264578\n    CONST082 = -1.97354559160624\n    CONST083 = 196.973412902922\n    CONST085 = -824.099508554641\n    CONST087 = -1.97354559160624\n    CONST088 = -305.867618423396\n    CONST089 = -305.867618423396\n    CONST090 = 721.755671955545\n    CONST091 = -305.867618423396\n    CONST092 = -300.731529981477\n    CONST093 = -300.731529981477\n    CONST094 = -1.75869118663323\n    CONST095 = -290.050781013267\n    CONST097 = 225.548647486108\n    CONST098 = 225.548647486108\n    CONST099 = -284.190565191299\n    CONST101 = -278.562471954228\n    CONST102 = -278.562471954228\n    CONST103 = -787.893651611688\n    CONST104 = -787.893651611688\n    CONST105 = 772.593289269975\n    CONST106 = 787.893651611688\n    CONST107 = 787.893651611688\n    CONST108 = 278.562471954228\n    CONST109 = -742.833258544608\n    CONST110 = -1.6581099521085\n    CONST112 = -1761.78376404427\n    CONST113 = -223.028471767059\n    CONST114 = -734.07656835178\n    CONST116 = -220.222970505534\n    CONST117 = 1321.3378230332\n    CONST118 = 1321.3378230332\n    CONST119 = -203.035546709287\n    CONST120 = -1.6581099521085\n    CONST121 = -196.973412902922\n    CONST122 = -196.973412902922\n    CONST123 = -696.40617988557\n    CONST125 = 338.322971229162\n    CONST126 = -1181.84047741753\n    CONST127 = -669.085415301178\n    CONST128 = -669.085415301178\n    CONST129 = -154.518657853995\n    CONST130 = -154.518657853995\n    CONST131 = 360.877835977772\n    CONST132 = -150.074981259369\n    CONST133 = -2707.14062279049\n    CONST134 = -146.815313670356\n    CONST135 = 880.891882022136\n    CONST136 = 1392.81235977114\n    CONST137 = 1392.81235977114\n    CONST138 = -131.315608601948\n    CONST139 = -131.315608601948\n    CONST141 = -125.841697431734\n    CONST142 = -125.841697431734\n    CONST143 = -122.415518921279\n    CONST145 = 406.071093418574\n    CONST146 = -103.107953136506\n    CONST147 = -103.107953136506\n    CONST148 = -101.517773354644\n    CONST149 = -98.486706451461\n    CONST150 = 412.04975427732\n    CONST151 = -94.7301883970997\n    CONST152 = -1114.24988781691\n    CONST153 = -88.2963759165686\n    CONST154 = -1624.2843736743\n    CONST155 = -82.8889148474622\n    CONST156 = -82.8889148474622\n    CONST158 = -590.920238708766\n    CONST159 = -77.3468749368713\n    CONST160 = -77.2593289269975\n    CONST161 = 2486.66744542387\n    CONST162 = -2626.31217203896\n    CONST165 = -571.272421632637\n    CONST166 = -56.2781179722634\n    CONST167 = -49.2433532257305\n    CONST168 = -49.2433532257305\n    CONST169 = 984.86706451461\n    CONST170 = -541.428124558099\n    CONST171 = -24.6216766128653\n    CONST172 = -22.862985426232\n    CONST173 = -16.4144510752435\n    CONST174 = -15.7883647328499\n    CONST175 = -14.0695294930659\n    CONST176 = -13.264879616868\n    CONST177 = -11.2774323743054\n    CONST178 = -14.5025390506634\n    CONST179 = -6.632439808434\n    CONST180 = -5.63871618715269\n    CONST181 = 1532.8847621298\n    CONST182 = -3.21913870529156\n    CONST183 = -2.72034486491732\n    CONST184 = -1.12774323743054\n    VAR05 = x * x * x * x * x\n    VAR06 = x * x * x * x\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR00 = VAR05 * VAR05\n    VAR01 = VAR05 * VAR06\n    VAR02 = VAR06 * VAR06\n    VAR03 = VAR06 * VAR07\n    VAR04 = VAR07 * VAR07\n    VAR14 = y * y * y * y * y\n    VAR15 = y * y * y * y\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR09 = VAR14 * VAR14\n    VAR10 = VAR14 * VAR15\n    VAR11 = VAR15 * VAR15\n    VAR12 = VAR15 * VAR16\n    VAR13 = VAR16 * VAR16\n    VAR23 = z * z * z * z * z\n    VAR24 = z * z * z * z\n    VAR25 = z * z * z\n    VAR26 = z * z\n    VAR18 = VAR23 * VAR23\n    VAR19 = VAR23 * VAR24\n    VAR20 = VAR24 * VAR24\n    VAR21 = VAR24 * VAR25\n    VAR22 = VAR25 * VAR25\n    Y00 = (CONST023 * VAR01 * z + CONST023 * VAR19 * x + CONST074 * VAR05 *\n        VAR23 + CONST080 * VAR03 * VAR25 + CONST080 * VAR07 * VAR21)\n    Y01 = y * (CONST002 * VAR07 * VAR22 + CONST010 * VAR01 + CONST045 *\n        VAR03 * VAR26 + CONST061 * VAR20 * x + CONST181 * VAR05 * VAR24)\n    Y02 = (CONST013 * VAR01 * z + CONST054 * VAR07 * VAR21 + CONST151 *\n        VAR03 * VAR25 + CONST174 * VAR19 * x + VAR17 * (-CONST039 * VAR05 *\n        VAR25 + CONST039 * VAR07 * VAR23 + CONST099 * VAR03 * z - CONST099 *\n        VAR21 * x))\n    Y03 = VAR16 * (CONST024 * VAR22 * x + CONST051 * VAR05 * VAR26 + \n        CONST133 * VAR07 * VAR24 + CONST159 * VAR03) + y * (CONST095 *\n        VAR03 * VAR26 - CONST119 * VAR05 * VAR24 + CONST145 * VAR07 * VAR22 +\n        CONST148 * VAR20 * x - CONST178 * VAR01)\n    Y04 = CONST009 * VAR01 * z + VAR03 * (CONST076 * VAR17 * z + CONST175 *\n        VAR25) + VAR05 * (CONST106 * VAR15 * z + CONST107 * VAR17 * VAR25 +\n        CONST167 * VAR23) + VAR07 * (CONST106 * VAR17 * VAR23 + CONST162 *\n        VAR15 * VAR25 + CONST175 * VAR21) + x * (CONST009 * VAR19 + \n        CONST075 * VAR17 * VAR21 + CONST106 * VAR15 * VAR23)\n    Y05 = VAR14 * (CONST077 * VAR05 + CONST112 * VAR07 * VAR26 + CONST135 *\n        VAR24 * x) + VAR16 * (-CONST114 * VAR07 * VAR24 + CONST114 * VAR22 *\n        x + CONST117 * VAR05 * VAR26 + CONST134 * VAR03) + y * (CONST014 *\n        VAR01 + CONST047 * VAR20 * x + CONST116 * VAR05 * VAR24 + CONST141 *\n        VAR03 * VAR26)\n    Y06 = CONST005 * VAR01 * z + VAR03 * (CONST011 * VAR25 + CONST102 *\n        VAR17 * z) + VAR05 * (CONST101 * VAR17 * VAR25 - CONST152 * VAR15 * z\n        ) + VAR07 * (CONST108 * VAR17 * VAR23 + CONST109 * VAR13 * z + \n        CONST176 * VAR21) + x * (CONST108 * VAR17 * VAR21 - CONST109 *\n        VAR13 * VAR25 + CONST152 * VAR15 * VAR23 + CONST179 * VAR19)\n    Y07 = VAR12 * (-CONST041 * VAR26 * x + CONST132 * VAR07) + VAR14 * (-\n        CONST062 * VAR05 + CONST103 * VAR07 * VAR26 + CONST126 * VAR24 * x\n        ) + VAR16 * (CONST083 * VAR05 * VAR26 + CONST121 * VAR03 - CONST158 *\n        VAR22 * x + CONST169 * VAR07 * VAR24) + y * (CONST015 * VAR01 + \n        CONST138 * VAR07 * VAR22 + CONST149 * VAR05 * VAR24 + CONST168 *\n        VAR20 * x)\n    Y08 = -CONST182 * VAR01 * z + VAR03 * (CONST016 * VAR25 + CONST129 *\n        VAR17 * z) + VAR05 * (CONST017 * VAR23 + CONST032 * VAR17 * VAR25 +\n        CONST105 * VAR15 * z) + VAR07 * (CONST008 * VAR15 * VAR25 + \n        CONST016 * VAR21 + CONST032 * VAR17 * VAR23 + CONST085 * VAR13 * z\n        ) + x * (CONST078 * VAR11 * z + CONST085 * VAR13 * VAR25 + CONST105 *\n        VAR15 * VAR23 + CONST129 * VAR17 * VAR21 - CONST182 * VAR19)\n    Y09 = CONST018 * VAR01 * y + VAR03 * (CONST042 * VAR26 * y + CONST113 *\n        VAR16) + VAR05 * (CONST020 * VAR14 + CONST056 * VAR24 * y + \n        CONST128 * VAR16 * VAR26) + VAR07 * (CONST031 * VAR14 * VAR26 + \n        CONST042 * VAR22 * y + CONST088 * VAR12 + CONST127 * VAR16 * VAR24\n        ) + x * (CONST018 * VAR20 * y + CONST020 * VAR14 * VAR24 + CONST026 *\n        VAR10 + CONST088 * VAR12 * VAR26 + CONST113 * VAR16 * VAR22)\n    Y10 = (CONST004 * VAR09 + CONST037 * VAR17 * VAR20 + CONST093 * VAR15 *\n        VAR22 + CONST131 * VAR13 * VAR24 + CONST147 * VAR11 * VAR26 + \n        CONST184 * VAR00 + CONST184 * VAR18 + VAR02 * (CONST036 * VAR17 + \n        CONST059 * VAR26) + VAR04 * (CONST092 * VAR15 + CONST098 * VAR17 *\n        VAR26 + CONST177 * VAR24) + VAR06 * (CONST063 * VAR15 * VAR26 + \n        CONST125 * VAR17 * VAR24 + CONST131 * VAR13 + CONST177 * VAR22) + \n        VAR08 * (CONST063 * VAR15 * VAR24 + CONST090 * VAR13 * VAR26 + \n        CONST097 * VAR17 * VAR22 + CONST146 * VAR11 + CONST180 * VAR20))\n    Y11 = CONST018 * VAR19 * y + VAR21 * (CONST042 * VAR08 * y + CONST113 *\n        VAR16) + VAR23 * (CONST020 * VAR14 + CONST056 * VAR06 * y + \n        CONST128 * VAR08 * VAR16) + VAR25 * (CONST031 * VAR08 * VAR14 + \n        CONST042 * VAR04 * y + CONST091 * VAR12 + CONST127 * VAR06 * VAR16\n        ) + z * (CONST018 * VAR02 * y + CONST020 * VAR06 * VAR14 + CONST027 *\n        VAR10 + CONST089 * VAR08 * VAR12 + CONST113 * VAR04 * VAR16)\n    Y12 = (CONST057 * VAR13 * VAR24 - CONST066 * VAR15 * VAR22 + CONST081 *\n        VAR00 - CONST081 * VAR18 - CONST153 * VAR11 * VAR26 + CONST160 *\n        VAR17 * VAR20 + VAR02 * (CONST030 * VAR26 + CONST046 * VAR17) + \n        VAR04 * (CONST066 * VAR15 - CONST129 * VAR17 * VAR26 + CONST182 *\n        VAR24) + VAR06 * (CONST065 * VAR15 * VAR26 + CONST150 * VAR13 - \n        CONST182 * VAR22) + VAR08 * (CONST006 * VAR20 - CONST066 * VAR15 *\n        VAR24 + CONST130 * VAR17 * VAR22 + CONST153 * VAR11))\n    Y13 = VAR12 * (CONST041 * VAR08 * z + CONST071 * VAR25) + VAR14 * (\n        CONST062 * VAR23 + CONST107 * VAR08 * VAR25 - CONST126 * VAR06 * z\n        ) + VAR16 * (CONST029 * VAR06 * VAR25 - CONST121 * VAR21 + CONST122 *\n        VAR08 * VAR23 + CONST158 * VAR04 * z) + y * (-CONST138 * VAR04 *\n        VAR25 - CONST149 * VAR06 * VAR23 - CONST168 * VAR02 * z + CONST173 *\n        VAR19)\n    Y14 = (CONST044 * VAR17 * VAR20 + CONST079 * VAR13 * VAR24 + CONST101 *\n        VAR15 * VAR22 + CONST110 * VAR00 + CONST120 * VAR18 + VAR02 * (\n        CONST043 * VAR17 + CONST070 * VAR26) + VAR04 * (CONST021 * VAR24 + \n        CONST101 * VAR15 + CONST101 * VAR17 * VAR26) + VAR06 * (CONST021 *\n        VAR22 + CONST079 * VAR13 + CONST123 * VAR17 * VAR24 + CONST137 *\n        VAR15 * VAR26) + VAR08 * (CONST007 * VAR20 + CONST101 * VAR17 *\n        VAR22 + CONST136 * VAR15 * VAR24 + CONST152 * VAR13 * VAR26))\n    Y15 = VAR14 * (CONST077 * VAR23 + CONST112 * VAR08 * VAR25 + CONST135 *\n        VAR06 * z) + VAR16 * (CONST114 * VAR04 * z - CONST114 * VAR06 *\n        VAR25 + CONST118 * VAR08 * VAR23 + CONST134 * VAR21) + y * (\n        CONST014 * VAR19 + CONST047 * VAR02 * z + CONST116 * VAR06 * VAR23 +\n        CONST142 * VAR08 * VAR21)\n    Y16 = (CONST001 * VAR18 + CONST094 * VAR00 - CONST139 * VAR15 * VAR22 +\n        CONST166 * VAR17 * VAR20 + VAR02 * (CONST019 * VAR26 - CONST166 *\n        VAR17) + VAR04 * (CONST022 * VAR24 + CONST104 * VAR17 * VAR26 + \n        CONST139 * VAR15) + VAR06 * (-CONST049 * VAR15 * VAR26 + CONST171 *\n        VAR22) + VAR08 * (CONST049 * VAR15 * VAR24 + CONST106 * VAR17 *\n        VAR22 + CONST172 * VAR20))\n    Y17 = VAR16 * (CONST050 * VAR21 - CONST133 * VAR06 * VAR25 + CONST154 *\n        VAR08 * VAR23 + CONST170 * VAR04 * z) + y * (CONST058 * VAR02 * z +\n        CONST060 * VAR04 * VAR25 - CONST095 * VAR08 * VAR21 + CONST119 *\n        VAR06 * VAR23 + CONST178 * VAR19)\n    Y18 = (CONST034 * VAR02 * VAR26 + CONST035 * VAR08 * VAR20 + CONST082 *\n        VAR00 + CONST087 * VAR18 + CONST155 * VAR04 * VAR24 + CONST156 *\n        VAR06 * VAR22 + VAR17 * (CONST025 * VAR04 * VAR26 + CONST025 *\n        VAR08 * VAR22 + CONST028 * VAR02 + CONST028 * VAR20 + CONST161 *\n        VAR06 * VAR24))\n    Y19 = y * (CONST002 * VAR04 * VAR25 + CONST010 * VAR19 + CONST045 *\n        VAR08 * VAR21 + CONST061 * VAR02 * z + CONST181 * VAR06 * VAR23)\n    Y20 = (-CONST143 * VAR02 * VAR26 + CONST143 * VAR08 * VAR20 + CONST165 *\n        VAR04 * VAR24 - CONST165 * VAR06 * VAR22 + CONST183 * VAR00 - \n        CONST183 * VAR18)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y00, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y01, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y02, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y03, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y04, mask=\n        output_row_offset + 4 < output_numel)\n    tl.store(output_ptr + output_row_offset + 5, Y05, mask=\n        output_row_offset + 5 < output_numel)\n    tl.store(output_ptr + output_row_offset + 6, Y06, mask=\n        output_row_offset + 6 < output_numel)\n    tl.store(output_ptr + output_row_offset + 7, Y07, mask=\n        output_row_offset + 7 < output_numel)\n    tl.store(output_ptr + output_row_offset + 8, Y08, mask=\n        output_row_offset + 8 < output_numel)\n    tl.store(output_ptr + output_row_offset + 9, Y09, mask=\n        output_row_offset + 9 < output_numel)\n    tl.store(output_ptr + output_row_offset + 10, Y10, mask=\n        output_row_offset + 10 < output_numel)\n    tl.store(output_ptr + output_row_offset + 11, Y11, mask=\n        output_row_offset + 11 < output_numel)\n    tl.store(output_ptr + output_row_offset + 12, Y12, mask=\n        output_row_offset + 12 < output_numel)\n    tl.store(output_ptr + output_row_offset + 13, Y13, mask=\n        output_row_offset + 13 < output_numel)\n    tl.store(output_ptr + output_row_offset + 14, Y14, mask=\n        output_row_offset + 14 < output_numel)\n    tl.store(output_ptr + output_row_offset + 15, Y15, mask=\n        output_row_offset + 15 < output_numel)\n    tl.store(output_ptr + output_row_offset + 16, Y16, mask=\n        output_row_offset + 16 < output_numel)\n    tl.store(output_ptr + output_row_offset + 17, Y17, mask=\n        output_row_offset + 17 < output_numel)\n    tl.store(output_ptr + output_row_offset + 18, Y18, mask=\n        output_row_offset + 18 < output_numel)\n    tl.store(output_ptr + output_row_offset + 19, Y19, mask=\n        output_row_offset + 19 < output_numel)\n    tl.store(output_ptr + output_row_offset + 20, Y20, mask=\n        output_row_offset + 20 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "4fdbbc9e-6823-484b-b2e9-e0ba18d392e8"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_apply_penalty(Logits, presence_penalty, freqency_penalty,\n    p_token_ids, p_token_counts, p_cumsum_seq_len, stride_logit_b,\n    stride_logit_s, BLOCK_P: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_freqency = tl.load(freqency_penalty + cur_batch)\n    cur_presence = tl.load(presence_penalty + cur_batch)\n    cur_batch_start_index = tl.load(p_cumsum_seq_len + cur_batch)\n    cur_batch_end_index = tl.load(p_cumsum_seq_len + cur_batch + 1)\n    cur_batch_id_offset = cur_batch_start_index + tl.arange(0, BLOCK_P)\n    batch_ids = tl.load(p_token_ids + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    batch_ids_count = tl.load(p_token_counts + cur_batch_id_offset, mask=\n        cur_batch_id_offset < cur_batch_end_index, other=0)\n    row_start_ptr = Logits + cur_batch * stride_logit_b\n    cur_offset = row_start_ptr + batch_ids\n    cur_logits = tl.load(cur_offset, mask=cur_batch_id_offset <\n        cur_batch_end_index, other=0.0)\n    freq_logits = cur_logits - batch_ids_count * cur_freqency\n    pre_logits = freq_logits - cur_presence\n    output_ptr = Logits + cur_batch * stride_logit_b + batch_ids\n    tl.store(output_ptr, pre_logits, mask=cur_batch_id_offset <\n        cur_batch_end_index)\n    return\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "373c1d13-56c1-4c31-b6e3-5b8a5dec3bec"
  },
  {
    "input": "@triton.jit\ndef _bwd_diag_kernel(Q, K, V, S, DO, DQ, DK, DV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_h = off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK\n    qk_block_offset = block_offset * d\n    v_block_offset = block_offset * e\n    o_block_offset = block_offset * e\n    Q_trans_block_ptr = Q + qk_offset + qk_block_offset + tl.arange(0, BLOCK)[\n        None, :] * d + tl.arange(0, d)[:, None]\n    K_block_ptr = K + qk_offset + qk_block_offset + tl.arange(0, BLOCK)[:, None\n        ] * d + tl.arange(0, d)[None, :]\n    V_trans_block_ptr = V + v_offset + v_block_offset + tl.arange(0, BLOCK)[\n        None, :] * e + tl.arange(0, e)[:, None]\n    DQ_block_ptr = DQ + qk_offset + qk_block_offset + tl.arange(0, BLOCK)[:,\n        None] * d + tl.arange(0, d)[None, :]\n    DK_trans_block_ptr = DK + qk_offset + qk_block_offset + tl.arange(0, BLOCK\n        )[None, :] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + v_block_offset + tl.arange(0, BLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    DO_block_ptr = DO + o_offset + o_block_offset + tl.arange(0, BLOCK)[:, None\n        ] * e + tl.arange(0, e)[None, :]\n    S_block_ptr = S + off_h\n    s = tl.load(S_block_ptr)\n    array = tl.arange(0, BLOCK)\n    index = array[:, None] - array[None, :]\n    s_index = s * index\n    s_index = tl.where(index >= 0, -s_index, float('-inf'))\n    diag_decay = tl.exp(s_index)\n    diag_decay_trans = tl.trans(diag_decay)\n    k = tl.load(K_block_ptr)\n    v_trans = tl.load(V_trans_block_ptr)\n    do = tl.load(DO_block_ptr)\n    q_trans = tl.load(Q_trans_block_ptr)\n    dqk = tl.dot(do, v_trans) * diag_decay\n    dq_diag = tl.dot(dqk, k)\n    dq = dq_diag\n    dk_diag_trans = tl.dot(q_trans, dqk)\n    qk_trans = tl.dot(k, q_trans) * diag_decay_trans\n    dv_diag = tl.dot(qk_trans, do)\n    dk_trans = dk_diag_trans\n    dv = dv_diag\n    tl.store(DQ_block_ptr, dq)\n    tl.store(DK_trans_block_ptr, dk_trans)\n    tl.store(DV_block_ptr, dv)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5140ed1b-5f4e-44ec-bdc7-ec72553828ea"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef softmax_bwd_kernel(p, dp, ds, D: 'tl.constexpr', B: 'tl.constexpr'):\n    i_n = tl.program_id(0)\n    o_d = tl.arange(0, B)\n    m_d = o_d < D\n    b_p = tl.load(p + i_n * D + o_d, mask=m_d, other=0.0)\n    b_dp = tl.load(dp + i_n * D + o_d, mask=m_d, other=0.0)\n    b_pp = tl.sum(b_p * b_dp, 0)\n    b_ds = b_p * b_dp - b_p * b_pp\n    tl.store(ds + i_n * D + o_d, b_ds, mask=m_d)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d0133910-2a68-40fd-acc1-4c7fa44ccdd4"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, Req_to_tokens, B_req_idx,\n    B_Start_Loc, B_Seqlen, B_Att_Start_Loc, B_Att_Seqlen, Att_Out,\n    stride_req_to_tokens_b, stride_req_to_tokens_s, stride_qbs, stride_qh,\n    stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, kv_group_num, sliding_window, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Att_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_att_seq_len = tl.load(B_Att_Seqlen + cur_batch)\n    cur_batch_start_index = tl.maximum(cur_batch_seq_len - sliding_window, 0)\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_att_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value = att_value\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "20af7b6f-1307-4a46-a4ca-8f35c340338e"
  },
  {
    "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=['D'])\n@triton.jit\ndef _ln_mul_dropout_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, N, D, BLOCK_N:\n    'tl.constexpr', BLOCK_D: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)\n    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    db = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = (rows[:, None] < N) & (cols[None, :] < D)\n        offs = rows[:, None] * D + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < D)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < D)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d7d19db7-334c-4cee-bc35-05b7dbfe9f0c"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, B, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "ebb1a36d-9f8b-4fad-86f2-e33c226e51f8"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV', 'USE_G', 'USE_GK', 'USE_GV'])\n@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not\n    None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})\n@triton.jit\ndef chunk_fwd_kernel_h(k, v, h, g, gk, gv, h0, ht, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', USE_G: 'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for i_t in range(NT):\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V),\n            (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        last_idx = min((i_t + 1) * BT, T) - 1\n        if USE_G:\n            if HEAD_FIRST:\n                b_g_last = tl.load(g + i_bh * T + last_idx)\n                p_g = g + i_bh * T + i_t * BT + tl.arange(0, BT)\n                p_g = tl.max_contiguous(tl.multiple_of(p_g, BT), BT)\n            else:\n                b_g_last = tl.load(g + i_b * T * H + last_idx * H + i_h)\n                p_g = g + i_b * T * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n            b_h *= tl.exp(b_g_last)\n            b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n            b_v = b_v * tl.exp(b_g_last - b_g)[:, None]\n        if USE_GK:\n            if HEAD_FIRST:\n                p_gk = tl.make_block_ptr(gk + i_bh * T * K, (K, T), (1, K),\n                    (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_bh * T * K + last_idx * K + i_k * BK +\n                    tl.arange(0, BK))\n            else:\n                p_gk = tl.make_block_ptr(gk + i_b * T * H * K + i_h * K, (K,\n                    T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_b * T * H * K + last_idx * H * K + i_h *\n                    K + i_k * BK + tl.arange(0, BK))\n            p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) <\n                K, other=0.0)\n            b_h *= tl.exp(b_gk_last)[:, None]\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_k = b_k * tl.exp(b_gk_last[:, None] - b_gk)\n        if USE_GV:\n            if HEAD_FIRST:\n                p_gv = tl.make_block_ptr(gv + i_bh * T * V, (T, V), (V, 1),\n                    (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_bh * T * V + last_idx * V + i_v * BV +\n                    tl.arange(0, BV))\n            else:\n                p_gv = tl.make_block_ptr(gv + i_b * T * H * V + i_h * V, (T,\n                    V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_b * T * H * V + last_idx * H * V + i_h *\n                    V + i_v * BV + tl.arange(0, BV))\n            p_gv_last = tl.max_contiguous(tl.multiple_of(p_gv_last, BV), BV)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) <\n                V, other=0.0)\n            b_h *= tl.exp(b_gv_last)[None, :]\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_v = b_v * tl.exp(b_gv_last[None, :] - b_gv)\n        b_h += tl.dot(b_k, b_v)\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5b3477b2-5e68-42c3-a680-55f9ab020975"
  },
  {
    "input": "@triton.jit\ndef chunk_simple_gla_fwd_kernel_o(q, k, v, h, g, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    p_g = g + i_bh * T + i_t * BT + tl.arange(0, BT)\n    b_g = tl.load(p_g)\n    b_o = b_o * tl.math.exp2(b_g)[:, None]\n    b_s = b_s * tl.math.exp2(b_g[:, None] - b_g[None, :])\n    b_s = tl.where(m_s, b_s, 0)\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "8784bdb9-17c9-4c57-8e02-4b6da217304d"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "438d95ea-ea71-4bbf-b04f-af6c067310b4"
  },
  {
    "input": "@triton.jit\ndef tl_softcapping_grad(dv: 'tl.tensor', v: 'tl.tensor', softcap: 'float'\n    ) ->tl.tensor:\n    v = v / softcap\n    return dv * (1 - v * v)\n",
    "category": "Math Utils",
    "subcategory": "exponential",
    "uuid": "aeaa8b2b-66f3-4eb2-87f1-541b101f4efa"
  },
  {
    "input": "@triton.jit\ndef _group_norm_backward_kernel(X_ptr, X_row_stride, X_col_stride, W_ptr,\n    Mean_ptr, Mean_ptr_row_stride, Mean_ptr_col_stride, RSTD_ptr, DX_ptr,\n    DW_ptr, DB_ptr, UPSTREAM_ptr, hidden_size: 'tl.constexpr',\n    channels_per_group: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', dtype:\n    'tl.constexpr'):\n    \"\"\"\n    References:\n    https://nn.labml.ai/normalization/group_norm/index.html\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n\n    The backprop equations are the same for group_norm and layer_norm\n    the only difference here is that we load the Mean, Rstd corresponding to the\n    group we're computing gradients for and the mean and rstd are computed over n-channels\n    so the total number of elements we compute the mean over is num_channels_per_group * hidden_size\n\n    We also need to load the Weights corresponding to the current channel to compute the gradients.\n    \"\"\"\n    batch_idx = tl.program_id(0)\n    group_idx = tl.program_id(1)\n    X_ptr += batch_idx * X_row_stride\n    DX_ptr += batch_idx * X_row_stride\n    UPSTREAM_ptr += batch_idx * X_row_stride\n    mean = tl.load(Mean_ptr + batch_idx * Mean_ptr_row_stride + group_idx *\n        Mean_ptr_col_stride)\n    rstd = tl.load(RSTD_ptr + batch_idx * Mean_ptr_row_stride + group_idx *\n        Mean_ptr_col_stride)\n    c1 = 0.0\n    c2 = 0.0\n    block_range = tl.arange(0, BLOCK_SIZE)\n    for channel_idx in range(group_idx * channels_per_group, (group_idx + 1\n        ) * channels_per_group):\n        dW = 0.0\n        dB = 0.0\n        W = tl.load(W_ptr + channel_idx)\n        for i in tl.range(0, hidden_size, BLOCK_SIZE):\n            hidden_size_offsets = i + block_range\n            mask = hidden_size_offsets < hidden_size\n            X = tl.load(X_ptr + channel_idx * X_col_stride +\n                hidden_size_offsets, mask=mask, other=0.0)\n            UPSTREAM_grad = tl.load(UPSTREAM_ptr + channel_idx *\n                X_col_stride + hidden_size_offsets, mask=mask, other=0.0)\n            x_hat = (X - mean) * rstd\n            dW += tl.sum(UPSTREAM_grad * x_hat)\n            dB += tl.sum(UPSTREAM_grad)\n            wdy = W * UPSTREAM_grad\n            c1 += tl.sum(x_hat * wdy)\n            c2 += tl.sum(wdy)\n        tl.atomic_add(DW_ptr + channel_idx, dW)\n        tl.atomic_add(DB_ptr + channel_idx, dB)\n    N = hidden_size * channels_per_group\n    c1 = c1 / N\n    c2 = c2 / N\n    for channel_idx in tl.range(group_idx * channels_per_group, (group_idx +\n        1) * channels_per_group):\n        W = tl.load(W_ptr + channel_idx)\n        for i in range(0, hidden_size, BLOCK_SIZE):\n            hidden_size_offsets = i + block_range\n            mask = hidden_size_offsets < hidden_size\n            X = tl.load(X_ptr + channel_idx * X_col_stride +\n                hidden_size_offsets, mask=mask, other=0.0)\n            UPSTREAM_grad = tl.load(UPSTREAM_ptr + channel_idx *\n                X_col_stride + hidden_size_offsets, mask=mask, other=0.0)\n            x_hat = (X - mean) * rstd\n            wdy = W * UPSTREAM_grad\n            dx = (wdy - (x_hat * c1 + c2)) * rstd\n            tl.store(DX_ptr + channel_idx * X_col_stride +\n                hidden_size_offsets, dx, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a507b8f8-8b4f-41b1-8ab5-e08ad7e1b076"
  },
  {
    "input": "@triton.jit\ndef _bwd_blocked_kernel_one_col(start_n, Q, K, V, Q_idx, K_idx, DO, DQ, DK,\n    DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_dom,\n    stride_dqm, stride_dkn, stride_dvn, stride_q_idxm, stride_k_idxn,\n    seqlen_q, block_size, headdim, v_headdim, smooth_block, BLOCK_HEADDIM:\n    'tl.constexpr', V_BLOCK_HEADDIM: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr', EVEN_V_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    block_id = start_n // block_size\n    block_offs = seqlen_q + start_n % block_size * BLOCK_M - (block_size - 1\n        ) * BLOCK_M // 2\n    begin_m = block_id * BLOCK_M * block_size\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)\n    k_idx_ptrs = K_idx + offs_n * stride_k_idxn\n    k_idx = tl.load(k_idx_ptrs)\n    k_ptrs = K + (k_idx[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (k_idx[:, None] * stride_vn + offs_vd[None, :])\n    dv = tl.zeros([BLOCK_N, V_BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if EVEN_HEADDIM:\n        k = tl.load(k_ptrs)\n    else:\n        k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    if EVEN_V_HEADDIM:\n        v = tl.load(v_ptrs)\n    else:\n        v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n    end_m = tl.minimum((block_id + 1) * BLOCK_M * block_size, seqlen_q)\n    for start_m in range(begin_m, end_m, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        if smooth_block:\n            q_idx_ptrs = (start_m + block_offs + offs_m\n                ) * stride_q_idxm % seqlen_q\n        else:\n            q_idx_ptrs = (start_m + offs_m) * stride_q_idxm\n        q_idx = tl.load(Q_idx + q_idx_ptrs)\n        q_ptrs = Q + (q_idx[:, None] * stride_qm + offs_d[None, :])\n        if EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        else:\n            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n        qk = tl.dot(q, tl.trans(k))\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + q_idx)\n        p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        do_ptrs = DO + (q_idx[:, None] * stride_dom + offs_vd[None, :])\n        if EVEN_V_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n        dv += tl.dot(tl.trans(p), do)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, tl.trans(v))\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + q_idx)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(tl.trans(ds), q)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        dq_ptrs = DQ + (q_idx[:, None] * stride_dqm + offs_d[None, :])\n        dq = tl.dot(ds, k)\n        if EVEN_HEADDIM:\n            tl.atomic_add(dq_ptrs, dq)\n        else:\n            tl.atomic_add(dq_ptrs, dq, mask=offs_d[None, :] < headdim)\n    dv_ptrs = DV + (k_idx[:, None] * stride_dvn + offs_vd[None, :])\n    dk_ptrs = DK + (k_idx[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dx(dk_ptrs, dk, offs_d, headdim, even_headdim=EVEN_HEADDIM)\n    _bwd_store_dx(dv_ptrs, dv, offs_vd, v_headdim, even_headdim=EVEN_V_HEADDIM)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5f447800-610f-43f3-82b7-15e349439288"
  },
  {
    "input": "@triton.jit\ndef cosh(x):\n    exp_x = tl.exp(x)\n    return (exp_x + 1.0 / exp_x) * 0.5\n",
    "category": "Activation Functions",
    "subcategory": "cosh",
    "uuid": "4f574d8a-2d6f-41d3-b398-e1bdfd028396"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c7db51e1-53be-4f4d-81ee-84bbfbcbc6c6"
  },
  {
    "input": "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim',\n    'spatial_dim'])\n@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic,\n    'BLOCK_SIZE_SPATIAL': lambda args: next_power_of_2(args['spatial_dim'])})\n@triton.jit\ndef nll_loss_backward_kernel(output_grad_pointer, target_pointer,\n    weight_pointer, sum_weights_pointer, input_grad_pointer, batch_dim,\n    spatial_dim, output_grad_batch_stride, output_grad_feat_stride,\n    target_batch_stride, target_spatial_stride, input_grad_batch_stride,\n    input_grad_feat_stride, input_grad_spatial_stride, reduction:\n    'tl.constexpr', weighted: 'tl.constexpr', BLOCK_SIZE_BATCH:\n    'tl.constexpr', BLOCK_SIZE_SPATIAL: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of negative log likelihood loss.\n\n    Args:\n        output_grad_pointer: Pointer to the loss's output gradients.\n            The output gradients must be of shape [batch_dim, spatial_dim]\n            if reduction is 'none', and otherwise [batch_dim/BLOCK_SIZE_BATCH].\n        target_pointer: Pointer to the target.\n            The target must be of shape [batch_dim, spatial_dim].\n        weight_pointer: Pointer to an optional class weight vector.\n            The class weight vector, if provided, must be of shape [feat_dim].\n        sum_weights_pointer: Pointer to the sum of the class weights if the classes were weighed.\n            The sum of weights must be a scalar.\n        input_grad_pointer: Pointer to a container the input's gradients are written to.\n            The container must be of shape [batch_dim, feat_dim, spatial_dim] and zeroed.\n        batch_dim: Batch dimension.\n        spatial_dim: Spatial dimension.\n        output_grad_batch_stride: Stride necessary to jump one element along the\n            output gradients' batch dimension.\n        output_grad_feat_stride: Stride necessary to jump one element along the\n            output gradients' feature dimension.\n        input_spatial_stride: Stride necessary to jump one element along the\n            input's spatial dimension.\n        target_batch_stride: Stride necessary to jump one element along the\n            target's batch dimension.\n        target_spatial_stride: Stride necessary to jump one element along the\n            target's spatial dimension.\n        input_grad_batch_stride: Stride necessary to jump one element along the\n            input gradient container's batch dimension.\n        input_grad_feat_stride: Stride necessary to jump one element along the\n            input gradient container's feature dimension.\n        input_grad_spatial_stride: Stride necessary to jump one element along the\n            input gradient container's spatial dimension.\n        reduction: Reduction strategy for the output whose gradient is calculated.\n            Options are 'none' for no reduction, 'mean' for averaging the loss\n            across all entries, and 'sum' for summing the loss across all entries.\n        weighted: Flag for weighing each class.\n        BLOCK_SIZE_BATCH: Block size across the batch dimension.\n        BLOCK_SIZE_SPATIAL: Block size across the spatial dimension.\n    \"\"\"\n    batch_pid = tl.program_id(axis=0)\n    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH\n        )\n    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)\n    batch_mask = batch_offset < batch_dim\n    spatial_mask = spatial_offset < spatial_dim\n    output_grad_mask = None\n    if reduction == 'none':\n        output_grad_pointer += output_grad_batch_stride * batch_offset[:, None\n            ] + output_grad_feat_stride * spatial_offset[None, :]\n        output_grad_mask = batch_mask[:, None] & spatial_mask[None, :]\n    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask)\n    input_grad = -output_grad\n    target_pointer += target_batch_stride * batch_offset[:, None\n        ] + target_spatial_stride * spatial_offset[None, :]\n    target = tl.load(target_pointer, mask=batch_mask[:, None] &\n        spatial_mask[None, :])\n    if weighted:\n        weight = tl.load(weight_pointer + target, mask=batch_mask[:, None] &\n            spatial_mask[None, :])\n        input_grad *= weight\n        if reduction == 'mean':\n            input_grad /= tl.load(sum_weights_pointer)\n    elif reduction == 'mean':\n        input_grad /= batch_dim * spatial_dim\n    input_grad_pointer += (input_grad_feat_stride * target + \n        input_grad_batch_stride * batch_offset[:, None] + \n        input_grad_spatial_stride * spatial_offset[None, :])\n    tl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] &\n        spatial_mask[None, :])\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "46122ee9-1068-4481-ab30-1d55b48627ec"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK'])\n@triton.jit\ndef fused_chunk_delta_rule_fwd_kernel(q, k, v, v_new, d, o, initial_state,\n    final_state, s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, B, H, T, scale,\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (DK, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BT), (0, 1))\n    p_d = tl.make_block_ptr(d + i_bh * s_k_h, (T, DK), (s_k_t, s_k_d), (0, \n        i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, DV), (s_v_t, s_v_d), (0, \n        i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_v_h, (T, DV), (\n        s_v_t, s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new + i_bh * s_v_h, (T, DV), (s_v_t,\n        s_v_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_v_prime = tl.dot(b_d, b_h, allow_tf32=False)\n        b_v = b_v - b_v_prime\n        tl.store(p_v_new, b_v, boundary_check=(0, 1))\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_v_new = tl.advance(p_v_new, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_d = tl.advance(p_d, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5a849848-4d6d-46b6-b27f-1bf42b8c1fe0"
  },
  {
    "input": "@triton.jit\ndef _splat_2d(to_splat, grad_image, w, batch_index, ix, iy, IH, IW, C:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW))\n    w = tl.view(w[:, None], (BLOCK_SIZE, 1))\n    offs = tl.view((batch_index * IW * IH * C + iy_ * IW * C + ix_ * C)[:,\n        None] + Coffs[None, :], (BLOCK_SIZE, C))\n    tl.atomic_add(grad_image + offs, w * to_splat)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "6441d83f-6a6e-4502-af15-c8aafaa757ff"
  },
  {
    "input": "@triton.jit\ndef triton_cross_merge_1b1(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr', BW:\n    'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp0 + i_w * BW * DH + tl.arange(\n        0, BW)[None, :] * DH + i_h * BH + tl.arange(0, BH)[:, None]\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NW - i_w - 1\n        ) * BW * DH + (BW - 1 - tl.arange(0, BW)[None, :]) * DH + (NH - i_h - 1\n        ) * BH + (BH - 1 - tl.arange(0, BH)[:, None]) + (DH - NH * BH) + (DW -\n        NW * BW) * DH\n    p_x1 = x + i_b * 4 * _tmp1 + _tmp2\n    p_x2 = p_x1 + _tmp1\n    p_x3 = p_x2 + _tmp1\n    p_x4 = p_x3 + _tmp1\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        tl.store(p_x1 + _idx, tl.load(p_y1 + _idx), mask=_mask_hw)\n        tl.store(p_x2 + _idx, tl.load(p_y2 + _idx), mask=_mask_hw)\n        tl.store(p_x3 + _idx, tl.load(p_y3 + _idx), mask=_mask_hw)\n        tl.store(p_x4 + _idx, tl.load(p_y4 + _idx), mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "a1d0207f-19a5-426e-af1a-766f92386893"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_fwd_kernel_h(k, v, h, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, s_hh, s_ht, H, T, TD,\n    DK, DV, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + i_bh * s_hh, (TD, DV), (s_ht, 1), (i_k * BK,\n        i_v * BV), (BK, BV), (1, 0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1))\n    for _ in range(0, T, BT):\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_h = d_b * b_h + tl.dot(b_k, b_v * d_i[:, None], allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_h = tl.advance(p_h, (DK, 0))\n    if STORE_FINAL_STATE:\n        p_ht = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_ht, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "bc2c9332-d131-447c-9c52-0c7a2c83d35f"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr',\n    offs_n: 'tl.constexpr', N_CTX: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr)\n        acc += tl.dot(p, v)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "c65fc144-c915-482e-8a25-1c8f7f072351"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_intra_V(q, k, g, A, s_k_h, s_k_t, s_k_d, scale, T:\n    'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (K, T), (s_k_d, s_k_t),\n            (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n        p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n        p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)\n        b_A = tl.dot(b_qg, b_kg, allow_tf32=False)\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n            BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((\n            i_t * BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_k * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_k = tl.load(p_k, boundary_check=(0,))\n            b_gk = tl.load(p_gk, boundary_check=(0,))\n            b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]) *\n                scale, 1)\n            b_A = tl.where(o_i >= j, b_A, 0.0)\n            tl.store(A + o_A + j, b_A, mask=m_A)\n            p_k = tl.advance(p_k, (K,))\n            p_gk = tl.advance(p_gk, (K,))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "628b17c2-84b7-4b34-b3a1-73a36c7b3ddc"
  },
  {
    "input": "@triton.jit\ndef print_line(str_line):\n    if tl.program_id(0) == 0 and tl.program_id(1) == 0:\n        None\n",
    "category": "Helper Functions",
    "subcategory": "",
    "uuid": "139834df-fabd-4d3d-95a6-e6d6455a07b6"
  },
  {
    "input": "@triton.jit\ndef chunk_rwkv6_bwd_kernel_intra(q, k, g, gs, dA, dq, dk, s_k_h, s_k_t,\n    s_k_d, T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    o_k = i_k * BK + tl.arange(0, BK)\n    o_q = i_t * BT + i_i * BC\n    m_k = o_k < K\n    p_gs = tl.make_block_ptr(gs + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_gn = tl.load(g + i_bh * T * K + (o_q - 1) * K + o_k, mask=m_k & (i_i >\n        0) & (o_q <= T), other=0)\n    b_gs = tl.load(p_gs, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kg, allow_tf32=False)\n    b_dq *= tl.exp(b_gs - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,))\n        b_gkj = tl.load(g + i_bh * T * K + (o_q + j) * K + o_k, mask=m_k &\n            (o_q + j < T), other=0)\n        m_i = o_i[:, None] > j\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tl.exp(b_gs -\n            b_gkj[None, :]), 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_dq = b_dq + tl.load(p_dq, boundary_check=(0, 1))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gs = tl.make_block_ptr(gs + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gs = tl.load(p_gs, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_gs - b_gn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(tl.trans(b_dA), b_qg, allow_tf32=False)\n    b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, BC):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gqj = tl.make_block_ptr(gs + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        b_gqj = tl.load(p_gqj, boundary_check=(0,))\n        m_i = o_i[:, None] < j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_dk = b_dk + tl.load(p_dk, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "bdce3797-4a1a-4838-ade8-9b59a5902ad7"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_K(v, g, do, dA, s_v_h, s_v_t, s_v_d, scale,\n    T: 'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    n_bh = tl.num_programs(2)\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (\n            i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_gv = tl.make_block_ptr(g + i_bh * s_v_h, (V, T), (s_v_d, s_v_t),\n            (i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n        p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (i_bh + i_v * n_bh) * T * BT, (T, BT),\n            (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_gn = tl.load(p_gn, boundary_check=(0,))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g - b_gn[None, :]) * scale\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = b_v * tl.exp(b_gn[:, None] - b_gv)\n        b_dA = tl.dot(b_do, b_vg, allow_tf32=False)\n        tl.store(p_dA, b_dA, boundary_check=(0, 1))\n    elif i_i == i_j:\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n            BT + i_j * BC) * V + i_v * BV,), (BV,), (0,))\n        p_gv = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,), ((\n            i_t * BT + i_j * BC) * V + i_v * BV,), (BV,), (0,))\n        p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        o_i = tl.arange(0, BC)\n        o_A = (i_bh + i_v * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.\n            arange(0, BC)) * BT + i_j * BC\n        m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n        for j in range(0, BC):\n            b_v = tl.load(p_v, boundary_check=(0,))\n            b_gv = tl.load(p_gv, boundary_check=(0,))\n            b_dA = tl.sum(b_do * b_v[None, :] * tl.exp(b_g - b_gv[None, :]), 1)\n            b_dA = tl.where(o_i >= j, b_dA, 0)\n            tl.store(dA + o_A + j, b_dA, mask=m_A)\n            p_v = tl.advance(p_v, (V,))\n            p_gv = tl.advance(p_gv, (V,))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0ee86c92-d5cf-403d-acf6-cbb095843877"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N':\n    256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K':\n    32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128,\n    'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K',\n    'NO_GROUPS'])\n@triton.jit\ndef matmul4_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, M, N, K,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n    stride_scales_g, stride_scales_n, stride_zeros_g, stride_zeros_n,\n    groupsize, NO_GROUPS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    \"\"\"\n    Compute the matrix multiplication C = A x B.\n    A is of shape (M, K) float16\n    B is of shape (K//8, N) int32\n    C is of shape (M, N) float16\n    scales is of shape (G, N) float16\n    zeros is of shape (G, N//8) int32\n    groupsize is an int specifying the size of groups for scales and zeros.\n    G is K // groupsize.\n    Set NO_GROUPS to groupsize == K, in which case G = 1 and the kernel is more efficient.\n    WARNING: This kernel assumes that K is a multiple of BLOCK_SIZE_K.\n    WARNING: This kernel assumes that N is a multiple of BLOCK_SIZE_N.\n    WARNING: This kernel assumes that groupsize is a multiple of BLOCK_SIZE_K.\n    \"\"\"\n    bits = 4\n    infearure_per_bits = 8\n    pid = tl.program_id(axis=0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    a_mask = offs_am[:, None] < M\n    b_ptrs = b_ptr + (offs_k[:, None] // infearure_per_bits * stride_bk + \n        offs_bn[None, :] * stride_bn)\n    scales_ptrs = scales_ptr + offs_bn * stride_scales_n\n    zeros_ptrs = zeros_ptr + offs_bn // infearure_per_bits * stride_zeros_n\n    shifter = offs_k % infearure_per_bits * bits\n    zeros_shifter = offs_bn % infearure_per_bits * bits\n    if NO_GROUPS:\n        scales = tl.load(scales_ptrs)\n        zeros = tl.load(zeros_ptrs)\n        zeros = zeros >> zeros_shifter & 15\n        zeros = zeros * scales\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, num_pid_k):\n        a = tl.load(a_ptrs, mask=a_mask, other=0.0)\n        b = tl.load(b_ptrs)\n        if not NO_GROUPS:\n            g_id = k // (groupsize // BLOCK_SIZE_K)\n            ptr = scales_ptrs + g_id * stride_scales_g\n            scales = tl.load(ptr)\n            ptr = zeros_ptrs + g_id * stride_zeros_g\n            zeros = tl.load(ptr)\n            zeros = zeros >> zeros_shifter & 15\n            zeros = zeros * scales\n        b = b >> shifter[:, None] & 15\n        b = b * scales[None, :] - zeros[None, :]\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K // infearure_per_bits * stride_bk\n    c = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :\n        ]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, accumulator, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "a56aa3eb-24b9-4480-86cd-51026d1234cd"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_delta_rule_fwd_kernel(q, k, v, v_new, d, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_d = tl.make_block_ptr(d + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new + i_bh * s_vo_h, (T, DV), (s_vo_t,\n        s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_v_prime = tl.dot(b_d, b_h, allow_tf32=False)\n        b_v = b_v - b_v_prime\n        tl.store(p_v_new, b_v, boundary_check=(0, 1))\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_v_new = tl.advance(p_v_new, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_d = tl.advance(p_d, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0aaa9bfa-839e-445f-9247-a58e1422419d"
  },
  {
    "input": "@triton.jit\ndef gelu_approx(x):\n    \"\"\"\n    GeLU_ activation - Gaussian error linear unit, with tanh approximation\n\n    .. _GeLU: https://arxiv.org/pdf/1606.08415.pdf\n    \"\"\"\n    return 0.5 * x * (1.0 + tanh(_sqrt2pi * x * (1.0 + 0.044715 * x * x)))\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "76a14b19-56b9-40b5-9b74-f9136b1b3d6e"
  },
  {
    "input": "@triton.jit\ndef second_order_fwd(coord_ptr: 'tl.tensor', output_ptr: 'tl.tensor',\n    block_size: 'tl.constexpr', coord_numel: 'tl.constexpr', output_numel:\n    'tl.constexpr', col_offset: 'tl.constexpr', output_stride: 'tl.constexpr'):\n    coord_stride = 3\n    block_id = tl.program_id(0)\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    CONST_00 = 3.87298334620742\n    CONST_01 = 2.23606797749979\n    CONST_02 = -1.11803398874989\n    CONST_03 = 1.93649167310371\n    Y20 = CONST_00 * x * z\n    Y21 = CONST_00 * x * y\n    Y23 = CONST_00 * y * z\n    Y22 = CONST_02 * x * x + CONST_01 * y * y + CONST_02 * z * z\n    Y24 = -CONST_03 * x * x + CONST_03 * z * z\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    tl.store(output_ptr + output_row_offset, Y20, mask=output_row_offset <\n        output_numel)\n    tl.store(output_ptr + output_row_offset + 1, Y21, mask=\n        output_row_offset + 1 < output_numel)\n    tl.store(output_ptr + output_row_offset + 2, Y22, mask=\n        output_row_offset + 2 < output_numel)\n    tl.store(output_ptr + output_row_offset + 3, Y23, mask=\n        output_row_offset + 3 < output_numel)\n    tl.store(output_ptr + output_row_offset + 4, Y24, mask=\n        output_row_offset + 4 < output_numel)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "86a12a69-3c37-4002-a78c-ce099f6c26f4"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att1_int8(Q, K, K_scale, sm_scale, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s,\n    stride_qbs, stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd,\n    stride_ksbs, stride_ksh, stride_ksd, att_stride_h, att_stride_bs,\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        off_ks = k_loc[:, None] * stride_ksbs + cur_head * stride_ksh\n        k_scale = tl.load(K_scale + off_ks, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k * k_scale, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "13c19b03-c9dd-423c-ac08-ff3e3e57f4b6"
  },
  {
    "input": "@triton.jit\ndef triton_cross_merge_bidi(x, y, BC: 'tl.constexpr', BH: 'tl.constexpr',\n    BW: 'tl.constexpr', DC: 'tl.constexpr', DH: 'tl.constexpr', DW:\n    'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    _tmp0 = i_c * BC * DH * DW\n    _tmp1 = DC * DH * DW\n    _tmp2 = _tmp0 + i_h * BH * DW + tl.arange(0, BH)[:, None\n        ] * DW + i_w * BW + tl.arange(0, BW)[None, :]\n    p_x = x + i_b * _tmp1 + _tmp2\n    p_y1 = y + i_b * 4 * _tmp1 + _tmp2\n    p_y2 = y + i_b * 4 * _tmp1 + _tmp1 + _tmp2\n    p_y3 = y + i_b * 4 * _tmp1 + 2 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    p_y4 = y + i_b * 4 * _tmp1 + 3 * _tmp1 + _tmp0 + (NH - i_h - 1\n        ) * BH * DW + (BH - 1 - tl.arange(0, BH)[:, None]) * DW + (NW - i_w - 1\n        ) * BW + (BW - 1 - tl.arange(0, BW)[None, :]) + (DH - NH * BH) * DW + (\n        DW - NW * BW)\n    for idxc in range(_for_C):\n        _idx = idxc * DH * DW\n        _y1 = tl.load(p_y1 + _idx, mask=_mask_hw)\n        _y2 = tl.load(p_y2 + _idx, mask=_mask_hw)\n        _y3 = tl.load(p_y3 + _idx, mask=_mask_hw)\n        _y4 = tl.load(p_y4 + _idx, mask=_mask_hw)\n        tl.store(p_x + _idx, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "dfba9de7-2663-4703-85da-4cb489dbd35a"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n    stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr', start_n, start_m, num_steps, MASK: 'tl.constexpr'\n    ):\n    offs_m = start_m + tl.arange(0, BLOCK_M1)\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    offs_k = tl.arange(0, HEAD_DIM)\n    qT_ptrs = Q + offs_m[None, :] * stride_tok + offs_k[:, None] * stride_d\n    do_ptrs = DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n    curr_m = start_m\n    step_m = BLOCK_M1\n    for blk_idx in range(num_steps):\n        qT = tl.load(qT_ptrs)\n        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n        m = tl.load(M + offs_m)\n        qkT = tl.dot(k, qT)\n        pT = tl.math.exp2(qkT - m[None, :])\n        if MASK:\n            mask = offs_m[None, :] >= offs_n[:, None]\n            pT = tl.where(mask, pT, 0.0)\n        do = tl.load(do_ptrs)\n        ppT = pT\n        ppT = ppT\n        dv += tl.dot(ppT, do)\n        Di = tl.load(D + offs_m)\n        dpT = tl.dot(v, tl.trans(do))\n        dsT = pT * (dpT - Di[None, :])\n        dsT = dsT\n        dk += tl.dot(dsT, tl.trans(qT))\n        curr_m += step_m\n        qT_ptrs += step_m * stride_tok\n        do_ptrs += step_m * stride_tok\n    return dk, dv\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "02e2bee9-a62a-4a10-b391-053adb562d00"
  },
  {
    "input": "@triton.jit\ndef _weighted_rms_norm_fwd(X, Y, W, Rstd, D, eps, stride_x, stride_y,\n    BLOCK_D: 'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x\n    Y += row * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    x = tl.load(X + cols, mask=cols < D, other=0.0)\n    _var = tl.zeros([BLOCK_D], dtype=tl.float32)\n    x_mean = tl.where(cols < D, x, 0.0)\n    _var += x_mean * x_mean\n    var = tl.sum(_var, axis=0) / D\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < D\n    y = x_mean * rstd\n    w = tl.load(W + cols, mask=mask)\n    y = y * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "c41bc8df-c6df-4f07-ac8d-d5a8c1bae74a"
  },
  {
    "input": "@triton.jit\ndef maximum_path(path, value, t_x, t_y, B, T, S, max_neg_val, BLOCK_SIZE_X:\n    'tl.constexpr'):\n    batch = tl.program_id(axis=0)\n    path += batch * T * S\n    value += batch * T * S\n    x_length = tl.load(t_x + batch)\n    y_length = tl.load(t_y + batch)\n    offs_prev = tl.arange(0, BLOCK_SIZE_X)\n    init = tl.where(offs_prev == 0, tl.load(value), max_neg_val)\n    tl.store(value + offs_prev * S, init, mask=offs_prev < x_length)\n    for j in range(1, y_length, 1):\n        v_cur = tl.load(value + offs_prev * S + (j - 1), mask=offs_prev <\n            x_length, other=max_neg_val)\n        v_prev = tl.load(value + (offs_prev - 1) * S + (j - 1), mask=(0 <\n            offs_prev) & (offs_prev < x_length), other=max_neg_val)\n        v = tl.maximum(v_cur, v_prev) + tl.load(value + offs_prev * S + j,\n            mask=offs_prev < x_length)\n        tl.store(value + offs_prev * S + j, v, mask=offs_prev < x_length)\n    index = x_length - 1\n    for j in range(y_length - 1, -1, -1):\n        tl.store(path + index * S + j, 1)\n        if index > 0:\n            v_left = tl.load(value + index * S + j - 1)\n            v_leftdown = tl.load(value + (index - 1) * S + j - 1)\n            if v_left < v_leftdown:\n                index += -1\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "f88538dd-dd96-45d8-8802-64ac91f58742"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel_backward(output_ptr, input_ptr, grad_ptr,\n    grad_row_stride, input_row_stride, output_row_stride, n_cols,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    grad_row_start_ptr = grad_ptr + row_idx * grad_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    grad_ptrs = grad_row_start_ptr + col_offsets\n    mask = col_offsets < n_cols\n    probs_row = tl.load(input_ptrs, mask=mask, other=0.0)\n    grad_row = tl.load(grad_ptrs, mask=mask, other=0.0)\n    dxhat = probs_row * grad_row\n    softmax_grad_output = dxhat - probs_row * tl.sum(dxhat, axis=0)\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_grad_output, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5e882fbf-5a61-49ff-8cbb-61123b581783"
  },
  {
    "input": "@triton.jit\ndef recompute_mask_kernel(mask, B, H, M, N, dropout_p, seed, offset):\n    row, b, h = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    offs_base = b * H * M * N + h * M * N + row * N\n    BLOCK: 'tl.constexpr' = 1024\n    offs_base += tl.arange(0, BLOCK)\n    for start_n in range(0, N, BLOCK):\n        offs = start_n + offs_base\n        rng_offs = offset + offs\n        pmask = tl.rand(seed, rng_offs, n_rounds=6) > dropout_p\n        row_mask = start_n + tl.arange(0, BLOCK) < N\n        tl.store(mask + offs, pmask, mask=row_mask)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "c1c781bb-dbef-4d05-8517-b5b14919c125"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, stride, N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "af108bdf-9d20-474d-a896-b9aeb7ea159a"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att1_int8(Q, K, K_scale, sm_scale, Req_to_tokens,\n    B_req_idx, B_Start_Loc, B_Seqlen, Att_Out, stride_req_to_tokens_b,\n    stride_req_to_tokens_s, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_ksbs, stride_ksh, stride_ksd, att_stride_h,\n    att_stride_bs, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    cur_batch_start_index = 0\n    cur_batch_end_index = cur_batch_seq_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * offs_n_new, mask=\n            offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        off_ks = k_loc[:, None] * stride_ksbs + cur_kv_head * stride_ksh\n        k_scale = tl.load(K_scale + off_ks, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k * k_scale, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5f2e9a0a-7b88-4fae-a71f-eb93e73b9076"
  },
  {
    "input": "@triton.jit\ndef parallel_based_fwd_kernel(q, k, v, o, z, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, scale, B: 'tl.constexpr', H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "c6766ac8-a1e8-4dbc-b46b-802e2ace4c88"
  },
  {
    "input": "@triton.jit\ndef relu(x):\n    return tl.max(x, 0.0)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "b1eec882-6e53-4b65-9bcc-f4deb7fb3a22"
  },
  {
    "input": "@triton.jit\ndef _bwd_dkv_reduce(Q, DO, S, DKV, b: 'tl.constexpr', h: 'tl.constexpr', n:\n    'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr', BLOCK:\n    'tl.constexpr', NUM_BLOCK: 'tl.constexpr', D_FBLOCK: 'tl.constexpr',\n    E_FBLOCK: 'tl.constexpr', NUM_FBLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_h = off_bh % h\n    off_d = tl.program_id(1)\n    off_e = tl.program_id(2)\n    kv_offset = off_bh * (NUM_BLOCK + 1) * d * e\n    d_offset = off_d * D_FBLOCK\n    e_offset = off_e * E_FBLOCK\n    DKV_block_ptr = (DKV + kv_offset + d_offset * e + e_offset + NUM_BLOCK *\n        d * e + tl.arange(0, D_FBLOCK)[:, None] * e + tl.arange(0, E_FBLOCK\n        )[None, :])\n    s_ptrs = S + off_h\n    s = tl.load(s_ptrs)\n    block_decay = tl.exp(-s * BLOCK)\n    dkv = tl.zeros([D_FBLOCK, E_FBLOCK], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        DKV_block_ptr -= d * e\n        dkv_current = tl.load(DKV_block_ptr)\n        tl.store(DKV_block_ptr, dkv)\n        dkv = block_decay * dkv + dkv_current\n    DKV_block_ptr += NUM_BLOCK * d * e\n    tl.store(DKV_block_ptr, dkv)\n",
    "category": "Gradient Operations",
    "subcategory": "gradient accumulation",
    "uuid": "ef7dc391-f026-4dc0-9d44-533b20a4a12d"
  },
  {
    "input": "@triton.jit\ndef blora_fwd_kernel_with_loraA_mask(x_ptr, x_stride_bs, x_stride_m, o_ptr,\n    o_stride_bsk, lA_ptr, lA_stride_l, lA_stride_hm, lB_ptr, lB_stride_l,\n    lB_stride_r, lA_mask1_ptr, lA_mask1_stride_m, lA_mask2_ptr,\n    lA_mask2_stride_rm, sel_ptr, k: 'tl.constexpr', m: 'tl.constexpr', r:\n    'tl.constexpr', rm: 'tl.constexpr', hm: 'tl.constexpr', hout:\n    'tl.constexpr', block_size_hout: 'tl.constexpr', block_size_hm:\n    'tl.constexpr', block_size_r: 'tl.constexpr'):\n    block_idx_bsk = tl.program_id(0)\n    block_idx_bs = block_idx_bsk // k\n    block_idx_hout = tl.program_id(1)\n    offsets_hout = block_idx_hout * block_size_hout + tl.arange(0,\n        block_size_hout)\n    offsets_hm = tl.arange(0, block_size_hm)\n    offsets_r = tl.arange(0, block_size_r)\n    offsets_m = tl.arange(0, m)\n    offsets_rm = tl.arange(0, rm)\n    block_mask_hout = offsets_hout < hout\n    block_mask_hout_row = offsets_hout[None, :] < hout\n    block_mask_r_col = offsets_r[:, None] < r\n    block_mask_r_row = offsets_r[None, :] < r\n    sel_ptr += block_idx_bsk\n    sel_idx = tl.load(sel_ptr)\n    x_block_ptrs = x_ptr + block_idx_bs * x_stride_bs + (offsets_m[:, None] *\n        x_stride_m + offsets_hm[None, :])\n    lA_block_ptrs = lA_ptr + sel_idx * lA_stride_l + (offsets_hm[:, None] *\n        lA_stride_hm + offsets_rm[None, :])\n    lB_block_ptrs = lB_ptr + sel_idx * lB_stride_l + (offsets_r[:, None] *\n        lB_stride_r + offsets_hout[None, :])\n    lA_mask1_ptrs = lA_mask1_ptr + (offsets_m[:, None] * lA_mask1_stride_m +\n        offsets_rm[None, :])\n    lA_mask2_ptrs = lA_mask2_ptr + (offsets_rm[:, None] *\n        lA_mask2_stride_rm + offsets_r[None, :])\n    o_block_ptrs = o_ptr + block_idx_bsk * o_stride_bsk + offsets_hout\n    compute_olA_dtype = tl.float16\n    olA = tl.zeros((m, rm), dtype=compute_olA_dtype)\n    for block_idx_hm in range(tl.cdiv(hm, block_size_hm)):\n        block_mask_hm_col = offsets_hm[:, None] < hm\n        block_mask_hm_row = offsets_hm[None, :] < hm\n        x = tl.load(x_block_ptrs, mask=block_mask_hm_row, other=0.0)\n        lA = tl.load(lA_block_ptrs, mask=block_mask_hm_col, other=0.0)\n        olA += tl.dot(x, lA)\n        offsets_hm += block_size_hm\n        x_block_ptrs += block_size_hm\n        lA_block_ptrs += block_size_hm * lA_stride_hm\n    lA_mask1 = tl.load(lA_mask1_ptrs)\n    lA_mask2 = tl.load(lA_mask2_ptrs, mask=block_mask_r_row, other=0.0)\n    olA = tl.dot(olA * lA_mask1, lA_mask2)\n    compute_olB_dtype = tl.float16\n    lB = tl.load(lB_block_ptrs, mask=block_mask_r_col & block_mask_hout_row,\n        other=0.0)\n    olB = tl.sum(tl.dot(olA, lB), axis=0)\n    tl.store(o_block_ptrs, olB, mask=block_mask_hout)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "01c681da-313d-43b5-8fff-f2c7045460ef"
  },
  {
    "input": "@triton.jit\ndef tanh(x):\n    return 2 * tl.sigmoid(2 * x) - 1\n",
    "category": "Activation Functions",
    "subcategory": "tanh",
    "uuid": "e95a8709-2221-49bb-adb6-9254cde7c25d"
  },
  {
    "input": "@triton.jit\ndef test_pid_conds(conds):\n    \"\"\"Test if condition on pids are fulfilled\n    E.g.:\n        '=0'    checks that pid_0 == 0\n        ',>1'   checks that pid_1 > 1\n        '>1,=0' checks that pid_0 > 1 and pid_1 == 0\n    \"\"\"\n    return _test_pid_conds(conds, tl.program_id(0).handle.data[0], tl.\n        program_id(1).handle.data[0], tl.program_id(2).handle.data[0])\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "d66b4322-2913-41de-b184-dc2f7a8036de"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim', 'dstate'])\n@triton.jit\ndef _chunk_scan_chunk_state_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, D_ptr, b_ptr, dstates_ptr, dx_ptr, ddt_ptr,\n    dD_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio,\n    stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim,\n    stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m,\n    stride_cb_csize_k, stride_dout_batch, stride_dout_seqlen,\n    stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk,\n    stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_D_head, stride_b_batch, stride_b_seqlen,\n    stride_b_head, stride_b_dstate, stride_dstates_batch,\n    stride_dstates_chunk, stride_dstates_head, stride_dstates_hdim,\n    stride_dstates_dstate, stride_dx_batch, stride_dx_seqlen,\n    stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk,\n    stride_ddt_head, stride_ddt_csize, stride_dD_batch, stride_dD_chunk,\n    stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D: 'tl.constexpr',\n    D_HAS_HDIM: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K:\n    'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr', IS_TRITON_22:\n    'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    b_ptr += (pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_b_head)\n    dstates_ptr += (pid_b * stride_dstates_batch + pid_c *\n        stride_dstates_chunk + pid_h * stride_dstates_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size_limit, other=0.0)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize)\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) *\n            stride_seq_idx_seqlen)\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last -\n            dA_cs_m), 0.0)\n    offs_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if IS_TRITON_22 and \n        BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_dstate[None,\n        :] * stride_b_dstate)\n    dstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_dstates_hdim + \n        offs_dstate[:, None] * stride_dstates_dstate)\n    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_dstate[None, :] < dstate), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_dstate[:, None] < dstate\n            ) & (offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates\n        acc = tl.dot(b, dstates) * scale[:, None]\n    else:\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n                (offs_dstate[None, :] < dstate - k), other=0.0)\n            dstates = tl.load(dstates_ptrs, mask=(offs_dstate[:, None] < \n                dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n            dstates = dstates\n            acc += tl.dot(b, dstates)\n            b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n            dstates_ptrs += BLOCK_SIZE_K * stride_dstates_dstate\n        acc *= scale[:, None]\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None,\n        :] * stride_cb_csize_k)\n    dout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    K_MAX = chunk_size_limit\n    K_MIN = pid_m * BLOCK_SIZE_M\n    cb_ptrs += K_MIN * stride_cb_csize_k\n    dout_ptrs += K_MIN * stride_dout_seqlen\n    dA_cumsum_ptrs += K_MIN * stride_dA_cs_csize\n    for k in range(K_MIN, K_MAX, BLOCK_SIZE_K):\n        k = tl.multiple_of(k, BLOCK_SIZE_K)\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k\n            [None, :] < K_MAX - k), other=0.0)\n        dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0)\n        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None,\n            :] < K_MAX)\n        cb = tl.where(mask, cb, 0.0)\n        cb = cb\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (pid_b * stride_dx_batch + pid_c * chunk_size *\n        stride_dx_seqlen + pid_h * stride_dx_head)\n    dx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :\n        ] * stride_dx_hdim)\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + \n            offs_n[None, :] * stride_dout_hdim)\n        dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] <\n            chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        dx += dout_res * D\n    tl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    if HAS_D:\n        dD_ptr += (pid_b * stride_dD_batch + pid_c * stride_dD_chunk + \n            pid_h * stride_dD_head + pid_m * stride_dD_csize)\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n            dD = tl.sum(dout_res * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        else:\n            dD = tl.sum(dout_res * x)\n            tl.store(dD_ptr, dD)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "aadc8c15-0365-4012-ab95-0cbfef8076a3"
  },
  {
    "input": "@triton.jit\ndef apply_dropout(input, drop_p, seed, offset):\n    \"\"\"\n    Randomly zeroes elements in the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element.\n        seed: Seed for generating the dropout mask.\n        offset: Offset to generate the mask for.\n\n    Returns:\n        Input with elements randomly zeroed out.\n    \"\"\"\n    random = tl.rand(seed, offset)\n    return tl.where(random < drop_p, 0, input / (1 - drop_p))\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "9cc45ce2-0df6-4fb1-9044-292bf547a808"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size',\n    'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_kernel(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m *\n        stride_ddA_cs_csize_m)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None,\n        :] * stride_cb_csize_n)\n    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr, 0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M\n    for start_n in range(lo, hi, BLOCK_SIZE_N):\n        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit - start_n), other=0.0)\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= dt_n\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n\n            [None, :] < chunk_size - start_n), other=0.0)\n        acc *= cb\n        dA_cs_n = tl.load(dA_cumsum_ptr + (start_n + offs_n) *\n            stride_dA_cs_csize, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=offs_n < \n            chunk_size - start_n - 1)\n        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n        tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N\n            ,), dtype=tl.float32), mask=offs_n < chunk_size - start_n - 1)\n        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "4d41e620-4e2d-4cc2-aec2-02a147e48ba7"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "a28b54e0-3133-4303-8420-d366b20229b9"
  },
  {
    "input": "@triton.jit\ndef concat_2D_jagged_w_prefix(OffsetsA, ValuesA, OffsetsB, ValuesB,\n    DenseSize, Out, D, stride_ad, stride_bd, stride_dense_batch, stride_od,\n    n_prefix_from_B, IS_DENSE_A: 'tl.constexpr', IS_DENSE_B: 'tl.constexpr',\n    BLOCK_D: 'tl.constexpr', IS_REPLACE: 'tl.constexpr'):\n    off_z = tl.program_id(1)\n    off_n = tl.program_id(0)\n    if IS_DENSE_A:\n        seq_start_a = off_z * DenseSize\n        seq_len_a = DenseSize\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_len_b = seq_end_b - seq_start_b\n    elif IS_DENSE_B:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = off_z * DenseSize\n        seq_len_b = DenseSize\n    else:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_len_b = seq_end_b - seq_start_b\n    if IS_REPLACE:\n        seq_len = seq_len_a\n    else:\n        seq_len = seq_len_a + seq_len_b\n    if off_n >= seq_len:\n        return\n    offs_d = tl.arange(0, BLOCK_D)\n    if IS_REPLACE:\n        out_seq_start = seq_start_a + off_n\n        out_seq_b_start = seq_len_a - seq_len_b\n    else:\n        out_seq_start = seq_start_a + seq_start_b + off_n\n        out_seq_b_start = seq_len_a + n_prefix_from_B\n    out_ptrs = Out + out_seq_start * stride_od + offs_d\n    if off_n < out_seq_b_start and off_n >= n_prefix_from_B:\n        off_a = off_n - n_prefix_from_B\n        if IS_DENSE_A:\n            in_ptrs = (ValuesA + off_a * stride_ad + off_z *\n                stride_dense_batch + offs_d)\n        else:\n            in_ptrs = ValuesA + (off_a + seq_start_a) * stride_ad + offs_d\n    else:\n        off_b = off_n - out_seq_b_start + n_prefix_from_B\n        if off_n < n_prefix_from_B:\n            off_b += out_seq_b_start - n_prefix_from_B\n        if IS_DENSE_B:\n            in_ptrs = (ValuesB + off_b * stride_bd + off_z *\n                stride_dense_batch + offs_d)\n        else:\n            in_ptrs = ValuesB + (off_b + seq_start_b) * stride_bd + offs_d\n    v = tl.load(in_ptrs, mask=offs_d < D)\n    tl.store(out_ptrs, v, mask=offs_d < D)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "b3422fee-7060-4a38-9650-e4f00183f617"
  },
  {
    "input": "@triton.jit\ndef _reg_matmul(pid_n, type_id, start_off, input, other, output, N,\n    stride_input_m, stride_input_k, stride_other_b, stride_other_k,\n    stride_other_n, stride_output_m, stride_output_n, out_dtype:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    TILE_M: 'tl.constexpr', TILE_N: 'tl.constexpr', TILE_K: 'tl.constexpr'):\n    offs_m = start_off + tl.arange(0, TILE_M)\n    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_k = tl.arange(0, TILE_K)\n    rn = tl.max_contiguous(tl.multiple_of(offs_n % N, TILE_N), TILE_N)\n    other_ptrs = other + type_id * stride_other_b + (offs_k[:, None] *\n        stride_other_k + rn[None, :] * stride_other_n)\n    b = tl.load(other_ptrs)\n    input_ptrs = input + (offs_m[:, None] * stride_input_m + offs_k[None, :\n        ] * stride_input_k)\n    output_ptrs = output + stride_output_m * offs_m[:, None\n        ] + stride_output_n * offs_n[None, :]\n    for _ in range(0, BLOCK_SIZE):\n        a = tl.load(input_ptrs)\n        acc = tl.dot(a, b, out_dtype=out_dtype)\n        if EVEN_N:\n            tl.store(output_ptrs, acc)\n        else:\n            mask_n = offs_n[None, :] < N\n            tl.store(output_ptrs, acc, mask=mask_n)\n        input_ptrs += TILE_M * stride_input_m\n        output_ptrs += TILE_M * stride_output_m\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "c751cc43-3ebb-443c-a0f7-aab98dd1fb13"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_V(q, k, g, dA, dq, dk, s_k_h, s_k_t, s_k_d,\n    T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d),\n            (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * tl.exp(b_gn[None, :] - b_gk)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kg, allow_tf32=False)\n    b_dq *= tl.exp(b_g - b_gn[None, :])\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gkj = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,))\n        b_gkj = tl.load(p_gkj, boundary_check=(0,))\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * tl.exp(b_g -\n            b_gkj[None, :]), 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gn = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g - b_gn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(tl.trans(b_dA), b_qg, allow_tf32=False)\n    b_dk *= tl.exp(b_gn[None, :] - b_gk)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, BC):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_gqj = tl.make_block_ptr(g + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        b_gqj = tl.load(p_gqj, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_gqj[\n            None, :] - b_gk), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "0bf17f5b-17b9-4283-9f90-fd649cfd83b6"
  },
  {
    "input": "@triton.jit\ndef _round(x):\n    return _floor(x + 0.5)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "0caafe0a-5fb6-4a31-8a14-718474af1631"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd(Q, K, V, sm_scale, DO, DQ, DK, DV, M, D, stride_z, stride_h,\n    stride_tok, stride_d, H, N_CTX, BLOCK_M1: 'tl.constexpr', BLOCK_N1:\n    'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr',\n    BLK_SLICE_FACTOR: 'tl.constexpr', HEAD_DIM: 'tl.constexpr'):\n    LN2: 'tl.constexpr' = 0.6931471824645996\n    bhid = tl.program_id(2)\n    off_chz = bhid * N_CTX\n    adj = stride_h * (bhid % H) + stride_z * (bhid // H)\n    pid = tl.program_id(0)\n    Q += adj\n    K += adj\n    V += adj\n    DO += adj\n    DQ += adj\n    DK += adj\n    DV += adj\n    M += off_chz\n    D += off_chz\n    offs_k = tl.arange(0, HEAD_DIM)\n    start_n = pid * BLOCK_N1\n    start_m = start_n\n    MASK_BLOCK_M1: 'tl.constexpr' = BLOCK_M1 // BLK_SLICE_FACTOR\n    offs_n = start_n + tl.arange(0, BLOCK_N1)\n    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, MASK_BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n,\n        start_m, num_steps, MASK=True)\n    start_m += num_steps * MASK_BLOCK_M1\n    num_steps = (N_CTX - start_m) // BLOCK_M1\n    dk, dv = _attn_bwd_dkdv(dk, dv, Q, k, v, sm_scale, DO, M, D, stride_tok,\n        stride_d, H, N_CTX, BLOCK_M1, BLOCK_N1, HEAD_DIM, start_n, start_m,\n        num_steps, MASK=False)\n    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dv_ptrs, dv)\n    dk *= sm_scale\n    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n    tl.store(dk_ptrs, dk)\n    start_m = pid * BLOCK_M2\n    end_n = start_m + BLOCK_M2\n    MASK_BLOCK_N2: 'tl.constexpr' = BLOCK_N2 // BLK_SLICE_FACTOR\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n        )\n    m = tl.load(M + offs_m)\n    m = m[:, None]\n    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, MASK_BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps *\n        MASK_BLOCK_N2, num_steps, MASK=True)\n    end_n -= num_steps * MASK_BLOCK_N2\n    num_steps = end_n // BLOCK_N2\n    dq = _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n        BLOCK_M2, BLOCK_N2, HEAD_DIM, start_m, end_n - num_steps * BLOCK_N2,\n        num_steps, MASK=False)\n    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n    dq *= LN2\n    tl.store(dq_ptrs, dq)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "9c01216e-f227-4a20-a806-2531ed033760"
  },
  {
    "input": "@triton.jit\ndef attention_fwd_kernel(q, k, v, h, o, s_qh, s_qt, s_qd, s_hh, s_ht, T,\n    scale, BT: 'tl.constexpr', BD: 'tl.constexpr', NT: 'tl.constexpr',\n    STORE: 'tl.constexpr', IFCOND: 'tl.constexpr'):\n    i_bh = tl.program_id(0)\n    b_h = tl.zeros([BD, BD], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qh, (BD, T), (s_qd, s_qt), (0,\n            i * BT), (BD, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_hh, (NT * BD, BD), (s_ht, s_qd\n            ), (i * BD, 0), (BD, BD), (1, 0))\n        p_o = tl.make_block_ptr(o + i_bh * s_qh, (T, BD), (s_qt, s_qd), (i *\n            BT, 0), (BT, BD), (1, 0))\n        if STORE:\n            tl.store(p_h, b_h)\n        b_q = tl.load(p_q)\n        b_q = b_q * scale\n        b_k = tl.load(p_k)\n        b_v = tl.load(p_v)\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if IFCOND:\n            if i == 0:\n                b_h = tl.dot(b_k, b_v, allow_tf32=False)\n            else:\n                b_o += tl.dot(b_q, b_h, allow_tf32=False)\n                b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False)\n            b_h += tl.dot(b_k, b_v, allow_tf32=False)\n        tl.store(p_o, b_o)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "d09c94e9-06ff-460d-a12f-f594ee3168cc"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_fwd_kernel_o(q, k, v, h, o, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_s = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, K), (s_qk_t, s_qk_d),\n            (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q * d_i[:, None], b_h, allow_tf32=False)\n        b_s += tl.dot(b_q, b_k, allow_tf32=False)\n    b_s *= d_s\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_o = (b_o + tl.dot(b_s, b_v, allow_tf32=False)) * scale\n    p_o = tl.make_block_ptr(o + i_bh * s_vo_h, (T, V), (s_vo_t, s_vo_d), (\n        i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "9dc7aebe-22a5-4436-a83a-73727e37d12e"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_INP': 32,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=3, num_warps=1), triton.Config({\n    'BLOCK_SIZE_INP': 64, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=3, num_warps\n    =8), triton.Config({'BLOCK_SIZE_INP': 128, 'BLOCK_SIZE_HIDDEN': 32},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 256,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 32, 'BLOCK_SIZE_HIDDEN': 64}, num_stages=3, num_warps\n    =8), triton.Config({'BLOCK_SIZE_INP': 64, 'BLOCK_SIZE_HIDDEN': 64},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 128,\n    'BLOCK_SIZE_HIDDEN': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 256, 'BLOCK_SIZE_HIDDEN': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_INP': 32, 'BLOCK_SIZE_HIDDEN':\n    128}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 64,\n    'BLOCK_SIZE_HIDDEN': 128}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 128, 'BLOCK_SIZE_HIDDEN': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_INP': 256, 'BLOCK_SIZE_HIDDEN':\n    128}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 32,\n    'BLOCK_SIZE_HIDDEN': 256}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 64, 'BLOCK_SIZE_HIDDEN': 256}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_INP': 128, 'BLOCK_SIZE_HIDDEN':\n    256}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 256,\n    'BLOCK_SIZE_HIDDEN': 256}, num_stages=3, num_warps=8)], key=['inp_size',\n    'hidden_size'], reset_to_zero=['per_slice_centered_and_squared_ptr'])\n@triton.jit\ndef center_and_square_kernel(inp_ptr, upper_end_of_slices_ptr,\n    per_slice_mean_ptr, per_slice_centered_and_squared_ptr, inp_size,\n    hidden_size, num_slices, stride_inp_inp_size, stride_inp_hidden_size,\n    BLOCK_SIZE_INP: 'tl.constexpr', BLOCK_SIZE_HIDDEN: 'tl.constexpr'):\n    \"\"\"Center and square each slice.\"\"\"\n    pid = tl.program_id(axis=0)\n    offs_am = pid * BLOCK_SIZE_INP + tl.arange(0, BLOCK_SIZE_INP)\n    ir_range_lower = 0\n    for slice_idx in range(0, num_slices):\n        current_mean = tl.load(per_slice_mean_ptr + slice_idx)\n        ir_range_upper = tl.load(upper_end_of_slices_ptr + slice_idx)\n        current_lower = ir_range_lower\n        if num_slices == 1:\n            num_k = tl.cdiv(ir_range_upper - ir_range_lower, BLOCK_SIZE_HIDDEN)\n        else:\n            num_k = tl.cdiv(hidden_size, BLOCK_SIZE_HIDDEN)\n        offs_k = current_lower + tl.arange(0, BLOCK_SIZE_HIDDEN)\n        a_ptrs = inp_ptr + (offs_am[:, None] * stride_inp_inp_size + offs_k\n            [None, :] * stride_inp_hidden_size)\n        for k in range(0, num_k):\n            current_upper = min(ir_range_upper, ir_range_lower + (k + 1) *\n                BLOCK_SIZE_HIDDEN, hidden_size)\n            inp_block = tl.load(a_ptrs, mask=(offs_am[:, None] < inp_size) &\n                (offs_k[None, :] < current_upper), other=current_mean)\n            inp_block = inp_block\n            a_centered = inp_block - current_mean\n            centered_and_squared = tl.sum(a_centered * a_centered)\n            tl.atomic_add(per_slice_centered_and_squared_ptr + slice_idx,\n                centered_and_squared)\n            offs_k += current_upper - current_lower\n            a_ptrs += (current_upper - current_lower) * stride_inp_hidden_size\n            current_lower = current_upper\n        ir_range_lower = ir_range_upper\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "bc528ee5-be55-4a1a-aed6-5dd63dfbf058"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_V(q, k, z, dA, dq, dk, s_k_h, s_k_t, s_k_d,\n    T: 'tl.constexpr', K: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BK: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_zq = tl.exp(b_zn[None, :] - b_z)\n    b_dq = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_kz = tl.exp(b_k - b_zn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kz, allow_tf32=False)\n    b_dq *= b_zq\n    o_i = tl.arange(0, BC)\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)\n        ) * BT + i_i * BC\n    m_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        p_kj = tl.make_block_ptr(k + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n        b_kj = tl.load(p_kj, boundary_check=(0,))\n        m_i = o_i[:, None] >= j\n        b_dq += tl.where(m_i, b_dA[:, None] * tl.exp(b_kj[None, :] - b_z), 0.0)\n    p_dq = tl.make_block_ptr(dq + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.debug_barrier()\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kz = tl.exp(b_k - b_zn[None, :])\n    b_dk = tl.zeros([BC, BK], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_qz = b_q * tl.exp(b_zn[None, :] - b_z)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(tl.trans(b_dA), b_qz, allow_tf32=False)\n    b_dk *= b_kz\n    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(\n        0, BC)\n    for j in range(0, BC):\n        p_qj = tl.make_block_ptr(q + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        p_zj = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (1,), ((i_t *\n            BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n        b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T,\n            other=0)\n        b_qj = tl.load(p_qj, boundary_check=(0,))\n        b_zj = tl.load(p_zj, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * tl.exp(b_k -\n            b_zj[None, :]), 0.0)\n    p_dk = tl.make_block_ptr(dk + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n        i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "12116103-76c7-4fe8-82b5-ca40e8392f1a"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DV, DK), (\n            1, DV), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n        if CHECK and i == 0:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        else:\n            b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n        b_dq *= scale\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    b_h = None\n    tl.debug_barrier()\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    m_s = o_i[:, None] <= o_i[None, :]\n    for i in range(1, tl.cdiv(T, BT) + 1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s = tl.where(m_s, b_s, 0)\n        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n        if CHECK and i == 1:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        else:\n            b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n            b_dv += tl.dot(b_k, b_dh, allow_tf32=False)\n            b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n        tl.store(p_dk, b_dk * scale, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "661be1ed-dad6-43fa-8860-00db2da8e654"
  },
  {
    "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate,\n    stride_state_batch, stride_state_dim, stride_state_dstate,\n    stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim,\n    stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch,\n    stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim,\n    stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim,\n    DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS:\n    'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    state_ptr += pid_b * stride_state_batch\n    x_ptr += pid_b * stride_x_batch\n    dt_ptr += pid_b * stride_dt_batch\n    B_ptr += pid_b * stride_B_batch\n    C_ptr += pid_b * stride_C_batch\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch\n    out_ptr += pid_b * stride_out_batch\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n    if DT_SOFTPLUS:\n        dt = tl.where(dt <= 20.0, tl.math.log1p(tl.exp(dt)), dt)\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n        dstate), other=0.0)\n    dA = tl.exp(A * dt[:, None])\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    dB = B[None, :] * dt[:, None]\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "8d2154d9-dd7f-4a57-81b6-61e6f0ad7175"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_retention_fwd_kernel(q, k, v, o, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b, d_o, d_h = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b\n        ), tl.math.exp2((BT - o_i - 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "f5da4fda-bb6b-4fe4-8a94-e11c992d60af"
  },
  {
    "input": "@triton.jit\ndef __flat_csr_masked_bmm_compute(CROW_INDICES, stride_crow_n,\n    stride_crow_r1, COL_INDICES, stride_col_n, stride_col_z, A, stride_a_n,\n    stride_a_h, stride_a_t, stride_a_d, B, stride_b_n, stride_b_h,\n    stride_b_t, stride_b_d, OUT_VALUES, stride_out_n, stride_out_z, N, R,\n    T_SRC, HID, GRID_ROW, GRID_COL, BLOCK_ROW: 'tl.constexpr', BLOCK_COL:\n    'tl.constexpr', BLOCK_HID: 'tl.constexpr'):\n    n = tl.program_id(0)\n    pid_ir = tl.program_id(1)\n    pid_icol = tl.program_id(2)\n    for _ir in range(BLOCK_ROW):\n        ir = _ir * GRID_ROW + pid_ir\n        ir_mask = ir < R\n        crow_start = tl.load(CROW_INDICES + n * stride_crow_n + ir *\n            stride_crow_r1, mask=ir_mask)\n        crow_end = tl.load(CROW_INDICES + n * stride_crow_n + (ir + 1) *\n            stride_crow_r1, mask=ir_mask)\n        index_row = ir\n        for ic in range(BLOCK_COL):\n            icol = ic + pid_icol * BLOCK_COL + crow_start\n            _index_col = tl.load(COL_INDICES + n * stride_col_n + icol *\n                stride_col_z, mask=(icol < crow_end) & ir_mask)\n            index_col = _index_col % T_SRC\n            index_head = _index_col // T_SRC\n            accumulator = 0.0\n            for ih in range(0, tl.cdiv(HID, BLOCK_HID)):\n                index_hids = tl.arange(0, BLOCK_HID) + ih * BLOCK_HID\n                index_hids_mask = index_hids < HID\n                a_vec = tl.load(A + n * stride_a_n + index_head *\n                    stride_a_h + index_row * stride_a_t + index_hids *\n                    stride_a_d, mask=index_hids_mask & ir_mask, other=0)\n                b_vec = tl.load(B + n * stride_b_n + index_head *\n                    stride_b_h + index_col * stride_b_t + index_hids *\n                    stride_b_d, mask=index_hids_mask & ir_mask, other=0)\n                t = tl.sum(a_vec * b_vec)\n                accumulator += t\n            tl.store(OUT_VALUES + n * stride_out_n + icol * stride_out_z,\n                accumulator, mask=(icol < crow_end) & ir_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "f80e8fc7-3c1a-431d-85cb-66e90f6b0e78"
  },
  {
    "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd,\n    stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row,\n    stride_dz_row, stride_dw_row, stride_db_row, M, N, eps,\n    rows_per_program, NORM_BEFORE_GATE: 'tl.constexpr', IS_RMS_NORM:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    RECOMPUTE_OUTPUT: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        elif RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask\n        =mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db,\n            mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "3c15b24d-31bd-47fa-a73f-1065bccf8cf2"
  },
  {
    "input": "@triton.jit\ndef _fwd_compute_O(A, V, GV, O, stride_a2, stride_a3, stride_a4, stride_v2,\n    stride_v3, stride_v4, BLOCK_N: 'tl.constexpr', BLOCK_DMODEL_V:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_v = tl.program_id(2)\n    a_offset = off_hz * stride_a2\n    v_offset = off_hz * stride_v2 + off_v * BLOCK_DMODEL_V\n    lo = 0\n    hi = BLOCK_N\n    V_ptr = V + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    O_ptr = O + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(lo + 16, hi, 16):\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        acc = tl.zeros([16, BLOCK_DMODEL_V], dtype=tl.float32)\n        for k_high in range(0, q_high, 16):\n            qk = tl.load(A_ptr + q_high * stride_a4 + k_high)\n            v = tl.load(V_ptr + k_high * stride_v4)\n            k_gv = tl.load(GV_ptr + k_high * stride_v4)\n            k_gv = tl.exp(q_gv_normalizer[None, :] - k_gv)\n            v = v * k_gv\n            output = tl.dot(qk, v, allow_tf32=False)\n            acc += output\n        tl.store(O_ptr + q_high * stride_v4, acc)\n    tl.store(O_ptr, tl.zeros([16, BLOCK_DMODEL_V], dtype=tl.float32))\n    tl.debug_barrier()\n    for q_high in range(lo, hi, 16):\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        qk = tl.load(A_ptr + q_high * stride_a4 + q_high)\n        v = tl.load(V_ptr + q_high * stride_v4)\n        k_gv = tl.load(GV_ptr + q_high * stride_v4)\n        k_gv2 = tl.exp(q_gv_normalizer[None, :] - k_gv)\n        v = v * k_gv2\n        output = tl.dot(qk, v, allow_tf32=False)\n        q_gv = tl.exp(k_gv - q_gv_normalizer[None, :])\n        prev = tl.load(O_ptr + q_high * stride_v4)\n        output += prev\n        output = output * q_gv\n        tl.store(O_ptr + q_high * stride_v4, output)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "294999f7-ba42-41ec-815c-1190d1bda520"
  },
  {
    "input": "@triton_autotune(configs=_get_bwd_dwdb_configs(), key=['D'])\n@triton.jit\ndef _rms_norm_bwd_dwdb(DW, FINAL_DW, N, D, BLOCK_N: 'tl.constexpr', BLOCK_D:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_D + tl.arange(0, BLOCK_D)\n    dw = tl.zeros((BLOCK_N, BLOCK_D), dtype=tl.float32)\n    for i in range(0, N, BLOCK_N):\n        rows = i + tl.arange(0, BLOCK_N)\n        mask = (rows[:, None] < N) & (cols[None, :] < D)\n        offs = rows[:, None] * D + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < D)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "1c395dd0-c2f5-421a-a152-10dba15ca49e"
  },
  {
    "input": "@triton.jit\ndef apply_dropout_grad(output_grad, drop_p, seed, offset):\n    \"\"\"\n    Calculates the input gradient of dropout.\n\n    Args:\n        output_grad: Output gradients. The output gradients must be\n            loaded and cannot be a pointer.\n        drop_p: Probability of dropping an element.\n        seed: Seed for generating the dropout mask.\n        offset: Offset to generate the mask for.\n\n    Returns:\n        Gradient of dropout.\n    \"\"\"\n    random = tl.rand(seed, offset)\n    return tl.where(random < drop_p, 0, output_grad / (1 - drop_p))\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "567b716a-e94f-41c5-9833-ee8bfd2ed1cb"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_bwd_dx_fused(DX, DY, DW, DB, X, W, B, Rstd, Lock, stride, N,\n    eps, GROUP_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    \"\"\"\n    Kernel invocation for backward pass of RMS normalization, computing gradients w.r.t. input\n\n    Params:\n        - DX (tensor): Gradient of the loss with respect to the inputs\n        - DY (tensor): Gradient of the loss with respect to the outputs\n        - DW (tensor): Gradient of the loss with respect to the scale tensor W\n        - DB (tensor): Gradient of the loss with respect to the bias tensor B\n        - X (tensor): Input tensor from the forward pass\n        - W (tensor): Scale tensor applied during the forward pass\n        - B (tensor): Bias tensor added during the forward pass\n        - Rstd (tensor): Reciprocal of the standard deviation used for normalization in the forward pass\n        - Lock (tensor): Lock tensor for atomic operations to prevent race conditions\n        - stride (int): Stride to be applied when accessing elements in the tensors\n        - N (int): Number of elements in each tensor\n        - eps (float): Small epsilon value used during the forward pass\n        - GROUP_SIZE_M (constexpr): Size of the group for M dimension, provided as a compile-time constant\n        - BLOCK_SIZE_N (constexpr): Size of the block for N dimension, provided as a compile-time constant\n\n    Return:\n        - None\n\n    Usage:\n        _rms_norm_bwd_dx_fused[grid, block](DX, DY, DW, DB, X, W, B, Rstd, Lock, stride, N, eps, GROUP_SIZE_M, BLOCK_SIZE_N)\n    \"\"\"\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE_N)\n    mask = cols < N\n    X += row * stride\n    DY += row * stride\n    DX += row * stride\n    lock_id = row % GROUP_SIZE_M\n    Lock += lock_id\n    Count = Lock + GROUP_SIZE_M\n    DW = DW + lock_id * N + cols\n    DB = DB + lock_id * N + cols\n    x = tl.load(X + cols, mask=mask, other=0)\n    dy = tl.load(DY + cols, mask=mask, other=0)\n    w = tl.load(W + cols, mask=mask)\n    rstd = tl.load(Rstd + row)\n    x_norm = x * rstd\n    wdy = w * dy\n    dx = wdy * rstd\n    tl.store(DX + cols, dx, mask=mask)\n    partial_dw = dy * x_norm\n    partial_db = dy\n    while tl.atomic_cas(Lock, 0, 1) == 1:\n        pass\n    count = tl.load(Count)\n    if count == 0:\n        tl.atomic_xchg(Count, 1)\n    else:\n        partial_dw += tl.load(DW, mask=mask)\n        partial_db += tl.load(DB, mask=mask)\n    tl.store(DW, partial_dw, mask=mask)\n    tl.store(DB, partial_db, mask=mask)\n    tl.atomic_xchg(Lock, 0)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "866a4140-035d-4bfc-88cc-4350ce70826b"
  },
  {
    "input": "@triton.jit\ndef mload2d(REG_ROWS: 'tl.constexpr', REG_COLS: 'tl.constexpr', i_base,\n    i_start_row, i_start_col, i_rows, i_cols, stride_row, stride_col):\n    off_rows = tl.arange(0, REG_ROWS) + i_start_row\n    off_cols = tl.arange(0, REG_COLS) + i_start_col\n    i_ptrs = i_base + off_rows[:, None] * stride_row + off_cols[None, :\n        ] * stride_col\n    row_overflow = i_start_row + REG_ROWS - i_rows\n    col_overflow = i_start_col + REG_COLS - i_cols\n    i_ptrs_mask = tl.full([REG_ROWS, REG_COLS], 1, dtype=tl.int1)\n    if row_overflow > 0:\n        i_ptrs_mask = i_ptrs_mask & (off_rows[:, None] < i_rows)\n    if col_overflow > 0:\n        i_ptrs_mask = i_ptrs_mask & (off_cols[None, :] < i_cols)\n    return tl.load(i_ptrs, mask=i_ptrs_mask, other=0.0)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "de34052a-48b8-499c-b125-c0dcb6c9553f"
  },
  {
    "input": "@triton.jit\ndef unpack64(merged):\n    tl.static_assert(merged.dtype == tl.uint64)\n    b = (merged & 4294967295).to(tl.uint32)\n    a = (merged >> 32).to(tl.uint32)\n    return a, b\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "187e725c-991b-4d98-b986-6de1edfd3001"
  },
  {
    "input": "@triton.jit\ndef _depth_inv_sphere(far, disparity_at_inf, n, step):\n    frac_step = (step + 1) / n\n    n_disp = (disparity_at_inf - 1) * frac_step + 1\n    return far * (1 / n_disp)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "0a3d72f9-a3b0-4ef2-890b-7cf957560e83"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4,\n    pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, D_ptr, dx_ptr, ddt_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_cb_batch, stride_cb_chunk, stride_cb_head,\n    stride_cb_csize_m, stride_cb_csize_k, stride_dout_batch,\n    stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch,\n    stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch,\n    stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize,\n    stride_D_head, stride_dx_batch, stride_dx_seqlen, stride_dx_head,\n    stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head,\n    stride_ddt_csize, HAS_D: 'tl.constexpr', D_HAS_HDIM: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None,\n        :] * stride_cb_csize_k)\n    dout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size_limit, other=0.0)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    K_MAX = chunk_size_limit\n    for k in range(0, K_MAX, BLOCK_SIZE_K):\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k\n            [None, :] < K_MAX - k), other=0.0)\n        dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (\n            offs_n[None, :] < hdim), other=0.0)\n        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0)\n        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None,\n            :] < K_MAX)\n        cb = tl.where(mask, cb, 0.0)\n        cb = cb\n        acc += tl.dot(cb, dout)\n        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dt_ptrs = dt_ptr + offs_m * stride_dt_csize\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0)\n    dx = acc * dt_m[:, None]\n    dx_ptr += (pid_b * stride_dx_batch + pid_c * chunk_size *\n        stride_dx_seqlen + pid_h * stride_dx_head)\n    dx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :\n        ] * stride_dx_hdim)\n    if HAS_D:\n        dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + \n            offs_n[None, :] * stride_dout_hdim)\n        dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] <\n            chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        dx += dout_res * D\n    tl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] *\n        stride_x_hdim)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    ddt = tl.sum(acc * x, axis=1)\n    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\n    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "87fe1480-3656-4648-8e16-ab0977181bc4"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, B, sm_scale, L, ml, O, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk, stride_oz, stride_oh,\n    stride_os, stride_om, stride_lz, stride_lh, stride_ls, stride_lm,\n    stride_bz, stride_bh, stride_bm, stride_bn, Z, H, M, N, P_SEQ, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    DIVISIBLE_N: 'tl.constexpr', HAS_BIAS: 'tl.constexpr', NUM_SPLITS:\n    'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    off_s = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    n_per_split = N // NUM_SPLITS\n    split_n_start = off_s * n_per_split\n    split_n_end = N if off_s + 1 == NUM_SPLITS else split_n_start + n_per_split\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_h * stride_kh\n    V += off_z * stride_vz + off_h * stride_vh\n    O += off_z * stride_oz + off_h * stride_oh + off_s * stride_os\n    if HAS_BIAS:\n        B += off_z * stride_bz + off_h * stride_bh\n    L += off_z * stride_lz + off_h * stride_lh + off_s * stride_ls\n    ml += off_z * stride_lz + off_h * stride_lh + off_s * stride_ls\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    o_ptrs = O + (offs_m[:, None] * stride_om + offs_k[None, :])\n    l_ptrs = L + offs_m\n    ml_ptrs = ml + offs_m\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    mask_m = offs_m < M\n    q = tl.load(q_ptrs, cache_modifier='.cg')\n    if BLOCK_DMODEL < 128:\n        I = tl.where(offs_k[:, None] == offs_k, tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 1.0, dtype=input_dtype), tl.full((BLOCK_DMODEL,\n            BLOCK_DMODEL), 0.0, dtype=input_dtype))\n        q = tl.dot(q, I)\n    offs_n_init = offs_n_base + split_n_start\n    k_ptrs = K + (offs_k[:, None] * stride_vk + offs_n_init[None, :] *\n        stride_vn)\n    v_ptrs = V + (offs_n_init[:, None] * stride_kn + offs_k[None, :] *\n        stride_kk)\n    if HAS_BIAS:\n        bias_ptrs = B + (offs_m[:, None] * stride_bm + offs_n_init[None, :] *\n            stride_bn)\n    for start_n in range(split_n_start, split_n_end, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        mask_n = offs_n < N\n        if DIVISIBLE_N:\n            k = tl.load(k_ptrs, cache_modifier='.cg')\n            v = tl.load(v_ptrs, cache_modifier='.cg')\n        else:\n            k = tl.load(k_ptrs, mask=mask_n[None, :], cache_modifier='.cg')\n            v = tl.load(v_ptrs, mask=mask_n[:, None], cache_modifier='.cg')\n        if HAS_BIAS:\n            b = tl.load(bias_ptrs, mask_m[:, None] & mask_n[None, :])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, k) * sm_scale\n        if HAS_BIAS:\n            s += b\n        if not DIVISIBLE_N:\n            s = tl.where(mask_n[None, :], s, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(s, 1))\n        alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n        p = tl.math.exp2((s - m_i_new[:, None]) * log2e)\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vn\n        if HAS_BIAS:\n            bias_ptrs += BLOCK_N * stride_bn\n    acc = acc * (1.0 / l_i[:, None])\n    l = l_i\n    tl.store(l_ptrs, l, mask=mask_m, cache_modifier='.cg')\n    tl.store(ml_ptrs, m_i, mask=mask_m, cache_modifier='.cg')\n    tl.store(o_ptrs, acc, mask=mask_m[:, None], cache_modifier='.cg')\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "d18f6300-0de7-46d4-8b7f-f0935a2751a5"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, D, stride_qz,\n    stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn,\n    stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, Z, H, N_CTX,\n    num_block, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', CAUSAL: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qk_scale = sm_scale * 1.44269504\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_qz + off_h * stride_qh\n    V += off_z * stride_qz + off_h * stride_qh\n    DO += off_z * stride_qz + off_h * stride_qh\n    DQ += off_z * stride_qz + off_h * stride_qh\n    DK += off_z * stride_qz + off_h * stride_qh\n    DV += off_z * stride_qz + off_h * stride_qh\n    for start_n in range(0, num_block):\n        if CAUSAL:\n            lo = start_n * BLOCK_M\n        else:\n            lo = 0\n        offs_qm = lo + tl.arange(0, BLOCK_M)\n        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_m = tl.arange(0, BLOCK_N)\n        offs_k = tl.arange(0, BLOCK_DMODEL)\n        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk\n            )\n        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk\n            )\n        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        D_ptrs = D + off_hz * N_CTX\n        l_ptrs = L + off_hz * N_CTX\n        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):\n            offs_m_curr = start_m + offs_m\n            q = tl.load(q_ptrs)\n            if CAUSAL:\n                qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :],\n                    float(0.0), float('-inf'))\n            else:\n                qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n            qk += tl.dot(q, tl.trans(k))\n            qk *= qk_scale\n            l_i = tl.load(l_ptrs + offs_m_curr)\n            p = tl.math.exp2(qk - l_i[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(tl.trans(p), do)\n            Di = tl.load(D_ptrs + offs_m_curr)\n            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]\n            dp += tl.dot(do, tl.trans(v))\n            ds = p * dp * sm_scale\n            dk += tl.dot(tl.trans(ds), q)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k)\n            tl.store(dq_ptrs, dq)\n            dq_ptrs += BLOCK_M * stride_qm\n            q_ptrs += BLOCK_M * stride_qm\n            do_ptrs += BLOCK_M * stride_qm\n        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] *\n            stride_qk)\n        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] *\n            stride_kk)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a9557bae-2655-4e06-9d78-980efcafa7ac"
  },
  {
    "input": "@triton.jit\ndef log_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = tl.log(x)\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "logarithm",
    "uuid": "c128b1f8-53b2-48da-afd2-f1c4a22920fe"
  },
  {
    "input": "@triton.jit\ndef embedding_backward_kernel(grad_output_ptr, grad_weight_ptr, indices_ptr,\n    n_elements, embedding_dim: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    grad_output = tl.load(grad_output_ptr + offsets_m[:, None] *\n        embedding_dim + offsets_n[None, :], mask=mask_m[:, None] & mask_n[\n        None, :], other=0.0)\n    grad_weight_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    tl.atomic_add(grad_weight_ptr + grad_weight_offsets, grad_output, mask=\n        mask_m[:, None] & mask_n[None, :])\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "0d1a404c-fdbb-47e7-bbbb-a42ddcd1db03"
  },
  {
    "input": "@triton.jit\ndef triton_bid_merge(y, x, BC: 'tl.constexpr', BT: 'tl.constexpr', d_head:\n    'tl.constexpr', n_heads: 'tl.constexpr', batch_size: 'tl.constexpr',\n    seq_len: 'tl.constexpr', NT: 'tl.constexpr'):\n    i_c = tl.program_id(0)\n    i_t = tl.program_id(1)\n    i_bh = tl.program_id(2)\n    batch_idx = i_bh // n_heads\n    head_idx = i_bh % n_heads\n    block_start_seq = i_t * BT\n    block_start_depth = i_c * BC\n    seq_range = tl.arange(0, BT)\n    depth_range = tl.arange(0, BC)\n    seq_idx = block_start_seq + seq_range\n    depth_idx = block_start_depth + depth_range\n    mask = (seq_idx < seq_len)[:, None] & (depth_idx < d_head)\n    offset_normal = (batch_idx * n_heads * seq_len * d_head + head_idx *\n        seq_len * d_head + seq_idx[:, None] * d_head + depth_idx)\n    offset_mirrored = (batch_idx * n_heads * seq_len * d_head + head_idx *\n        seq_len * d_head + (seq_len - seq_idx - 1)[:, None] * d_head +\n        depth_idx + batch_size * n_heads * seq_len * d_head)\n    normal_vals = tl.load(y + offset_normal, mask=mask)\n    mirrored_vals = tl.load(y + offset_mirrored, mask=mask)\n    combined_vals = normal_vals + mirrored_vals\n    tl.store(x + offset_normal, combined_vals, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "249c80d4-6efc-4b44-bd24-61f279486a85"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "231a6a4b-10dd-4fe0-b2ba-6d01fcfcc5bb"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_INP': 32,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=3, num_warps=1), triton.Config({\n    'BLOCK_SIZE_INP': 64, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=3, num_warps\n    =8), triton.Config({'BLOCK_SIZE_INP': 128, 'BLOCK_SIZE_HIDDEN': 32},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 256,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 32, 'BLOCK_SIZE_HIDDEN': 64}, num_stages=3, num_warps\n    =8), triton.Config({'BLOCK_SIZE_INP': 64, 'BLOCK_SIZE_HIDDEN': 64},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 128,\n    'BLOCK_SIZE_HIDDEN': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 256, 'BLOCK_SIZE_HIDDEN': 64}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_INP': 32, 'BLOCK_SIZE_HIDDEN':\n    128}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 64,\n    'BLOCK_SIZE_HIDDEN': 128}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 128, 'BLOCK_SIZE_HIDDEN': 128}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_INP': 256, 'BLOCK_SIZE_HIDDEN':\n    128}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 32,\n    'BLOCK_SIZE_HIDDEN': 256}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_INP': 64, 'BLOCK_SIZE_HIDDEN': 256}, num_stages=3,\n    num_warps=8), triton.Config({'BLOCK_SIZE_INP': 128, 'BLOCK_SIZE_HIDDEN':\n    256}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_INP': 256,\n    'BLOCK_SIZE_HIDDEN': 256}, num_stages=3, num_warps=8)], key=['inp_size',\n    'hidden_size'], reset_to_zero=['per_slice_sum_ptr'])\n@triton.jit\ndef per_slice_sum_kernel(inp_ptr, upper_end_of_slices_ptr,\n    per_slice_sum_ptr, inp_size, hidden_size, num_slices,\n    stride_inp_inp_size, stride_inp_hidden_size, BLOCK_SIZE_INP:\n    'tl.constexpr', BLOCK_SIZE_HIDDEN: 'tl.constexpr'):\n    \"\"\"Compute the sum per slice.\"\"\"\n    pid = tl.program_id(axis=0)\n    offs_am = pid * BLOCK_SIZE_INP + tl.arange(0, BLOCK_SIZE_INP)\n    ir_range_lower = 0\n    for slice_idx in range(0, num_slices):\n        ir_range_upper = tl.load(upper_end_of_slices_ptr + slice_idx)\n        current_lower = ir_range_lower\n        if num_slices == 1:\n            num_k = tl.cdiv(ir_range_upper - ir_range_lower, BLOCK_SIZE_HIDDEN)\n        else:\n            num_k = tl.cdiv(hidden_size, BLOCK_SIZE_HIDDEN)\n        offs_k = current_lower + tl.arange(0, BLOCK_SIZE_HIDDEN)\n        a_ptrs = inp_ptr + (offs_am[:, None] * stride_inp_inp_size + offs_k\n            [None, :] * stride_inp_hidden_size)\n        for k in range(0, num_k):\n            current_upper = min(ir_range_upper, ir_range_lower + (k + 1) *\n                BLOCK_SIZE_HIDDEN, hidden_size)\n            inp_block = tl.load(a_ptrs, mask=(offs_am[:, None] < inp_size) &\n                (offs_k[None, :] < current_upper), other=0.0)\n            inp_block = inp_block\n            tl.atomic_add(per_slice_sum_ptr + slice_idx, tl.sum(inp_block))\n            offs_k += current_upper - current_lower\n            a_ptrs += (current_upper - current_lower) * stride_inp_hidden_size\n            current_lower = current_upper\n        ir_range_lower = ir_range_upper\n",
    "category": "Kernel Operations",
    "subcategory": "kernel configuration",
    "uuid": "adfb19cc-f8be-4035-b277-a5d591781c32"
  },
  {
    "input": "@triton.jit\ndef add_mask2_kernel(x_ptr, z_ptr, N0, B0: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    off_x = block_id * B0 + tl.arange(0, B0)\n    mask = off_x < N0\n    x = tl.load(x_ptr + off_x, mask=mask)\n    x = x + 10.0\n    tl.store(z_ptr + off_x, x, mask=mask)\n    return\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "096c91ff-1907-4cbd-b214-175aa6cc2e48"
  },
  {
    "input": "@triton.autotune(DEFAULT_DEQUANT_CONFIGS, key=['numels'])\n@triton.jit\ndef dequant_kernel_248(g_idx_ptr, scales_ptr, qweight_ptr, qzeros_ptr,\n    out_ptr, numels, maxq: 'tl.constexpr', bits: 'tl.constexpr',\n    outfeatures: 'tl.constexpr', num_groups: 'tl.constexpr', X_BLOCK:\n    'tl.constexpr'):\n    xoffset = tl.program_id(0) * X_BLOCK\n    x_index = xoffset + tl.arange(0, X_BLOCK)\n    xmask = x_index < numels\n    row_idx = x_index // outfeatures\n    col_idx = x_index % outfeatures\n    elements_per_feature: 'tl.constexpr' = 32 // bits\n    g_idx = tl.load(g_idx_ptr + row_idx, None, eviction_policy='evict_last')\n    qweights = tl.load(qweight_ptr + (col_idx + outfeatures * (row_idx //\n        elements_per_feature)), None)\n    wf_weights = row_idx % elements_per_feature * bits\n    wf_zeros = col_idx % elements_per_feature * bits\n    tmp1 = g_idx + num_groups\n    tmp2 = g_idx < 0\n    tl.device_assert(g_idx >= 0, 'index out of bounds: 0 <= tmp0 < 0')\n    groups = tl.where(tmp2, tmp1, g_idx)\n    scales = tl.load(scales_ptr + (col_idx + outfeatures * groups), None)\n    weights = qweights >> wf_weights\n    weights = weights & maxq\n    qzero_ncols: 'tl.constexpr' = outfeatures // elements_per_feature\n    qzeros = tl.load(qzeros_ptr + (qzero_ncols * groups + col_idx //\n        elements_per_feature), None, eviction_policy='evict_last')\n    zeros = qzeros >> wf_zeros\n    zeros = zeros & maxq\n    weights = weights - zeros\n    weights = weights\n    weights = scales * weights\n    tl.store(out_ptr + x_index, weights, mask=xmask)\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "bc2f3760-dab8-4156-b189-318ac775c743"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(O_block, l_i, m_i, Q_block, K_block_ptr, V_block_ptr,\n    block_index_q, softmax_scale, BLOCK_SIZE_Q: 'tl.constexpr',\n    BLOCK_SIZE_KV: 'tl.constexpr', STAGE: 'tl.constexpr', offs_q:\n    'tl.constexpr', offs_kv: 'tl.constexpr', SEQ_LEN: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, block_index_q * BLOCK_SIZE_Q\n    elif STAGE == 2:\n        lo, hi = block_index_q * BLOCK_SIZE_Q, (block_index_q + 1\n            ) * BLOCK_SIZE_Q\n        lo = tl.multiple_of(lo, BLOCK_SIZE_Q)\n    else:\n        lo, hi = 0, SEQ_LEN\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_kv in range(lo, hi, BLOCK_SIZE_KV):\n        start_kv = tl.multiple_of(start_kv, BLOCK_SIZE_KV)\n        K_block = tl.load(K_block_ptr)\n        QK_block = tl.dot(Q_block, K_block)\n        if STAGE == 2:\n            mask = offs_q[:, None] >= start_kv + offs_kv[None, :]\n            QK_block = QK_block * softmax_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(QK_block, 1))\n            QK_block -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(QK_block, 1) * softmax_scale)\n            QK_block = QK_block * softmax_scale - m_ij[:, None]\n        P_block = tl.math.exp(QK_block)\n        l_ij = tl.sum(P_block, 1)\n        alpha = tl.math.exp(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        V_block = tl.load(V_block_ptr)\n        P_block = P_block\n        O_block = O_block * alpha[:, None]\n        O_block = tl.dot(P_block, V_block, O_block)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_SIZE_KV, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_SIZE_KV))\n    return O_block, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "957444ff-9f63-468c-b7e8-5d5a52d95d3d"
  },
  {
    "input": "@triton.jit\ndef _inner_paged_attn_unroll_0_kernel(q, k_cache, v_cache, stride_km,\n    block_base_ptrs, base_offs_kv, alibi_slope, block_offs, seq_len, qkv,\n    qk_max, exp_sum, BLOCK_SIZE: 'tl.constexpr', LO: 'tl.constexpr', HI:\n    'tl.constexpr'):\n    for block_idx in range(LO, HI, 1):\n        offs_kv_0 = tl.load(block_base_ptrs + block_idx + 0\n            ) * stride_km + base_offs_kv\n        k_0 = tl.load(k_cache + offs_kv_0)\n        v_0 = tl.load(v_cache + offs_kv_0)\n        _qk_0 = tl.sum(q[None, :] * k_0, axis=1)\n        if alibi_slope is not None:\n            _qk_0 += alibi_slope * ((block_idx + 0) * BLOCK_SIZE +\n                block_offs - seq_len + 1)\n        _qk_max = tl.maximum(tl.max(_qk_0, axis=0), qk_max)\n        exp_tmp = tl.exp(_qk_0 - _qk_max)\n        _exp_sum = exp_sum * tl.exp(qk_max - _qk_max) + tl.sum(exp_tmp, axis=0)\n        qkv_sum_tmp = tl.exp(_qk_0[:, None] - _qk_max) * v_0\n        qkv = (qkv * (exp_sum * tl.exp(qk_max - _qk_max)) + qkv_sum_tmp\n            ) / _exp_sum\n        qk_max = _qk_max\n        exp_sum = _exp_sum\n    return qkv, qk_max, exp_sum\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "a18f6e25-ca4d-4728-b322-f4ebf2353b2b"
  },
  {
    "input": "@triton.autotune(configs=_generate_configs(), reset_to_zero=['grad_other'],\n    key=['N', 'K', 'stddev_tile_size_m', 'avg_tile_size_m'],\n    prune_configs_by={'early_config_prune': functools.partial(\n    _early_config_prune, is_weight=True), 'perf_model': _weight_perf_model,\n    'top_k': 100 if GlobalConfig.with_perf_model else 10}, use_cuda_graph=\n    True, rep=20)\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % args['TILE_SIZE_K'] ==\n    0, 'EVEN_N': lambda args: args['N'] % args['TILE_SIZE_N'] == 0})\n@triton.jit\ndef split_matmul_kernel(input, input_slices, input_tiles, grad_output,\n    grad_other, grad_other_tiles, K, N, stride_input_m, stride_input_k,\n    stride_grad_output_m, stride_grad_output_n, stride_grad_other_b,\n    stride_grad_other_k, stride_grad_other_n, stddev_tile_size_m,\n    avg_tile_size_m, out_dtype: 'tl.constexpr', NUM_BLOCKS: 'tl.constexpr',\n    NUM_TILES: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', TILE_SIZE_M:\n    'tl.constexpr', TILE_SIZE_N: 'tl.constexpr', TILE_SIZE_K:\n    'tl.constexpr', EVEN_K: 'tl.constexpr', EVEN_N: 'tl.constexpr',\n    DETERMINISTIC: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    grid_k = tl.cdiv(K, TILE_SIZE_K)\n    grid_n = tl.cdiv(N, TILE_SIZE_N)\n    next_id = pid // (grid_k * grid_n)\n    next_next_id = tl.load(input_tiles + 5 * next_id + 4)\n    tile_id = pid % (grid_k * grid_n)\n    pid_k = tile_id // grid_n\n    pid_n = tile_id % grid_n\n    if next_next_id == 0:\n        slice_id = tl.load(input_tiles + 5 * next_id + 0)\n        type_id = tl.load(input_tiles + 5 * next_id + 1)\n        start_off = tl.load(input_tiles + 5 * next_id + 2)\n        slice_start = tl.load(input_slices + 5 * slice_id + 2)\n        slice_end = tl.load(input_slices + 5 * slice_id + 3)\n        M = slice_end - slice_start\n        _dynamic_matmul(pid_k, pid_n, next_id, input + start_off *\n            stride_input_m, grad_output + start_off * stride_grad_output_m,\n            grad_other + type_id * stride_grad_other_b, grad_other_tiles,\n            stride_input_m, stride_input_k, stride_grad_output_m,\n            stride_grad_output_n, stride_grad_other_b, stride_grad_other_k,\n            stride_grad_other_n, K, N, M, TILE_SIZE_M * BLOCK_SIZE,\n            out_dtype=out_dtype, BLOCK_LENGTH=TILE_SIZE_M * BLOCK_SIZE,\n            TILE_K=TILE_SIZE_K, TILE_N=TILE_SIZE_N, TILE_M=TILE_SIZE_M,\n            EVEN_K=EVEN_K, EVEN_N=EVEN_N, EVEN_M=True, DETERMINISTIC=\n            DETERMINISTIC)\n    else:\n        _split_noncontiguous_block(pid_k, pid_n, input, input_slices,\n            input_tiles, grad_output, grad_other, grad_other_tiles,\n            stride_input_m, stride_input_k, stride_grad_output_m,\n            stride_grad_output_n, stride_grad_other_b, stride_grad_other_k,\n            stride_grad_other_n, K, N, next_id, next_next_id, out_dtype=\n            out_dtype, BLOCK_SIZE=BLOCK_SIZE, NUM_TILES=NUM_TILES, TILE_K=\n            TILE_SIZE_K, TILE_N=TILE_SIZE_N, TILE_M=TILE_SIZE_M, EVEN_K=\n            EVEN_K, EVEN_N=EVEN_N, DETERMINISTIC=DETERMINISTIC)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "ff0d1fee-5612-4158-a978-11c5d855fd0c"
  },
  {
    "input": "@triton.jit\ndef _gelu_and_mul_kernel(input_ptr, output_ptr, stride_input_m,\n    stride_input_n, stride_output_m, stride_output_n, size_m, size_n,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    tid = tl.program_id(0)\n    input_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    output_m_offsets = tid * BLOCK_M + tl.arange(0, BLOCK_M)\n    pid = tl.program_id(1)\n    input_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    output_n_offsets = pid * BLOCK_N + tl.arange(0, BLOCK_N)\n    up_offsets = input_m_offsets[:, None] * stride_input_m + (input_n_offsets\n        [None, :] + size_n) * stride_input_n\n    gate_offsets = input_m_offsets[:, None] * stride_input_m + input_n_offsets[\n        None, :] * stride_input_n\n    res_offsets = output_m_offsets[:, None\n        ] * stride_output_m + output_n_offsets[None, :] * stride_output_n\n    up = tl.load(input_ptr + up_offsets, mask=(input_n_offsets < size_n)[\n        None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = tl.load(input_ptr + gate_offsets, mask=(input_n_offsets < size_n\n        )[None, :] * (input_m_offsets < size_m)[:, None], other=0.0)\n    gate = gelu(gate)\n    gate = gate\n    tl.store(output_ptr + res_offsets, up * gate, mask=(output_n_offsets <\n        size_n)[None, :] * (output_m_offsets < size_m)[:, None])\n",
    "category": "Activation Functions",
    "subcategory": "gelu",
    "uuid": "e6704f8f-30c7-4961-954f-ee635c6118b4"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv4_backward_kernel(w_ptr, w_s_c, u_ptr, u_s_c, k_ptr,\n    k_s_b, k_s_t, k_s_c, v_ptr, v_s_b, v_s_t, v_s_c, state_ptr, state_s_b,\n    state_s_abe, state_s_t, state_s_c, gwkv_ptr, gwkv_s_b, gwkv_s_t,\n    gwkv_s_c, gstate_out_ptr, gstate_out_s_b, gstate_out_s_abe,\n    gstate_out_s_c, gw_ptr, gw_s_c, gu_ptr, gu_s_c, gk_ptr, gk_s_b, gk_s_t,\n    gk_s_c, gv_ptr, gv_s_b, gv_s_t, gv_s_c, gstate_ptr, gstate_s_b,\n    gstate_s_abe, gstate_s_c, tsz, chans, BLOCK_SIZE_C: 'tl.constexpr'):\n    b_idx = tl.program_id(0)\n    c_idx = tl.program_id(1)\n    cs = c_idx * BLOCK_SIZE_C + tl.arange(0, BLOCK_SIZE_C)\n    cmask = cs < chans\n    k_ptr = k_ptr + b_idx * k_s_b\n    v_ptr = v_ptr + b_idx * v_s_b\n    alpha_ptr = state_ptr + b_idx * state_s_b\n    beta_ptr = state_ptr + b_idx * state_s_b + state_s_abe\n    eps_ptr = state_ptr + b_idx * state_s_b + 2 * state_s_abe\n    gk_ptr = gk_ptr + b_idx * gk_s_b\n    gv_ptr = gv_ptr + b_idx * gv_s_b\n    gwkv_ptr = gwkv_ptr + b_idx * gwkv_s_b\n    galpha_out_ptr = gstate_out_ptr + b_idx * gstate_out_s_b\n    gbeta_out_ptr = gstate_out_ptr + b_idx * gstate_out_s_b + gstate_out_s_abe\n    geps_out_ptr = (gstate_out_ptr + b_idx * gstate_out_s_b + 2 *\n        gstate_out_s_abe)\n    galpha = tl.load(galpha_out_ptr + gstate_out_s_c * cs, mask=cmask)\n    gbeta = tl.load(gbeta_out_ptr + gstate_out_s_c * cs, mask=cmask)\n    geps = tl.load(geps_out_ptr + gstate_out_s_c * cs, mask=cmask)\n    w = tl.load(w_ptr + w_s_c * cs, mask=cmask)\n    u = tl.load(u_ptr + u_s_c * cs, mask=cmask)\n    gw = tl.zeros_like(w)\n    gu = tl.zeros_like(u)\n    alpha_prev = tl.load(alpha_ptr + tsz * state_s_t + state_s_c * cs, mask\n        =cmask)\n    beta_prev = tl.load(beta_ptr + tsz * state_s_t + state_s_c * cs, mask=cmask\n        )\n    eps_prev = tl.load(eps_ptr + tsz * state_s_t + state_s_c * cs, mask=cmask)\n    for t in range(tsz):\n        tc = tsz - t - 1\n        kt = tl.load(k_ptr + tc * k_s_t + k_s_c * cs, mask=cmask)\n        vt = tl.load(v_ptr + tc * v_s_t + v_s_c * cs, mask=cmask)\n        alpha_curr = alpha_prev\n        beta_curr = beta_prev\n        eps_curr = eps_prev\n        alpha_prev = tl.load(alpha_ptr + tc * state_s_t + state_s_c * cs,\n            mask=cmask)\n        beta_prev = tl.load(beta_ptr + tc * state_s_t + state_s_c * cs,\n            mask=cmask)\n        eps_prev = tl.load(eps_ptr + tc * state_s_t + state_s_c * cs, mask=\n            cmask)\n        ukt = u + kt\n        tau = tl.maximum(ukt, eps_prev)\n        e1 = tl.exp(eps_prev - tau)\n        e2 = tl.exp(ukt - tau)\n        euke = tl.exp(ukt + eps_prev - 2 * tau)\n        denom = e1 * beta_prev + e2\n        denom_sq = denom * denom\n        gwkvt = tl.load(gwkv_ptr + tc * gwkv_s_t + gwkv_s_c * cs, mask=cmask)\n        guk = gwkvt * e2 * (e1 * beta_prev * vt - e1 * alpha_prev) / denom_sq\n        gu += guk\n        gk = guk\n        gv = gwkvt * e2 / denom\n        galpha_wkv = gwkvt * e1 / denom\n        gbeta_wkv = -gwkvt * e1 * (e2 * vt + e1 * alpha_prev) / denom_sq\n        geps_wkv_denom = e1 * beta_prev + e2\n        geps_wkv = gwkvt * euke * (alpha_prev - vt * beta_prev) / (\n            geps_wkv_denom * geps_wkv_denom)\n        e1 = tl.exp(w + eps_prev - eps_curr)\n        e2 = tl.exp(kt - eps_curr)\n        galpha_we = galpha * e1 * alpha_prev\n        gw += galpha_we\n        gk += galpha * e2 * vt\n        gv += galpha * e2\n        geps += galpha * -alpha_curr\n        gbeta_we = gbeta * e1 * beta_prev\n        gw += gbeta_we\n        gk += gbeta * e2\n        geps += gbeta * -beta_curr\n        geps_mask = w + eps_prev > kt\n        geps_we = tl.where(geps_mask, geps, tl.zeros_like(geps))\n        gw += geps_we\n        gk += tl.where(geps_mask, tl.zeros_like(geps), geps)\n        tl.store(gk_ptr + tc * gk_s_t + gk_s_c * cs, gk, mask=cmask)\n        tl.store(gv_ptr + tc * gv_s_t + gv_s_c * cs, gv, mask=cmask)\n        galpha = galpha * e1 + galpha_wkv\n        gbeta = gbeta * e1 + gbeta_wkv\n        geps = galpha_we + gbeta_we + geps_we + geps_wkv\n    galpha_ptr = gstate_ptr + b_idx * gstate_s_b\n    gbeta_ptr = gstate_ptr + b_idx * gstate_s_b + gstate_s_abe\n    geps_ptr = gstate_ptr + b_idx * gstate_s_b + 2 * gstate_s_abe\n    tl.store(galpha_ptr + gstate_s_c * cs, galpha, mask=cmask)\n    tl.store(gbeta_ptr + gstate_s_c * cs, gbeta, mask=cmask)\n    tl.store(geps_ptr + gstate_s_c * cs, geps, mask=cmask)\n    gw_temp = tl.load(gw_ptr + gw_s_c * cs, mask=cmask)\n    gw_temp += gw\n    tl.store(gw_ptr + gw_s_c * cs, gw_temp, mask=cmask)\n    gu_temp = tl.load(gu_ptr + gu_s_c * cs, mask=cmask)\n    gu_temp += gu\n    tl.store(gu_ptr + gu_s_c * cs, gu_temp, mask=cmask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "59d5f787-495a-439c-8b1b-695e05d29232"
  },
  {
    "input": "@triton.jit\ndef _softmax_core(input_ptrs, output_ptrs, mask_ptrs, bias_ptrs,\n    col_offsets, n_cols, use_mask: 'tl.constexpr', use_bias: 'tl.constexpr'):\n    row = tl.load(input_ptrs, mask=col_offsets < n_cols, other=-float('inf'))\n    if use_bias:\n        bias = tl.load(bias_ptrs, mask=col_offsets < n_cols, other=float(\n            '-inf'))\n        row += bias\n    if use_mask:\n        mask = tl.load(mask_ptrs, mask=col_offsets < n_cols, other=float(\n            '-inf'))\n        row = tl.where(mask == 0, float('-1e20'), row)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    tl.store(output_ptrs, softmax_output, mask=col_offsets < n_cols)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "0b716d39-a733-4360-a285-cc65be1922fb"
  },
  {
    "input": "@triton.jit\ndef moe_align_block_size_stage3(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    last_cumsum = 0\n    off_cnt = num_experts * num_experts\n    for i in range(1, num_experts + 1):\n        token_cnt = tl.load(tokens_cnts_ptr + off_cnt + i - 1)\n        last_cumsum = last_cumsum + tl.cdiv(token_cnt, block_size) * block_size\n        tl.store(cumsum_ptr + i, last_cumsum)\n    tl.store(total_tokens_post_pad_ptr, last_cumsum)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "fd229b9b-3566-4c5f-acee-8e5fa079ccc1"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128},\n    num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_bwd_old_kernel(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m *\n        stride_ddA_cs_csize_m)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    end_offset = chunk_size - BLOCK_SIZE_N\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + ((end_offset + offs_n[None, :]) * stride_x_seqlen + \n        offs_k[:, None] * stride_x_hdim)\n    dt_ptrs = dt_ptr + (end_offset + offs_n) * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + (end_offset +\n        offs_n[None, :]) * stride_cb_csize_n)\n    ddAcs_ptrs = ddA_cumsum_ptr + (end_offset + offs_n) * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    for start_n in range(0, chunk_size - pid_m * BLOCK_SIZE_M, BLOCK_SIZE_N):\n        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n        offset = chunk_size - BLOCK_SIZE_N - start_n\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit - offset) & (offs_n[None, :] >= -offset),\n            other=0.0)\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n >= -offset, other=0.0)\n        acc *= dt_n\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n\n            [None, :] >= -offset), other=0.0)\n        acc *= cb\n        dA_cs_n = tl.load(dA_cumsum_ptr + offset + offs_n *\n            stride_dA_cs_csize, mask=offs_n >= -offset, other=0.0)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] <= offset + offs_n[None, :] - 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=offs_n >=\n            1 - offset)\n        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n    for start_n in range(pid_m * BLOCK_SIZE_M, 0, -BLOCK_SIZE_N):\n        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N\n            ,), dtype=tl.float32), mask=offs_n >= 1 - start_n)\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "4252e249-7489-4790-b73d-4ceb0d17647e"
  },
  {
    "input": "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD_FUSED, key=['max_seqlen_q',\n    'max_seqlen_k', 'head_dim'])\n@triton.jit\ndef tuned_attn_bwd(Q, K, V, B, sm_scale, Out, DO, DK, DV, DQ, DB, L, D,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n    stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn, stride_dkk,\n    stride_dvz, stride_dvh, stride_dvk, stride_dvn, stride_dqz, stride_dqh,\n    stride_dqm, stride_dqk, stride_dbz, stride_dbh, stride_dbm, stride_dbn,\n    num_head_q, num_head_k, cu_seqlens_q, cu_seqlens_k, num_seqlens,\n    max_seqlen_q, max_seqlen_k, head_dim, dropout_p, philox_seed_ptr,\n    philox_offset1, philox_offset2, BLOCK_DMODEL: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr', BIAS_TYPE: 'tl.constexpr', BLOCK_M1: 'tl.constexpr',\n    BLOCK_N1: 'tl.constexpr', BLOCK_M2: 'tl.constexpr', BLOCK_N2:\n    'tl.constexpr', BLK_SLICE_FACTOR: 'tl.constexpr'):\n    bare_attn_bwd(Q, K, V, B, sm_scale, Out, DO, DK, DV, DQ, DB, L, D,\n        stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n        stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n        stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n        stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn,\n        stride_dkk, stride_dvz, stride_dvh, stride_dvk, stride_dvn,\n        stride_dqz, stride_dqh, stride_dqm, stride_dqk, stride_dbz,\n        stride_dbh, stride_dbm, stride_dbn, num_head_q, num_head_k,\n        cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k,\n        head_dim, dropout_p, philox_seed_ptr, philox_offset_base,\n        BLOCK_DMODEL, CAUSAL, ENABLE_DROPOUT, PADDED_HEAD, BIAS_TYPE,\n        BLOCK_M1, BLOCK_N1, BLOCK_M2, BLOCK_N2, BLK_SLICE_FACTOR)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "bebd6f1e-88c2-4e0c-ab09-61ca4ec7c774"
  },
  {
    "input": "@triton.jit\ndef liger_cross_entropy_kernel(X_ptr, X_stride, Y_ptr, Y_stride, loss_ptr,\n    loss_stride, n_cols, n_non_ignore, ignore_index, label_smoothing:\n    'tl.constexpr', reduction: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    Y_ptr += program_id * Y_stride\n    y = tl.load(Y_ptr)\n    X_ptr += program_id * X_stride\n    if y == ignore_index:\n        for i in range(0, n_cols, BLOCK_SIZE):\n            X_offsets = i + tl.arange(0, BLOCK_SIZE)\n            tl.store(X_ptr + X_offsets, 0.0, mask=X_offsets < n_cols)\n        return\n    loss_ptr += program_id * loss_stride\n    m = float('-inf')\n    d = 0.0\n    ori_X_y = tl.load(X_ptr + y)\n    scaled_x_sum = 0.0\n    eps = label_smoothing / n_cols\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        block_max = tl.max(X_block)\n        if label_smoothing > 0:\n            scaled_x_sum += tl.sum(tl.where(X_offsets < n_cols, -eps *\n                X_block, 0.0))\n        m_new = tl.maximum(m, block_max)\n        d = d * tl.exp(m - m_new) + tl.sum(tl.exp(X_block - m_new))\n        m = m_new\n    for i in range(0, n_cols, BLOCK_SIZE):\n        X_offsets = i + tl.arange(0, BLOCK_SIZE)\n        X_block = tl.load(X_ptr + X_offsets, mask=X_offsets < n_cols, other\n            =float('-inf'))\n        if reduction == 'mean':\n            X_block = (tl.exp(X_block - m) / d - eps) / n_non_ignore\n        else:\n            X_block = tl.exp(X_block - m) / d - eps\n        tl.store(X_ptr + X_offsets, X_block, mask=X_offsets < n_cols)\n    tl.debug_barrier()\n    loss = -(ori_X_y - m - tl.log(d))\n    if label_smoothing > 0:\n        smooth_loss = scaled_x_sum + label_smoothing * (m + tl.log(d))\n        loss = loss * (1 - label_smoothing) + smooth_loss\n    if reduction == 'mean':\n        loss = loss / n_non_ignore\n    X_y = tl.load(X_ptr + y)\n    if reduction == 'mean':\n        X_y += -(1 - label_smoothing) / n_non_ignore\n    else:\n        X_y += -(1 - label_smoothing)\n    tl.store(loss_ptr, loss)\n    tl.store(X_ptr + y, X_y)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "9198c8a3-9564-4153-b6ab-e7f7aea3df9c"
  },
  {
    "input": "@triton.jit\ndef calc_p_loss(input, target, size, p_loss: 'tl.constexpr', reduction:\n    'tl.constexpr'):\n    \"\"\"\n    Measures the L1 or squared L2 norm of the difference between the input\n    and target (i.e., mean absolute error or mean squared error).\n\n    Args:\n        input: Input.\n            The input must be of shape [BLOCK_SIZE].\n        target: Target.\n            The target must be of shape [BLOCK_SIZE].\n        size: Number of elements in the input and target.\n            This value is used only if reduction is 'mean'.\n        p_loss: p-norm used to compute the error.\n            Options are 1 for MAE and 2 for MSE.\n        reduction: Reduction strategy for the output.\n            Options are 'none' for no reduction, 'mean' for averaging the error\n            across all entries, and 'sum' for summing the error across all entries.\n\n    Returns:\n        Error.\n    \"\"\"\n    input = input\n    target = target\n    diff = input - target\n    if p_loss == 1:\n        error = tl.abs(diff)\n    elif p_loss == 2:\n        error = diff * diff\n    if reduction == 'none':\n        output = error\n    elif reduction == 'mean':\n        output = tl.sum(error) / size\n    elif reduction == 'sum':\n        output = tl.sum(error)\n    return output\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "a1d9fe1e-b9c4-4b1e-ad1e-0d1ccf91a847"
  },
  {
    "input": "@triton.jit\ndef triton_batch_lora_B(output, x, w, a_start, a_len, a_loc, batch_req_bins,\n    a_scaling, qkvo_offset: 'tl.constexpr', NUM_TOKENS: 'tl.constexpr',\n    HIDDEN: 'tl.constexpr', MAX_LORA_RANK: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    return\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "df3242b2-6074-47d2-bf85-ca1ef70ec51d"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "5bfcb405-ca03-48ec-ad60-ebef18a87dc4"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, M, D, sqz, sqh,\n    sqm, sqd, skz, skh, skn, skd, svz, svh, svn, svd, Z, H, N_CTX_Q,\n    N_CTX_KV, BLOCK: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    N_PREFIX_Q: 'tl.constexpr'):\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    BLOCK_M: 'tl.constexpr' = BLOCK\n    BLOCK_N: 'tl.constexpr' = BLOCK\n    Q += off_z * sqz + off_h * sqh\n    K += off_z * skz + off_h * skh\n    V += off_z * svz + off_h * svh\n    DO += off_z * sqz + off_h * sqh\n    DQ += off_z * sqz + off_h * sqh\n    DK += off_z * skz + off_h * skh\n    DV += off_z * svz + off_h * svh\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    D_ptrs = D + off_hz * N_CTX_Q\n    m_ptrs = M + off_hz * N_CTX_Q\n    for start_n in range(0, N_CTX_KV, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + tl.arange(0, BLOCK_N)\n        k_ptrs = K + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        v_ptrs = V + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dv = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n        dk = tl.zeros([BLOCK_N, BLOCK_DMODEL], dtype=tl.float32)\n        k = tl.load(k_ptrs)\n        v = tl.load(v_ptrs)\n        if start_n < N_PREFIX_Q * BLOCK_M:\n            start_q_index = 0\n        elif N_CTX_Q <= start_n - N_PREFIX_Q * BLOCK_M:\n            start_q_index = start_n - N_PREFIX_Q * BLOCK_M\n        else:\n            first_start_m = start_n - N_PREFIX_Q * BLOCK_M\n            first_start_m = tl.multiple_of(first_start_m, BLOCK_M)\n            offs_m = first_start_m + tl.arange(0, BLOCK_M)\n            offs_m_real = offs_m + N_PREFIX_Q * BLOCK_M\n            offs_m_real += tl.where(tl.arange(0, BLOCK_M) == BLOCK_M - 1, -1, 0\n                )\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk = tl.where(offs_m_real[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n            m = tl.load(m_ptrs + offs_m)\n            m_ = m\n            last_p = tl.exp(qk * sm_scale - m_[:, None])\n            do = tl.load(do_ptrs)\n            dv += tl.dot(tl.trans(last_p), do, allow_tf32=False)\n            Di = tl.load(D_ptrs + offs_m)\n            last_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:,\n                None]\n            last_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            ds = last_p * last_dp * sm_scale\n            dk += tl.dot(tl.trans(ds), q, allow_tf32=False)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k, allow_tf32=False)\n            tl.store(dq_ptrs, dq)\n            start_q_index = first_start_m + BLOCK_M\n        for start_m in range(start_q_index, N_CTX_Q, BLOCK_M):\n            start_m = tl.multiple_of(start_m, BLOCK_M)\n            offs_m = start_m + tl.arange(0, BLOCK_M)\n            q_ptrs = Q + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            do_ptrs = DO + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            dq_ptrs = DQ + (offs_m[:, None] * sqm + offs_d[None, :] * sqd)\n            q = tl.load(q_ptrs)\n            qk = tl.dot(q, tl.trans(k), allow_tf32=False)\n            qk *= sm_scale\n            landmark_qk = tl.max(tl.where(tl.arange(0, BLOCK_N)[None, :] ==\n                BLOCK_N - 1, qk, float('-inf')), 1)\n            normal_qk = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N -\n                1, float('-inf'), qk)\n            m = tl.load(m_ptrs + offs_m)\n            m_ = m\n            p = tl.exp(landmark_qk - m_)\n            do = tl.load(do_ptrs)\n            normal_m = tl.max(normal_qk, 1)\n            normal_p = tl.exp(normal_qk - normal_m[:, None])\n            normal_p_normalized = normal_p / tl.sum(normal_p, 1)[:, None]\n            normal_kv = tl.dot(normal_p_normalized, v, allow_tf32=False)\n            normal_D = tl.sum(do * normal_kv, 1)\n            dv += tl.dot(tl.trans(p[:, None] * normal_p_normalized), do,\n                allow_tf32=False)\n            Di = tl.load(D_ptrs + offs_m)\n            dp = tl.zeros([BLOCK_M], dtype=tl.float32) - Di\n            dp += normal_D\n            landmark_ds = p * dp\n            normal_dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32\n                ) - normal_D[:, None]\n            normal_dp += tl.dot(do, tl.trans(v), allow_tf32=False)\n            normal_ds = p[:, None] * normal_p_normalized * normal_dp\n            ds = tl.where(tl.arange(0, BLOCK_N)[None, :] == BLOCK_N - 1,\n                landmark_ds[:, None], normal_ds)\n            ds *= sm_scale\n            dk += tl.dot(tl.trans(ds), q, allow_tf32=False)\n            dq = tl.load(dq_ptrs)\n            dq += tl.dot(ds, k, allow_tf32=False)\n            tl.store(dq_ptrs, dq)\n        dv_ptrs = DV + (offs_n[:, None] * svn + offs_d[None, :] * svd)\n        dk_ptrs = DK + (offs_n[:, None] * skn + offs_d[None, :] * skd)\n        tl.store(dv_ptrs, dv)\n        tl.store(dk_ptrs, dk)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "194aac88-13d0-425c-ab22-a0d0891e1b3c"
  },
  {
    "input": "@triton.jit\ndef _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n    s_k_h, s_k_t, s_k_d, s_v_h, s_v_t, s_v_d, scale, B: 'tl.constexpr', H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BTL: 'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr'):\n    p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (0, \n        i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, 0), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\n    b_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n    b_dq *= scale\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (V, T), (s_v_d, s_v_t), (i_v *\n        BV, i_c * BTL), (BV, BTS), (0, 1))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(2 * b_ds * b_s, b_k, allow_tf32=False)\n        p_k = tl.advance(p_k, (BTS, 0))\n        p_v = tl.advance(p_v, (0, BTS))\n        o_k += BTS\n    p_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * s_k_h, (T, K), (\n        s_k_t, s_k_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "71b85afd-db5a-4c29-9f08-b22c9884204e"
  },
  {
    "input": "@triton.jit\ndef __flat_csr_sdbmm_compute(CROW_INDICES, stride_crow_n, stride_crow_r,\n    COL_INDICES, stride_col_n, stride_col_z, VALUES, stride_v_n, stride_v_z,\n    OTHER, stride_other_n, stride_other_h, stride_other_t, stride_other_d,\n    OUTPUT, stride_output_n, stride_output_h, stride_output_t,\n    stride_output_d, TEMP_COUNT_HEAD, stride_tch_n, stride_tch_r,\n    stride_tch_h, N, R, Z, H, T_DST, T_SRC, HID, MAX_ROW_Z: 'tl.constexpr',\n    MAX_ROW_T: 'tl.constexpr', BLOCK_HID: 'tl.constexpr', BLOCK_H:\n    'tl.constexpr', BLOCK_R: 'tl.constexpr', BLOCK_COL_HEAD: 'tl.constexpr',\n    GRID_COL_HEAD: 'tl.constexpr'):\n    n = tl.program_id(0)\n    pid_ir = tl.program_id(1)\n    grid_ir = tl.num_programs(1)\n    pid_hid = tl.program_id(2)\n    for _ir in range(BLOCK_R):\n        ir = _ir * grid_ir + pid_ir\n        ir_mask = ir < R\n        crow_start = tl.load(CROW_INDICES + n * stride_crow_n + ir *\n            stride_crow_r, mask=ir_mask)\n        crow_end = tl.load(CROW_INDICES + n * stride_crow_n + (ir + 1) *\n            stride_crow_r, mask=ir_mask)\n        for ih in range(H):\n            ch_start = tl.load(TEMP_COUNT_HEAD + n * stride_tch_n + ir *\n                stride_tch_r + (ih - 1) * stride_tch_h, mask=(ih - 1 >= 0) &\n                (ih - 1 < H) & ir_mask, other=0)\n            ch_end = tl.load(TEMP_COUNT_HEAD + n * stride_tch_n + ir *\n                stride_tch_r + ih * stride_tch_h, mask=(ih < H) & ir_mask,\n                other=-1)\n            ch_len = ch_end - ch_start\n            per_head_col_indices_mask = tl.arange(0, MAX_ROW_T) < ch_len\n            per_head_col_indices = tl.load(COL_INDICES + n * stride_col_n +\n                (tl.arange(0, MAX_ROW_T) + ch_start + crow_start) *\n                stride_col_z, mask=per_head_col_indices_mask & ir_mask, other=1\n                ) % T_SRC\n            row_values = tl.load(VALUES + n * stride_v_n + (tl.arange(0,\n                MAX_ROW_T) + ch_start + crow_start) * stride_v_z, mask=\n                per_head_col_indices_mask & ir_mask, other=0)\n            hid_range = tl.arange(0, BLOCK_HID) + pid_hid * BLOCK_HID\n            hid_mask = hid_range < HID\n            other_mask = per_head_col_indices_mask[:, None] & hid_mask[None, :\n                ] & ir_mask\n            other = tl.load(OTHER + n * stride_other_n + ih *\n                stride_other_h + per_head_col_indices[:, None] *\n                stride_other_t + hid_range[None, :] * stride_other_d, mask=\n                other_mask, other=0)\n            tl.static_assert(other.shape[0] == MAX_ROW_T)\n            tl.static_assert(other.shape[1] == BLOCK_HID)\n            other_mul = row_values[:, None] * other\n            other_sum = tl.sum(other_mul, axis=0)\n            tl.store(OUTPUT + n * stride_output_n + ih * stride_output_h + \n                ir * stride_output_t + (tl.arange(0, BLOCK_HID) + pid_hid *\n                BLOCK_HID) * stride_output_d, other_sum, mask=(tl.arange(0,\n                BLOCK_HID) + pid_hid * BLOCK_HID < HID) & ir_mask)\n",
    "category": "Helper Functions",
    "subcategory": "shape manipulation",
    "uuid": "96331043-9c37-47d1-8bc4-ce8380094823"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BK', 'BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_dv(k, g, A, do, dh, dv, T: 'tl.constexpr', H:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    if HEAD_FIRST:\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, \n            i_t * BT), (BT, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT, i_v * BV), (BT, BV), (1, 0))\n    else:\n        p_A = tl.make_block_ptr(A + i_b * T * H * BT + i_h * BT, (BT, T), (\n            1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dv = tl.make_block_ptr(dv + i_b * T * H * V + i_h * V, (T, V), (H *\n            V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],\n        b_A, 0.0)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = tl.dot(b_A, b_do, allow_tf32=False)\n    for i_k in range(tl.cdiv(K, BK)):\n        o_k = i_k * BK + tl.arange(0, BK)\n        m_k = o_k < K\n        if HEAD_FIRST:\n            p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_gk = tl.make_block_ptr(g + i_bh * T * K, (T, K), (K, 1), (i_t *\n                BT, i_k * BK), (BT, BK), (1, 0))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_bh * T * K + min(\n                i_t * BT + BT, T) * K - K + o_k, BK), BK)\n        else:\n            p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_gk = tl.make_block_ptr(g + i_b * T * H * K + i_h * K, (T, K),\n                (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n            p_gn = tl.max_contiguous(tl.multiple_of(g + i_b * T * H * K + (\n                min(i_t * BT + BT, T) - 1) * H * K + i_h * K + o_k, BK), BK)\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_gn = tl.exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)\n        b_k = b_k * b_gn\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh)\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "f7e71f1d-7533-4a57-9559-c4cda6748135"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "bbefe96f-c1a1-4a04-80a0-3b393cc39244"
  },
  {
    "input": "@triton.jit\ndef offsets_from_base(ptrs, base_ptr):\n    \"\"\"Return offsets for which ptrs = base_ptr + offsets\"\"\"\n    return ptrs - base_ptr\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "8abff29c-8f0f-4073-9c47-36d29d51f724"
  },
  {
    "input": "@triton.jit\ndef _softplus(x):\n    z = tl.where(x >= 0, x + tl.log(1 + tl.exp(-x)), tl.log(1 + tl.exp(x)))\n    return z\n",
    "category": "Activation Functions",
    "subcategory": "softplus",
    "uuid": "e590e270-b02b-4547-941c-73b5eeb07136"
  },
  {
    "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(64, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BLOCK_N': lambda kwargs: min(64, triton.next_power_of_2(kwargs['size_inp_1'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'STRIDE_INP_1_IS_1': lambda kwargs: kwargs['stride_inp_1'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n    'STRIDE_OUT_1_IS_1': lambda kwargs: kwargs['stride_out_1'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] * kwargs['BLOCK_N'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_3d_kernel(output_ptr, input_ptr, bs, size_inp_0, size_inp_1,\n    batch_stride_inp, stride_inp_0, stride_inp_1, batch_stride_out,\n    stride_out_0, stride_out_1, BATCH_STRIDE_INP_IS_1: 'tl.constexpr',\n    STRIDE_INP_0_IS_1: 'tl.constexpr', STRIDE_INP_1_IS_1: 'tl.constexpr',\n    BATCH_STRIDE_OUT_IS_1: 'tl.constexpr', STRIDE_OUT_0_IS_1:\n    'tl.constexpr', STRIDE_OUT_1_IS_1: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    grid_n = tl.cdiv(size_inp_1, BLOCK_N)\n    pid_m = pid // grid_n\n    pid_n = pid - pid_m * grid_n\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + (rm[:, None] * (1 if STRIDE_INP_0_IS_1 else\n        stride_inp_0) + rn[None, :] * (1 if STRIDE_INP_1_IS_1 else\n        stride_inp_1))\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + (rm[:, None] * (1 if STRIDE_OUT_0_IS_1 else\n        stride_out_0) + rn[None, :] * (1 if STRIDE_OUT_1_IS_1 else\n        stride_out_1))\n    mask = (rm < size_inp_0)[:, None] & (rn < size_inp_1)[None, :]\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "01cd2625-6a89-4a12-bc8e-dafcf61c24aa"
  },
  {
    "input": "@triton.jit\ndef silu(x):\n    return x * tl.sigmoid(x)\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "36ec7ee9-ee16-4b86-a75c-6565e8fd4c28"
  },
  {
    "input": "@triton.jit\ndef bid_fused_recurrent_gla_fwd_kernel(q, k, v, gk, gv, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', NK: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', REVERSE: 'tl.constexpr', USE_GK:\n    'tl.constexpr', USE_GV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + 0\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + 0\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + 0\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + 0\n    inv_p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    inv_p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    inv_p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    inv_p_o = o + (i_bh + i_k * B * H + NK * B * H\n        ) * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    if USE_GK:\n        p_gk = gk + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + 0\n        inv_p_gk = gk + B * H * s_qk_h + i_bh * s_qk_h + i_k * BK + tl.arange(\n            0, BK) + (T - 1) * DK\n    if USE_GV:\n        p_gv = gv + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + ((T - 1) *\n            DV if REVERSE else 0)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    inv_h = tl.zeros([BV, BK], dtype=tl.float32)\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _inv_k = tl.load(inv_p_k, mask=mask_bk, other=0)\n        _inv_v = tl.load(inv_p_v, mask=mask_bv, other=0)\n        _inv_q = tl.load(inv_p_q, mask=mask_bk, other=0) * scale\n        if USE_GK:\n            _gk = tl.load(p_gk, mask=mask_bk, other=0)\n            h = h * _gk[None, :]\n            _inv_gk = tl.load(inv_p_gk, mask=mask_bk, other=0)\n            inv_h = inv_h * _inv_gk[None, :]\n        if USE_GV:\n            _gv = tl.load(p_gv, mask=mask_bv, other=0)\n            h = h * _gv[:, None]\n        h += _k[None, :] * _v[:, None]\n        inv_h += _inv_k[None, :] * _inv_v[:, None]\n        _o = h * _q[None, :]\n        _inv_o = inv_h * _inv_q[None, :]\n        _o = tl.sum(_o, axis=1)\n        _inv_o = tl.sum(_inv_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        tl.store(inv_p_o, _inv_o, mask=mask_bv)\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n        inv_p_q += -DK\n        inv_p_k += -DK\n        inv_p_o += -DV\n        inv_p_v += -DV\n        if USE_GK:\n            p_gk += DK\n            inv_p_gk += -DK\n        if USE_GV:\n            p_gv += -DV if REVERSE else DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5cc26ffd-3ab0-4472-a851-5ef43d225c30"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h,\n    s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_1o = tl.zeros([BV, BK], dtype=tl.float32)\n    b_h_2o = tl.zeros([BV, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    for i in range(0, tl.cdiv(T, BT)):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (DV, T), (s_vo_d, s_vo_t\n            ), (i_v * BV, i * BT), (BV, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i * BT\n        b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i * BT < T)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dq += tl.dot(b_do, b_h_1o, allow_tf32=False)\n        if i_v == 0:\n            b_dq += b_dz[:, None] * k_1o\n        b_dq_2o = tl.dot(b_do, b_h_2o, allow_tf32=False) * 0.5\n        if i_v == 0:\n            b_dq_2o += b_dz[:, None] * k_2o * 0.5\n        b_dq_2o = tl.reshape(b_dq_2o, [BT, BK, BK])\n        b_dq += tl.sum(b_dq_2o * b_q[:, :, None], axis=1)\n        b_dq += tl.sum(b_dq_2o * b_q[:, None, :], axis=2)\n        b_dq *= scale\n        b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[:, None]\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n        b_s = tl.where(m_s, b_s, 0)\n        b_dq += tl.dot(b_ds * (1 + b_s), b_k, allow_tf32=False)\n        tl.store(p_dq, b_dq, boundary_check=(0, 1))\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_h_2o = b_h_2o + tl.dot(b_v, b_k_2o, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_v, b_k, allow_tf32=False)\n        if i_v == 0:\n            k_1o += tl.sum(b_k, axis=0)[None, :]\n            k_2o += tl.sum(b_k_2o, axis=0)[None, :]\n    tl.debug_barrier()\n    b_h_1o = None\n    b_h_2o = None\n    b_dh_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_dh_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    b_dh_0o = tl.zeros([BV], dtype=tl.float32)\n    m_s = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\n    dq_1o = tl.zeros([1, BK], dtype=tl.float32)\n    dq_2o = tl.zeros([BK * BK, 1], dtype=tl.float32)\n    for i in range(tl.cdiv(T, BT) * BT - BT, -BT, -BT):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d\n            ), (i, i_k * BK), (BT, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d\n            ), (i, i_v * BV), (BT, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, DV), (s_vo_t,\n            s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * s_qk_h, (T, DK\n            ), (s_qk_t, s_qk_d), (i, i_k * BK), (BT, BK), (1, 0))\n        p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * s_vo_h, (T, DV\n            ), (s_vo_t, s_vo_d), (i, i_v * BV), (BT, BV), (1, 0))\n        p_dz = dz + i_bh * T + tl.arange(0, BT) + i\n        b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n        b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n        b_dz = tl.load(p_dz, mask=tl.arange(0, BT) + i < T)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        b_ds = tl.where(m_s, b_ds, 0)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False)\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds *= 1 + b_s\n        b_dk += tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n        b_dv += tl.dot(b_s2, b_do, allow_tf32=False)\n        b_k_2o = b_k[:, :, None] * b_k[:, None, :]\n        b_k_2o = tl.reshape(b_k_2o, [BT, BK * BK])\n        b_dv += tl.dot(b_k, b_dh_1o, allow_tf32=False)\n        b_dv += tl.dot(b_k_2o, b_dh_2o, allow_tf32=False)\n        b_dv += b_dh_0o\n        b_dk += tl.dot(b_v, tl.trans(b_dh_1o), allow_tf32=False)\n        if i_v == 0:\n            b_dk += dq_1o\n        b_dk_2o = tl.dot(b_dh_2o, tl.trans(b_v), allow_tf32=False)\n        if i_v == 0:\n            b_dk_2o += dq_2o\n        b_dk_2o = tl.reshape(b_dk_2o, [BK, BK, BT])\n        b_k_fp32 = tl.trans(b_k)\n        b_dk2 = tl.sum(b_dk_2o * b_k_fp32[:, None, :], axis=0)\n        b_dk2 += tl.sum(b_dk_2o * b_k_fp32[None, :, :], axis=1)\n        b_dk += tl.trans(b_dk2)\n        b_dh_0o += tl.sum(b_do, axis=0)\n        b_dh_1o = b_dh_1o + tl.dot(b_q, b_do, allow_tf32=False)\n        b_q_2o = b_q[None, :, :] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BK * BK, BT])\n        b_dh_2o = b_dh_2o + tl.dot(b_q_2o, b_do, allow_tf32=False) * 0.5\n        if i_v == 0:\n            dq_1o += tl.sum(b_dz[None, :] * b_q, axis=1)[None, :]\n            dq_2o += (tl.sum(b_dz[None, :] * b_q_2o, axis=1) * 0.5)[:, None]\n        tl.store(p_dk, b_dk, boundary_check=(0, 1))\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5ad965e1-f90d-44c7-8557-fa647ce32307"
  },
  {
    "input": "@triton.jit\ndef _bwd_q_kernel(Q, K, V, B, sm_scale, DO, DQ, L, D, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk, stride_bz, stride_bh,\n    stride_bm, stride_bn, stride_doz, stride_doh, stride_dom, stride_dok,\n    stride_dqz, stride_dqh, stride_dqm, stride_dqk, Z, H, M, N, P_SEQ,\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', LARGER_M: 'tl.constexpr',\n    DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_h * stride_kh\n    V += off_z * stride_vz + off_h * stride_vh\n    if HAS_BIAS:\n        B += off_z * stride_bz + off_h * stride_bh\n    DO += off_z * stride_doz + off_h * stride_doh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    DQ += off_z * stride_dqz + off_h * stride_dqh\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_n_init = offs_n_base\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    k_ptrs = K + (offs_n_init[:, None] * stride_kn + offs_k[None, :] *\n        stride_kk)\n    v_ptrs = V + (offs_n_init[:, None] * stride_vn + offs_k[None, :] *\n        stride_vk)\n    if HAS_BIAS:\n        bias_ptrs = B + (offs_m[:, None] * stride_bm + offs_n_init[None, :] *\n            stride_bn)\n    dq_ptrs = DQ + (offs_m[:, None] * stride_dqm + offs_k[None, :] * stride_dqk\n        )\n    do_ptrs = DO + (offs_m[:, None] * stride_dom + offs_k[None, :] * stride_dok\n        )\n    d_ptrs = D + offs_m\n    l_ptrs = L + offs_m\n    mask_m = offs_m < M\n    if DIVISIBLE_M:\n        q = tl.load(q_ptrs)\n        do = tl.load(do_ptrs)\n        delta = tl.load(d_ptrs)\n        l = tl.load(l_ptrs)\n    else:\n        q = tl.load(q_ptrs, mask=mask_m[:, None])\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n        delta = tl.load(d_ptrs, mask=mask_m)\n        l = tl.load(l_ptrs, mask=mask_m)\n    dq = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if CAUSAL:\n        hi = tl.minimum(N, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = N\n    for start_n in range(0, hi, BLOCK_N):\n        offs_n = start_n + offs_n_base\n        mask_n = offs_n < N\n        if DIVISIBLE_N:\n            v = tl.load(v_ptrs)\n            k = tl.load(k_ptrs)\n        else:\n            v = tl.load(v_ptrs, mask=mask_n[:, None])\n            k = tl.load(k_ptrs, mask=mask_n[:, None])\n        if HAS_BIAS:\n            if DIVISIBLE_M and DIVISIBLE_N:\n                b = tl.load(bias_ptrs)\n            else:\n                b = tl.load(bias_ptrs, mask=mask_m[:, None] & mask_n[None, :])\n        if not DIVISIBLE_N:\n            valid_mask = mask_n\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k)) * sm_scale\n        if HAS_BIAS:\n            s += b\n        p = tl.math.exp2((s - l[:, None]) * log2e)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_N:\n            ds = tl.where(valid_mask, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        dq += tl.dot(ds, k)\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vn\n        if HAS_BIAS:\n            bias_ptrs += BLOCK_N * stride_bn\n    dq *= sm_scale\n    if DIVISIBLE_M:\n        tl.store(dq_ptrs, dq)\n    else:\n        tl.store(dq_ptrs, dq, mask=mask_m[:, None])\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "78b2612b-be6b-4855-aa26-b98d825f96cc"
  },
  {
    "input": "@triton.heuristics({'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None}\n    )\n@triton.jit\ndef parallel_delta_rule_fwd_kernel(q, k, k2, v, beta, o, o_new, attn, s_k_h,\n    s_k_t, s_v_h, s_v_t, T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', OUTPUT_ATTENTIONS: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT,\n        0), (BT, BK), (1, 0))\n    b_q = tl.zeros([BT, BK], dtype=tl.float32)\n    b_q += tl.load(p_q, boundary_check=(0, 1))\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT,\n        0), (BT, BV), (1, 0))\n    b_o += tl.load(p_o, boundary_check=(0, 1))\n    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (0,\n            offset), (BK, BS), (0, 1))\n        p_k2 = tl.make_block_ptr(k2 + i_bh * s_k_h, (T, K), (s_k_t, 1), (\n            offset, 0), (BS, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            offset, 0), (BS, BV), (1, 0))\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,),\n            (BS,), (0,))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_beta = tl.load(p_beta, boundary_check=(0,))\n        m_s = tl.arange(0, BT) >= offset - i_t * BT + BS\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = tl.where(m_s[:, None], b_s, 0)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        b_k2 = tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]\n        b_q -= tl.dot(b_s, b_k2, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(attn + i_bh * T * T, (T, T), (T, 1), (\n                i_t * BT, offset), (BT, BS), (1, 0))\n            tl.store(p_a, b_s, boundary_check=(0, 1))\n    for offset in range(i_t * BT - BS, -BS, -BS):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (1, s_k_t), (0,\n            offset), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n            offset, 0), (BS, BV), (1, 0))\n        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,),\n            (BS,), (0,))\n        p_k2 = tl.make_block_ptr(k2 + i_bh * s_k_h, (T, K), (s_k_t, 1), (\n            offset, 0), (BS, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_beta = tl.load(p_beta, boundary_check=(0,))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        b_k2 = tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]\n        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False)\n        if OUTPUT_ATTENTIONS:\n            p_a = tl.make_block_ptr(attn + i_bh * T * T, (T, T), (T, 1), (\n                i_t * BT, offset), (BT, BS), (1, 0))\n            tl.store(p_a, b_s, boundary_check=(0, 1))\n    p_o_new = tl.make_block_ptr(o_new + i_bh * s_v_h, (T, V), (s_v_t, 1), (\n        i_t * BT, 0), (BT, BV), (1, 0))\n    tl.store(p_o_new, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "09f02af4-a4cb-41e9-a40f-14a4d6bbfd07"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef chunk_hgrn_bwd_kernel_h(g, gc, dx, do, T: 'tl.constexpr', D:\n    'tl.constexpr', BT: 'tl.constexpr', BD: 'tl.constexpr'):\n    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    BC = min(BT, T - i_t * BT)\n    NT = tl.num_programs(1)\n    p_g = g + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_gc = gc + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_dx = dx + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    p_do = do + (i_b * T + i_t * BT + BC - 1) * D + o_d\n    if i_t == NT - 1:\n        b_gc = tl.zeros([BD], dtype=tl.float32)\n    else:\n        b_gc = tl.load(g + (i_b * T + i_t * BT + BT) * D + o_d, mask=mask,\n            other=0)\n    b_dh = tl.zeros([BD], dtype=tl.float32)\n    for _ in range(BC - 1, -1, -1):\n        tl.store(p_gc, b_gc, mask=mask)\n        b_g = tl.load(p_g, mask=mask, other=0)\n        b_do = tl.load(p_do, mask=mask, other=0)\n        b_gc = b_gc + b_g\n        b_dh = b_dh + b_do\n        b_dx = b_dh\n        b_dh = b_dh * tl.exp(b_g)\n        tl.store(p_dx, b_dx, mask=mask)\n        p_g -= D\n        p_gc -= D\n        p_dx -= D\n        p_do -= D\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a8cec61f-8a13-4678-9756-e5c6acde2ba2"
  },
  {
    "input": "@triton.jit\ndef gemm_split_k_kernel(a_ptr, b_ptr, c_ptr, stride_am, stride_ak,\n    stride_bk, stride_bn, stride_cm, stride_cn, m, n, k, block_m:\n    'tl.constexpr', block_n: 'tl.constexpr', block_k: 'tl.constexpr',\n    split_k: 'tl.constexpr', group_m: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    grid_k = tl.cdiv(k, block_k * split_k)\n    pid_m, pid_n = grouped_launch(pid, m, n, block_m, block_n, group_m)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_k = pid_k * block_k + tl.arange(0, block_k)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, block_m), block_m)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, block_n), block_n)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k_ in range(0, grid_k):\n        k_remaining = k - k_ * (block_k * split_k)\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < k_remaining, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < k_remaining, other=0.0)\n        acc = tl.dot(a, b, acc, out_dtype=tl.float32)\n        a_ptrs += block_k * split_k * stride_ak\n        b_ptrs += block_k * split_k * stride_bk\n    acc\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    mask = (offs_m < m)[:, None] & (offs_n < n)[None, :]\n    tl.atomic_add(c_ptrs, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "5860c49b-5232-4d69-9f7d-89bd63ecfd64"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config(\n    {'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config\n    ({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.\n    Config({'BLOCK_SIZE': 2048})], key=['dim'])\n@triton.jit\ndef _state_passing_fwd_kernel(states_ptr, out_ptr, final_states_ptr,\n    dA_cs_ptr, initstates_ptr, seq_idx_ptr, dim, nchunks, seqlen,\n    chunk_size, stride_states_batch, stride_states_chunk,\n    stride_states_head, stride_states_dim, stride_out_batch,\n    stride_out_chunk, stride_out_head, stride_out_dim,\n    stride_final_states_batch, stride_final_states_head,\n    stride_final_states_dim, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_initstates_batch, stride_initstates_head,\n    stride_initstates_dim, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    HAS_INITSTATES: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\n    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    final_states_ptr += (pid_b * stride_final_states_batch + pid_h *\n        stride_final_states_head)\n    if HAS_INITSTATES:\n        initstates_ptr += (pid_b * stride_initstates_batch + pid_h *\n            stride_initstates_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += pid_b * stride_seq_idx_batch\n    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    states_ptrs = states_ptr + offs_m * stride_states_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim\n    if not HAS_INITSTATES:\n        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    else:\n        initstates_ptrs = initstates_ptr + offs_m * stride_initstates_dim\n        states = tl.load(initstates_ptrs, mask=offs_m < dim, other=0.0)\n    tl.store(out_ptrs, states, mask=offs_m < dim)\n    out_ptrs += stride_out_chunk\n    seq_idx = 0\n    for c in range(nchunks):\n        new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0)\n        dA_cs = tl.load(dA_cs_ptr)\n        scale = tl.exp(dA_cs)\n        if HAS_SEQ_IDX:\n            seq_idx_new = tl.load(seq_idx_ptr + (min((c + 1) * chunk_size,\n                seqlen) - 1) * stride_seq_idx_seqlen)\n            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n            seq_idx = seq_idx_new\n        states = scale * states + new_states\n        if c < nchunks - 1:\n            tl.store(out_ptrs, states, mask=offs_m < dim)\n        else:\n            tl.store(final_states_ptrs, states, mask=offs_m < dim)\n        states_ptrs += stride_states_chunk\n        dA_cs_ptr += stride_dA_cs_chunk\n        out_ptrs += stride_out_chunk\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "d89a7807-8a9a-4bc0-82cf-1e2040ccf12f"
  },
  {
    "input": "@triton.jit\ndef moe_align_block_size_stage1(topk_ids_ptr, sorted_token_ids_ptr,\n    expert_ids_ptr, total_tokens_post_pad_ptr, tokens_cnts_ptr, cumsum_ptr,\n    num_experts: 'tl.constexpr', block_size: 'tl.constexpr', numel:\n    'tl.constexpr', tokens_per_thread: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(0)\n    start_idx = pid * tokens_per_thread\n    off_c = (pid + 1) * num_experts\n    for i in range(tokens_per_thread):\n        if start_idx + i < numel:\n            idx = tl.load(topk_ids_ptr + start_idx + i)\n            token_cnt = tl.load(tokens_cnts_ptr + off_c + idx)\n            tl.store(tokens_cnts_ptr + off_c + idx, token_cnt + 1)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "561de1a1-9660-438b-b1d7-07e239c6f751"
  },
  {
    "input": "@triton.jit\ndef mars_adamw_kernel(param_ptr, grad_ptr, exp_avg_ptr, exp_avg_sq_ptr,\n    prev_grad_ptr, lr, beta1, beta2, eps, weight_decay, gamma,\n    max_grad_norm, step, bias_correction1, bias_correction2, n_elements,\n    BLOCK_SIZE: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    param = tl.load(param_ptr + offsets, mask=mask)\n    grad = tl.load(grad_ptr + offsets, mask=mask)\n    exp_avg = tl.load(exp_avg_ptr + offsets, mask=mask)\n    exp_avg_sq = tl.load(exp_avg_sq_ptr + offsets, mask=mask)\n    prev_grad = tl.load(prev_grad_ptr + offsets, mask=mask)\n    grad_diff = grad - prev_grad\n    correction = gamma * beta1 / (1 - beta1) * grad_diff\n    c_t = grad + correction\n    c_t_norm = tl.sqrt(tl.sum(c_t * c_t))\n    scale = tl.where(c_t_norm > max_grad_norm, max_grad_norm / c_t_norm, 1.0)\n    c_t = c_t * scale\n    exp_avg = beta1 * exp_avg + (1 - beta1) * c_t\n    exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * (c_t * c_t)\n    tl.store(prev_grad_ptr + offsets, grad, mask=mask)\n    step_size = lr / bias_correction1\n    denom = tl.sqrt(exp_avg_sq) / tl.sqrt(bias_correction2) + eps\n    update = exp_avg / denom\n    param = param - step_size * (update + weight_decay * param)\n    tl.store(param_ptr + offsets, param, mask=mask)\n    tl.store(exp_avg_ptr + offsets, exp_avg, mask=mask)\n    tl.store(exp_avg_sq_ptr + offsets, exp_avg_sq, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "dd071c1c-f7f4-4400-b53d-7680b03a1a00"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_KV(v, z, A, do, dv, s_v_h, s_v_t, s_v_d, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * V + i_v * BV,), (BV,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (i_i *\n            BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_zn[None, :] - b_z)\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n    b_dv *= tl.exp(b_v - b_zn[None, :])\n    o_i = tl.arange(0, BC)\n    for j in range(0, BC):\n        p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T * BT,), (1,), ((i_t *\n            BT + i_i * BC + j) * BT + i_i * BC,), (BC,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(p_A, boundary_check=(0,))\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_do = tl.load(p_do, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(m_i, tl.exp(b_v - b_z[None, :]) * b_A[:, None] *\n            b_do[None, :], 0.0)\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "cross attention",
    "uuid": "bcd321d4-26bb-4b5d-9716-2e2714324b4c"
  },
  {
    "input": "@triton.jit\ndef _complex_operator_element_(_x_real_a, _a_imag_a, _x_imag_a, _a_real_a,\n    start, num, interval, offset_b, offset_n, L, C, last_interval, BLOCK_M:\n    'tl.constexpr'):\n    offset_t = tl.program_id(0)\n    range_batch = tl.arange(0, BLOCK_M) + offset_b * L * C + offset_n * BLOCK_M\n    range_time = (tl.arange(0, num) * interval + start) * C\n    range_2dim = range_batch[:, None] + range_time[None, :]\n    ptr = range_2dim\n    ptr_last = range_2dim - last_interval * C\n    x_real_a = tl.load(_x_real_a + ptr)\n    x_real_a_last = tl.load(_x_real_a + ptr_last)\n    a_imag_a = tl.load(_a_imag_a + ptr)\n    a_imag_a_last = tl.load(_a_imag_a + ptr_last)\n    x_imag_a = tl.load(_x_imag_a + ptr)\n    x_imag_a_last = tl.load(_x_imag_a + ptr_last)\n    a_real_a = tl.load(_a_real_a + ptr)\n    a_real_a_last = tl.load(_a_real_a + ptr_last)\n    x_real_a = x_real_a + a_real_a * x_real_a_last - a_imag_a * x_imag_a_last\n    x_imag_a = x_imag_a + a_real_a * x_imag_a_last + a_imag_a * x_real_a_last\n    tl.store(_x_real_a + ptr, x_real_a)\n    tl.store(_x_imag_a + ptr, x_imag_a)\n    a_real_a_next = a_real_a * a_real_a_last - a_imag_a * a_imag_a_last\n    a_imag_a_next = a_imag_a * a_real_a_last - a_real_a * a_imag_a_last\n    tl.store(_a_real_a + ptr, a_real_a_next)\n    tl.store(_a_imag_a + ptr, a_imag_a_next)\n",
    "category": "Math Utils",
    "subcategory": "trigonometry",
    "uuid": "12896564-d0bf-4b77-bf0b-533407eee23f"
  },
  {
    "input": "@triton.jit\ndef parallel_rebased_fwd_kernel(q, k, v, o, z, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, scale, B, H, T, K: 'tl.constexpr', V: 'tl.constexpr', BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'\n    ):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(V, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_c *\n        BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, 0), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (0, \n        i_v * BV), (BTS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = b_q * scale\n    b_o = tl.zeros([BTL, BV], dtype=tl.float32)\n    b_z = tl.zeros([BTL], dtype=tl.float32)\n    for _ in range(0, i_c * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_z += tl.sum(b_s, axis=1)\n        b_o = b_o + tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n    tl.debug_barrier()\n    o_q = tl.arange(0, BTL)\n    o_k = tl.arange(0, BTS)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (i_k *\n        BK, i_c * BTL), (BK, BTS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_c *\n        BTL, i_v * BV), (BTS, BV), (1, 0))\n    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        m_s = o_q[:, None] >= o_k[None, :]\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        p_k = tl.advance(p_k, (0, BTS))\n        p_v = tl.advance(p_v, (BTS, 0))\n        o_k += BTS\n    p_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * s_v_h, (T, V), (\n        s_v_t, s_v_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    tl.store(p_z, b_z, mask=i_c * BTL + tl.arange(0, BTL) < T)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "98f13a5e-b62c-439c-8bee-8a13defe2e95"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size},\n    num_warps=num_warps) for block_size, num_warps in itertools.product([32,\n    64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=[\n    'n_audios', 'audio_len'])\n@triton.jit\ndef apply_clip_kernel(samples_ptr, min, max, output_ptr, n_audios,\n    audio_len, BLOCK_SIZE: 'tl.constexpr'):\n    audio_idx = tl.program_id(0)\n    if audio_idx >= n_audios:\n        return\n    for i in range(0, audio_len, BLOCK_SIZE):\n        sample_idx = i + tl.arange(0, BLOCK_SIZE)\n        mask = sample_idx < audio_len\n        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx,\n            mask=mask)\n        result = tl.where(samples > max, max, samples)\n        result = tl.where(result < min, min, result)\n        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result,\n            mask=mask)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "df82c3b5-f2fa-4b6e-a648-0a296117c052"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "c4d8bce4-338a-43de-b813-5472cf5f4489"
  },
  {
    "input": "@eval(\n    \"\"\"triton.heuristics({\n    'BLOCK_M': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_0'])),\n    'BLOCK_N': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_1'])),\n    'BLOCK_K': lambda kwargs: min(32, triton.next_power_of_2(kwargs['size_inp_2'])),\n    'BATCH_STRIDE_INP_IS_1': lambda kwargs: kwargs['batch_stride_inp'] == 1,\n    'STRIDE_INP_0_IS_1': lambda kwargs: kwargs['stride_inp_0'] == 1,\n    'STRIDE_INP_1_IS_1': lambda kwargs: kwargs['stride_inp_1'] == 1,\n    'STRIDE_INP_2_IS_1': lambda kwargs: kwargs['stride_inp_2'] == 1,\n    'BATCH_STRIDE_OUT_IS_1': lambda kwargs: kwargs['batch_stride_out'] == 1,\n    'STRIDE_OUT_0_IS_1': lambda kwargs: kwargs['stride_out_0'] == 1,\n    'STRIDE_OUT_1_IS_1': lambda kwargs: kwargs['stride_out_1'] == 1,\n    'STRIDE_OUT_2_IS_1': lambda kwargs: kwargs['stride_out_2'] == 1,\n})\"\"\"\n    )\n@eval(\n    \"\"\"triton.heuristics({\n    'num_warps': lambda kwargs: max(1, min(16, kwargs['BLOCK_M'] * kwargs['BLOCK_N'] * kwargs['BLOCK_K'] // 32)),\n})\"\"\"\n    )\n@triton.jit\ndef copy_4d_kernel(output_ptr, input_ptr, bs, size_inp_0, size_inp_1,\n    size_inp_2, batch_stride_inp, stride_inp_0, stride_inp_1, stride_inp_2,\n    batch_stride_out, stride_out_0, stride_out_1, stride_out_2,\n    BATCH_STRIDE_INP_IS_1: 'tl.constexpr', STRIDE_INP_0_IS_1:\n    'tl.constexpr', STRIDE_INP_1_IS_1: 'tl.constexpr', STRIDE_INP_2_IS_1:\n    'tl.constexpr', BATCH_STRIDE_OUT_IS_1: 'tl.constexpr',\n    STRIDE_OUT_0_IS_1: 'tl.constexpr', STRIDE_OUT_1_IS_1: 'tl.constexpr',\n    STRIDE_OUT_2_IS_1: 'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_batch = tl.program_id(1)\n    grid_m = tl.cdiv(size_inp_0, BLOCK_M)\n    grid_n = tl.cdiv(size_inp_1, BLOCK_N)\n    grid_k = tl.cdiv(size_inp_2, BLOCK_K)\n    pid_m = pid // (grid_n * grid_k)\n    pid_nk = pid - pid_m * (grid_n * grid_k)\n    pid_n = pid_nk // grid_k\n    pid_k = pid_nk - pid_n * grid_k\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    rk = pid_k * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = input_ptr + (1 if BATCH_STRIDE_INP_IS_1 else batch_stride_inp\n        ) * pid_batch + (rm[:, None, None] * (1 if STRIDE_INP_0_IS_1 else\n        stride_inp_0) + rn[None, :, None] * (1 if STRIDE_INP_1_IS_1 else\n        stride_inp_1) + rk[None, None, :] * (1 if STRIDE_INP_2_IS_1 else\n        stride_inp_2))\n    B = output_ptr + (1 if BATCH_STRIDE_OUT_IS_1 else batch_stride_out\n        ) * pid_batch + (rm[:, None, None] * (1 if STRIDE_OUT_0_IS_1 else\n        stride_out_0) + rn[None, :, None] * (1 if STRIDE_OUT_1_IS_1 else\n        stride_out_1) + rk[None, None, :] * (1 if STRIDE_OUT_2_IS_1 else\n        stride_out_2))\n    mask = (rm < size_inp_0)[:, None, None] & (rn < size_inp_1)[None, :, None\n        ] & (rk < size_inp_2)[None, None, :]\n    a = tl.load(A, mask=mask)\n    tl.store(B, a, mask=mask)\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "92e59b85-5b2e-49f8-99ce-ba89bd3c8f01"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_cumsum_fwd_kernel(s, z, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (\n            i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "94a22af2-0c3c-4c44-b914-f57d498d9234"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE,\n    D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm,\n    stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k,\n    headdim, ATOMIC_ADD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr', EVEN_M:\n    'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    begin_m = 0 if not IS_CAUSAL else start_n * BLOCK_N // BLOCK_M * BLOCK_M\n    offs_qm = begin_m + tl.arange(0, BLOCK_M)\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_d[None, :])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_d[None, :])\n    do_ptrs = DO + (offs_qm[:, None] * stride_dom + offs_d[None, :])\n    dq_ptrs = DQ + (offs_qm[:, None] * stride_dqm + offs_d[None, :])\n    if BIAS_TYPE == 'vector':\n        b_ptrs = Bias + offs_n\n    elif BIAS_TYPE == 'matrix':\n        b_ptrs = Bias + (offs_qm[:, None] * stride_bm + offs_n[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\n    if begin_m >= seqlen_q:\n        dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n        dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n        _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n            headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n        return\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n            v = tl.load(v_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n        v = tl.load(v_ptrs, mask=offs_n[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n        v = tl.load(v_ptrs, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim), other=0.0)\n    num_block_m = tl.cdiv(seqlen_q, BLOCK_M)\n    for start_m in range(begin_m, num_block_m * BLOCK_M, BLOCK_M):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        offs_m_curr = start_m + offs_m\n        if EVEN_M & EVEN_HEADDIM:\n            q = tl.load(q_ptrs)\n        elif EVEN_HEADDIM:\n            q = tl.load(q_ptrs, mask=offs_m_curr[:, None] < seqlen_q, other=0.0\n                )\n        else:\n            q = tl.load(q_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) & (\n                offs_d[None, :] < headdim), other=0.0)\n        qk = tl.dot(q, k, trans_b=True)\n        if not EVEN_N:\n            qk = tl.where(offs_n[None, :] < seqlen_k, qk, float('-inf'))\n        if IS_CAUSAL:\n            qk = tl.where(offs_m_curr[:, None] >= offs_n[None, :], qk,\n                float('-inf'))\n        if BIAS_TYPE != 'none':\n            tl.debug_barrier()\n            if BIAS_TYPE == 'vector':\n                if EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=offs_n < seqlen_k, other=0.0)\n                bias = bias[None, :]\n            elif BIAS_TYPE == 'matrix':\n                if EVEN_M & EVEN_N:\n                    bias = tl.load(b_ptrs)\n                else:\n                    bias = tl.load(b_ptrs, mask=(offs_m_curr[:, None] <\n                        seqlen_q) & (offs_n[None, :] < seqlen_k), other=0.0)\n            qk = qk * softmax_scale + bias\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        lse_i = tl.load(LSE + offs_m_curr)\n        if BIAS_TYPE == 'none':\n            p = tl.exp(qk * softmax_scale - lse_i[:, None])\n        else:\n            p = tl.exp(qk - lse_i[:, None])\n        if EVEN_M & EVEN_HEADDIM:\n            do = tl.load(do_ptrs)\n        else:\n            do = tl.load(do_ptrs, mask=(offs_m_curr[:, None] < seqlen_q) &\n                (offs_d[None, :] < headdim), other=0.0)\n        dv += tl.dot(p, do, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        dp = tl.dot(do, v, trans_b=True)\n        if not EVEN_HEADDIM:\n            tl.debug_barrier()\n        Di = tl.load(D + offs_m_curr)\n        ds = p * (dp - Di[:, None]) * softmax_scale\n        dk += tl.dot(ds, q, trans_a=True)\n        if not EVEN_M & EVEN_HEADDIM:\n            tl.debug_barrier()\n        if not ATOMIC_ADD:\n            if EVEN_M & EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, eviction_policy='evict_last')\n            elif EVEN_HEADDIM:\n                dq = tl.load(dq_ptrs, mask=offs_m_curr[:, None] < seqlen_q,\n                    other=0.0, eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q,\n                    eviction_policy='evict_last')\n            else:\n                dq = tl.load(dq_ptrs, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), other=0.0,\n                    eviction_policy='evict_last')\n                dq += tl.dot(ds, k)\n                tl.store(dq_ptrs, dq, mask=(offs_m_curr[:, None] < seqlen_q\n                    ) & (offs_d[None, :] < headdim), eviction_policy=\n                    'evict_last')\n        else:\n            dq = tl.dot(ds, k)\n            if EVEN_M & EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq)\n            elif EVEN_HEADDIM:\n                tl.atomic_add(dq_ptrs, dq, mask=offs_m_curr[:, None] < seqlen_q\n                    )\n            else:\n                tl.atomic_add(dq_ptrs, dq, mask=(offs_m_curr[:, None] <\n                    seqlen_q) & (offs_d[None, :] < headdim))\n        dq_ptrs += BLOCK_M * stride_dqm\n        q_ptrs += BLOCK_M * stride_qm\n        do_ptrs += BLOCK_M * stride_dom\n        if BIAS_TYPE == 'matrix':\n            b_ptrs += BLOCK_M * stride_bm\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_d[None, :])\n    _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n        headdim, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM)\n",
    "category": "Attention Mechanisms",
    "subcategory": "causal attention",
    "uuid": "9e5a1cca-3924-4e83-baa3-1d1a20255dfd"
  },
  {
    "input": "@triton.jit\ndef parallel_rebased_bwd_kernel(q, k, v, do, dz, dq, dk, dv, s_qk_h, s_qk_t,\n    s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL: 'tl.constexpr',\n    BTS: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr'):\n    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    i_k = i_kv // NV\n    i_v = i_kv % NV\n    i_h = i_bh % H\n    _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq,\n        s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL\n        =BTL, BTS=BTS, BK=BK, BV=BV, DK=DK, DV=DV)\n    tl.debug_barrier()\n    _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n        dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale,\n        BTL, BTS, BK, BV, DK, DV)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e651ecf4-221d-4442-8140-5e426ab4deb8"
  },
  {
    "input": "@triton.jit\ndef fwd_prepare_wy_repr(A, x, k, cumsum, cumdecay, NT, DK, BT:\n    'tl.constexpr', BK: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_x = x + i_bh * NT * BT * DK + (i_t * BT + tl.arange(0, BT)[:, None]\n        ) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    p_k = k + i_bh * NT * BT * DK + (i_t * BT + tl.arange(0, BT)[:, None]\n        ) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    S = tl.load(p_x)\n    p_A = A + i_bh * NT * BT * BT + i_t * BT * BT + tl.arange(0, BT)\n    S_cumdecay = tl.load(p_k)\n    for i in range(BT):\n        attn = tl.load(p_A)\n        mask = tl.arange(0, BT) < i\n        attn = tl.where(mask, attn, 0)\n        new = tl.sum(attn[:, None] * S, axis=0)\n        new_cumdecay = tl.sum(attn[:, None] * S_cumdecay, axis=0)\n        mask = tl.arange(0, BT) == i\n        S = tl.where(mask[:, None], S - new[None, :], S)\n        S_cumdecay = tl.where(mask[:, None], S_cumdecay - new_cumdecay[None,\n            :], S_cumdecay)\n        p_A += BT\n    p_cumsum = cumsum + i_bh * BT * NT * DK + (i_t * BT + tl.arange(0, BT)[\n        :, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    tl.store(p_cumsum, S)\n    p_cumdecay = cumdecay + i_bh * BT * NT * DK + (i_t * BT + tl.arange(0,\n        BT)[:, None]) * DK + tl.arange(0, BK)[None, :] + i_k * BK\n    tl.store(p_cumdecay, S_cumdecay)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "863bd6d1-d487-44a3-8628-b980947148da"
  },
  {
    "input": "@triton.jit\ndef split_2D_jagged_w_prefix(JaggedIn, DenseSize, OffsetsA, OffsetsB, OutA,\n    OutB, D, stride_id, stride_ad, stride_bd, n_prefix_to_B, IS_DENSE_A:\n    'tl.constexpr', IS_DENSE_B: 'tl.constexpr', BLOCK_D: 'tl.constexpr',\n    IS_REPLACE: 'tl.constexpr'):\n    off_z = tl.program_id(1)\n    off_n = tl.program_id(0)\n    if IS_DENSE_A:\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_start_a = off_z * DenseSize\n        seq_len_a = DenseSize\n        seq_len_b = seq_end_b - seq_start_b\n    elif IS_DENSE_B:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = off_z * DenseSize\n        seq_len_b = DenseSize\n    else:\n        seq_start_a = tl.load(OffsetsA + off_z)\n        seq_end_a = tl.load(OffsetsA + off_z + 1)\n        seq_len_a = seq_end_a - seq_start_a\n        seq_start_b = tl.load(OffsetsB + off_z)\n        seq_end_b = tl.load(OffsetsB + off_z + 1)\n        seq_len_b = seq_end_b - seq_start_b\n    if IS_REPLACE:\n        seq_len = seq_len_a\n    else:\n        seq_len = seq_len_a + seq_len_b\n    if off_n >= seq_len:\n        return\n    if IS_REPLACE:\n        seq_start = seq_start_a\n        out_seq_b_start = seq_len_a - seq_len_b\n    else:\n        seq_start = seq_start_a + seq_start_b\n        out_seq_b_start = seq_len_a + n_prefix_to_B\n    offs_d = tl.arange(0, BLOCK_D)\n    in_ptrs = JaggedIn + (seq_start + off_n) * stride_id + offs_d\n    if off_n < out_seq_b_start and off_n >= n_prefix_to_B:\n        off_a = off_n - n_prefix_to_B\n        out_ptrs = OutA + (off_a + seq_start_a) * stride_ad + offs_d\n    else:\n        off_b = off_n - out_seq_b_start + n_prefix_to_B\n        if off_n < n_prefix_to_B:\n            off_b += out_seq_b_start - n_prefix_to_B\n        out_ptrs = OutB + (off_b + seq_start_b) * stride_bd + offs_d\n    v = tl.load(in_ptrs, mask=offs_d < D)\n    tl.store(out_ptrs, v, mask=offs_d < D)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "14d479ad-ddb4-4085-b1fb-02b732753789"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BL': 128, 'BK': 128, 'BV': 128},\n    num_warps=8), triton.Config({'BL': 128, 'BK': 64, 'BV': 64}, num_warps=\n    4), triton.Config({'BL': 64, 'BK': 64, 'BV': 64}, num_warps=2)], key=[\n    'L', 'DK', 'DV'])\n@triton.jit\ndef _fwd_kv_kernel(K, V, S, Z, stride_qk_bh, stride_qk_l, stride_qk_d,\n    stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk,\n    stride_s_dv, stride_z_bh, scale, B: 'tl.constexpr', H: 'tl.constexpr',\n    L: 'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr', BL:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1\n        ), tl.program_id(2)\n    NV = tl.cdiv(DV, BV)\n    K_block_ptr = tl.make_block_ptr(base=K + off_bs_head * stride_qk_bh,\n        shape=(DK, L), strides=(stride_qk_d, stride_qk_l), offsets=(start_k *\n        BK, 0), block_shape=(BK, BL), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + off_bs_head * stride_vo_bh,\n        shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(0, \n        start_v * BV), block_shape=(BL, BV), order=(1, 0))\n    s = tl.zeros([BK, BV], dtype=tl.float32)\n    z = tl.zeros([BK], dtype=tl.float32)\n    for _ in range(0, L, BL):\n        k = tl.load(K_block_ptr, boundary_check=(0, 1))\n        v = tl.load(V_block_ptr, boundary_check=(0, 1))\n        v = v * scale\n        s += tl.dot(k, v, allow_tf32=False)\n        z += tl.sum(k, axis=1) / L\n        K_block_ptr = tl.advance(K_block_ptr, (0, BL))\n        V_block_ptr = tl.advance(V_block_ptr, (BL, 0))\n    S_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh,\n        shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(\n        start_k * BK, start_v * BV), block_shape=(BK, BV), order=(1, 0))\n    tl.store(S_block_ptr, s, boundary_check=(0, 1))\n    Z_block_ptr = Z + off_bs_head * stride_z_bh + start_k * BK + tl.arange(\n        0, BK)\n    tl.store(Z_block_ptr, z, mask=start_k * BK + tl.arange(0, BK) < DK)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "a7f09e67-0901-48b3-b22b-058e4ebf140f"
  },
  {
    "input": "@triton.jit\ndef leaky_relu(x):\n    \"\"\"\n    LeakyReLU_ activation\n\n    .. _LeakyReLU: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html\n    \"\"\"\n    scale = 0.01 + 0.0\n    scale = scale\n    return tl.where(x >= 0, x, scale * x)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "dadcf85c-ce9b-45bd-93e2-4934a1ac9a61"
  },
  {
    "input": "@triton.jit\ndef bwd_inner_chunk(q, k, g, dA, dq, dk, s_qk_h, s_qk_t, s_qk_d, B, H, T,\n    scale, BT: 'tl.constexpr', BK: 'tl.constexpr', DK: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < DK\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_qk_h + i_k * BK + i_t * BT * DK + tl.arange(0, BK)\n    p_dq = dq + i_bh * s_qk_h + i_k * BK + i_t * BT * DK + tl.arange(0, BK)\n    p_gq = g + i_bh * s_qk_h + i_k * BK + i_t * BT * DK + tl.arange(0, BK)\n    p_dA = dA + i_bh * (tl.cdiv(T, BT) * BT * BT) + i_t * BT * BT + tl.arange(\n        0, BT)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0)\n        gq = tl.load(p_gq, mask=mask, other=0)\n        score = tl.math.exp2(gq[None, :] - b_g)\n        score = tl.where(o_i[:, None] <= i, score, 0)\n        _dA = tl.load(p_dA)\n        _dA = tl.where(o_i <= i, _dA, 0)\n        b_dk += _dA[:, None] * score * _q[None, :]\n        b_dq = tl.sum(_dA[:, None] * score * b_k, axis=0)\n        tl.store(p_dq, b_dq, mask=mask)\n        p_q += DK\n        p_dq += DK\n        p_gq += DK\n        p_dA += BT\n    p_dk = tl.make_block_ptr(dk + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d),\n        (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "fce3bf48-418d-49b2-b05d-bfdf78a3cbae"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['D'])\n@triton.jit\ndef logsigmoid_fwd_kernel(x, y, temperature, T: 'tl.constexpr', D:\n    'tl.constexpr', B: 'tl.constexpr'):\n    i = tl.program_id(0)\n    o_i = i * B + tl.arange(0, B)\n    m_i = o_i < T\n    b_x = tl.load(x + o_i, mask=m_i, other=0.0)\n    b_m = tl.minimum(0.0, b_x)\n    b_z = 1.0 + tl.exp(-tl.abs(b_x))\n    b_y = (b_m - tl.log(b_z)) / temperature\n    tl.store(y + o_i, b_y, mask=m_i)\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "bdc1982e-d7c2-49e9-9556-9dd08de8e07e"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, N_CTX, sliding_window_offset, sliding_window_size, BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    SLIDING_WINDOW: 'tl.constexpr', IS_EVEN_M: 'tl.constexpr', IS_EVEN_N:\n    'tl.constexpr', COMPLEMENT_SLIDING_WINDOW: 'tl.constexpr'):\n    if SLIDING_WINDOW and not COMPLEMENT_SLIDING_WINDOW:\n        if COMPLEMENT_SLIDING_WINDOW:\n            lo = 0\n            hi = ((start_m + 1) * BLOCK_M + sliding_window_offset -\n                sliding_window_size + BLOCK_N - 1) // BLOCK_N * BLOCK_N\n        else:\n            lo = (start_m * BLOCK_M + sliding_window_offset -\n                sliding_window_size + 1) // BLOCK_N * BLOCK_N\n            hi = ((start_m + 1) * BLOCK_M - 1 + sliding_window_offset + BLOCK_N\n                ) // BLOCK_N * BLOCK_N\n            if lo < 0:\n                lo = 0\n            if hi > N_CTX:\n                hi = N_CTX\n            lo = tl.multiple_of(lo, BLOCK_N)\n            K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n            V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    else:\n        lo, hi = 0, N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        if IS_EVEN_N:\n            k = tl.load(K_block_ptr)\n        else:\n            k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk = qk * qk_scale\n        if SLIDING_WINDOW:\n            dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[\n                None, :] + start_m * BLOCK_M - start_n + sliding_window_offset\n            if COMPLEMENT_SLIDING_WINDOW:\n                mask = dist >= sliding_window_size\n            else:\n                mask = (dist >= 0) & (dist < sliding_window_size)\n            qk = tl.where(mask, qk, float('-inf'))\n        if not IS_EVEN_N:\n            qk = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :\n                ], qk, float('-inf'))\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk = qk - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        if SLIDING_WINDOW:\n            p = tl.where(mask, p, 0)\n        if not IS_EVEN_N:\n            p = tl.where((tl.arange(0, BLOCK_N) + start_n < N_CTX)[None, :],\n                p, 0)\n        l_ij = tl.sum(p, 1)\n        tmp = m_i - m_ij\n        alpha_mask = tmp != tmp\n        alpha = tl.math.exp2(tmp)\n        alpha = tl.where(alpha_mask, 1.0, alpha)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        if IS_EVEN_N:\n            v = tl.load(V_block_ptr)\n        else:\n            v = tl.load(V_block_ptr, boundary_check=(0, 1), padding_option=\n                'zero')\n        acc += tl.dot(p, v)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "94fdd439-feca-4996-b64f-f0006cceea92"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c246093a-8078-4339-9514-bd5e89e83257"
  },
  {
    "input": "@triton.jit\ndef print_once(*txt):\n    print_if(*txt, conds='=0,=0,=0')\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "39b7aa45-d642-4637-bed7-9b3c9d7534d1"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.\n    Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}),\n    triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dz_kernel(dout_ptr, out_ptr, z_ptr, x_ptr, D_ptr,\n    outz_ptr, dz_ptr, dout_x_ptr, dD_ptr, ddA_cumsum_ptr, chunk_size, hdim,\n    batch, seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head,\n    stride_out_hdim, stride_z_batch, stride_z_seqlen, stride_z_head,\n    stride_z_hdim, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_D_head, stride_outz_batch, stride_outz_seqlen,\n    stride_outz_head, stride_outz_hdim, stride_dz_batch, stride_dz_seqlen,\n    stride_dz_head, stride_dz_hdim, stride_doutx_batch, stride_doutx_seqlen,\n    stride_doutx_head, stride_doutx_hdim, stride_dD_batch, stride_dD_chunk,\n    stride_dD_head, stride_dD_csize, stride_dD_hdim, stride_ddA_cs_batch,\n    stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_D:\n    'tl.constexpr', D_HAS_HDIM: 'tl.constexpr', HAS_DDACS: 'tl.constexpr',\n    RECOMPUTE_OUTPUT: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dout_x_ptr += (pid_b * stride_doutx_batch + pid_c * chunk_size *\n        stride_doutx_seqlen + pid_h * stride_doutx_head)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    z_ptr += (pid_b * stride_z_batch + pid_c * chunk_size * stride_z_seqlen +\n        pid_h * stride_z_head)\n    dz_ptr += (pid_b * stride_dz_batch + pid_c * chunk_size *\n        stride_dz_seqlen + pid_h * stride_dz_head)\n    if RECOMPUTE_OUTPUT:\n        outz_ptr += (pid_b * stride_outz_batch + pid_c * chunk_size *\n            stride_outz_seqlen + pid_h * stride_outz_head)\n    if HAS_DDACS:\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    if HAS_D:\n        x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size *\n            stride_x_seqlen + pid_h * stride_x_head)\n        dD_ptr += (pid_b * stride_dD_batch + pid_c * stride_dD_chunk + \n            pid_h * stride_dD_head + pid_m * stride_dD_csize)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    dout_x_ptrs = dout_x_ptr + (offs_m[:, None] * stride_doutx_seqlen + \n        offs_n[None, :] * stride_doutx_hdim)\n    out_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None,\n        :] * stride_out_hdim)\n    z_ptrs = z_ptr + (offs_m[:, None] * stride_z_seqlen + offs_n[None, :] *\n        stride_z_hdim)\n    dz_ptrs = dz_ptr + (offs_m[:, None] * stride_dz_seqlen + offs_n[None, :\n        ] * stride_dz_hdim)\n    if RECOMPUTE_OUTPUT:\n        outz_ptrs = outz_ptr + (offs_m[:, None] * stride_outz_seqlen + \n            offs_n[None, :] * stride_outz_hdim)\n    if HAS_D:\n        x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None,\n            :] * stride_x_hdim)\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    out = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    z = tl.load(z_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < hdim), other=0.0)\n    z_sigmoid = tl.sigmoid(z)\n    if RECOMPUTE_OUTPUT:\n        outz = out * z * z_sigmoid\n        tl.store(outz_ptrs, outz, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_n[None, :] < hdim))\n    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))\n    tl.store(dz_ptrs, dz, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim))\n    dout *= z * z_sigmoid\n    tl.store(dout_x_ptrs, dout, mask=(offs_m[:, None] < chunk_size_limit) &\n        (offs_n[None, :] < hdim))\n    if HAS_D:\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        out -= x * D\n    if HAS_DDACS:\n        ddA_cs = tl.sum(dout * out, axis=1)\n        tl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs,\n            mask=offs_m < chunk_size)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "530836c4-7b82-4a86-8d8d-2146b3cb915a"
  },
  {
    "input": "@triton.jit\ndef _sample_2d(image, w, batch_index, ix, iy, IH, IW, C: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    image_offs = image + batch_index * IW * IH * C + iy_ * IW * C + ix_ * C\n    mask_w = w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW))\n    if C == 1:\n        val = tl.view(tl.load(image_offs), (BLOCK_SIZE,))\n        out = tl.view(val * mask_w, (BLOCK_SIZE,))\n        return out\n    else:\n        val = tl.view(tl.load(image_offs[:, None] + Coffs[None, :]), (\n            BLOCK_SIZE, C))\n        mask_w_bcast = tl.view(mask_w[:, None], (BLOCK_SIZE, 1))\n        return val * mask_w_bcast\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "b261f77b-270c-4907-900a-e41775af87ce"
  },
  {
    "input": "@triton.jit\ndef max_fn(x, y):\n    return tl.math.max(x, y)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "b57d5b71-0a08-4eda-b0a2-2b3f3b2f4332"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8),\n    triton.Config({}, num_stages=2, num_warps=4), triton.Config({},\n    num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1\n    )], key=['K'])\n@triton.jit\ndef quantize_int8_perrow_kernel(fpa_ptr, a_ptr, as_ptr, M, K, stride_fpam,\n    stride_fpak, stride_am, stride_ak, stride_asm, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n    a_scale = a_max / 127.0\n    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :\n        ] * stride_fpak\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K,\n            other=0.0)\n        inta = fpa / a_scale[:, None]\n        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n    tl.store(as_ptr + as_offs, a_scale)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "5cbc9440-03b0-4334-9b98-80fdba9f1653"
  },
  {
    "input": "@triton.jit\ndef chunk_gsa_bwd_kernel_intra_K(v, g, do, dA, scale, T: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BC: 'tl.constexpr', BV:\n    'tl.constexpr', NC: 'tl.constexpr', NG: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i, i_j = i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC\n        ) % NC\n    i_bg = i_bh // NG\n    n_bh = tl.num_programs(2)\n    o_v = i_v * BV + tl.arange(0, BV)\n    m_v = o_v < V\n    if i_t * BT + i_i * BC > T:\n        return\n    p_dA = tl.make_block_ptr(dA + (i_bh + i_v * n_bh) * T * BT, (T, BT), (\n        BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    b_dA = tl.zeros([BC, BC], dtype=tl.float32)\n    if i_i > i_j:\n        p_v = tl.make_block_ptr(v + i_bg * T * V, (V, T), (1, V), (i_v * BV,\n            i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_gv = tl.make_block_ptr(g + i_bg * T * V, (V, T), (1, V), (i_v *\n            BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n        p_gn = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t *\n            BT + i_i * BC) * V + o_v, BV), BV)\n        p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT +\n            i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        b_gn = tl.load(p_gn, mask=m_v, other=0.0)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g - b_gn[None, :]) * scale\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_vg = b_v * tl.exp(b_gn[:, None] - b_gv)\n        b_dA = tl.dot(b_do, b_vg)\n    elif i_i == i_j:\n        p_g = tl.make_block_ptr(g + i_bg * T * V, (T, V), (V, 1), (i_t * BT +\n            i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t *\n            BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n        p_v = tl.max_contiguous(tl.multiple_of(v + i_bg * T * V + (i_t * BT +\n            i_j * BC) * V + o_v, BV), BV)\n        p_gv = tl.max_contiguous(tl.multiple_of(g + i_bg * T * V + (i_t *\n            BT + i_j * BC) * V + o_v, BV), BV)\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n        m_v = o_v < V\n        o_i = tl.arange(0, BC)\n        m_dA = o_i[:, None] >= o_i[None, :]\n        for j in range(0, min(BC, T - i_t * BT - i_j * BC)):\n            b_v = tl.load(p_v, mask=m_v, other=0)\n            b_gv = tl.load(p_gv, mask=m_v, other=0)\n            b_dAj = tl.sum(b_do * b_v[None, :] * tl.exp(b_g - b_gv[None, :]), 1\n                )\n            b_dA = tl.where((o_i == j)[None, :], b_dAj[:, None], b_dA)\n            p_v += V\n            p_gv += V\n        b_dA = tl.where(m_dA, b_dA, 0.0)\n    tl.store(p_dA, b_dA, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "27a4680c-472a-4e93-b4b7-25ef6bfe60ac"
  },
  {
    "input": "@triton.autotune([triton.Config({'BLOCK_SIZE_Q': BLOCK_SIZE_Q,\n    'BLOCK_SIZE_KV': BLOCK_SIZE_KV}, num_stages=num_stages, num_warps=\n    num_warps) for BLOCK_SIZE_Q in [64, 128] for BLOCK_SIZE_KV in [32, 64] for\n    num_stages in [3, 4, 7] for num_warps in [2, 4]], key=['SEQ_LEN',\n    'HEAD_DIM'])\n@triton.jit\ndef _attn_fwd(Q, K, V, softmax_scale, M, O, stride_Q_batch, stride_Q_head,\n    stride_Q_seq, stride_Q_dim, stride_K_batch, stride_K_head, stride_K_seq,\n    stride_K_dim, stride_V_batch, stride_V_head, stride_V_seq, stride_V_dim,\n    stride_O_batch, stride_O_head, stride_O_seq, stride_O_dim, BATCH_SIZE,\n    NUM_HEADS: 'tl.constexpr', SEQ_LEN: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', BLOCK_SIZE_Q: 'tl.constexpr', BLOCK_SIZE_KV:\n    'tl.constexpr', STAGE: 'tl.constexpr'):\n    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\n    block_index_q = tl.program_id(0)\n    index_batch_head = tl.program_id(1)\n    index_batch = index_batch_head // NUM_HEADS\n    index_head = index_batch_head % NUM_HEADS\n    qvk_offset = index_batch * stride_Q_batch + index_head * stride_Q_head\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(SEQ_LEN,\n        HEAD_DIM), strides=(stride_Q_seq, stride_Q_dim), offsets=(\n        block_index_q * BLOCK_SIZE_Q, 0), block_shape=(BLOCK_SIZE_Q,\n        HEAD_DIM), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(SEQ_LEN,\n        HEAD_DIM), strides=(stride_V_seq, stride_V_dim), offsets=(0, 0),\n        block_shape=(BLOCK_SIZE_KV, HEAD_DIM), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(HEAD_DIM,\n        SEQ_LEN), strides=(stride_K_dim, stride_K_seq), offsets=(0, 0),\n        block_shape=(HEAD_DIM, BLOCK_SIZE_KV), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=O + qvk_offset, shape=(SEQ_LEN,\n        HEAD_DIM), strides=(stride_O_seq, stride_O_dim), offsets=(\n        block_index_q * BLOCK_SIZE_Q, 0), block_shape=(BLOCK_SIZE_Q,\n        HEAD_DIM), order=(1, 0))\n    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\n    offs_kv = tl.arange(0, BLOCK_SIZE_KV)\n    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0\n    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\n    Q_block = tl.load(Q_block_ptr)\n    if STAGE == 1 or STAGE == 3:\n        O_block, l_i, m_i = _attn_fwd_inner(O_block, l_i, m_i, Q_block,\n            K_block_ptr, V_block_ptr, block_index_q, softmax_scale,\n            BLOCK_SIZE_Q, BLOCK_SIZE_KV, 4 - STAGE, offs_q, offs_kv, SEQ_LEN)\n    if STAGE == 3:\n        O_block, l_i, m_i = _attn_fwd_inner(O_block, l_i, m_i, Q_block,\n            K_block_ptr, V_block_ptr, block_index_q, softmax_scale,\n            BLOCK_SIZE_Q, BLOCK_SIZE_KV, 2, offs_q, offs_kv, SEQ_LEN)\n    m_i += tl.math.log(l_i)\n    O_block = O_block / l_i[:, None]\n    m_ptrs = M + index_batch_head * SEQ_LEN + offs_q\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, O_block)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "c284a68e-435d-4ad7-b7be-eb46928792be"
  },
  {
    "input": "@triton.jit\ndef scaled_gemm_splitk(a_ptr, b_ptr, c_ptr, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, scale_a, scale_b, m, n, k, block_m:\n    'tl.constexpr', block_n: 'tl.constexpr', block_k: 'tl.constexpr',\n    split_k: 'tl.constexpr', group_m: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_k = tl.program_id(1)\n    grid_k = tl.cdiv(k, block_k * split_k)\n    pid_m, pid_n = column_major(pid, m, n, block_m, block_n)\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    offs_k = pid_k * block_k + tl.arange(0, block_k)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_m, block_m), block_m)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_n, block_n), block_n)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((block_m, block_n), dtype=tl.float32)\n    for k_ in range(0, grid_k):\n        k_remaining = k - k_ * (block_k * split_k)\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < k_remaining, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k[:, None] < k_remaining, other=0.0)\n        acc = tl.dot(a, b, acc, out_dtype=tl.float32)\n        a_ptrs += block_k * split_k * stride_ak\n        b_ptrs += block_k * split_k * stride_bk\n    acc = scale_a * scale_b * acc\n    acc\n    offs_m = pid_m * block_m + tl.arange(0, block_m)\n    offs_n = pid_n * block_n + tl.arange(0, block_n)\n    c_ptrs = c_ptr + (offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n        )\n    mask = (offs_m < m)[:, None] & (offs_n < n)[None, :]\n    tl.atomic_add(c_ptrs, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "77a60da5-6c54-47bd-9a03-1371c77ac86d"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(Q, K, V, softmax_scale, dO, dQ, dK, dV, M, D, stride_batch,\n    stride_head, stride_seq, stride_dim, NUM_HEADS, SEQ_LEN, BLOCK_Q:\n    'tl.constexpr', BLOCK_KV: 'tl.constexpr', HEAD_DIM: 'tl.constexpr',\n    STAGE: 'tl.constexpr'):\n    index_batch_head = tl.program_id(2)\n    index_batch = index_batch_head // NUM_HEADS\n    index_head = index_batch_head % NUM_HEADS\n    offset_batch_head = stride_batch * index_batch + stride_head * index_head\n    offset_batch_head_seq = index_batch_head * SEQ_LEN\n    Q += offset_batch_head\n    K += offset_batch_head\n    V += offset_batch_head\n    dO += offset_batch_head\n    dQ += offset_batch_head\n    dK += offset_batch_head\n    dV += offset_batch_head\n    M += offset_batch_head_seq\n    D += offset_batch_head_seq\n    offs_dim = tl.arange(0, HEAD_DIM)\n    index_block_kv = tl.program_id(0)\n    start_q = index_block_kv * BLOCK_Q\n    offs_q = start_q + tl.arange(0, BLOCK_Q)\n    Q_block = tl.load(Q + offs_q[:, None] * stride_seq + offs_dim[None, :] *\n        stride_dim)\n    dQ_block = tl.zeros([BLOCK_Q, HEAD_DIM], dtype=tl.float32)\n    dO_block = tl.load(dO + offs_q[:, None] * stride_seq + offs_dim[None, :\n        ] * stride_dim)\n    M_block = tl.load(M + offs_q)\n    M_block = M_block[:, None]\n    offs_kv = tl.arange(0, BLOCK_KV)\n    kT_ptrs = K + offs_kv[None, :] * stride_seq + offs_dim[:, None\n        ] * stride_dim\n    vT_ptrs = V + offs_kv[None, :] * stride_seq + offs_dim[:, None\n        ] * stride_dim\n    Di = tl.load(D + offs_q)\n    curr_kv = 0\n    num_steps = SEQ_LEN // BLOCK_KV\n    for blk_idx in range(num_steps):\n        K_T_block = tl.load(kT_ptrs)\n        V_T_block = tl.load(vT_ptrs)\n        QK_block = softmax_scale * tl.dot(Q_block, K_T_block)\n        P_block = tl.math.exp(QK_block - M_block)\n        if STAGE == 3:\n            offs_kv = curr_kv + tl.arange(0, BLOCK_KV)\n            mask_block = offs_q[:, None] >= offs_kv[None, :]\n            P_block = tl.where(mask_block, P_block, 0.0)\n        dP_block = tl.dot(dO_block, V_T_block)\n        dS_block = P_block * (dP_block - Di[:, None])\n        dS_block = dS_block\n        dQ_block += softmax_scale * tl.dot(dS_block, tl.trans(K_T_block))\n        curr_kv += BLOCK_KV\n        kT_ptrs += BLOCK_KV * stride_seq\n        vT_ptrs += BLOCK_KV * stride_seq\n    dQ_block_ptrs = dQ + offs_q[:, None] * stride_seq + offs_dim[None, :\n        ] * stride_dim\n    tl.store(dQ_block_ptrs, dQ_block)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "a8218b04-69a9-4b20-8ece-4d2aba6d2df7"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_bwd_dwdb(A, DOut, Mean, Var, DW, DB, M, N, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    UNROLL: 'tl.constexpr' = 4\n    for i in range(0, M, BLOCK_SIZE_M * UNROLL):\n        for j in range(UNROLL):\n            rows = i + j * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n            mask = (rows[:, None] < M) & (cols[None, :] < N)\n            offs = rows[:, None] * N + cols[None, :]\n            a = tl.load(A + offs, mask=mask, other=0.0)\n            dout = tl.load(DOut + offs, mask=mask, other=0.0)\n            mean = tl.load(Mean + rows, mask=rows < M, other=0.0)\n            rstd = tl.load(Var + rows, mask=rows < M, other=0.0)\n            a_hat = (a - mean[:, None]) * rstd[:, None]\n            dw += dout * a_hat\n            db += dout\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(DW + cols, sum_dw, mask=cols < N)\n    tl.store(DB + cols, sum_db, mask=cols < N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "724bc321-3541-4aa4-84a0-c70c88a8833b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS':\n    32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2)],\n    key=['chunk_size', 'K'])\n@triton.jit\ndef _bmm_chunk_bwd_kernel(a_ptr, dout_ptr, db_ptr, res_ptr, seqlen,\n    chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head,\n    stride_ak, stride_dout_batch, stride_dout_chunk, stride_dout_head,\n    stride_dout_csize_m, stride_dout_csize_n, stride_db_batch,\n    stride_db_seqlen, stride_db_head, stride_db_k, stride_res_batch,\n    stride_res_seqlen, stride_res_head, stride_res_k, dot_dtype:\n    'tl.constexpr', HAS_RESIDUAL: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_CS: 'tl.constexpr'\n    ):\n    pid_b = tl.program_id(axis=1)\n    pid_ch = tl.program_id(axis=2)\n    pid_c = pid_ch // ngroups\n    pid_h = pid_ch - pid_c * ngroups\n    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    a_ptr += (pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen +\n        pid_h * stride_a_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * stride_dout_chunk + \n        pid_h * stride_dout_head)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_cs = tl.arange(0, BLOCK_SIZE_CS)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_csize_n + offs_cs\n        [None, :] * stride_dout_csize_m)\n    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] *\n        stride_ak)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size) & (\n            offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),\n            other=0.0)\n        a = tl.load(a_ptrs, mask=(offs_cs[:, None] < chunk_size_limit - cs *\n            BLOCK_SIZE_CS) & (offs_n[None, :] < K), other=0.0)\n        acc += tl.dot(dout, a)\n        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_RESIDUAL:\n        res_ptr += (pid_b * stride_res_batch + pid_c * chunk_size *\n            stride_res_seqlen + pid_h * stride_res_head)\n        res_ptrs = res_ptr + (offs_m[:, None] * stride_res_seqlen + offs_n[\n            None, :] * stride_res_k)\n        res = tl.load(res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_n[None, :] < K))\n        acc += res\n    db = acc\n    db_ptr += (pid_b * stride_db_batch + pid_c * chunk_size *\n        stride_db_seqlen + pid_h * stride_db_head)\n    db_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :\n        ] * stride_db_k)\n    tl.store(db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < K))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b2aba609-4d13-42e9-ad03-863a78650aa6"
  },
  {
    "input": "@triton.jit\ndef fwd_sequential_scan(v, f1, hidden, B, L, C, BLOCK_M: 'tl.constexpr'):\n    offset_b = tl.program_id(0)\n    if offset_b >= B:\n        return\n    offset_n = tl.program_id(1)\n    ptr = tl.arange(0, BLOCK_M) + offset_b * L * C + offset_n * BLOCK_M\n    h1 = tl.zeros([BLOCK_M], dtype=tl.float32)\n    for _ in range(L):\n        x0 = tl.load(v + ptr)\n        decay1 = tl.load(f1 + ptr)\n        h1 = (h1 - x0) * decay1 + x0\n        tl.store(hidden + ptr, h1)\n        ptr += C\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "a78b39c9-987a-42b8-9864-a6b78a794d2e"
  },
  {
    "input": "@triton.jit\ndef fused_layer_norm_kernel(x_ptr, w_ptr, b_ptr, z_ptr, H, eps=1e-05,\n    BLOCK_SIZE: 'tl.constexpr'=512):\n    row_id = tl.program_id(0)\n    x_ptr += row_id * H\n    z_ptr += row_id * H\n    x_mean = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for i in range(0, H, BLOCK_SIZE):\n        offset = i + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(x_ptr + offset, mask=offset < H, other=0.0)\n        x_mean += x\n    x_mean = tl.sum(x_mean) / H\n    x_var = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for i in range(0, H, BLOCK_SIZE):\n        offset = i + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(x_ptr + offset, mask=offset < H, other=x_mean)\n        x = x\n        x_var += (x - x_mean) * (x - x_mean)\n    x_var = tl.sum(x_var) / H\n    rstd = 1 / tl.sqrt(x_var + eps)\n    for i in range(0, H, BLOCK_SIZE):\n        offset = i + tl.arange(0, BLOCK_SIZE)\n        mask = offset < H\n        x = tl.load(x_ptr + offset, mask=mask, other=0.0)\n        w = tl.load(w_ptr + offset, mask=mask, other=0.0)\n        b = tl.load(b_ptr + offset, mask=mask, other=0.0)\n        z = (x - x_mean) * rstd\n        z = z * w + b\n        tl.store(z_ptr + offset, z, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "28dae5a0-0fbe-40a6-ac9e-a2cdd46e8a5a"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BS': 32}, num_warps=2), triton.\n    Config({'BS': 32}, num_warps=4), triton.Config({'BS': 32}, num_warps=8),\n    triton.Config({'BS': 64}, num_warps=2), triton.Config({'BS': 64},\n    num_warps=4), triton.Config({'BS': 64}, num_warps=8), triton.Config({\n    'BS': 128}, num_warps=2), triton.Config({'BS': 128}, num_warps=4),\n    triton.Config({'BS': 128}, num_warps=8)], key=['S'])\n@triton.jit\ndef recurrent_reversed_cumsum_fwd_kernel(s, z, s_s_h, s_s_t, T:\n    'tl.constexpr', S: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_s = i_s * BS + tl.arange(0, BS)\n    mask = o_s < S\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(T - 1, -1, -1):\n        b_s = tl.load(s + i_bh * s_s_h + i_t * s_s_t + o_s, mask=mask, other=0)\n        b_z = b_z + b_s\n        tl.store(z + i_bh * s_s_h + i_t * s_s_t + o_s, b_z, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "5b4a9f5b-a382-4d05-93fb-cda9218fea54"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4, pre_hook=init_to_zero([\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages\n    =3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.\n    Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size',\n    'dstate', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_prev_kernel(dout_ptr, prev_states_ptr, C_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, dstate, hdim,\n    batch, seqlen, nchunks, nheads_ngroups_ratio, stride_dout_batch,\n    stride_dout_seqlen, stride_dout_head, stride_dout_hdim,\n    stride_prev_states_batch, stride_prev_states_chunk,\n    stride_prev_states_head, stride_prev_states_hdim,\n    stride_prev_states_dstate, stride_C_batch, stride_C_seqlen,\n    stride_C_head, stride_C_dstate, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: 'tl.constexpr',\n    BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    prev_states_ptr += (pid_b * stride_prev_states_batch + pid_c *\n        stride_prev_states_chunk + pid_h * stride_prev_states_head)\n    C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen +\n        pid_h // nheads_ngroups_ratio * stride_C_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n        stride_prev_states_dstate + offs_k[:, None] * stride_prev_states_hdim)\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None, :] *\n        stride_C_dstate)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    prev_states = tl.load(prev_states_ptrs, mask=(offs_k[:, None] < hdim) &\n        (offs_n[None, :] < dstate), other=0.0)\n    prev_states = prev_states\n    acc = tl.dot(dout, prev_states)\n    c = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n\n        [None, :] < dstate), other=0.0)\n    ddA_cs = tl.sum(acc * c, axis=1)\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0\n        )\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_m)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    ddA_cs *= scale\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "3ef3be46-95bb-4417-9da2-a58ebacb817c"
  },
  {
    "input": "@triton.jit\ndef bmm_kernel(x_ptr, x_stride_b, x_stride_m, x_stride_k, y_ptr, y_stride_b,\n    y_stride_k, y_stride_n, o_ptr, o_stride_b, o_stride_m, o_stride_n, m:\n    'tl.constexpr', n: 'tl.constexpr', k: 'tl.constexpr', block_size_m:\n    'tl.constexpr', block_size_n: 'tl.constexpr', block_size_k:\n    'tl.constexpr', group_size_m: 'tl.constexpr'):\n    batch_idx = tl.program_id(axis=0)\n    block_idx = tl.program_id(axis=1)\n    num_blocks_m = tl.cdiv(m, block_size_m)\n    num_blocks_n = tl.cdiv(n, block_size_n)\n    num_blocks_g = group_size_m * num_blocks_n\n    group_idx = block_idx // num_blocks_g\n    first_block_m = group_idx * group_size_m\n    group_size_m_ = min(num_blocks_m - first_block_m, group_size_m)\n    block_idx_m = first_block_m + block_idx % group_size_m_\n    block_idx_n = block_idx % num_blocks_g // group_size_m_\n    block_ptrs_m = (block_idx_m * block_size_m + tl.arange(0, block_size_m)\n        ) % m\n    block_ptrs_n = (block_idx_n * block_size_n + tl.arange(0, block_size_n)\n        ) % n\n    offsets_k = tl.arange(0, block_size_k)\n    x_block_ptrs = x_ptr + (block_ptrs_m[:, None] * x_stride_m + offsets_k[\n        None, :] * x_stride_k) + batch_idx * x_stride_b\n    y_block_ptrs = y_ptr + (offsets_k[:, None] * y_stride_k + block_ptrs_n[\n        None, :] * y_stride_n) + batch_idx * y_stride_b\n    o = tl.zeros((block_size_m, block_size_n), dtype=tl.float32)\n    for block_idx_k in range(0, tl.cdiv(k, block_size_k)):\n        block_offs_k = block_idx_k * block_size_k + offsets_k\n        x = tl.load(x_block_ptrs, mask=block_offs_k[None, :] < k, other=0.0)\n        y = tl.load(y_block_ptrs, mask=block_offs_k[:, None] < k, other=0.0)\n        o += tl.dot(x, y)\n        x_block_ptrs += block_size_k * x_stride_k\n        y_block_ptrs += block_size_k * y_stride_k\n    block_ptrs_m_ = block_idx_m * block_size_m + tl.arange(0, block_size_m)[\n        :, None]\n    block_ptrs_n_ = block_idx_n * block_size_n + tl.arange(0, block_size_n)[\n        None, :]\n    o_ptrs = o_ptr + (block_ptrs_m_ * o_stride_m + block_ptrs_n_ * o_stride_n\n        ) + batch_idx * o_stride_b\n    block_mask_mn = (block_ptrs_m_ < m) & (block_ptrs_n_ < n)\n    tl.store(o_ptrs, o, mask=block_mask_mn)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "50c9f8f2-c798-4544-95de-6c891a832340"
  },
  {
    "input": "@triton.jit\ndef mul_relu_block_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: 'tl.constexpr',\n    B1: 'tl.constexpr'):\n    block_id_x = tl.program_id(0)\n    block_id_y = tl.program_id(1)\n    off_x = block_id_x * B0 + tl.arange(0, B0)\n    off_y = block_id_y * B1 + tl.arange(0, B1)\n    off_z = off_y[:, None] * N0 + off_x[None, :]\n    mask_x = off_x < N0\n    mask_y = off_y < N1\n    mask_z = mask_y[:, None] & mask_x[None, :]\n    x = tl.load(x_ptr + off_x, mask=mask_x)\n    y = tl.load(y_ptr + off_y, mask=mask_y)\n    z = x[None, :] * y[:, None]\n    relu_z = tl.where(z > 0, z, 0.0)\n    tl.store(z_ptr + off_z, relu_z, mask=mask_z)\n    return\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "b09379e4-9392-40f1-8080-6adf21d1d993"
  },
  {
    "input": "@triton.jit\ndef mask_2d(offs0, offs1, max0, max1):\n    return (tl.expand_dims(offs0, 1) < max0) & (tl.expand_dims(offs1, 0) < max1\n        )\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "17468480-c18a-44e5-ad86-e16df2fa420d"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "57e83e30-d8cb-48ff-9585-ee7e0a7d7dd1"
  },
  {
    "input": "@triton.jit\ndef _plane_grid_sample_one(gi, feature_grid, feature_grid_size, batch_index,\n    ix_in, iy_in, IH, IW, C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    mask_out_of_bounds_samples: 'tl.constexpr'):\n    ix, iy, ix0, iy0, grid_numel = _get_plane_grid_sample_info(gi, ix_in,\n        iy_in, IH, IW, feature_grid_size, C, BLOCK_SIZE)\n    V00x, V00y, V10x, V10y, V01x, V01y, V11x, V11y, x, y = (\n        _get_plane_grid_sample_locs_weights(ix, iy, ix0, iy0))\n    sampled = _sample_2d(feature_grid, (1 - x) * (1 - y), batch_index, V00x,\n        V00y, IH, IW, C, BLOCK_SIZE) + _sample_2d(feature_grid, x * (1 - y),\n        batch_index, V01x, V01y, IH, IW, C, BLOCK_SIZE) + _sample_2d(\n        feature_grid, (1 - x) * y, batch_index, V10x, V10y, IH, IW, C,\n        BLOCK_SIZE) + _sample_2d(feature_grid, x * y, batch_index, V11x,\n        V11y, IH, IW, C, BLOCK_SIZE)\n    return sampled, grid_numel\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "95f10ed5-3d10-4853-9a8e-b7a2815b9534"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_based_fwd_kernel(q, k, v, o, z, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'\n    ):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_h_0o = tl.zeros([BV], dtype=tl.float32)\n    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\n    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\n    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\n    k_1o = tl.zeros([1, BK], dtype=tl.float32)\n    k_0o = 0\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT])\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1)) * scale\n        b_o = tl.zeros([BT, BV], dtype=tl.float32)\n        b_z = tl.zeros([BT], dtype=tl.float32)\n        b_o += b_h_0o\n        b_z += k_0o\n        b_o += tl.dot(b_q, b_h_1o, allow_tf32=False)\n        b_z += tl.sum(b_q * k_1o, axis=1)\n        b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK])\n        b_o += tl.dot(b_q_2o, b_h_2o, allow_tf32=False) * 0.5\n        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n        k_1o += tl.sum(b_k, axis=1)[None, :]\n        k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n        k_0o += BT\n        b_s = tl.dot(b_q, b_k, allow_tf32=False)\n        b_s = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_z += tl.sum(b_s, axis=1)\n        b_o += tl.dot(b_s, b_v, allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        tl.store(p_z, b_z, mask=i * BT + tl.arange(0, BT) < T)\n        b_h_2o = b_h_2o + tl.dot(b_k_2o, b_v, allow_tf32=False)\n        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n        p_z += BT\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "fbdb2475-0001-4b61-b12a-8400c7e1a32f"
  },
  {
    "input": "@triton.jit\ndef third_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    CONST002 = 6.48074069840786\n    CONST005 = 12.9614813968157\n    CONST007 = -3.96862696659689\n    CONST008 = -12.5499003980111\n    CONST009 = -10.2469507659596\n    CONST010 = -7.93725393319377\n    CONST011 = -6.27495019900557\n    CONST012 = -5.1234753829798\n    CONST013 = -4.8605555238059\n    CONST014 = -3.24037034920393\n    CONST015 = -1.62018517460197\n    VAR08 = x * x\n    VAR17 = y * y\n    VAR26 = z * z\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += (CONST008 * g_6 * x * z - CONST009 * g_1 * y * z + CONST009 *\n        g_5 * x * y + CONST010 * g_3 * x * y + CONST014 * g_4 * x * z + g_0 *\n        (CONST011 * VAR08 - CONST011 * VAR26) + g_2 * (CONST002 * VAR17 + \n        CONST013 * VAR08 + CONST015 * VAR26))\n    g_y += (CONST005 * g_2 * x * y + CONST005 * g_4 * y * z - CONST009 *\n        g_1 * x * z + g_3 * (CONST007 * VAR08 + CONST007 * VAR26 - CONST010 *\n        VAR17) + g_5 * (CONST012 * VAR08 - CONST012 * VAR26))\n    g_z += (-CONST008 * g_0 * x * z - CONST009 * g_1 * x * y - CONST009 *\n        g_5 * y * z + CONST010 * g_3 * y * z + CONST014 * g_2 * x * z + g_4 *\n        (CONST002 * VAR17 + CONST013 * VAR26 + CONST015 * VAR08) + g_6 * (\n        CONST011 * VAR08 - CONST011 * VAR26))\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "6e661231-205b-46f0-b38f-db9dd49c04fe"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.\n    Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4),\n    triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64},\n    num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({\n    'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton\n    .Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps\n    =2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128},\n    num_warps=8)], key=['D'])\n@triton.jit\ndef fused_recurrent_hgrn_fwd_kernel(x, g, o, h0, ht, T: 'tl.constexpr', D:\n    'tl.constexpr', BD: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr'):\n    i_d, i_b = tl.program_id(0), tl.program_id(1)\n    o_d = i_d * BD + tl.arange(0, BD)\n    mask = o_d < D\n    p_x = x + i_b * T * D + o_d\n    p_g = g + i_b * T * D + o_d\n    p_o = o + i_b * T * D + o_d\n    b_h = tl.zeros([BD], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_b * D + o_d\n        b_h += tl.load(p_h0, mask=mask, other=0)\n    for _ in range(0, T):\n        b_x = tl.load(p_x, mask=mask, other=0)\n        b_g = tl.load(p_g, mask=mask, other=0)\n        b_h = tl.exp(b_g) * b_h + b_x\n        tl.store(p_o, b_h, mask=mask)\n        p_x += D\n        p_g += D\n        p_o += D\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_b * D + o_d\n        tl.store(p_ht, b_h, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "a0ffc5dc-c580-4b55-b267-e1c7f0944ba1"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_dqk(Q, K, GK, DA, DQ, DK, DGK, stride_q1, stride_q2,\n    stride_q3, stride_q4, stride_a1, stride_a2, stride_a3, stride_a4, Z, H,\n    N_CTX, D, BLOCK_DMODEL_QK: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_k = tl.program_id(2)\n    qk_offset = off_hz * stride_q2 + BLOCK_DMODEL_QK * off_k\n    a_offset = off_hz * stride_a2\n    lo = 0\n    hi = BLOCK_N\n    Q_ptr = Q + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DQ_ptr = DQ + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    K_ptr = K + qk_offset + start_m * stride_q3 + tl.arange(0, BLOCK_DMODEL_QK\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    GK_K_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    GK_Q_ptr = GK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DA_ptr = DA + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(lo + 16, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        dq2 = tl.zeros([16, BLOCK_DMODEL_QK], dtype=tl.float32)\n        for k_high in range(0, q_high, 16):\n            k = tl.load(K_ptr + k_high * stride_q4)\n            k_gk = tl.load(GK_K_ptr + k_high * stride_q4)\n            dqk = tl.load(DA_ptr + q_high * stride_a4 + k_high)\n            k_gk = tl.exp(q_normalizer[None, :] - k_gk)\n            k = k * k_gk\n            dq2 += tl.dot(dqk, k, allow_tf32=False)\n        dq2 = dq2\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_gk = tl.exp(q_gk - q_normalizer[None, :])\n        dq = dq2 * q_gk\n        dq_gk = dq * q\n        DQ_ptr = DQ + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + q_high * stride_q4\n        tl.store(DQ_ptr, dq)\n        DGK_Q_ptr = DGK + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + q_high * stride_q4\n        tl.store(DGK_Q_ptr, dq_gk)\n    tl.debug_barrier()\n    for k_high in range(lo, hi - 16, 16):\n        k = tl.load(K_ptr + k_high * stride_q4)\n        k_gk = tl.load(GK_K_ptr + k_high * stride_q4)\n        dk = tl.zeros([16, BLOCK_DMODEL_QK], dtype=tl.float32)\n        dgk = tl.zeros([16, BLOCK_DMODEL_QK], dtype=tl.float32)\n        for q_high in range(k_high + 16, hi, 16):\n            q = tl.load(Q_ptr + q_high * stride_q4)\n            q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n                q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n            q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n            q_gk = tl.exp(q_gk - q_normalizer[None, :])\n            q = q * q_gk\n            dqk = tl.load(DA_ptr + q_high * stride_a4 + k_high)\n            k_gk2 = tl.exp(q_normalizer[None, :] - k_gk)\n            dk2 = tl.dot(tl.trans(dqk), q, allow_tf32=False)\n            dk += dk2 * k_gk2\n            dgk -= dk2 * k * k_gk2\n        DK_ptr = DK + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + k_high * stride_q4\n        tl.store(DK_ptr, dk)\n        DGK_K_ptr = DGK + qk_offset + start_m * stride_q3 + tl.arange(0,\n            BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None\n            ] * stride_q4 + k_high * stride_q4\n        prev = tl.load(DGK_K_ptr)\n        tl.store(DGK_K_ptr, prev + dgk)\n    tl.debug_barrier()\n    DK_ptr = DK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DGK_K_ptr = DGK + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    DQ_ptr = DQ + qk_offset + start_m * stride_q3 + tl.arange(0,\n        BLOCK_DMODEL_QK)[None, :] + tl.arange(0, 16)[:, None] * stride_q4\n    for q_high in range(lo, hi, 16):\n        q = tl.load(Q_ptr + q_high * stride_q4)\n        q_gk = tl.load(GK_Q_ptr + q_high * stride_q4)\n        q_normalizer = tl.load(GK + qk_offset + start_m * stride_q3 + \n            q_high * stride_q4 + tl.arange(0, BLOCK_DMODEL_QK))\n        q_gk2 = tl.exp(q_gk - q_normalizer[None, :])\n        q2 = q * q_gk2\n        q_gk3 = tl.exp(q_normalizer[None, :] - q_gk)\n        k = tl.load(K_ptr + q_high * stride_q4)\n        k2 = k * q_gk3\n        dqk = tl.load(DA_ptr + q_high * stride_a4 + q_high)\n        dqk = tl.where(tl.arange(0, 16)[:, None] >= tl.arange(0, 16)[None,\n            :], dqk, 0.0)\n        dk2 = tl.dot(tl.trans(dqk), q2, allow_tf32=False)\n        dk = dk2 * q_gk3\n        prev_dk = tl.load(DK_ptr + q_high * stride_q4)\n        tl.store(DK_ptr + q_high * stride_q4, dk + prev_dk)\n        dgk = -dk * k\n        dq2 = tl.dot(dqk, k2, allow_tf32=False)\n        dq = dq2 * q_gk2\n        prev_dq = tl.load(DQ_ptr + q_high * stride_q4)\n        tl.store(DQ_ptr + q_high * stride_q4, dq + prev_dq)\n        dgk += dq * q\n        prev_dq_gk = tl.load(DGK_K_ptr + q_high * stride_q4)\n        tl.store(DGK_K_ptr + q_high * stride_q4, dgk + prev_dq_gk)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "0c4a1163-bbdd-4692-b84b-73d681846db0"
  },
  {
    "input": "@triton.jit\ndef _dot_tf32_f32_3x(a, b, trans_a=False, trans_b=False):\n    \"\"\"Perform the 3-pass tf32 dot function.\"\"\"\n    tl.static_assert(a.dtype == tl.float32)\n    tl.static_assert(b.dtype == tl.float32)\n    a_ = a.to(tl.uint32, bitcast=True) & 4294959104\n    b_ = b.to(tl.uint32, bitcast=True) & 4294959104\n    a_err = a - a_\n    b_err = b - b_\n    if trans_a:\n        a_ = tl.trans(a_)\n        a_err = tl.trans(a_err)\n    if trans_b:\n        b_ = tl.trans(b_)\n        b_err = tl.trans(b_err)\n    return tl.dot(a_, b_, out_dtype=tl.float32) + (tl.dot(a_, b_err,\n        out_dtype=tl.float32) + tl.dot(a_err, b_, out_dtype=tl.float32))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "49453ff5-ba2f-49f8-95d7-b4f1229e0ab3"
  },
  {
    "input": "@triton.jit\ndef maximum_path_cp(path, value, t_x, t_y, B, T, S, max_neg_val,\n    BLOCK_SIZE_X: 'tl.constexpr'):\n    batch = tl.program_id(axis=0)\n    path += batch * T * S\n    value += batch * T * S\n    x_length = tl.load(t_x + batch)\n    y_length = tl.load(t_y + batch)\n    offs_prev = tl.arange(0, BLOCK_SIZE_X)\n    init = tl.where(offs_prev == 0, tl.load(value), max_neg_val)\n    tl.store(value + offs_prev * S, init, mask=offs_prev < x_length)\n    for j in range(1, y_length, 1):\n        v_cur = tl.load(value + offs_prev * S + (j - 1), mask=offs_prev <\n            x_length, other=max_neg_val)\n        v_prev = tl.load(value + (offs_prev - 1) * S + (j - 1), mask=(0 <\n            offs_prev) & (offs_prev < x_length), other=max_neg_val)\n        v = tl.maximum(v_cur, v_prev) + tl.load(value + offs_prev * S + j,\n            mask=offs_prev < x_length)\n        tl.store(value + offs_prev * S + j, v, mask=offs_prev < x_length)\n    index = x_length - 1\n    for j in range(y_length - 1, -1, -1):\n        tl.store(path + index * S + j, 1)\n        if index > 0:\n            v_left = tl.load(value + index * S + j - 1)\n            v_leftdown = tl.load(value + (index - 1) * S + j - 1)\n            if v_left < v_leftdown:\n                index += -1\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "95652b4d-263b-4221-9ef7-7ae9beb955a6"
  },
  {
    "input": "@triton.jit\ndef _dynamic_matmul(pid_k, pid_n, next_id, input, grad_output, grad_other,\n    grad_other_tiles, stride_input_m, stride_input_k, stride_grad_output_m,\n    stride_grad_output_n, stride_grad_other_b, stride_grad_other_k,\n    stride_grad_other_n, K, N, M, length, out_dtype: 'tl.constexpr',\n    BLOCK_LENGTH: 'tl.constexpr', TILE_K: 'tl.constexpr', TILE_N:\n    'tl.constexpr', TILE_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_K:\n    'tl.constexpr', EVEN_M: 'tl.constexpr', DETERMINISTIC: 'tl.constexpr'):\n    offs_k = pid_k * TILE_K + tl.arange(0, TILE_K)\n    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)\n    offs_m = tl.arange(0, TILE_M)\n    acc = tl.zeros((TILE_K, TILE_N), dtype=out_dtype)\n    mask_k = offs_k[:, None] < K if not EVEN_K else True\n    mask_n = offs_n[None, :] < N if not EVEN_N else True\n    input_ptrs = input + (offs_m[None, :] * stride_input_m + offs_k[:, None\n        ] * stride_input_k)\n    grad_output_ptrs = grad_output + (offs_m[:, None] *\n        stride_grad_output_m + offs_n[None, :] * stride_grad_output_n)\n    m_iter = length // TILE_M if EVEN_M else tl.cdiv(length, TILE_M)\n    for m in range(0, m_iter):\n        if EVEN_K:\n            if EVEN_M:\n                a = tl.load(input_ptrs)\n            else:\n                a = tl.load(input_ptrs, mask=offs_m[None, :] + m * TILE_M <\n                    length, other=0.0)\n        elif EVEN_M:\n            a = tl.load(input_ptrs, mask=mask_k, other=0.0)\n        else:\n            a = tl.load(input_ptrs, mask=mask_k & (offs_m[None, :] + m *\n                TILE_M < length), other=0.0)\n        if EVEN_N:\n            if EVEN_M:\n                b = tl.load(grad_output_ptrs)\n            else:\n                b = tl.load(grad_output_ptrs, mask=offs_m[:, None] + m *\n                    TILE_M < length, other=0.0)\n        elif EVEN_M:\n            b = tl.load(grad_output_ptrs, mask=mask_n)\n        else:\n            b = tl.load(grad_output_ptrs, mask=mask_n & (offs_m[:, None] + \n                m * TILE_M < length), other=0.0)\n        acc += tl.dot(a, b, out_dtype=out_dtype)\n        input_ptrs += TILE_M * stride_input_m\n        grad_output_ptrs += TILE_M * stride_grad_output_m\n    acc = acc\n    if DETERMINISTIC:\n        if M <= BLOCK_LENGTH:\n            c_ptrs = grad_other + stride_grad_other_k * offs_k[:, None\n                ] + stride_grad_other_n * offs_n[None, :]\n            if EVEN_N and EVEN_K:\n                tl.store(c_ptrs, acc)\n            else:\n                c_mask = mask_k & mask_n\n                tl.store(c_ptrs, acc, mask=c_mask)\n        else:\n            c_ptrs = (grad_other_tiles + next_id * stride_grad_other_b + \n                stride_grad_other_k * offs_k[:, None] + stride_grad_other_n *\n                offs_n[None, :])\n            if EVEN_N and EVEN_K:\n                tl.store(c_ptrs, acc)\n            else:\n                c_mask = mask_k & mask_n\n                tl.store(c_ptrs, acc, mask=c_mask)\n    else:\n        c_ptrs = grad_other + stride_grad_other_k * offs_k[:, None\n            ] + stride_grad_other_n * offs_n[None, :]\n        if M <= BLOCK_LENGTH:\n            if EVEN_N and EVEN_K:\n                tl.store(c_ptrs, acc)\n            else:\n                c_mask = mask_k & mask_n\n                tl.store(c_ptrs, acc, mask=c_mask)\n        elif EVEN_N and EVEN_K:\n            tl.atomic_add(c_ptrs, acc)\n        else:\n            c_mask = mask_k & mask_n\n            tl.atomic_add(c_ptrs, acc, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "42a21926-3b51-467d-a394-e2220c6cfbc0"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_destindex_copy_kv(K, Dest_loc, Out, stride_k_bs, stride_k_h,\n    stride_k_d, stride_o_bs, stride_o_h, stride_o_d, head_num, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_HEAD: 'tl.constexpr'):\n    cur_index = tl.program_id(0)\n    offs_h = tl.arange(0, BLOCK_HEAD)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    dest_index = tl.load(Dest_loc + cur_index)\n    k_ptrs = K + cur_index * stride_k_bs + stride_k_h * offs_h[:, None\n        ] + stride_k_d * offs_d[None, :]\n    o_ptrs = Out + dest_index * stride_o_bs + stride_o_h * offs_h[:, None\n        ] + stride_o_d * offs_d[None, :]\n    k = tl.load(k_ptrs, mask=offs_h[:, None] < head_num, other=0.0)\n    tl.store(o_ptrs, k, mask=offs_h[:, None] < head_num)\n    return\n",
    "category": "Data Movement",
    "subcategory": "copying",
    "uuid": "4725e829-f43d-4f32-91e1-d670aed8589c"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_bwd_dwdb(DW, DB, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'):\n    \"\"\"\n    Kernel invocation for backward pass of RMS normalization, computing and aggregating gradients w.r.t. weights and biases\n\n    Params:\n        - DW (tensor): Intermediate gradient tensor for the scale factors, W\n        - DB (tensor): Intermediate gradient tensor for the biases, B\n        - FINAL_DW (tensor): Aggregated gradient tensor for the scale factors, to be updated\n        - FINAL_DB (tensor): Aggregated gradient tensor for the biases, to be updated\n        - M (int): Number of groups or batch size dimension\n        - N (int): Dimensionality of the feature vectors or the number of features\n        - BLOCK_SIZE_M (constexpr): Compile-time constant defining the block size in the M dimension\n        - BLOCK_SIZE_N (constexpr): Compile-time constant defining the block size in the N dimension\n\n    Return:\n        - None\n\n    Usage:\n        _rms_norm_bwd_dwdb[grid, block](DW, DB, FINAL_DW, FINAL_DB, M, N, BLOCK_SIZE_M, BLOCK_SIZE_N)\n    \"\"\"\n    pid = tl.program_id(0)\n    cols = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dw = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    db = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for i in range(0, M, BLOCK_SIZE_M):\n        rows = i + tl.arange(0, BLOCK_SIZE_M)\n        mask = (rows[:, None] < M) & (cols[None, :] < N)\n        offs = rows[:, None] * N + cols[None, :]\n        dw += tl.load(DW + offs, mask=mask, other=0.0)\n        db += tl.load(DB + offs, mask=mask, other=0.0)\n    sum_dw = tl.sum(dw, axis=0)\n    sum_db = tl.sum(db, axis=0)\n    tl.store(FINAL_DW + cols, sum_dw, mask=cols < N)\n    tl.store(FINAL_DB + cols, sum_db, mask=cols < N)\n",
    "category": "Gradient Operations",
    "subcategory": "gradient accumulation",
    "uuid": "c6c6c196-0383-419b-824e-83c4a1aec627"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.jit\ndef _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean,\n    Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N,\n    eps, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HAS_RESIDUAL:\n    'tl.constexpr', STORE_RESIDUAL_OUT: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x_row\n    Y += row * stride_y_row\n    if HAS_RESIDUAL:\n        RESIDUAL += row * stride_res_row\n    if STORE_RESIDUAL_OUT:\n        RESIDUAL_OUT += row * stride_res_out_row\n    cols = tl.arange(0, BLOCK_N)\n    x = tl.load(X + cols, mask=cols < N, other=0.0)\n    if HAS_RESIDUAL:\n        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0)\n        x += residual\n    if STORE_RESIDUAL_OUT:\n        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\n    if not IS_RMS_NORM:\n        mean = tl.sum(x, axis=0) / N\n        tl.store(Mean + row, mean)\n        xbar = tl.where(cols < N, x - mean, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    else:\n        xbar = tl.where(cols < N, x, 0.0)\n        var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    mask = cols < N\n    w = tl.load(W + cols, mask=mask)\n    if HAS_BIAS:\n        b = tl.load(B + cols, mask=mask)\n    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    y = x_hat * w + b if HAS_BIAS else x_hat * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "0ea893c9-41ea-4826-baaa-d639f4176473"
  },
  {
    "input": "@triton.jit\ndef _bwd_q_kernel(Q1, K1, Q2, K2, V, sm_scale, DO, DQ1, DQ2, L, D,\n    stride_q1z, stride_q1h, stride_q1m, stride_q1k, stride_k1z, stride_k1h,\n    stride_k1n, stride_k1k, stride_q2z, stride_q2h, stride_q2m, stride_q2k,\n    stride_k2z, stride_k2h, stride_k2n, stride_k2k, stride_vz, stride_vh,\n    stride_vn, stride_vk, stride_doz, stride_doh, stride_dom, stride_dok,\n    stride_dq1z, stride_dq1h, stride_dq1m, stride_dq1k, stride_dq2z,\n    stride_dq2h, stride_dq2m, stride_dq2k, Z, H, M, N, P_SEQ, w:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', CAUSAL: 'tl.constexpr', LARGER_M:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    input_dtype = Q1.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q1 += off_z * stride_q1z + off_h * stride_q1h\n    Q2 += off_z * stride_q2z + off_h * stride_q2h\n    K1 += off_z * stride_k1z + off_h * stride_k1h\n    K2 += off_z * stride_k2z + off_h * stride_k2h\n    V += off_z * stride_vz + off_h * stride_vh\n    DO += off_z * stride_doz + off_h * stride_doh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    DQ1 += off_z * stride_dq1z + off_h * stride_dq1h\n    DQ2 += off_z * stride_dq2z + off_h * stride_dq2h\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_n_init = offs_n_base\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q1_ptrs = Q1 + (offs_m[:, None] * stride_q1m + offs_k[None, :] * stride_q1k\n        )\n    q2_ptrs = Q2 + (offs_m[:, None] * stride_q2m + offs_k[None, :] * stride_q2k\n        )\n    k1_ptrs = K1 + (offs_n_init[:, None] * stride_k1n + offs_k[None, :] *\n        stride_k1k)\n    k2_ptrs = K2 + (offs_n_init[:, None] * stride_k2n + offs_k[None, :] *\n        stride_k2k)\n    v_ptrs = V + (offs_n_init[:, None] * stride_vn + offs_k[None, :] *\n        stride_vk)\n    dq1_ptrs = DQ1 + (offs_m[:, None] * stride_dq1m + offs_k[None, :] *\n        stride_dq1k)\n    dq2_ptrs = DQ2 + (offs_m[:, None] * stride_dq2m + offs_k[None, :] *\n        stride_dq2k)\n    do_ptrs = DO + (offs_m[:, None] * stride_dom + offs_k[None, :] * stride_dok\n        )\n    d_ptrs = D + offs_m\n    l_ptrs = L + offs_m\n    if DIVISIBLE_M:\n        q1 = tl.load(q1_ptrs)\n        q2 = tl.load(q2_ptrs)\n        do = tl.load(do_ptrs)\n        delta = tl.load(d_ptrs)\n        l = tl.load(l_ptrs)\n    else:\n        mask_m = offs_m < M\n        q1 = tl.load(q1_ptrs, mask=mask_m[:, None])\n        q2 = tl.load(q2_ptrs, mask=mask_m[:, None])\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n        delta = tl.load(d_ptrs, mask=mask_m)\n        l = tl.load(l_ptrs, mask=mask_m)\n    dq1 = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    dq2 = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if CAUSAL:\n        hi = tl.minimum(N, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = N\n    for start_n in range(0, hi, BLOCK_N):\n        offs_n = start_n + offs_n_base\n        if DIVISIBLE_N:\n            v = tl.load(v_ptrs)\n            k1 = tl.load(k1_ptrs)\n            k2 = tl.load(k2_ptrs)\n        else:\n            mask_n = offs_n < N\n            v = tl.load(v_ptrs, mask=mask_n[:, None])\n            k1 = tl.load(k1_ptrs, mask=mask_n[:, None])\n            k2 = tl.load(k2_ptrs, mask=mask_n[:, None])\n        piecewise_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :] + w\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.where(piecewise_mask, tl.dot(q2, tl.trans(k2)), tl.dot(q1,\n            tl.trans(k1)))\n        p = tl.math.exp2(s * qk_scale - l[:, None] * log2e)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_N:\n            ds = tl.where(mask_n, ds, 0.0)\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            ds = tl.where(causal_mask, ds, 0.0)\n        ds2 = tl.where(piecewise_mask, ds, 0.0)\n        ds1 = tl.where(piecewise_mask, 0.0, ds)\n        dq1 += tl.dot(ds1, k1)\n        dq2 += tl.dot(ds2, k2)\n        k1_ptrs += BLOCK_N * stride_k1n\n        k2_ptrs += BLOCK_N * stride_k2n\n        v_ptrs += BLOCK_N * stride_vn\n    dq1 *= sm_scale\n    dq2 *= sm_scale\n    if DIVISIBLE_M:\n        tl.store(dq1_ptrs, dq1)\n        tl.store(dq2_ptrs, dq2)\n    else:\n        tl.store(dq1_ptrs, dq1, mask=mask_m[:, None])\n        tl.store(dq2_ptrs, dq2, mask=mask_m[:, None])\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "8dd7daab-15f3-4061-baad-8b38220eaf57"
  },
  {
    "input": "@triton.jit\ndef bwd_inner_dq(dq, qk_scale, bias_scale, DB_block_ptr, store_db, q,\n    kt_ptrs, k_stride, vt_ptrs, v_stride, B_block_ptr, do, Di, l_i,\n    seqlen_q, seqlen_k, head_dim, start_q, lo, hi, dropout_p, philox_seed,\n    batch_philox_offset, max_seqlen_k, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', FULL_BLOCKS:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr',\n    PADDED_HEAD: 'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    offs_q = start_q + tl.arange(0, BLOCK_M)\n    offs_k = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    ld_offs_d = None if not PADDED_HEAD else tl.arange(0, BLOCK_DMODEL)\n    kt_ptrs += lo * k_stride\n    vt_ptrs += lo * v_stride\n    if BIAS_TYPE == 1:\n        B_block_ptr = tl.advance(B_block_ptr, (0, lo))\n        DB_block_ptr = tl.advance(DB_block_ptr, (0, lo))\n    \"\"\"\n           K1   K2      (d)V      dO\n    Q1    qk11 qk12     (d)v1     dO1\n    Q2    qk21 qk22     (d)v2     dO2\n\n    QK: (seqlen_q, seqlen_k)\n    dO: (seqlen_q, hdim)\n    dV: (seqlen_k, hdim)\n    \"\"\"\n    for start_k in range(lo, hi, BLOCK_N):\n        offs_k_curr = offs_k[None, :] + start_k\n        if not FULL_BLOCKS:\n            kt = load_fn(kt_ptrs, ld_offs_d, offs_k + start_k, head_dim,\n                seqlen_k)\n        else:\n            kt = load_fn(kt_ptrs, ld_offs_d, None, head_dim, seqlen_k)\n        if not FULL_BLOCKS:\n            vt = load_fn(vt_ptrs, ld_offs_d, offs_k + start_k, head_dim,\n                seqlen_k)\n        else:\n            vt = load_fn(vt_ptrs, ld_offs_d, None, head_dim, seqlen_k)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += dot(BLOCK_M, BLOCK_DMODEL, BLOCK_DMODEL, q, kt)\n        if not FULL_BLOCKS:\n            k_boundary = tl.full((BLOCK_M,), seqlen_k, dtype=tl.int32)\n            mask = offs_k_curr < k_boundary[:, None]\n            qk = tl.where(mask, qk, float('-inf'))\n        if CAUSAL:\n            qk = tl.where(offs_q[:, None] >= offs_k_curr, qk, float('-inf'))\n        if BIAS_TYPE == 0:\n            pass\n        elif BIAS_TYPE == 1:\n            bias = tl.load(B_block_ptr, boundary_check=(0, 1),\n                padding_option='zero')\n            qk += bias * bias_scale\n        else:\n            tl.static_assert(False, f'Unsupported BIAS_TYPE {BIAS_TYPE}')\n        p = tl.math.exp2(qk_scale * qk - l_i[:, None])\n        if not FULL_BLOCKS or CAUSAL:\n            if qk_scale == 0.0:\n                p = tl.where(libdevice.isnan(p), 0.0, p)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += dot(BLOCK_M, BLOCK_DMODEL, BLOCK_DMODEL, do, vt)\n        if ENABLE_DROPOUT:\n            philox_offset = (batch_philox_offset + start_q * max_seqlen_k +\n                start_k)\n            keep = dropout_mask(philox_seed, philox_offset, dropout_p,\n                BLOCK_M, BLOCK_N, max_seqlen_k)\n            dp = tl.where(keep, dp / (1 - dropout_p), 0)\n        ds = p * (dp - Di[:, None])\n        if BLOCK_M == 1:\n            dq += tl.view(kt, [BLOCK_DMODEL]) * ds\n        else:\n            dq = tl.dot(ds, tl.trans(kt), acc=dq)\n        if BIAS_TYPE == 1:\n            if store_db:\n                tl.store(DB_block_ptr, ds, boundary_check=(0, 1))\n        kt_ptrs += BLOCK_N * k_stride\n        vt_ptrs += BLOCK_N * v_stride\n        if BIAS_TYPE == 1:\n            B_block_ptr = tl.advance(B_block_ptr, (0, BLOCK_N))\n            DB_block_ptr = tl.advance(DB_block_ptr, (0, BLOCK_N))\n    return dq\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "5945f12e-acaa-4e27-8408-798bfc308aea"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, L, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vn, stride_vk, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, Z_H_N_CTX, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', IS_CAUSAL:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    qvk_offset = off_hz * stride_qh\n    vk_offset = qvk_offset // stride_qm\n    K_block_ptr = tl.make_block_ptr(base=K, shape=(BLOCK_DMODEL, Z_H_N_CTX),\n        strides=(stride_kk, stride_kn), offsets=(0, vk_offset), block_shape\n        =(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_vn, stride_vk), offsets=(vk_offset, 0), block_shape\n        =(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    Q_ptrs = Q + qvk_offset + offs_m[:, None] * stride_qm + offs_k[None, :\n        ] * stride_qk\n    q = tl.load(Q_ptrs)\n    q = q * qk_scale\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if IS_CAUSAL:\n            qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n                float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n    O_block_ptr = tl.make_block_ptr(base=Out, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(vk_offset +\n        start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(\n        1, 0))\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "311e283d-329c-4b64-ade6-318bdfe2fe62"
  },
  {
    "input": "@triton.jit\ndef _paged_attn_v2_reduce_kernel(out_ptr, m_i_ptr, l_i_ptr, tmp_out_ptr,\n    context_lens_ptr, max_num_partitions, stride_o0, stride_o1, stride_o2,\n    HEAD_SIZE: 'tl.constexpr', QUERY_GROUP_SIZE: 'tl.constexpr',\n    NUM_KV_HEADS: 'tl.constexpr', PARTITION_SIZE: 'tl.constexpr',\n    NUM_PARTITIONS: 'tl.constexpr'):\n    seq_idx = tl.program_id(0)\n    kv_head_idx = tl.program_id(1)\n    context_len = tl.load(context_lens_ptr + seq_idx)\n    num_partitions = tl.cdiv(context_len, PARTITION_SIZE)\n    group_head_offset = tl.arange(0, QUERY_GROUP_SIZE)[:, None\n        ] * HEAD_SIZE + tl.arange(0, HEAD_SIZE)[None, :]\n    if num_partitions == 1:\n        tmp_out_offset = ((seq_idx * NUM_KV_HEADS + kv_head_idx) *\n            max_num_partitions * QUERY_GROUP_SIZE * HEAD_SIZE +\n            group_head_offset)\n        tmp_out = tl.load(tmp_out_ptr + tmp_out_offset)\n        out_offset = (seq_idx * stride_o0 + kv_head_idx * QUERY_GROUP_SIZE *\n            stride_o1 + group_head_offset * stride_o2)\n        tl.store(out_ptr + out_offset, tmp_out)\n        return\n    ml_offset = (seq_idx * NUM_KV_HEADS + kv_head_idx\n        ) * max_num_partitions * QUERY_GROUP_SIZE + tl.arange(0, NUM_PARTITIONS\n        )[:, None] * QUERY_GROUP_SIZE + tl.arange(0, QUERY_GROUP_SIZE)[None, :]\n    mask = tl.arange(0, NUM_PARTITIONS)[:, None] < num_partitions\n    m_i = tl.load(m_i_ptr + ml_offset, mask=mask, other=float('-inf'))\n    m = tl.max(m_i, axis=0)\n    l_i = tl.load(l_i_ptr + ml_offset, mask=mask, other=0.0)\n    l_i *= tl.exp(m_i - m[None, :])\n    l = tl.sum(l_i, axis=0)\n    r = l_i / l[None, :]\n    r = tl.reshape(r, (NUM_PARTITIONS, QUERY_GROUP_SIZE, 1))\n    tmp_out_offset = (seq_idx * NUM_KV_HEADS + kv_head_idx\n        ) * max_num_partitions * QUERY_GROUP_SIZE * HEAD_SIZE + tl.arange(0,\n        NUM_PARTITIONS)[:, None, None\n        ] * QUERY_GROUP_SIZE * HEAD_SIZE + tl.arange(0, QUERY_GROUP_SIZE)[\n        None, :, None] * HEAD_SIZE + tl.arange(0, HEAD_SIZE)[None, None, :]\n    tmp_out = tl.load(tmp_out_ptr + tmp_out_offset, mask=mask[:, :, None],\n        other=0.0)\n    out = tl.sum(tmp_out * r, axis=0)\n    out_offset = (seq_idx * stride_o0 + kv_head_idx * QUERY_GROUP_SIZE *\n        stride_o1 + group_head_offset * stride_o2)\n    tl.store(out_ptr + out_offset, out)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "bf598516-7e44-48bc-bfa4-01db477c6550"
  },
  {
    "input": "@triton.jit\ndef _parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk,\n    dv, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BTL:\n    'tl.constexpr', BTS: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', DK: 'tl.constexpr', DV: 'tl.constexpr'):\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v,\n        boundary_check=(0, 1))\n    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV],\n        dtype=tl.float32)\n    for i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n        if i_v == 0:\n            b_ds += b_dz[None, :] * scale\n        else:\n            b_ds = b_ds\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)\n    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t\n            ), (i_k * BK, i), (BK, BTS), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (DV, T), (s_vo_d,\n            s_vo_t), (i_v * BV, i), (BV, BTS), (0, 1))\n        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n        m_s = o_k[:, None] <= o_q[None, :]\n        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n        b_s2 = 1 + b_s + 0.5 * b_s * b_s\n        b_s = tl.where(m_s, b_s, 0)\n        b_s2 = tl.where(m_s, b_s2, 0)\n        b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n        if i_v == 0:\n            b_ds += b_dz[None, :]\n        else:\n            b_ds = b_ds\n        b_ds = tl.where(m_s, b_ds, 0) * scale\n        b_dv += tl.dot(b_s2, tl.trans(b_do), allow_tf32=False)\n        b_dk += tl.dot(b_ds + b_ds * b_s, tl.trans(b_q), allow_tf32=False)\n        o_q += BTS\n    p_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * s_qk_h, (T, DK), (\n        s_qk_t, s_qk_d), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f118ffe5-396c-49ee-ab8a-208de56ea916"
  },
  {
    "input": "@triton.jit\ndef mul_relu_block_back_kernel(x_ptr, y_ptr, dz_ptr, dx_ptr, N0, N1, B0:\n    'tl.constexpr', B1: 'tl.constexpr'):\n    block_id_i = tl.program_id(0)\n    block_id_j = tl.program_id(1)\n    off_i = block_id_i * B0 + tl.arange(0, B0)\n    off_j = block_id_j * B1 + tl.arange(0, B1)\n    off_ji = off_j[:, None] * N0 + off_i[None, :]\n    mask_i = off_i < N0\n    mask_j = off_j < N1\n    mask_ji = mask_j[:, None] & mask_i[None, :]\n    x = tl.load(x_ptr + off_ji, mask=mask_ji)\n    y = tl.load(y_ptr + off_j, mask=mask_j)\n    dz = tl.load(dz_ptr + off_ji, mask=mask_ji)\n    df = tl.where(x * y[:, None] > 0, 1.0, 0.0)\n    dxy_x = y[:, None]\n    dx = df * dxy_x * dz\n    tl.store(dx_ptr + off_ji, dx, mask=mask_ji)\n    return\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "7ab09d5f-74b5-4ff2-b615-248c0cc3acdf"
  },
  {
    "input": "@triton.jit\ndef sigmoid(input):\n    \"\"\"\n    Applies sigmoid to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by sigmoid.\n    \"\"\"\n    return 1 / (1 + tl.exp(-input))\n",
    "category": "Activation Functions",
    "subcategory": "sigmoid",
    "uuid": "5cdb4665-72e8-4ab2-b71e-3fadf4325dde"
  },
  {
    "input": "@triton.jit\ndef div_scalar_kernel(x_ptr, scalar, output_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    output = x / scalar\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "6d614b54-1176-4549-a464-3d34b3a3462d"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': b_n,\n    'BLOCK_SIZE_K': b_k}, num_warps=w) for b_n, b_k, w in itertools.product\n    ([(4 ** n) for n in range(6)], [(4 ** n) for n in range(4)], [2, 4, 8])\n    ], key=['N'])\n@triton.jit\ndef triton_sum_kernel_2D_result_dim_1_sum_then_buffer(input_ptr, output_ptr,\n    M: 'tl.constexpr', N: 'tl.constexpr', K: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"\n    Modification to triton_sum_kernel_2D_result_dim_1() which uses a buffer to store intermediate results,\n    enabling reducing over a large middle dimension for 3D input tensors\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\n    buffer = tl.zeros((1, BLOCK_SIZE_K), dtype=tl.float32)\n    block_start_k = pid_k * BLOCK_SIZE_K\n    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\n    mask_k = offsets_k < K\n    for block_start_n in range(0, N, BLOCK_SIZE_N):\n        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n        mask_n = offsets_n < N\n        idxs_base = offsets_n[:, None] * K + offsets_k\n        idxs = idxs_base + pid_m * N * K\n        mask = mask_n[:, None] & mask_k\n        input = tl.load(input_ptr + idxs, mask=mask, other=0)\n        buffer += tl.sum(input, axis=0)\n    buffer_view = buffer.reshape((BLOCK_SIZE_K,))\n    output_offsets = pid_m * K + offsets_k\n    tl.store(output_ptr + output_offsets, buffer_view, mask=mask_k)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "4b376b9a-6d01-4c8a-844b-f35baf064ce3"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_kernel(X, Y, W, B, Mean, Rstd, stride, N, eps, BLOCK_SIZE:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * stride\n    X += row * stride\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(X + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols, mask=cols < N, other=0.0)\n        x = tl.where(cols < N, x - mean, 0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        b = tl.load(B + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = (x - mean) * rstd\n        y = x_hat * w + b\n        tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "980ec0ec-d9fc-431f-97ca-79a61ef14f6e"
  },
  {
    "input": "@triton.jit\ndef parallel_retention_bwd_kernel_dkv(i_bh, i_t, i_k, i_v, i_h, q, k, v, do,\n    dk, dv, scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr',\n    K: 'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr'):\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    d_b = tl.math.exp2(b_b * BS)\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, \n        i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, \n        i_v * BV), (BT, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    NTS = tl.cdiv(T, BS)\n    d_h = tl.math.exp2((BT - tl.arange(0, BT)) * b_b)\n    b_kd = b_k * d_h[:, None]\n    d_q = tl.math.exp2(tl.arange(0, BS) * b_b)\n    for i in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i, i_k *\n            BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i, i_v *\n            BV), (BS, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * d_q[:, None]\n        b_dk *= d_b\n        b_dv *= d_b\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n        b_s = tl.dot(b_kd, tl.trans(b_q), allow_tf32=False)\n        b_dk += tl.dot(b_ds, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, b_do, allow_tf32=False)\n    b_dk *= d_h[:, None] * scale\n    b_dv *= scale\n    tl.debug_barrier()\n    o_q, o_k = tl.arange(0, BS), tl.arange(0, BT)\n    for i in range(i_t * BT, (i_t + 1) * BT, BS):\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i, i_k *\n            BK), (BS, BK), (1, 0))\n        p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i, i_v *\n            BV), (BS, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        m_s = o_k[:, None] <= o_q[None, :]\n        d_s = tl.where(m_s, tl.math.exp2((-o_k[:, None] + o_q[None, :]) *\n            b_b), 0) * scale\n        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False) * d_s\n        b_s = tl.dot(b_k, tl.trans(b_q), allow_tf32=False) * d_s\n        b_dk += tl.dot(b_ds, b_q, allow_tf32=False)\n        b_dv += tl.dot(b_s, b_do, allow_tf32=False)\n        o_q += BS\n    p_dk = tl.make_block_ptr(dk + (i_v * B * H + i_bh) * T * K, (T, K), (K,\n        1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_k * B * H + i_bh) * T * V, (T, V), (V,\n        1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "c69d06e0-fd02-4a62-9005-ea313837034c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64,\n    'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128},\n    num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_stable_bwd_slow_kernel(x_ptr, dout_ptr, dt_ptr,\n    dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen,\n    nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk,\n    stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head,\n    stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen +\n        pid_h * stride_x_head)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + pid_h * stride_dA_cs_head)\n    cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h //\n        nheads_ngroups_ratio * stride_cb_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m *\n        stride_ddA_cs_csize_m)\n    start = (chunk_size - 1 // BLOCK_SIZE_N) * BLOCK_SIZE_N\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + ((start + offs_n[None, :]) * stride_x_seqlen + offs_k[\n        :, None] * stride_x_hdim)\n    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + (start +\n        offs_n[None, :]) * stride_cb_csize_n)\n    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n\n    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_k[None, :] < hdim), other=0.0)\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=\n        offs_m < chunk_size, other=0.0)\n    lo, hi = start, pid_m * BLOCK_SIZE_M // BLOCK_SIZE_N * BLOCK_SIZE_N - 1\n    for start_n in range(lo, hi, -BLOCK_SIZE_N):\n        tl.multiple_of(start_n, BLOCK_SIZE_N)\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit - start_n), other=0.0)\n        acc = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= dt_n\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n\n            [None, :] < chunk_size - start_n), other=0.0)\n        acc *= cb\n        dA_cs_n = tl.load(dA_cumsum_ptr + (start_n + offs_n) *\n            stride_dA_cs_csize, mask=offs_n < chunk_size - start_n, other=0.0)\n        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        mask = offs_m[:, None] <= start_n + offs_n[None, :] - 1\n        acc = tl.where(mask, acc, 0.0)\n        rowsum_new = rowsum + tl.sum(acc, axis=1)\n        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n        rowsum = rowsum_new\n        acc = tl.where(mask, acc, 0.0)\n        ddA_cs = tl.sum(acc, axis=0)\n        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=(offs_n <\n            chunk_size - start_n) & (offs_n >= 1 - start_n))\n        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n    for start_n in range(hi, -1, -BLOCK_SIZE_N):\n        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N\n            ,), dtype=tl.float32), mask=offs_n >= 1 - start_n)\n        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "2c4030f5-ff78-4e88-915e-7f7b8f0a506d"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_K(q, k, z, h, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_A = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\n        b_A += tl.dot(b_q, b_k, allow_tf32=False)\n    p_z = tl.make_block_ptr(z + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    p_zp = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_p * V +\n        i_v * BV,), (BV,), (0,))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_o = b_o * tl.exp(b_zp[None, :] - b_z)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_A = tl.where(m_s, b_A, 0.0)\n    if i_v == 0:\n        tl.store(p_A, b_A, boundary_check=(0, 1))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "b6148716-9cfa-4954-8dc0-9251d13698a4"
  },
  {
    "input": "@triton.jit\ndef barrier_test_kernel(signal_pad_ptrs, RANK: 'tl.constexpr', WORLD_SIZE:\n    'tl.constexpr'):\n    blockwise_barrier(signal_pad_ptrs, None, RANK, WORLD_SIZE)\n",
    "category": "Memory Management",
    "subcategory": "memory alignment",
    "uuid": "3665d492-1c9a-4c21-ae4c-c04eda947da5"
  },
  {
    "input": "@triton.jit\ndef _fwd_rms_kernel(out_ptr_base, stride_out_row, in_ptr_base, stride_x_row,\n    stride_x_col, weight_ptr, num_rows: 'tl.constexpr', num_cols:\n    'tl.constexpr', block_size: 'tl.constexpr'):\n    row_index = tl.program_id(0)\n    in_ptr_row = in_ptr_base + row_index * stride_x_row\n    out_ptr_row = out_ptr_base + row_index * stride_out_row\n    in_block_ptr = tl.make_block_ptr(base=in_ptr_base, shape=(num_rows,\n        num_cols), strides=(stride_x_row, stride_x_col), offsets=(row_index,\n        0), block_shape=(1, block_size), order=(1, 0))\n    variance_row = 0.0\n    eps = 1e-08\n    test = tl.zeros([5], dtype=tl.float32)\n    summer = 0.0\n    variance = 0.0\n    for col_index in range(0, block_size // num_cols):\n        col_block = tl.load(in_block_ptr[0], boundary_check=(0, 1))\n        variance += tl.sum(col_block * col_block, axis=0)\n        in_block_ptr = tl.advance(in_block_ptr, (0, block_size))\n        col_offsets = col_index + tl.arange(0, block_size)\n        col_mask = col_offsets < num_cols\n        col_block2 = tl.load(in_ptr_row + col_offsets, mask=col_mask, other=0.0\n            )\n        variance_row += tl.sum(col_block2 * col_block2, axis=0)\n    tl.device_print('summer, variance: ', summer, variance)\n    variance /= num_cols\n    rstdev = 1 / tl.sqrt(variance + eps)\n    for start_col in range(0, num_cols, block_size):\n        col_offsets = start_col + tl.arange(0, block_size)\n        col_mask = col_offsets < num_cols\n        weights = tl.load(weight_ptr + col_offsets, mask=col_mask)\n        in_block = tl.load(in_ptr_row + col_offsets, mask=col_mask, other=\n            0.0, eviction_policy='evict_first')\n        col_block_rms = in_block * rstdev\n        out = weights * col_block_rms\n        tl.store(out_ptr_row + col_offsets, out, mask=col_mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "f861369b-36e4-421b-ac86-5897603d8b48"
  },
  {
    "input": "@triton.jit\ndef _sample_3d(image, w, batch_index, ix, iy, iz, ID: 'tl.constexpr', IH:\n    'tl.constexpr', IW: 'tl.constexpr', C: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr'):\n    channel_bcast = tl.full((1, C), 1.0, dtype=tl.float32)\n    Coffs = tl.arange(0, C)\n    ix_ = tl.minimum(tl.maximum(ix, 0.0), IW - 1)\n    iy_ = tl.minimum(tl.maximum(iy, 0.0), IH - 1)\n    iz_ = tl.minimum(tl.maximum(iz, 0.0), ID - 1)\n    val = tl.view(tl.load((image + batch_index * ID * IW * IH * C + iz_ *\n        IW * IH * C + iy_ * IW * C + ix_ * C)[:, None] + Coffs[None, :]), (\n        BLOCK_SIZE, C))\n    return val * tl.view((w * ((iy >= 0) * (iy < IH) * (ix >= 0) * (ix < IW\n        ) * (iz < ID) * (iz >= 0)))[:, None] * channel_bcast, (BLOCK_SIZE, C))\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "b86ffa0d-bc2e-4cee-a6e3-02a095598bb9"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N'])\n@triton.jit\ndef _rms_norm_fwd_kernel(X, stride_x, Y, stride_y, W, Rstd, eps, M, N,\n    block_N: 'tl.constexpr'):\n    row = tl.program_id(0)\n    cols = tl.arange(0, block_N)\n    mask = cols < N\n    x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0)\n    w = tl.load(W + cols, mask=mask, other=0.0)\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Rstd + row, rstd)\n    x_hat = x * rstd\n    y = x_hat * w\n    tl.store(Y + row * stride_y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "91fdc1b5-54a2-4af2-82bc-c6e8d0c9736b"
  },
  {
    "input": "@triton.jit\ndef nop_kernel():\n    pass\n",
    "category": "Helper Functions",
    "subcategory": "",
    "uuid": "d6298f4e-2ff4-4952-a324-ec56ea340b63"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_preprocess(O, DO, Delta, Z, H, N_CTX, BLOCK_M: 'tl.constexpr',\n    HEAD_DIM: 'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_hz = tl.program_id(1)\n    off_n = tl.arange(0, HEAD_DIM)\n    o = tl.load(O + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    do = tl.load(DO + off_hz * HEAD_DIM * N_CTX + off_m[:, None] * HEAD_DIM +\n        off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hz * N_CTX + off_m, delta)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "907267e2-49a6-4572-b9b8-bb37e99b474d"
  },
  {
    "input": "@triton.jit\ndef max_fn(x, y):\n    return tl.math.max(x, y)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "d7ab03c9-4a27-47c8-a9d9-0450173c293a"
  },
  {
    "input": "@triton.jit\ndef fw_kernel(feature_grid, feature_grid_sizes, directions, origins,\n    grid_idx, near, far, splatting_feature, mask, num_samples:\n    'tl.constexpr', num_samples_inf: 'tl.constexpr', num_rays:\n    'tl.constexpr', grid_channel: 'tl.constexpr', NUM_GRIDS: 'tl.constexpr',\n    feature_channel: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr',\n    mask_out_of_bounds_samples: 'tl.constexpr', contract_coords:\n    'tl.constexpr', disparity_at_inf: 'tl.constexpr'):\n    (tot_num_samples, pid, offs, offs_mask, offs_features,\n        offs_features_mask, center_x, center_y, center_z, ray_x, ray_y,\n        ray_z, near_buffer, far_buffer, grid_idx_buffer,\n        sample_index_buffer, feature_buffer, mask_buffer) = (fwbw_splatter_init\n        (directions, origins, grid_idx, near, far, splatting_feature, mask,\n        num_samples, num_samples_inf, num_rays, grid_channel,\n        feature_channel, BLOCK_SIZE))\n    feature_buffer = feature_buffer * mask_buffer\n    for step in range(tot_num_samples):\n        if step < num_samples:\n            depth = depth_lin(near_buffer, far_buffer, num_samples, step)\n        else:\n            depth = depth_inv_sphere(far_buffer, disparity_at_inf,\n                num_samples_inf, step - num_samples)\n        sample_x = center_x + depth * ray_x\n        sample_y = center_y + depth * ray_y\n        sample_z = center_z + depth * ray_z\n        if contract_coords:\n            sample_x, sample_y, sample_z = contract_pi(sample_x, sample_y,\n                sample_z)\n        splat_grid_rep(feature_buffer, feature_grid, feature_grid_sizes,\n            grid_idx_buffer, sample_x, sample_y, sample_z, grid_channel,\n            NUM_GRIDS, BLOCK_SIZE, mask_out_of_bounds_samples)\n",
    "category": "Kernel Operations",
    "subcategory": "kernel configuration",
    "uuid": "4079de6a-091d-4ba1-9ae7-c8060d5b966e"
  },
  {
    "input": "@triton.jit\ndef zeroth_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "afb325b5-cb70-4b08-9fa5-c21ebf70d906"
  },
  {
    "input": "@triton.jit\ndef _mixed_mm_kernel(A, B, scales_ptr, zeros_ptr, C, M, N, K, stride_am,\n    stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scale_k,\n    stride_scale_n, IS_BFLOAT16: 'tl.constexpr', QGROUP_SIZE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr', EVEN_K:\n    'tl.constexpr', TRANSPOSED: 'tl.constexpr'=False, GROUP_M:\n    'tl.constexpr'=8, acc_dtype: 'tl.constexpr'=tl.float32, input_precision:\n    'tl.constexpr'='ieee', fp8_fast_accum: 'tl.constexpr'=False, DEBUG:\n    'tl.constexpr'=False):\n    \"\"\"Mixed matmul kernel\n\n    A has shape (M, K) and is float16, bfloat16, or float32\n\n    B is i4 / s4 and has shape (K // 2, N) and is packed as uint8 / int8. See `packed_2xint4` for details.\n\n    Scales and zeros are of shape (NUM_GROUPS, N) and are same dtype as A, where NUM_GROUPS = (K // QGROUP_SIZE)\n    QGROUP_SIZE should be a multiple of BLOCK_K such that a vector of scales / zeros is loaded and broadcasted to block shape\n    per mainloop iteration.\n\n    In the transposed case, A is M x N and B is K x N, and we reduce along \"N\":\n    - TLDR: we are loading rows of A and B blocks at a time, dequantizing and transposing each block of B to achieve the overall\n    effect of a transposed matmul. This is necessary to perform a transposed matmul without unpacking and repacking the B matrix.\n        - Indexing remains the same for A (the reduction dim (BLK_K / K) corresponds to axis 1 of A -- \"N\" above)\n            - We load a BLK_M x BLK_K block of A\n        - Indexing for B is now flipped: N <-> K\n            - We load BLK_N x BLK_K block of B (remembering that the reduction dimension is axis 1 of B)\n            - We dequantize and transpose to BLK_K x BLK_N\n            - scale / zero indexing also change, since we are now iterating along the non-grouping dim within the mac loop and along\n            the grouping dim across blocks.\n        - Each mac loop calculates BLK_M x BLK_N -> M x \"N\"(= K)\n        - Within the mac loop for each block, we iterate along axis=1 for **both** A and B since axis = 1 is now the reduction dim for B.\n\n    NOTE: Assumes that the quantization grouping was done along the K dimension originally (i.e., QGROUP_SIZE consecutive elements\n    of original weight matrix in the K dimension were grouped together when calculating min / max scaling factors).\n    \"\"\"\n    if not TRANSPOSED:\n        tl.static_assert(QGROUP_SIZE % BLOCK_K == 0)\n    else:\n        tl.static_assert(QGROUP_SIZE % BLOCK_N == 0)\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    if not DEBUG:\n        ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    else:\n        ram = rm\n    rak = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    if not TRANSPOSED:\n        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n        if not DEBUG:\n            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n        else:\n            rbn = rn\n        rbk = pid_z * BLOCK_K // 2 + tl.arange(0, BLOCK_K // 2)\n    else:\n        rn = (pid_n * BLOCK_N // 2 + tl.arange(0, BLOCK_N // 2)) % N\n        if not DEBUG:\n            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N // 2), \n                BLOCK_N // 2)\n        else:\n            rbn = rn\n        rbk = rak\n    A = A + (ram[:, None] * stride_am + rak[None, :] * stride_ak)\n    if not TRANSPOSED:\n        B = B + (rbk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    else:\n        B = B + (rbn[:, None] * stride_bk + rbk[None, :] * stride_bn)\n    if not TRANSPOSED:\n        offsets_scale_n = pid_n * stride_scale_n * BLOCK_N + tl.arange(0,\n            BLOCK_N) * stride_scale_n\n    else:\n        scale_offset_k = pid_n * BLOCK_N * stride_scale_k // QGROUP_SIZE\n        offsets_scale_n = tl.arange(0, BLOCK_K) * stride_scale_n\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        if EVEN_K:\n            a = tl.load(A)\n            qb = tl.load(B)\n        else:\n            k_remaining_a = K - k * (BLOCK_K * SPLIT_K)\n            if not TRANSPOSED:\n                k_remaining_b = K - k * (BLOCK_K * SPLIT_K) // 2\n            else:\n                k_remaining_b = K - k * (BLOCK_K * SPLIT_K)\n            _0 = tl.zeros((1, 1), dtype=C.dtype.element_ty)\n            a = tl.load(A, mask=rak[None, :] < k_remaining_a, other=_0)\n            qb = tl.load(B, mask=rbk[:, None] < k_remaining_b, other=_0)\n        if not TRANSPOSED:\n            scale_offset_k = (k * BLOCK_K * SPLIT_K * stride_scale_k //\n                QGROUP_SIZE)\n        else:\n            offsets_scale_n = k * stride_scale_n * BLOCK_K + tl.arange(0,\n                BLOCK_K) * stride_scale_n\n        scales = tl.load(scales_ptr + offsets_scale_n + scale_offset_k)\n        zeros = tl.load(zeros_ptr + offsets_scale_n + scale_offset_k)\n        _4_i8 = tl.full((1,), 4, dtype=tl.int8)\n        qb_lo = qb << _4_i8 >> _4_i8\n        qb_hi = qb >> _4_i8\n        if IS_BFLOAT16:\n            dq_b = tl.join(qb_lo.to(tl.float16), qb_hi.to(tl.float16)).permute(\n                0, 2, 1)\n        else:\n            dq_b = tl.join(qb_lo, qb_hi).permute(0, 2, 1)\n        if not TRANSPOSED:\n            dq_b = dq_b.reshape(BLOCK_K, BLOCK_N)\n        else:\n            dq_b = dq_b.reshape(BLOCK_N, BLOCK_K)\n        zeros = zeros[None, :]\n        scales = scales[None, :]\n        dq_b = (dq_b - zeros) * scales\n        if TRANSPOSED:\n            dq_b = tl.trans(dq_b)\n        if fp8_fast_accum:\n            acc = tl.dot(a, dq_b, acc, out_dtype=acc_dtype, input_precision\n                =input_precision)\n        else:\n            acc += tl.dot(a, dq_b, out_dtype=acc_dtype, input_precision=\n                input_precision)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        if not TRANSPOSED:\n            B += BLOCK_K * SPLIT_K * stride_bk // 2\n        else:\n            B += BLOCK_K * SPLIT_K * stride_bn\n    acc = acc\n    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d0d4ad56-338d-471a-90a8-57afe21bc623"
  },
  {
    "input": "@triton.jit\ndef layernorm_forward(Y, Y_row_stride, X, X_row_stride, W, b, r, mu, n_cols,\n    eps, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx\n    mu += row_idx\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    mean_X = tl.sum(X_row, axis=0) / n_cols\n    XX = X_row - mean_X\n    row_var = tl.sum(XX * XX, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    tl.store(mu, mean_X)\n    output = XX * inv_var * W_row + b_row\n    tl.store(Y + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "0de1ac1e-90fb-4091-9608-8e1046d6c506"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.jit\ndef chunk_retention_bwd_kernel_dqkv(q, k, v, h, do, dh, dq, dk, dv, scale,\n    H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V:\n    'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    n_bh = tl.num_programs(2)\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((min(BT, T - i_t *\n        BT) - o_i - 1) * b_b)\n    d_q = d_q * scale\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0\n        ) * scale\n    if HEAD_FIRST:\n        p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK,\n            i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT,\n            i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T), (1, \n            H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_k = tl.make_block_ptr(k + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_dk = tl.zeros([BT, BK], dtype=tl.float32)\n    b_ds = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t *\n                BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * T * V, (T,\n                V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_b * H) * T * V + \n                i_h * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV),\n                (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * NT * K * V, (V, NT * K), (1, V),\n            (i_v * BV, i_t * K + i_k * BK), (BV, BK), (0, 1))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V, (NT * K, V), (V, 1\n            ), (i_t * K + i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dh = tl.load(p_dh, boundary_check=(0, 1))\n        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n        b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(b_s,\n            b_do, allow_tf32=False)\n        tl.store(p_dv, b_dv, boundary_check=(0, 1))\n    b_ds = b_ds * d_s\n    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)\n    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))\n    if HEAD_FIRST:\n        p_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t *\n            BT, i_k * BK), (BT, BK), (1, 0))\n    else:\n        p_dq = tl.make_block_ptr(dq + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dk = tl.make_block_ptr(dk + i_b * T * H * K + i_h * K, (T, K), (H *\n            K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dq, b_dq, boundary_check=(0, 1))\n    tl.store(p_dk, b_dk, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "276b6dfa-1c11-43fb-9cfe-4d7c7f831a9a"
  },
  {
    "input": "@triton.jit\ndef leaky_relu(input, negative_slope):\n    \"\"\"\n    Applies leaky ReLU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n        negative_slope: Slope of the negative component.\n\n    Returns:\n        Input transformed by leaky ReLU.\n    \"\"\"\n    return relu(input) + negative_slope * tl.minimum(0, input)\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "9c8e6f2c-a760-4cf0-a73e-9954765ff8b2"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BT', 'BK', 'BV', 'USE_G', 'USE_GK', 'USE_GV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_bwd_kernel_dh(q, g, gk, gv, do, dh, dht, dh0, scale, T:\n    'tl.constexpr', HQ: 'tl.constexpr', H: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr', NG:\n    'tl.constexpr', USE_G: 'tl.constexpr', USE_GK: 'tl.constexpr', USE_GV:\n    'tl.constexpr', STORE_INITIAL_STATE_GRADIENT: 'tl.constexpr',\n    USE_FINAL_STATE_GRADIENT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_bg = i_bh // NG\n    i_b, i_h = i_bh // H, i_bh % H\n    i_hg = i_h // NG\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        last_idx = min(i_t * BT + BT, T) - 1\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * HQ * K + i_h * K, (K, T),\n                (1, HQ * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_b * T * HQ * V + i_h * V, (T, V\n                ), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        if USE_G:\n            if HEAD_FIRST:\n                p_g = g + i_bg * T + i_t * BT + tl.arange(0, BT)\n                p_g = tl.max_contiguous(tl.multiple_of(p_g, BT), BT)\n                b_g_last = tl.load(g + i_bg * T + last_idx)\n            else:\n                p_g = g + i_b * T * H + (i_t * BT + tl.arange(0, BT)\n                    ) * H + i_hg\n                b_g_last = tl.load(g + i_b * T * H + last_idx * H + i_hg)\n            b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n            b_q = b_q * tl.exp(b_g)[None, :]\n            b_dh *= tl.exp(b_g_last)\n        if USE_GK:\n            if HEAD_FIRST:\n                p_gk = tl.make_block_ptr(gk + i_bg * T * K, (K, T), (1, K),\n                    (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_bg * T * K + last_idx * K + i_k * BK +\n                    tl.arange(0, BK))\n            else:\n                p_gk = tl.make_block_ptr(gk + i_b * T * H * K + i_hg * K, (\n                    K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n                p_gk_last = (gk + i_b * T * H * K + last_idx * H * K + i_hg *\n                    K + i_k * BK + tl.arange(0, BK))\n            p_gk_last = tl.max_contiguous(tl.multiple_of(p_gk_last, BK), BK)\n            b_gk = tl.load(p_gk, boundary_check=(0, 1))\n            b_q = b_q * tl.exp(b_gk)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) <\n                K, other=0.0)\n            b_dh *= tl.exp(b_gk_last)[:, None]\n        if USE_GV:\n            if HEAD_FIRST:\n                p_gv = tl.make_block_ptr(gv + i_bg * T * V, (T, V), (V, 1),\n                    (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_bg * T * V + last_idx * V + i_v * BV +\n                    tl.arange(0, BV))\n            else:\n                p_gv = tl.make_block_ptr(gv + i_b * T * H * V + i_hg * V, (\n                    T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n                p_gv_last = (gv + i_b * T * H * V + last_idx * H * V + i_hg *\n                    V + i_v * BV + tl.arange(0, BV))\n            p_gv_last = tl.max_contiguous(tl.multiple_of(p_gv_last, BV), BV)\n            b_gv = tl.load(p_gv, boundary_check=(0, 1))\n            b_do = b_do * tl.exp(b_gv)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) <\n                V, other=0.0)\n            b_dh *= tl.exp(b_gv_last)[None, :]\n        b_dh += tl.dot(b_q, b_do)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "7ca422aa-c517-462e-999e-de3de9917099"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', STAGE:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(\n        BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n        ), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    q = tl.load(Q_block_ptr)\n    if STAGE & 1:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL, BLOCK_N,\n            1, offs_m, offs_n)\n    tl.debug_barrier()\n    if STAGE & 2:\n        acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, start_m, qk_scale, BLOCK_M, BLOCK_DMODEL, BLOCK_N,\n            2, offs_m, offs_n)\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\n    m_ptrs = M + off_hz * N_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e5d92489-fe13-4689-8165-d7e827b4c1d3"
  },
  {
    "input": "@triton.jit\ndef fourth_order_bwd(coord_ptr: 'tl.tensor', coord_grad_ptr: 'tl.tensor',\n    sph_grad_ptr: 'tl.tensor', block_size: 'tl.constexpr', coord_numel:\n    'tl.constexpr', output_numel: 'tl.constexpr', col_offset:\n    'tl.constexpr', output_stride: 'tl.constexpr'):\n    block_id = tl.program_id(0)\n    coord_stride = 3\n    coord_striding = tl.arange(0, block_size) * coord_stride\n    coord_row_offset = coord_striding + block_size * coord_stride * block_id\n    x = tl.load(coord_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    y = tl.load(coord_ptr + coord_row_offset + 1, mask=coord_row_offset + 1 <\n        coord_numel)\n    z = tl.load(coord_ptr + coord_row_offset + 2, mask=coord_row_offset + 2 <\n        coord_numel)\n    output_striding = tl.arange(0, block_size) * output_stride\n    output_row_offset = (output_striding + block_size * output_stride *\n        block_id + col_offset)\n    g_0 = tl.load(sph_grad_ptr + output_row_offset, mask=output_row_offset <\n        output_numel)\n    g_1 = tl.load(sph_grad_ptr + output_row_offset + 1, mask=\n        output_row_offset + 1 < output_numel)\n    g_2 = tl.load(sph_grad_ptr + output_row_offset + 2, mask=\n        output_row_offset + 2 < output_numel)\n    g_3 = tl.load(sph_grad_ptr + output_row_offset + 3, mask=\n        output_row_offset + 3 < output_numel)\n    g_4 = tl.load(sph_grad_ptr + output_row_offset + 4, mask=\n        output_row_offset + 4 < output_numel)\n    g_5 = tl.load(sph_grad_ptr + output_row_offset + 5, mask=\n        output_row_offset + 5 < output_numel)\n    g_6 = tl.load(sph_grad_ptr + output_row_offset + 6, mask=\n        output_row_offset + 6 < output_numel)\n    g_7 = tl.load(sph_grad_ptr + output_row_offset + 7, mask=\n        output_row_offset + 7 < output_numel)\n    g_8 = tl.load(sph_grad_ptr + output_row_offset + 8, mask=\n        output_row_offset + 8 < output_numel)\n    CONST000 = 2.0\n    CONST001 = 4.5\n    CONST002 = 2.25\n    CONST006 = 9.48683298050514\n    CONST008 = 12.0\n    CONST012 = 28.4604989415154\n    CONST014 = 40.2492235949962\n    CONST015 = -37.6497011940334\n    CONST016 = -6.70820393249937\n    CONST017 = -26.6223590239483\n    CONST018 = -21.3453742061366\n    CONST019 = -20.1246117974981\n    CONST020 = -18.8248505970167\n    CONST021 = -18.0\n    CONST022 = -14.2302494707577\n    CONST023 = -10.0623058987491\n    CONST024 = -9.0\n    CONST025 = -8.87411967464942\n    CONST026 = -7.11512473537885\n    CONST027 = -6.27495019900557\n    CONST028 = -3.35410196624968\n    VAR07 = x * x * x\n    VAR08 = x * x\n    VAR16 = y * y * y\n    VAR17 = y * y\n    VAR25 = z * z * z\n    VAR26 = z * z\n    g_x = tl.load(coord_grad_ptr + coord_row_offset, mask=coord_row_offset <\n        coord_numel)\n    g_y = tl.load(coord_grad_ptr + coord_row_offset + 1, mask=\n        coord_row_offset + 1 < coord_numel)\n    g_z = tl.load(coord_grad_ptr + coord_row_offset + 2, mask=\n        coord_row_offset + 2 < coord_numel)\n    g_x += CONST015 * g_7 * x * y * z + CONST022 * g_5 * x * y * z + g_0 * (\n        CONST017 * VAR08 * z - CONST025 * VAR25) + g_1 * y * (CONST020 *\n        VAR08 - CONST020 * VAR26) + g_2 * (-CONST019 * VAR17 * z + CONST023 *\n        VAR08 * z + CONST028 * VAR25) + g_3 * (CONST006 * VAR16 + CONST018 *\n        VAR08 * y + CONST026 * VAR26 * y) + g_4 * (CONST000 * x * (CONST002 *\n        VAR26 + CONST024 * VAR17) + CONST001 * VAR07) + g_6 * (-CONST016 *\n        VAR07 + CONST019 * VAR17 * x) + g_8 * (CONST017 * VAR26 * x - \n        CONST025 * VAR07)\n    g_y += CONST000 * g_6 * y * (CONST023 * VAR08 - CONST023 * VAR26\n        ) + CONST014 * g_2 * x * y * z + g_1 * (-CONST020 * VAR26 * x + \n        CONST027 * VAR07) + g_3 * (CONST026 * VAR07 + x * (CONST012 * VAR17 +\n        CONST026 * VAR26)) + g_4 * (CONST008 * VAR16 + CONST021 * VAR08 * y +\n        CONST021 * VAR26 * y) + g_5 * (CONST026 * VAR25 + z * (CONST012 *\n        VAR17 + CONST026 * VAR08)) + g_7 * (CONST020 * VAR08 * z - CONST027 *\n        VAR25)\n    g_z += -CONST015 * g_1 * x * y * z + CONST022 * g_3 * x * y * z + g_0 * (\n        -CONST017 * VAR26 * x + CONST025 * VAR07) + g_2 * (CONST028 * VAR07 +\n        x * (-CONST019 * VAR17 + CONST023 * VAR26)) + g_4 * (CONST001 *\n        VAR08 * z + CONST001 * VAR25 + CONST021 * VAR17 * z) + g_5 * (\n        CONST006 * VAR16 + CONST018 * VAR26 * y + CONST026 * VAR08 * y\n        ) + g_6 * (CONST016 * VAR25 - CONST019 * VAR17 * z) + g_7 * y * (\n        CONST020 * VAR08 - CONST020 * VAR26) + g_8 * (CONST017 * VAR08 * z -\n        CONST025 * VAR25)\n    tl.store(coord_grad_ptr + coord_row_offset, g_x, mask=coord_row_offset <\n        coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 1, g_y, mask=\n        coord_row_offset + 1 < coord_numel)\n    tl.store(coord_grad_ptr + coord_row_offset + 2, g_z, mask=\n        coord_row_offset + 2 < coord_numel)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "816deb68-269e-4afa-99c4-ecd25ab3fa99"
  },
  {
    "input": "@triton.jit\ndef mish(input):\n    \"\"\"\n    Applies Mish to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by Mish.\n    \"\"\"\n    return input * tanh(tl.log(1 + tl.exp(input)))\n",
    "category": "Activation Functions",
    "subcategory": "softplus",
    "uuid": "82a17016-27eb-4256-a670-c430dad7b0da"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c02f65c5-28f3-47eb-a836-25ef7e0505b9"
  },
  {
    "input": "@triton.jit\ndef _bwd_q_kernel(Q, K, V, B, sm_scale, DO, DQ, L, D, stride_qz, stride_qh,\n    stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk,\n    stride_vz, stride_vh, stride_vn, stride_vk, stride_bz, stride_bh,\n    stride_bm, stride_bn, stride_doz, stride_doh, stride_dom, stride_dok,\n    stride_dqz, stride_dqh, stride_dqm, stride_dqk, Z, H, M, N, P_SEQ,\n    BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', LARGER_M: 'tl.constexpr',\n    DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr', HAS_BIAS:\n    'tl.constexpr'):\n    input_dtype = Q.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    Q += off_z * stride_qz + off_h * stride_qh\n    K += off_z * stride_kz + off_h * stride_kh\n    V += off_z * stride_vz + off_h * stride_vh\n    if HAS_BIAS:\n        B += off_z * stride_bz + off_h * stride_bh\n    DO += off_z * stride_doz + off_h * stride_doh\n    D += (off_z * H + off_h) * M\n    L += (off_z * H + off_h) * M\n    DQ += off_z * stride_dqz + off_h * stride_dqh\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_n_init = offs_n_base\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q_ptrs = Q + (offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n    k_ptrs = K + (offs_n_init[:, None] * stride_kn + offs_k[None, :] *\n        stride_kk)\n    v_ptrs = V + (offs_n_init[:, None] * stride_vn + offs_k[None, :] *\n        stride_vk)\n    if HAS_BIAS:\n        bias_ptrs = B + (offs_m[:, None] * stride_bm + offs_n_init[None, :] *\n            stride_bn)\n    dq_ptrs = DQ + (offs_m[:, None] * stride_dqm + offs_k[None, :] * stride_dqk\n        )\n    do_ptrs = DO + (offs_m[:, None] * stride_dom + offs_k[None, :] * stride_dok\n        )\n    d_ptrs = D + offs_m\n    l_ptrs = L + offs_m\n    mask_m = offs_m < M\n    if DIVISIBLE_M:\n        q = tl.load(q_ptrs)\n        do = tl.load(do_ptrs)\n        delta = tl.load(d_ptrs)\n        l = tl.load(l_ptrs)\n    else:\n        q = tl.load(q_ptrs, mask=mask_m[:, None])\n        do = tl.load(do_ptrs, mask=mask_m[:, None])\n        delta = tl.load(d_ptrs, mask=mask_m)\n        l = tl.load(l_ptrs, mask=mask_m)\n    dq = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if CAUSAL:\n        hi = tl.minimum(N, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = N\n    for start_n in range(0, hi, BLOCK_N):\n        offs_n = start_n + offs_n_base\n        mask_n = offs_n < N\n        if DIVISIBLE_N:\n            v = tl.load(v_ptrs)\n            k = tl.load(k_ptrs)\n        else:\n            v = tl.load(v_ptrs, mask=mask_n[:, None])\n            k = tl.load(k_ptrs, mask=mask_n[:, None])\n        if HAS_BIAS:\n            if DIVISIBLE_M and DIVISIBLE_N:\n                b = tl.load(bias_ptrs)\n            else:\n                b = tl.load(bias_ptrs, mask=mask_m[:, None] & mask_n[None, :])\n        if not DIVISIBLE_N:\n            valid_mask = mask_n\n        if CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.dot(q, tl.trans(k)) * sm_scale\n        if HAS_BIAS:\n            s += b\n        p = tl.math.exp2((s - l[:, None]) * log2e)\n        dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        dp += tl.dot(do, tl.trans(v))\n        ds = p * (dp - delta[:, None])\n        if not DIVISIBLE_N:\n            ds = tl.where(valid_mask, ds, 0.0)\n        if CAUSAL:\n            ds = tl.where(causal_mask, ds, 0.0)\n        dq += tl.dot(ds, k)\n        k_ptrs += BLOCK_N * stride_kn\n        v_ptrs += BLOCK_N * stride_vn\n        if HAS_BIAS:\n            bias_ptrs += BLOCK_N * stride_bn\n    dq *= sm_scale\n    if DIVISIBLE_M:\n        tl.store(dq_ptrs, dq)\n    else:\n        tl.store(dq_ptrs, dq, mask=mask_m[:, None])\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "c6b2d92a-e28f-4605-be8e-e64f4388e954"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess(Out, DO, Delta, BLOCK_M: 'tl.constexpr', D_HEAD:\n    'tl.constexpr'):\n    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_n = tl.arange(0, D_HEAD)\n    o = tl.load(Out + off_m[:, None] * D_HEAD + off_n[None, :])\n    do = tl.load(DO + off_m[:, None] * D_HEAD + off_n[None, :])\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_m, delta)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "e04c5893-619d-4111-85d5-cb55a896d0d3"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_bwd_kernel_intra_KV(g, A, do, dv, s_v_h, s_v_t, s_v_d, T:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BC:\n    'tl.constexpr', BV: 'tl.constexpr', NC: 'tl.constexpr'):\n    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_t, i_i = i_c // NC, i_c % NC\n    p_gv = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_gn = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (s_v_d,), ((i_t *\n        BT + i_i * BC + BC - 1) * V + i_v * BV,), (BV,), (0,))\n    b_gn = tl.load(p_gn, boundary_check=(0,))\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_dv = tl.zeros([BC, BV], dtype=tl.float32)\n    for i_j in range(i_i + 1, NC):\n        p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (i_i *\n            BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T, V), (s_v_t, s_v_d),\n            (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_do = b_do * tl.exp(b_g - b_gn[None, :])\n        b_A = tl.load(p_A, boundary_check=(0, 1))\n        b_dv += tl.dot(b_A, b_do, allow_tf32=False)\n    b_dv *= tl.exp(b_gn[None, :] - b_gv)\n    o_i = tl.arange(0, BC)\n    for j in range(0, BC):\n        p_g = tl.make_block_ptr(g + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        p_A = tl.make_block_ptr(A + i_bh * T * BT, (T * BT,), (1,), ((i_t *\n            BT + i_i * BC + j) * BT + i_i * BC,), (BC,), (0,))\n        p_do = tl.make_block_ptr(do + i_bh * s_v_h, (T * V,), (1,), ((i_t *\n            BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n        b_A = tl.load(p_A, boundary_check=(0,))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_do = tl.load(p_do, boundary_check=(0,))\n        m_i = o_i[:, None] <= j\n        b_dv += tl.where(m_i, tl.exp(b_g[None, :] - b_gv) * b_A[:, None] *\n            b_do[None, :], 0.0)\n    p_dv = tl.make_block_ptr(dv + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n        i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    tl.store(p_dv, b_dv, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "29e612b2-b31f-49d5-b1a2-fbd8b9bc1c4b"
  },
  {
    "input": "@triton.jit\ndef bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g,\n    dg, s_qk_h, s_qk_t, s_qk_d, B, H, T, scale, BT: 'tl.constexpr', BK:\n    'tl.constexpr', DK: 'tl.constexpr'):\n    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * DK\n    p_g = g + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1\n        ) * DK\n    p_dg = dg + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (i_c * BT +\n        BT - 1) * DK\n    p_dq_inner = dq_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * DK\n    p_dk_inner = dk_inner + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * DK\n    p_dq_inter = dq_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * DK\n    p_dk_inter = dk_inter + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        i_c * BT + BT - 1) * DK\n    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)\n    mask = i_k * BK + tl.arange(0, BK) < DK\n    last_g = tl.zeros([BK], dtype=tl.float32)\n    for j in range(BT - 1, -1, -1):\n        _g = tl.load(p_g, mask=mask, other=0)\n        if j == BT - 1:\n            last_g = _g\n        _dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n        _dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n        _dq2 *= tl.math.exp2(_g)\n        _dq = _dq1 + _dq2\n        tl.store(p_dq_inter, _dq, mask=mask)\n        _dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n        _dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n        _dk2 *= tl.math.exp2(last_g - _g)\n        _dk = _dk1 + _dk2\n        tl.store(p_dk_inter, _dk, mask=mask)\n        _q = tl.load(p_q, mask=mask, other=0)\n        _k = tl.load(p_k, mask=mask, other=0)\n        _dg = _dq * _q - _dk * _k\n        cum_grad_dg += _dg\n        tl.store(p_dg, cum_grad_dg, mask=mask)\n        p_g -= DK\n        p_k -= DK\n        p_q -= DK\n        p_dq_inner -= DK\n        p_dk_inner -= DK\n        p_dq_inter -= DK\n        p_dk_inter -= DK\n        p_dg -= DK\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b838481d-5a01-4b0b-82ac-aa67013204e7"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4),\n    triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3,\n    num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64},\n    num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4)], key=['chunk_size',\n    'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dcb_kernel(x_ptr, dout_ptr, cb_ptr, dt_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, dcb_ptr, ddA_cumsum_ptr, chunk_size, hdim,\n    batch, seqlen, nheads, nheads_per_program, ngroups, stride_x_batch,\n    stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch,\n    stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_cb_batch,\n    stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n,\n    stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize,\n    stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head,\n    stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen,\n    stride_dcb_batch, stride_dcb_chunk, stride_dcb_split, stride_dcb_group,\n    stride_dcb_csize_m, stride_dcb_csize_n, stride_ddA_cs_batch,\n    stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m,\n    stride_ddA_cs_csize_n, HAS_DDA_CS: 'tl.constexpr', HAS_SEQ_IDX:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (\n        pid_g * (nheads // ngroups) + pid_s * nheads_per_program\n        ) * stride_x_head\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dout_head)\n    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g *\n        (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dA_cs_head)\n    if HAS_DDA_CS:\n        cb_ptr += (pid_b * stride_cb_batch + pid_c * stride_cb_chunk + \n            pid_g * stride_cb_head)\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n            nheads_per_program) * stride_ddA_cs_head + pid_m *\n            stride_ddA_cs_csize_m)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    x_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] *\n        stride_x_hdim)\n    dt_ptrs = dt_ptr + offs_n * stride_dt_csize\n    if HAS_DDA_CS:\n        cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[\n            None, :] * stride_cb_csize_n)\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\n    if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n        dcb_ptr += (pid_b * stride_dcb_batch + pid_c * stride_dcb_chunk + \n            pid_g * stride_dcb_group + pid_s * stride_dcb_split)\n        dcb_ptrs = dcb_ptr + (offs_m[:, None] * stride_dcb_csize_m + offs_n\n            [None, :] * stride_dcb_csize_n)\n        tl.store(dcb_ptrs, tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=\n            dcb_ptr.dtype.element_ty), mask=(offs_m[:, None] < chunk_size) &\n            (offs_n[None, :] < chunk_size))\n        return\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n\n            [None, :] < chunk_size), other=0.0)\n    nheads_iter = min(nheads_per_program, nheads // ngroups - pid_s *\n        nheads_per_program)\n    for h in range(nheads_iter):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_k[None, :] < hdim), other=0.0)\n        x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :\n            ] < chunk_size_limit_n), other=0.0)\n        dcb = tl.dot(dout, x)\n        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0)\n        dcb *= dt_n\n        dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask\n            =offs_m < chunk_size_limit, other=0.0)\n        dA_cs_n = tl.load(dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask\n            =offs_n < chunk_size_limit, other=0.0)\n        dcb *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n        if HAS_DDA_CS:\n            tl.static_assert(not HAS_SEQ_IDX,\n                'HAS_SEQ_IDX not supported with HAS_DDA_CS yet')\n            ddA_cs = dcb * cb\n            mask = offs_m[:, None] >= offs_n[None, :] + 1\n            ddA_cs = tl.where(mask, ddA_cs, 0.0)\n            ddA_cs = tl.cumsum(ddA_cs, axis=1)\n            ddA_cs = tl.where(mask, ddA_cs, 0.0)\n            ddA_cs = tl.sum(ddA_cs, axis=0)\n            tl.store(ddA_cumsum_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=\n                offs_n < chunk_size - 1)\n            tl.store(ddA_cumsum_ptr, 0.0)\n        acc += dcb\n        dout_ptrs += stride_dout_head\n        x_ptrs += stride_x_head\n        dt_ptrs += stride_dt_head\n        dA_cumsum_ptr += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptr += stride_ddA_cs_head\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    if HAS_SEQ_IDX:\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n        seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen,\n            mask=offs_n < chunk_size_limit, other=-2)\n        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\n    mask = offs_m[:, None] >= offs_n[None, :]\n    acc = tl.where(mask, acc, 0.0)\n    dcb_ptr += (pid_b * stride_dcb_batch + pid_c * stride_dcb_chunk + pid_g *\n        stride_dcb_group + pid_s * stride_dcb_split)\n    dcb_ptrs = dcb_ptr + (offs_m[:, None] * stride_dcb_csize_m + offs_n[\n        None, :] * stride_dcb_csize_n)\n    tl.store(dcb_ptrs, acc, mask=(offs_m[:, None] < chunk_size) & (offs_n[\n        None, :] < chunk_size))\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "bd54effc-13f8-4510-a86c-20127bb774c6"
  },
  {
    "input": "@triton.jit\ndef _bwd_preprocess_do_o_dot(Out, DO, Delta, stride_ob, stride_oh,\n    stride_om, stride_dob, stride_doh, stride_dom, nheads, seqlen_q,\n    seqlen_q_rounded, headdim, BLOCK_M: 'tl.constexpr', BLOCK_HEADDIM:\n    'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_d = tl.arange(0, BLOCK_HEADDIM)\n    o = tl.load(Out + off_b * stride_ob + off_h * stride_oh + offs_m[:,\n        None] * stride_om + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    do = tl.load(DO + off_b * stride_dob + off_h * stride_doh + offs_m[:,\n        None] * stride_dom + offs_d[None, :], mask=(offs_m[:, None] <\n        seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\n    delta = tl.sum(o * do, axis=1)\n    tl.store(Delta + off_hb * seqlen_q_rounded + offs_m, delta)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "e5b5c91f-c3eb-4451-931d-2b346645fe61"
  },
  {
    "input": "@triton.jit\ndef moe_weigths_bp_kernel(dr_ptr, dr_stride_bsk, dr_stride_hout, dk_ptr,\n    dk_stride_bsk, dk_stride_hout, dw_ptr, dw_stride_bsk, dw_stride_hout,\n    hout: 'tl.constexpr', block_size_hout: 'tl.constexpr'):\n    block_idx_bsk = tl.program_id(0)\n    block_idx_hout = tl.program_id(1)\n    offsets_hout = block_idx_hout * block_size_hout + tl.arange(0,\n        block_size_hout)\n    block_mask_hout = offsets_hout < hout\n    dr_ptrs = (dr_ptr + block_idx_bsk * dr_stride_bsk + offsets_hout *\n        dr_stride_hout)\n    dk_ptrs = (dk_ptr + block_idx_bsk * dk_stride_bsk + offsets_hout *\n        dk_stride_hout)\n    dw_ptrs = (dw_ptr + block_idx_bsk * dw_stride_bsk + offsets_hout *\n        dw_stride_hout)\n    dr = tl.load(dr_ptrs, mask=block_mask_hout, other=0.0)\n    dk = tl.load(dk_ptrs, mask=block_mask_hout, other=0.0)\n    dw = dr * dk\n    tl.store(dw_ptrs, dw, mask=block_mask_hout)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "37f7f546-8384-43e2-a0f4-981017086cdd"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.jit\ndef chunk_cumsum_bwd_kernel(ds, dz, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr',\n    S: 'tl.constexpr', BT: 'tl.constexpr', BS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\n    b_ds = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n        p_ds = tl.make_block_ptr(ds + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_dz = tl.make_block_ptr(dz + i_bh * s_s_h, (T, S), (s_s_t, s_s_d),\n            (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_dz = tl.load(p_dz, boundary_check=(0, 1))\n        b_c = b_ds[None, :] + tl.dot(m_s, b_dz, allow_tf32=False)\n        tl.store(p_ds, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_ds += tl.sum(b_dz, 0)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "eef4f094-adfd-49e1-9fa9-ccc012a315ec"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    128}, num_stages=3, num_warps=4, pre_hook=init_to_zero([\n    'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N':\n    32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'\n    ])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128},\n    num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])),\n    triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3,\n    num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config(\n    {'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4,\n    pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size',\n    'dstate', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_dc_kernel(dout_ptr, prev_states_ptr, C_ptr,\n    dA_cumsum_ptr, seq_idx_ptr, dc_ptr, ddA_cumsum_ptr, chunk_size, dstate,\n    hdim, batch, seqlen, nheads, nheads_per_program, ngroups,\n    stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_prev_states_batch, stride_prev_states_chunk,\n    stride_prev_states_head, stride_prev_states_hdim,\n    stride_prev_states_dstate, stride_C_batch, stride_C_seqlen,\n    stride_C_head, stride_C_dstate, stride_dA_cs_batch, stride_dA_cs_chunk,\n    stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch,\n    stride_seq_idx_seqlen, stride_dc_batch, stride_dc_seqlen,\n    stride_dc_split, stride_dc_group, stride_dc_dstate, stride_ddA_cs_batch,\n    stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize,\n    HAS_DDA_CS: 'tl.constexpr', HAS_SEQ_IDX: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_sg = tl.program_id(axis=2)\n    pid_s = pid_sg // ngroups\n    pid_g = pid_sg - pid_s * ngroups\n    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\n    pid_m = tl.program_id(axis=0) // num_pid_n\n    pid_n = tl.program_id(axis=0) % num_pid_n\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dout_head)\n    dc_ptr += (pid_b * stride_dc_batch + pid_c * chunk_size *\n        stride_dc_seqlen + pid_g * stride_dc_group + pid_s * stride_dc_split)\n    prev_states_ptr += (pid_b * stride_prev_states_batch + pid_c *\n        stride_prev_states_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_prev_states_head)\n    dA_cumsum_ptr += (pid_b * stride_dA_cs_batch + pid_c *\n        stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n        nheads_per_program) * stride_dA_cs_head)\n    if HAS_DDA_CS:\n        C_ptr += (pid_b * stride_C_batch + pid_c * chunk_size *\n            stride_C_seqlen + pid_g * stride_C_head)\n        ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n            stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s *\n            nheads_per_program) * stride_ddA_cs_head)\n    if HAS_SEQ_IDX:\n        seq_idx_ptr += (pid_b * stride_seq_idx_batch + pid_c * chunk_size *\n            stride_seq_idx_seqlen)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[\n        None, :] * stride_dout_hdim)\n    prev_states_ptrs = prev_states_ptr + (offs_n[None, :] *\n        stride_prev_states_dstate + offs_k[:, None] * stride_prev_states_hdim)\n    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\n    if HAS_DDA_CS:\n        C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None,\n            :] * stride_C_dstate)\n        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    if HAS_DDA_CS:\n        c = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < dstate), other=0.0)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=\n            pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen,\n            mask=offs_m < chunk_size_limit, other=-1)\n    nheads_iter = min(nheads_per_program, nheads // ngroups - pid_s *\n        nheads_per_program)\n    for h in range(nheads_iter):\n        dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) &\n            (offs_k[None, :] < hdim), other=0.0)\n        prev_states = tl.load(prev_states_ptrs, mask=(offs_k[:, None] <\n            hdim) & (offs_n[None, :] < dstate), other=0.0)\n        prev_states = prev_states\n        dc = tl.dot(dout, prev_states)\n        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit,\n            other=0.0)\n        if not HAS_SEQ_IDX:\n            scale = tl.exp(dA_cs_m)\n        else:\n            scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n        dc *= scale[:, None]\n        if HAS_DDA_CS:\n            ddA_cs = tl.sum(dc * c, axis=1)\n            tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n        acc += dc\n        dout_ptrs += stride_dout_head\n        prev_states_ptrs += stride_prev_states_head\n        dA_cumsum_ptrs += stride_dA_cs_head\n        if HAS_DDA_CS:\n            ddA_cumsum_ptrs += stride_ddA_cs_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    dc_ptrs = dc_ptr + (offs_m[:, None] * stride_dc_seqlen + offs_n[None, :\n        ] * stride_dc_dstate)\n    tl.store(dc_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < dstate))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "0a302f0c-2234-48c4-a3a8-f2572b6551e1"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, L, Out, stride_qz, stride_qh, stride_qm,\n    stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n    stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om,\n    stride_on, Z, H, N_CTX, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    qvk_offset = off_hz * stride_qh\n    Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(\n        BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n        ), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\n    V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0),\n        block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    qk_scale = sm_scale * 1.44269504\n    q = tl.load(Q_block_ptr)\n    q = q * qk_scale\n    lo = 0\n    hi = (start_m + 1) * BLOCK_M if IS_CAUSAL else N_CTX\n    for start_n in range(lo, hi, BLOCK_N):\n        k = tl.load(K_block_ptr)\n        v = tl.load(V_block_ptr)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        if IS_CAUSAL:\n            qk = tl.where(offs_m[:, None] >= start_n + offs_n[None, :], qk,\n                float('-inf'))\n        qk += tl.dot(q, k)\n        m_i_new = tl.maximum(m_i, tl.max(qk, 1))\n        alpha = tl.math.exp2(m_i - m_i_new)\n        p = tl.math.exp2(qk - m_i_new[:, None])\n        acc_scale = l_i * 0 + alpha\n        acc *= acc_scale[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n    acc = acc / l_i[:, None]\n    l_ptrs = L + off_hz * N_CTX + offs_m\n    tl.store(l_ptrs, m_i + tl.math.log2(l_i))\n    O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n        BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m *\n        BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "7257933d-bc88-40c3-85d7-d35ff4c41d0b"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel_dav(V, GV, A, O, DO, DA, DV, DGV, Z, H, stride_a1,\n    stride_a2, stride_a3, stride_a4, stride_v1, stride_v2, stride_v3,\n    stride_v4, BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_DMODEL_V: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_v = tl.program_id(2)\n    a_offset = off_hz * stride_a2\n    da_offset = (off_v * Z * H + off_hz) * stride_a2\n    v_offset = off_hz * stride_v2 + off_v * BLOCK_DMODEL_V\n    lo = 0\n    hi = BLOCK_N\n    DO_ptr = DO + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    O_ptr = O + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    DV_ptr = DV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    DGV_ptr = DGV + v_offset + start_m * stride_v3 + tl.arange(0,\n        BLOCK_DMODEL_V)[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    DA_ptr = DA + da_offset + start_m * stride_a3 + tl.arange(0, 16)[None, :\n        ] + tl.arange(0, 16)[:, None] * stride_a4\n    for q_high in range(lo, hi, 16):\n        do = tl.load(DO_ptr + q_high * stride_v4)\n        o = tl.load(O_ptr + q_high * stride_v4)\n        tl.store(DGV_ptr + q_high * stride_v4, do * o)\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        q_gv = tl.load(GV_ptr + q_high * stride_v4)\n        q_gv = tl.exp(q_gv - q_gv_normalizer[None, :])\n        do = do * q_gv\n        tl.store(DO_ptr + q_high * stride_v4, do)\n    tl.debug_barrier()\n    V_ptr = V + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        :, None] + tl.arange(0, 16)[None, :] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[:, None] + tl.arange(0, 16)[None, :] * stride_v4\n    for q_high in range(lo + 16, hi, 16):\n        do = tl.load(DO_ptr + q_high * stride_v4)\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        for k_high in range(0, q_high, 16):\n            v = tl.load(V_ptr + k_high * stride_v4)\n            k_gv = tl.load(GV_ptr + k_high * stride_v4)\n            k_gv = tl.exp(q_gv_normalizer[:, None] - k_gv)\n            v2 = v * k_gv\n            dqk = tl.dot(do, v2, allow_tf32=False)\n            tl.store(DA_ptr + q_high * stride_a4 + k_high, dqk)\n    tl.debug_barrier()\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[:, None\n        ] + tl.arange(0, 16)[None, :] * stride_a4\n    V_ptr = V + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V)[\n        None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    GV_ptr = GV + v_offset + start_m * stride_v3 + tl.arange(0, BLOCK_DMODEL_V\n        )[None, :] + tl.arange(0, 16)[:, None] * stride_v4\n    for k_high in range(0, hi, 16):\n        dv = tl.zeros([16, BLOCK_DMODEL_V], dtype=tl.float32)\n        k_gv = tl.load(GV_ptr + k_high * stride_v4)\n        for q_high in range(k_high + 16, BLOCK_N, 16):\n            do = tl.load(DO_ptr + q_high * stride_v4)\n            kq = tl.load(A_ptr + q_high * stride_a4 + k_high)\n            q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 +\n                q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n            k_gv2 = tl.exp(q_gv_normalizer[None, :] - k_gv)\n            dv2 = tl.dot(kq, do, allow_tf32=False)\n            dv += dv2 * k_gv2\n        v = tl.load(V_ptr + k_high * stride_v4)\n        tl.store(DV_ptr + k_high * stride_v4, dv)\n        prev_dv = tl.load(DGV_ptr + k_high * stride_v4)\n        tl.store(DGV_ptr + k_high * stride_v4, prev_dv - dv * v)\n    tl.debug_barrier()\n    A_ptr = A + a_offset + start_m * stride_a3 + tl.arange(0, 16)[:, None\n        ] + tl.arange(0, 16)[None, :] * stride_a4\n    for q_high in range(lo, hi, 16):\n        do = tl.load(DO_ptr + q_high * stride_v4)\n        q_gv_normalizer = tl.load(GV + v_offset + start_m * stride_v3 + \n            q_high * stride_v4 + tl.arange(0, BLOCK_DMODEL_V))\n        v = tl.load(V_ptr + q_high * stride_v4)\n        k_gv = tl.load(GV_ptr + q_high * stride_v4)\n        k_gv = tl.exp(q_gv_normalizer[None, :] - k_gv)\n        v2 = v * k_gv\n        dqk = tl.dot(do, tl.trans(v2), allow_tf32=False)\n        dqk = tl.where(tl.arange(0, 16)[:, None] >= tl.arange(0, 16)[None,\n            :], dqk, 0.0)\n        tl.store(DA_ptr + q_high * stride_a4 + q_high, dqk)\n        kq = tl.load(A_ptr + q_high * stride_a4 + q_high)\n        dv2 = tl.dot(kq, do, allow_tf32=False)\n        dv = dv2 * k_gv\n        prev_dv = tl.load(DV_ptr + q_high * stride_v4)\n        tl.store(DV_ptr + q_high * stride_v4, prev_dv + dv)\n        prev_gdv = tl.load(DGV_ptr + q_high * stride_v4)\n        prev_gdv -= dv * v\n        tl.store(DGV_ptr + q_high * stride_v4, prev_gdv)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "39891e58-1326-49bd-9783-0b503d5cb2bc"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_linear_attn_fwd_kernel(q, k, v, o, initial_state,\n    final_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_o = o + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    mask_kv = mask_bk[None, :] & mask_bv[:, None]\n    h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for _ in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        h += _k[None, :] * _v[:, None]\n        _o = h * _q[None, :]\n        _o = tl.sum(_o, axis=1)\n        tl.store(p_o, _o, mask=mask_bv)\n        p_q += DK\n        p_k += DK\n        p_o += DV\n        p_v += DV\n    if STORE_FINAL_STATE:\n        p_final_s = final_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[None, :]) * DV + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_final_s, h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "aaa459fe-53c1-455e-8f7d-2eafe736e4b0"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner_ws(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, desc_k,\n    desc_v, Q, qvk_offset, stride_kn, stride_vn, stride_vk, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr', fp8_v: 'tl.constexpr',\n    ENABLE_TMA: 'tl.constexpr', LOOP_SCHEDULE: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    if not ENABLE_TMA:\n        K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n        V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in tl.range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        with tl.async_task([0]):\n            if ENABLE_TMA:\n                k = tl._experimental_descriptor_load(desc_k, [start_n + \n                    qvk_offset // stride_kn, 0], [BLOCK_N, HEAD_DIM], Q.\n                    dtype.element_ty)\n            else:\n                k = tl.load(K_block_ptr)\n        with tl.async_task([1, 2]):\n            if ENABLE_TMA:\n                k = tl.trans(k)\n            qk = tl.dot(q, k)\n            if STAGE == 2:\n                mask = offs_m[:, None] >= start_n + offs_n[None, :]\n                qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n                m_ij = tl.maximum(m_i, tl.max(qk, 1))\n                qk -= m_ij[:, None]\n            else:\n                m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n                qk = qk * qk_scale - m_ij[:, None]\n            p = tl.math.exp2(qk)\n            l_ij = tl.sum(p, 1)\n            alpha = tl.math.exp2(m_i - m_ij)\n            l_i = l_i * alpha + l_ij\n            acc = acc * alpha[:, None]\n        with tl.async_task([0]):\n            if ENABLE_TMA:\n                if fp8_v:\n                    v = tl._experimental_descriptor_load(desc_v, [\n                        qvk_offset // stride_vn, start_n], [HEAD_DIM,\n                        BLOCK_N], Q.dtype.element_ty)\n                else:\n                    v = tl._experimental_descriptor_load(desc_v, [\n                        qvk_offset // stride_vk + start_n, 0], [BLOCK_N,\n                        HEAD_DIM], Q.dtype.element_ty)\n            else:\n                v = tl.load(V_block_ptr)\n        with tl.async_task([1, 2]):\n            if fp8_v:\n                if ENABLE_TMA:\n                    v = tl.trans(v)\n                p = p\n            else:\n                p = p\n            acc = tl.dot(p, v, acc)\n            m_i = m_ij\n        if not ENABLE_TMA:\n            V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n            K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "9aac132a-ac07-4d9b-889f-eb2fad770b60"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, x_stride0, x_stride1, y_stride0, y_stride1,\n    N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * y_stride0\n    X += row * x_stride0\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols * x_stride1, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols * y_stride1, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "42c81ff8-144a-4090-babc-d25a2f5b307f"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, B_ptr, B_row_stride, Mean_ptr, Mean_row_stride,\n    RSTD_ptr, RSTD_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    References:\n    https://arxiv.org/abs/1607.06450\n    https://github.com/karpathy/llm.c/blob/master/doc/layernorm/layernorm.md\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    Mean_ptr += row_idx * Mean_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    B_row = tl.load(B_ptr + col_offsets, mask=mask, other=0)\n    mean = tl.sum(X_row, axis=0) / n_cols\n    var = tl.sum((X_row - mean) * (X_row - mean), axis=0) / n_cols\n    rstd = rsqrt(var + eps)\n    tl.store(Mean_ptr, mean)\n    tl.store(RSTD_ptr, rstd)\n    Y_row = (X_row - mean) * rstd * W_row + B_row\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "c4798277-26f1-4232-9ea7-69f5efbfe606"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q, K, V, sm_scale, B_Start_Loc, B_Seqlen, Out,\n    Req_to_tokens, B_req_idx, stride_qbs, stride_qh, stride_qd, stride_kbs,\n    stride_kh, stride_kd, stride_vbs, stride_vh, stride_vd, stride_obs,\n    stride_oh, stride_od, stride_req_to_tokens_b, stride_req_to_tokens_s,\n    kv_group_num, b_prompt_cache_len, head_dim: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_m = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    prompt_cache_len = tl.load(b_prompt_cache_len + cur_batch)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch) - prompt_cache_len\n    cur_batch_req_idx = tl.load(B_req_idx + cur_batch)\n    block_start_loc = BLOCK_M * start_m\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    off_q = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd\n    q = tl.load(Q + off_q, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim), other=0.0)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)\n    block_end_loc = tl.minimum((start_m + 1) * BLOCK_M + prompt_cache_len, \n        cur_batch_seq_len + prompt_cache_len)\n    for start_n in range(0, block_mask * block_end_loc, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        kv_loc = tl.load(Req_to_tokens + stride_req_to_tokens_b *\n            cur_batch_req_idx + stride_req_to_tokens_s * (start_n + offs_n),\n            mask=start_n + offs_n < block_end_loc, other=0)\n        off_k = kv_loc[None, :\n            ] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None\n            ] * stride_kd\n        k = tl.load(K + off_k, mask=(start_n + offs_n[None, :] <\n            block_end_loc) & (offs_d[:, None] < head_dim), other=0.0)\n        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        qk += tl.dot(q, k)\n        qk *= sm_scale\n        qk = tl.where(offs_m[:, None] + prompt_cache_len >= start_n +\n            offs_n[None, :], qk, float('-100000000.0'))\n        m_ij = tl.max(qk, 1)\n        p = tl.exp(qk - m_ij[:, None])\n        l_ij = tl.sum(p, 1)\n        m_i_new = tl.maximum(m_i, m_ij)\n        alpha = tl.exp(m_i - m_i_new)\n        beta = tl.exp(m_ij - m_i_new)\n        l_i_new = alpha * l_i + beta * l_ij\n        p_scale = beta / l_i_new\n        p = p * p_scale[:, None]\n        acc_scale = l_i / l_i_new * alpha\n        acc_scale = tl.where(offs_m + prompt_cache_len >= start_n,\n            acc_scale, 1.0)\n        acc = acc * acc_scale[:, None]\n        off_v = kv_loc[:, None\n            ] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :\n            ] * stride_vd\n        v = tl.load(V + off_v, mask=(start_n + offs_n[:, None] <\n            block_end_loc) & (offs_d[None, :] < head_dim), other=0.0)\n        p = p\n        acc += tl.dot(p, v)\n        l_i = l_i_new\n        m_i = m_i_new\n    off_o = (cur_batch_in_all_start_index + offs_m[:, None]\n        ) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < cur_batch_seq_len) & (\n        offs_d[None, :] < head_dim))\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "b37c0501-ce4b-4192-a35e-e2483059f1fe"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_fused(X, Y, W, M, V, stride, N, BLOCK_SIZE: 'tl.constexpr'\n    ):\n    row = tl.program_id(0)\n    cols = tl.arange(0, BLOCK_SIZE)\n    mask = cols < N\n    X += row * stride\n    Y += row * stride\n    x = tl.load(X + cols, mask=mask, other=0)\n    mean = tl.sum(x, axis=0) / N\n    xmean = tl.where(mask, x - mean, 0.0)\n    var = tl.sum(xmean * xmean, axis=0) / N\n    rstd = 1 / tl.sqrt(var + 1e-05)\n    xhat = xmean * rstd\n    tl.store(M + row, mean)\n    tl.store(V + row, rstd)\n    w = tl.load(W + cols, mask=mask)\n    y = xhat * w\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "6d707d2c-ae71-472a-978d-07953447fb58"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel(Q1, K1, Q2, K2, V, sm_scale, L, O, stride_q1z, stride_q1h,\n    stride_q1m, stride_q1k, stride_k1z, stride_k1h, stride_k1n, stride_k1k,\n    stride_q2z, stride_q2h, stride_q2m, stride_q2k, stride_k2z, stride_k2h,\n    stride_k2n, stride_k2k, stride_vz, stride_vh, stride_vn, stride_vk,\n    stride_oz, stride_oh, stride_om, stride_ok, Z, H, M, N, P_SEQ, w:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_DMODEL: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr', IS_CAUSAL: 'tl.constexpr', LARGER_M:\n    'tl.constexpr', DIVISIBLE_M: 'tl.constexpr', DIVISIBLE_N: 'tl.constexpr'):\n    input_dtype = Q1.dtype.element_ty\n    start_m = tl.program_id(0)\n    off_h = tl.program_id(1)\n    off_z = tl.program_id(2)\n    log2e: 'tl.constexpr' = 1.4426950408889634\n    qk_scale = sm_scale * log2e\n    Q1 += off_z * stride_q1z + off_h * stride_q1h\n    Q2 += off_z * stride_q2z + off_h * stride_q2h\n    K1 += off_z * stride_k1z + off_h * stride_k1h\n    K2 += off_z * stride_k2z + off_h * stride_k2h\n    V += off_z * stride_vz + off_h * stride_vh\n    O += off_z * stride_oz + off_h * stride_oh\n    L += (off_z * H + off_h) * M\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n_base = tl.arange(0, BLOCK_N)\n    offs_n_init = offs_n_base\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    q1_ptrs = Q1 + (offs_m[:, None] * stride_q1m + offs_k[None, :] * stride_q1k\n        )\n    q2_ptrs = Q2 + (offs_m[:, None] * stride_q2m + offs_k[None, :] * stride_q2k\n        )\n    k1_ptrs = K1 + (offs_n_init[:, None] * stride_k1n + offs_k[None, :] *\n        stride_k1k)\n    k2_ptrs = K2 + (offs_n_init[:, None] * stride_k2n + offs_k[None, :] *\n        stride_k2k)\n    v_ptrs = V + (offs_n_init[:, None] * stride_vn + offs_k[None, :] *\n        stride_vk)\n    o_ptrs = O + (offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)\n    l_ptrs = L + offs_m\n    m_i = tl.full([BLOCK_M], value=-float('inf'), dtype=tl.float32)\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n    if DIVISIBLE_M:\n        q1 = tl.load(q1_ptrs)\n        q2 = tl.load(q2_ptrs)\n    else:\n        mask_m = offs_m < M\n        q1 = tl.load(q1_ptrs, mask=mask_m[:, None])\n        q2 = tl.load(q2_ptrs, mask=mask_m[:, None])\n    I = tl.where(offs_k[:, None] == offs_k, tl.full((BLOCK_DMODEL,\n        BLOCK_DMODEL), 1.0, dtype=input_dtype), tl.full((BLOCK_DMODEL,\n        BLOCK_DMODEL), 0.0, dtype=input_dtype))\n    q1 = tl.dot(q1, I)\n    q2 = tl.dot(q2, I)\n    if IS_CAUSAL:\n        hi = tl.minimum(N, P_SEQ + (start_m + 1) * BLOCK_M)\n        if LARGER_M:\n            hi = tl.maximum(0, hi)\n    else:\n        hi = N\n    for start_n in range(0, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        offs_n = start_n + offs_n_base\n        piecewise_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :] + w\n        if DIVISIBLE_N:\n            k1 = tl.load(k1_ptrs)\n            k2 = tl.load(k2_ptrs)\n            v = tl.load(v_ptrs)\n        else:\n            mask_n = offs_n < N\n            k1 = tl.load(k1_ptrs, mask=mask_n[:, None])\n            k2 = tl.load(k2_ptrs, mask=mask_n[:, None])\n            v = tl.load(v_ptrs, mask=mask_n[:, None])\n        s = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n        s += tl.where(piecewise_mask, tl.dot(q2, tl.trans(k2)), tl.dot(q1,\n            tl.trans(k1)))\n        if not DIVISIBLE_N:\n            s = tl.where(mask_n, s, float('-inf'))\n        if IS_CAUSAL:\n            causal_mask = P_SEQ + offs_m[:, None] >= offs_n[None, :]\n            s = tl.where(causal_mask, s, float('-inf'))\n        m_i_new = tl.maximum(m_i, tl.max(s, 1))\n        alpha = tl.math.exp2((m_i - m_i_new) * qk_scale)\n        p = tl.math.exp2(s * qk_scale - m_i_new[:, None] * qk_scale)\n        acc *= alpha[:, None]\n        acc += tl.dot(p, v)\n        l_i = l_i * alpha + tl.sum(p, 1)\n        m_i = m_i_new\n        k1_ptrs += BLOCK_N * stride_k1n\n        k2_ptrs += BLOCK_N * stride_k2n\n        v_ptrs += BLOCK_N * stride_vn\n    if IS_CAUSAL and LARGER_M:\n        is_empty_line = offs_m + P_SEQ < 0\n        acc = tl.where(is_empty_line[:, None], 0.0, acc * (1.0 / l_i[:, None]))\n        l_i = tl.where(is_empty_line, float('-inf'), m_i * sm_scale + tl.\n            log(l_i))\n    else:\n        acc = acc * (1.0 / l_i[:, None])\n        l_i = m_i * sm_scale + tl.log(l_i)\n    if DIVISIBLE_M:\n        tl.store(l_ptrs, l_i)\n        tl.store(o_ptrs, acc)\n    else:\n        tl.store(l_ptrs, l_i, mask=mask_m)\n        tl.store(o_ptrs, acc, mask=mask_m[:, None])\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "3b6f095c-1efe-40e4-b1c0-60cc983d45aa"
  },
  {
    "input": "@triton.jit\ndef _attn_bwd_dq(dq, q, K, V, do, m, D, stride_tok, stride_d, H, N_CTX,\n    BLOCK_M2: 'tl.constexpr', BLOCK_N2: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', start_m, start_n, num_steps, MASK: 'tl.constexpr'):\n    offs_m = start_m + tl.arange(0, BLOCK_M2)\n    offs_n = start_n + tl.arange(0, BLOCK_N2)\n    offs_k = tl.arange(0, BLOCK_DMODEL)\n    kT_ptrs = K + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    vT_ptrs = V + offs_n[None, :] * stride_tok + offs_k[:, None] * stride_d\n    Di = tl.load(D + offs_m)\n    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n    curr_n = start_n\n    step_n = BLOCK_N2\n    for blk_idx in range(num_steps):\n        kT = tl.load(kT_ptrs)\n        vT = tl.load(vT_ptrs)\n        qk = tl.dot(q, kT)\n        p = tl.math.exp2(qk - m)\n        if MASK:\n            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n            mask = offs_m[:, None] >= offs_n[None, :]\n            p = tl.where(mask, p, 0.0)\n        dp = tl.dot(do, vT)\n        ds = p * (dp - Di[:, None])\n        ds = ds\n        dq += tl.dot(ds, tl.trans(kT))\n        curr_n += step_n\n        kT_ptrs += step_n * stride_tok\n        vT_ptrs += step_n * stride_tok\n    return dq\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ca017cab-49d1-4f60-8e74-432491bccaeb"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, S, DO, DQ, DK, DV, KV, DKV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', DBLOCK:\n    'tl.constexpr', NUM_DBLOCK: 'tl.constexpr', EBLOCK: 'tl.constexpr',\n    NUM_EBLOCK: 'tl.constexpr'):\n    off_d = tl.program_id(0)\n    off_e = tl.program_id(1)\n    off_bh = tl.program_id(2)\n    off_h = off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    kv_offset = off_bh * d * e\n    d_offset = off_d * DBLOCK\n    e_offset = off_e * EBLOCK\n    dqk_offset = off_e * b * h * n * d\n    dv_offset = off_d * b * h * n * e\n    d_offset = off_d * DBLOCK\n    e_offset = off_e * EBLOCK\n    kv_d_offset = d_offset * e\n    S_block_ptr = S + off_h\n    s = tl.load(S_block_ptr)\n    block_decay = tl.exp(-s * BLOCK)\n    DQ_block_ptr = DQ + qk_offset + dqk_offset + d_offset + tl.arange(0, BLOCK\n        )[:, None] * d + tl.arange(0, DBLOCK)[None, :]\n    K_block_ptr = K + qk_offset + d_offset + tl.arange(0, BLOCK)[:, None\n        ] * d + tl.arange(0, DBLOCK)[None, :]\n    V_trans_block_ptr = V + v_offset + e_offset + tl.arange(0, BLOCK)[None, :\n        ] * e + tl.arange(0, EBLOCK)[:, None]\n    DO_block_ptr = DO + o_offset + e_offset + tl.arange(0, BLOCK)[:, None\n        ] * e + tl.arange(0, EBLOCK)[None, :]\n    KV_trans_block_ptr = KV + kv_offset + kv_d_offset + e_offset + tl.arange(\n        0, DBLOCK)[None, :] * e + tl.arange(0, EBLOCK)[:, None]\n    DKV_block_ptr = DKV + kv_offset + kv_d_offset + e_offset + tl.arange(0,\n        DBLOCK)[:, None] * e + tl.arange(0, EBLOCK)[None, :]\n    array = tl.arange(0, BLOCK)\n    index = array[:, None] - array[None, :]\n    s_index = s * index\n    s_index = tl.where(index >= 0, -s_index, float('-inf'))\n    diag_decay = tl.exp(s_index)\n    diag_decay_trans = tl.trans(diag_decay)\n    KV_trans = tl.load(KV_trans_block_ptr)\n    kv_trans = tl.zeros([EBLOCK, DBLOCK], dtype=tl.float32)\n    for i in range(NUM_BLOCK):\n        q_decay = tl.exp(-s * array[:, None])\n        k_decay = tl.exp(-s * (BLOCK - array[:, None]))\n        do = tl.load(DO_block_ptr)\n        k = tl.load(K_block_ptr)\n        v_trans = tl.load(V_trans_block_ptr)\n        dq_none_diag = tl.dot(do, kv_trans) * q_decay + tl.dot(do, KV_trans\n            ) * tl.exp(-s * (i * BLOCK + array[:, None]))\n        dqk = tl.dot(do, v_trans) * diag_decay\n        dq_diag = tl.dot(dqk, k)\n        dq = dq_none_diag + dq_diag\n        tl.store(DQ_block_ptr, dq)\n        DQ_block_ptr += BLOCK * d\n        DO_block_ptr += BLOCK * e\n        K_block_ptr += BLOCK * d\n        V_trans_block_ptr += BLOCK * e\n        kv_trans = block_decay * kv_trans + tl.dot(v_trans, k * k_decay)\n    Q_trans_block_ptr = Q + qk_offset + d_offset + n * d + tl.arange(0, BLOCK)[\n        None, :] * d + tl.arange(0, DBLOCK)[:, None]\n    K_block_ptr = K + qk_offset + d_offset + n * d + tl.arange(0, BLOCK)[:,\n        None] * d + tl.arange(0, DBLOCK)[None, :]\n    V_trans_block_ptr = V + v_offset + e_offset + n * e + tl.arange(0, BLOCK)[\n        None, :] * e + tl.arange(0, EBLOCK)[:, None]\n    DK_trans_block_ptr = (DK + qk_offset + dqk_offset + d_offset + n * d + \n        tl.arange(0, BLOCK)[None, :] * d + tl.arange(0, DBLOCK)[:, None])\n    DV_block_ptr = DV + v_offset + dv_offset + e_offset + n * e + tl.arange(\n        0, BLOCK)[:, None] * e + tl.arange(0, EBLOCK)[None, :]\n    DO_block_ptr = DO + o_offset + e_offset + n * e + tl.arange(0, BLOCK)[:,\n        None] * e + tl.arange(0, EBLOCK)[None, :]\n    DKV = tl.load(DKV_block_ptr)\n    dkv = tl.zeros([DBLOCK, EBLOCK], dtype=tl.float32)\n    for i in range(NUM_BLOCK - 1, -1, -1):\n        K_block_ptr -= BLOCK * d\n        V_trans_block_ptr -= BLOCK * e\n        DK_trans_block_ptr -= BLOCK * d\n        DV_block_ptr -= BLOCK * e\n        DO_block_ptr -= BLOCK * e\n        Q_trans_block_ptr -= BLOCK * d\n        k = tl.load(K_block_ptr)\n        v_trans = tl.load(V_trans_block_ptr)\n        do = tl.load(DO_block_ptr)\n        q_trans = tl.load(Q_trans_block_ptr)\n        k_decay_trans = tl.exp(-s * (BLOCK - array[None, :]))\n        k_decay = tl.exp(-s * (BLOCK - array[:, None]))\n        q_decay_trans = tl.exp(-s * array[None, :])\n        dqk = tl.dot(do, v_trans) * diag_decay\n        dk_diag_trans = tl.dot(q_trans, dqk)\n        dk_none_diag_trans = tl.dot(dkv, v_trans) * k_decay_trans + tl.dot(DKV,\n            v_trans) * tl.exp(-s * (n - i * BLOCK - array[None, :]))\n        dk_trans = dk_none_diag_trans + dk_diag_trans\n        qk_trans = tl.dot(k, q_trans) * diag_decay_trans\n        dv_diag = tl.dot(qk_trans, do)\n        dv_none_diag = tl.dot(k, dkv) * k_decay + tl.dot(k, DKV) * tl.exp(-\n            s * (n - i * BLOCK - array[:, None]))\n        dv = dv_none_diag + dv_diag\n        tl.store(DK_trans_block_ptr, dk_trans)\n        tl.store(DV_block_ptr, dv)\n        dkv = block_decay * dkv + tl.dot(q_trans * q_decay_trans, do)\n    DKV = tl.exp(-s * n) * DKV + dkv\n    tl.store(DKV_block_ptr, DKV)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d4c66896-0ab6-4189-8be0-0f222245d457"
  },
  {
    "input": "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd,\n    stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row,\n    stride_dz_row, stride_dw_row, stride_db_row, M, N, eps,\n    rows_per_program, NORM_BEFORE_GATE: 'tl.constexpr', IS_RMS_NORM:\n    'tl.constexpr', HAS_BIAS: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    RECOMPUTE_OUTPUT: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    group = tl.program_id(1)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row + group * N\n    if HAS_Z:\n        Z += row_start * stride_z_row + group * N\n        DZ += row_start * stride_dz_row + group * N\n    DY += row_start * stride_dy_row + group * N\n    DX += row_start * stride_dx_row + group * N\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row + group * N\n    if not IS_RMS_NORM:\n        Mean += group * M\n    Rstd += group * M\n    W += group * N\n    w = tl.load(W + cols, mask=mask)\n    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n        B += group * N\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            x_og = x\n            x = x_og * z * tl.sigmoid(z)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if HAS_Z and NORM_BEFORE_GATE:\n            z = tl.load(Z + cols, mask=mask, other=0.0)\n            z_sigmoid = tl.sigmoid(z)\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            if RECOMPUTE_OUTPUT:\n                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dy *= z * z_sigmoid\n        elif RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        if not IS_RMS_NORM:\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            dx = (wdy - xhat * c1) * rstd\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if HAS_Z and not NORM_BEFORE_GATE:\n            z_sigmoid = tl.sigmoid(z)\n            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n            tl.store(DZ + cols, dz, mask=mask)\n            dx *= z * z_sigmoid\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_Z:\n            Z += stride_z_row\n            DZ += stride_dz_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask\n        =mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db,\n            mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "570a196b-6112-4583-8855-3f1c29aa0281"
  },
  {
    "input": "@triton.jit\ndef embedding_forward_kernel(embeddings_ptr, indices_ptr, output_ptr,\n    n_elements, embedding_dim: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr'):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offsets_m = start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < n_elements\n    indices = tl.load(indices_ptr + offsets_m, mask=mask_m, other=0)\n    offsets_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < embedding_dim\n    embedding_offsets = indices[:, None] * embedding_dim + offsets_n[None, :]\n    embeddings = tl.load(embeddings_ptr + embedding_offsets, mask=mask_m[:,\n        None] & mask_n[None, :], other=0.0)\n    output_offsets = offsets_m[:, None] * embedding_dim + offsets_n[None, :]\n    tl.store(output_ptr + output_offsets, embeddings, mask=mask_m[:, None] &\n        mask_n[None, :])\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "414420e3-d26b-4b31-b3ca-89a386bead5f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=2), triton.Config({},\n    num_warps=4), triton.Config({}, num_warps=8)], key=['S'])\n@triton.jit\ndef softmax_fwd_kernel(s, p, s_s_h, s_s_t, s_s_d, T: 'tl.constexpr', S:\n    'tl.constexpr', BT: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    p_s = tl.make_block_ptr(s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    p_p = tl.make_block_ptr(p + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t *\n        BT, 0), (BT, S), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_m = tl.max(b_s, 1)\n    b_s = tl.exp(b_s - b_m[:, None])\n    b_z = tl.sum(b_s, 1)\n    b_p = tl.where(b_s != 0, b_s / b_z[:, None], 0.0)\n    tl.store(p_p, b_p, boundary_check=(0, 1))\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "e84fe43d-5dc4-467f-bf6e-0a678417c4c7"
  },
  {
    "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    nheads, rotary_dim, seqlen_ro, CACHE_KEY_SEQLEN, stride_out_batch,\n    stride_out_seqlen, stride_out_nheads, stride_out_headdim,\n    stride_x_batch, stride_x_seqlen, stride_x_nheads, stride_x_headdim,\n    BLOCK_K: 'tl.constexpr', IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr',\n    IS_VARLEN: 'tl.constexpr', INTERLEAVED: 'tl.constexpr', CONJUGATE:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim_half + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "86b3d654-a6f0-45f6-95ec-6c3bc2f59631"
  },
  {
    "input": "@triton.jit\ndef _rms_layernorm_forward(Y, Y_row_stride, X, X_row_stride, W,\n    W_row_stride, r, r_row_stride, n_cols, eps, BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n        Fast RMS Layernorm kernel\n        Inspiration from a Triton tutorial:\n        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y += row_idx * Y_row_stride\n    X += row_idx * X_row_stride\n    r += row_idx * r_row_stride\n    X_row = tl.load(X + col_offsets, mask=mask, other=0)\n    W_row = tl.load(W + col_offsets, mask=mask, other=0)\n    row_var = tl.sum(X_row * X_row, axis=0) / n_cols\n    inv_var = tl.math.rsqrt(row_var + eps)\n    tl.store(r, inv_var)\n    normed = X_row * inv_var\n    normed = normed\n    output = normed * W_row\n    tl.store(Y + col_offsets, output, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "d8bb6b51-7b15-4488-bd3a-bb7fea7bd7ad"
  },
  {
    "input": "@triton.jit\ndef _bwd_kernel(Q, K, V, sm_scale, Out, DO, DQ, DK, DV, L, D, stride_dqa,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vn, stride_vk, Z, H,\n    N_CTX, Z_H_N_CTX, SQ_Z_H_N_CTX, BLOCK_M: 'tl.constexpr', BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr', SEQUENCE_PARALLEL:\n    'tl.constexpr', CAUSAL: 'tl.constexpr', MMA_V3: 'tl.constexpr'):\n    qk_scale = sm_scale * 1.44269504\n    off_hz = tl.program_id(0)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    Q_block_ptr = tl.make_block_ptr(base=Q, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_qm, stride_qk), offsets=(0, 0), block_shape=(\n        BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    K_block_ptr = tl.make_block_ptr(base=K, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_kn, stride_kk), offsets=(0, 0), block_shape=(\n        BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    V_block_ptr = tl.make_block_ptr(base=V, shape=(Z_H_N_CTX, BLOCK_DMODEL),\n        strides=(stride_vn, stride_vk), offsets=(0, 0), block_shape=(\n        BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    DO_block_ptr = tl.make_block_ptr(base=DO, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    if SEQUENCE_PARALLEL:\n        DQ_block_ptr = tl.make_block_ptr(base=DQ, shape=(SQ_Z_H_N_CTX,\n            BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n            block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    else:\n        DQ_block_ptr = tl.make_block_ptr(base=DQ, shape=(Z_H_N_CTX,\n            BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0),\n            block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    DK_block_ptr = tl.make_block_ptr(base=DK, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_kn, stride_kk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    DV_block_ptr = tl.make_block_ptr(base=DV, shape=(Z_H_N_CTX,\n        BLOCK_DMODEL), strides=(stride_vn, stride_vk), offsets=(0, 0),\n        block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\n    num_block_n = tl.cdiv(N_CTX, BLOCK_N)\n    if not SEQUENCE_PARALLEL:\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(Q, K, V, sm_scale, qk_scale, Out, DO,\n                DQ, DK, DV, L, D, Q_block_ptr, K_block_ptr, V_block_ptr,\n                DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,\n                stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n                stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n                stride_vh, stride_vn, stride_vk, Z, H, N_CTX, off_h, off_z,\n                off_hz, start_n, num_block_n, BLOCK_M=BLOCK_M, BLOCK_DMODEL\n                =BLOCK_DMODEL, BLOCK_N=BLOCK_N, SEQUENCE_PARALLEL=\n                SEQUENCE_PARALLEL, CAUSAL=CAUSAL, MMA_V3=MMA_V3)\n    else:\n        start_n = tl.program_id(1)\n        _bwd_kernel_one_col_block(Q, K, V, sm_scale, qk_scale, Out, DO, DQ,\n            DK, DV, L, D, Q_block_ptr, K_block_ptr, V_block_ptr,\n            DO_block_ptr, DQ_block_ptr, DK_block_ptr, DV_block_ptr,\n            stride_dqa, stride_qz, stride_qh, stride_qm, stride_qk,\n            stride_kz, stride_kh, stride_kn, stride_kk, stride_vz,\n            stride_vh, stride_vn, stride_vk, Z, H, N_CTX, off_h, off_z,\n            off_hz, start_n, num_block_n, BLOCK_M=BLOCK_M, BLOCK_DMODEL=\n            BLOCK_DMODEL, BLOCK_N=BLOCK_N, SEQUENCE_PARALLEL=\n            SEQUENCE_PARALLEL, CAUSAL=CAUSAL, MMA_V3=MMA_V3)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "f09a6e59-131a-4bce-aec2-53865e7acdc0"
  },
  {
    "input": "@triton.jit\ndef dequantize(x_, scale, shift, PACKED_PER_VAL: 'tl.constexpr'):\n    \"\"\"PACKED_PER_VAL is the number of values packed into each element x_.\n    For example, for int4 quantization and x_ of type int32, PACKED_PER_VAL is 8.\n    \"\"\"\n    BLOCK_N: 'tl.constexpr' = x_.shape[0]\n    BLOCK_DMODEL_PACKED: 'tl.constexpr' = x_.shape[1]\n    offsets = tl.arange(0, PACKED_PER_VAL) * (32 // PACKED_PER_VAL)\n    quant_offset = x_[:, :, None, :] >> offsets\n    quant_offset = tl.reshape(quant_offset, (BLOCK_N, BLOCK_DMODEL_PACKED *\n        PACKED_PER_VAL))\n    if PACKED_PER_VAL == 4:\n        fp8_type = (tl.float8e4b8 if torch.version.hip is not None else tl.\n            float8e4nv)\n        dequant = quant_offset.to(tl.uint8).to(fp8_type, bitcast=True\n            ) * scale + shift\n    else:\n        quant_offset = (quant_offset & 15).to(tl.uint16)\n        quant_offset = quant_offset * 32768.0\n        scale_512 = scale * 512\n        dequant = quant_offset * scale_512 + shift\n    return dequant\n",
    "category": "Linear Operations",
    "subcategory": "dequantization",
    "uuid": "db9a4f28-efff-4a30-af35-407c519bcb91"
  },
  {
    "input": "@triton.jit\ndef __flat_csr_elmul_compute(CROW_INDICES, stride_crow_n, stride_crow_r,\n    COL_INDICES, stride_col_n, stride_col_z, IN_VALUES, stride_in_n,\n    stride_in_z, OUT_VALUES, stride_out_n, stride_out_z, OTHER,\n    stride_other_n, stride_other_h, stride_other_tdst, stride_other_tsrc, N,\n    H, T_DST, T_SRC, R, Z, MAX_ROW_Z: 'tl.constexpr', BLOCK_R: 'tl.constexpr'):\n    n = tl.program_id(0)\n    ir = tl.program_id(1)\n    ir = ir * BLOCK_R + tl.arange(0, BLOCK_R)\n    ir_mask = ir < R\n    crow_start = tl.load(CROW_INDICES + n * stride_crow_n + ir *\n        stride_crow_r, mask=ir_mask)\n    crow_end = tl.load(CROW_INDICES + n * stride_crow_n + (ir + 1) *\n        stride_crow_r, mask=ir_mask)\n    idx_ht = tl.load(COL_INDICES + n * stride_col_n + (tl.arange(0,\n        MAX_ROW_Z)[None, :] + crow_start[:, None]) * stride_col_z, mask=tl.\n        arange(0, MAX_ROW_Z)[None, :] < crow_end[:, None] - crow_start[:,\n        None] and ir_mask[:, None])\n    idx_heads = idx_ht // T_SRC\n    idx_cols = idx_ht % T_SRC\n    in_values = tl.load(IN_VALUES + n * stride_in_n + (tl.arange(0,\n        MAX_ROW_Z)[None, :] + crow_start[:, None]) * stride_in_z, mask=tl.\n        arange(0, MAX_ROW_Z)[None, :] < crow_end[:, None] - crow_start[:,\n        None] and ir_mask[:, None])\n    other_values = tl.load(OTHER + n * stride_other_n + idx_heads *\n        stride_other_h + ir[:, None] * stride_other_tdst + idx_cols *\n        stride_other_tsrc, mask=tl.arange(0, MAX_ROW_Z)[None, :] < crow_end\n        [:, None] - crow_start[:, None] and ir_mask[:, None])\n    out_values = in_values * other_values\n    tl.store(OUT_VALUES + n * stride_out_n + (tl.arange(0, MAX_ROW_Z)[None,\n        :] + crow_start[:, None]) * stride_out_z, out_values, mask=tl.\n        arange(0, MAX_ROW_Z)[None, :] < crow_end[:, None] - crow_start[:,\n        None] and ir_mask[:, None])\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "625084f8-7651-40ec-99a1-80b934a93662"
  },
  {
    "input": "@triton.jit\ndef _triton_gemm(a_ptr, b_ptr, c_ptr, stride_am, stride_ak, stride_bk,\n    stride_bn, stride_cm, stride_cn, M, N, K, BLOCK_SIZE_M: 'tl.constexpr',\n    BLOCK_SIZE_N: 'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr',\n    GROUP_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = pid // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + pid % group_size_m\n    pid_n = pid % num_pid_in_group // group_size_m\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        a_data = tl.load(a_ptrs, mask=offs_k[None, :] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        b_data = tl.load(b_ptrs, mask=offs_k[:, None] < K - k *\n            BLOCK_SIZE_K, other=0.0)\n        acc += tl.dot(a_data, b_data)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    acc = acc\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + (offs_cm[:, None] * stride_cm + offs_cn[None, :] *\n        stride_cn)\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    tl.store(c_ptrs, acc, mask=c_mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "d631c3af-9f6b-443c-8fb3-068837bab79b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128,\n    'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=\n    init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K',\n    'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])\n@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args[\n    'BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args[\n    'BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args[\n    'BLOCK_HEADDIM']})\n@triton.jit\ndef _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale,\n    stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn,\n    stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm,\n    stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm,\n    stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn,\n    nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim,\n    CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: 'tl.constexpr',\n    IS_CAUSAL: 'tl.constexpr', BLOCK_HEADDIM: 'tl.constexpr',\n    SEQUENCE_PARALLEL: 'tl.constexpr', EVEN_M: 'tl.constexpr', EVEN_N:\n    'tl.constexpr', EVEN_HEADDIM: 'tl.constexpr', BLOCK_M: 'tl.constexpr',\n    BLOCK_N: 'tl.constexpr'):\n    off_hb = tl.program_id(1)\n    off_b = off_hb // nheads\n    off_h = off_hb % nheads\n    Q += off_b * stride_qb + off_h * stride_qh\n    K += off_b * stride_kb + off_h * stride_kh\n    V += off_b * stride_vb + off_h * stride_vh\n    DO += off_b * stride_dob + off_h * stride_doh\n    DQ += off_b * stride_dqb + off_h * stride_dqh\n    DK += off_b * stride_dkb + off_h * stride_dkh\n    DV += off_b * stride_dvb + off_h * stride_dvh\n    if BIAS_TYPE != 'none':\n        Bias += off_b * stride_bb + off_h * stride_bh\n    D += off_hb * seqlen_q_rounded\n    LSE += off_hb * seqlen_q_rounded\n    if not SEQUENCE_PARALLEL:\n        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n        for start_n in range(0, num_block_n):\n            _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK,\n                DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n                stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n                seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=\n                BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n                EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n                BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n    else:\n        start_n = tl.program_id(0)\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV,\n            LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn,\n            stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n            seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=\n            BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM,\n            EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM,\n            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "4634e91f-ecf1-4a53-b7fd-fd6bdd072e48"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'UNROLL_FACTOR': uf}) for uf in [1,\n    2, 4, 8]], key=['POWER_OF_2_MAX_SEQ_LEN', 'QUERY_GROUP_SIZE',\n    'USE_PARTITIONING', 'BLOCK_SIZE', 'HEAD_SIZE', 'PARTITION_SIZE'])\n@triton.jit\ndef _paged_attn_wo_mma_kernel(exp_sums, max_logits, out, q, k_cache,\n    v_cache, scale, block_tables, seq_lens, max_num_blocks_per_seq,\n    alibi_slopes, stride_qm, stride_qn, stride_om, stride_on, stride_ok,\n    stride_km, stride_kn, stride_kk, stride_exp_m, stride_exp_n, BLOCK_SIZE:\n    'tl.constexpr', HEAD_SIZE: 'tl.constexpr', QUERY_GROUP_SIZE:\n    'tl.constexpr', PARTITION_SIZE: 'tl.constexpr', POWER_OF_2_MAX_SEQ_LEN:\n    'tl.constexpr', USE_PARTITIONING: 'tl.constexpr', UNROLL_FACTOR:\n    'tl.constexpr'):\n    head_idx = tl.program_id(axis=0)\n    kv_head_idx = head_idx // QUERY_GROUP_SIZE\n    seq_idx = tl.program_id(axis=1)\n    par_idx = tl.program_id(axis=2)\n    seq_len = tl.load(seq_lens + seq_idx)\n    if par_idx * PARTITION_SIZE >= seq_len:\n        return\n    num_context_blocks = tl.cdiv(seq_len, BLOCK_SIZE)\n    if USE_PARTITIONING:\n        num_blocks_per_par = PARTITION_SIZE // BLOCK_SIZE\n        start_block_idx = par_idx * num_blocks_per_par\n        end_block_idx = tl.minimum(start_block_idx + num_blocks_per_par,\n            num_context_blocks)\n    else:\n        start_block_idx = 0\n        end_block_idx = num_context_blocks\n    if alibi_slopes is None:\n        alibi_slope = 0.0\n    else:\n        alibi_slope = tl.load(alibi_slopes + head_idx)\n    block_offs = tl.arange(0, BLOCK_SIZE)\n    head_size_offs = tl.arange(0, HEAD_SIZE)\n    q = tl.load(q + seq_idx * stride_qm + head_idx * stride_qn + head_size_offs\n        )\n    q = q * scale\n    qkv = tl.zeros([BLOCK_SIZE, HEAD_SIZE], dtype=tl.float32)\n    qk_max = float('-inf')\n    exp_sum = 0.0\n    fp16_0 = tl.zeros([1, 1], dtype=k_cache.dtype.element_ty)\n    base_offs_kv = kv_head_idx * stride_kn + block_offs[:, None\n        ] * stride_kk + head_size_offs[None, :]\n    block_base_ptrs = block_tables + seq_idx * max_num_blocks_per_seq\n    hi_unroll = (end_block_idx - 1) // UNROLL_FACTOR * UNROLL_FACTOR\n    if UNROLL_FACTOR == 1:\n        qkv, qk_max, exp_sum = _inner_paged_attn_unroll_0_kernel(q, k_cache,\n            v_cache, stride_km, block_base_ptrs, base_offs_kv, alibi_slope,\n            block_offs, seq_len, qkv, qk_max, exp_sum, BLOCK_SIZE,\n            start_block_idx, hi_unroll)\n    elif UNROLL_FACTOR == 2:\n        qkv, qk_max, exp_sum = _inner_paged_attn_unroll_2_kernel(q, k_cache,\n            v_cache, stride_km, block_base_ptrs, base_offs_kv, alibi_slope,\n            block_offs, seq_len, qkv, qk_max, exp_sum, BLOCK_SIZE,\n            start_block_idx, hi_unroll)\n    elif UNROLL_FACTOR == 4:\n        qkv, qk_max, exp_sum = _inner_paged_attn_unroll_4_kernel(q, k_cache,\n            v_cache, stride_km, block_base_ptrs, base_offs_kv, alibi_slope,\n            block_offs, seq_len, qkv, qk_max, exp_sum, BLOCK_SIZE,\n            start_block_idx, hi_unroll)\n    elif UNROLL_FACTOR == 8:\n        qkv, qk_max, exp_sum = _inner_paged_attn_unroll_8_kernel(q, k_cache,\n            v_cache, stride_km, block_base_ptrs, base_offs_kv, alibi_slope,\n            block_offs, seq_len, qkv, qk_max, exp_sum, BLOCK_SIZE,\n            start_block_idx, hi_unroll)\n    tl.debug_barrier()\n    for block_idx in range(hi_unroll, end_block_idx):\n        physical_block_idx = tl.load(block_tables + seq_idx *\n            max_num_blocks_per_seq + block_idx)\n        mask = block_offs[:, None] < seq_len - block_idx * BLOCK_SIZE\n        offs_kv = physical_block_idx * stride_km + base_offs_kv\n        k = tl.load(k_cache + offs_kv, mask=mask, other=fp16_0)\n        v = tl.load(v_cache + offs_kv, mask=mask, other=fp16_0)\n        _qk = tl.sum(q[None, :] * k, axis=1)\n        _qk = tl.where(block_offs < seq_len - block_idx * BLOCK_SIZE, _qk,\n            float('-inf'))\n        _qk += alibi_slope * (block_idx * BLOCK_SIZE + block_offs - seq_len + 1\n            )\n        _qk_max = tl.maximum(tl.max(_qk, axis=0), qk_max)\n        _exp_sum = exp_sum * tl.exp(qk_max - _qk_max) + tl.sum(tl.exp(_qk -\n            _qk_max), axis=0)\n        qkv = qkv * (exp_sum * tl.exp(qk_max - _qk_max)) + tl.exp(_qk[:,\n            None] - _qk_max) * v\n        qkv = qkv / _exp_sum\n        qk_max = _qk_max\n        exp_sum = _exp_sum\n    if USE_PARTITIONING:\n        offs_exp = seq_idx * stride_exp_m + head_idx * stride_exp_n + par_idx\n        tl.store(exp_sums + offs_exp, exp_sum)\n        tl.store(max_logits + offs_exp, qk_max)\n    offs_out = (seq_idx * stride_om + head_idx * stride_on + par_idx *\n        stride_ok + head_size_offs)\n    tl.store(out + offs_out, tl.sum(qkv, axis=0))\n",
    "category": "Attention Mechanisms",
    "subcategory": "paged attention",
    "uuid": "0ea3ea77-e816-4b5e-bf29-945f935f1ff2"
  },
  {
    "input": "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})\n@triton.jit\ndef cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr,\n    labels_ptr, smoothing, logit_scale, lse_square_scale, ignored_index,\n    total_classes, class_start_idx, n_cols, n_rows, logits_row_stride,\n    BLOCK_SIZE: 'tl.constexpr', HAS_SMOOTHING: 'tl.constexpr', SPLIT:\n    'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    col_block_idx = tl.program_id(1)\n    logits_ptr = logits_ptr + row_idx * logits_row_stride\n    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    label_idx = tl.load(labels_ptr + row_idx)\n    logits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols,\n        other=-float('inf')) * logit_scale\n    max_logits = tl.max(logits, 0)\n    if HAS_SMOOTHING:\n        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\n    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits\n    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\n    if label_idx == ignored_index:\n        loss = 0.0\n        z_loss = 0.0\n    else:\n        label_idx -= class_start_idx\n        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols,\n            (col_block_idx + 1) * BLOCK_SIZE):\n            logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n            if HAS_SMOOTHING:\n                loss = (lse if not SPLIT else 0.0\n                    ) - smoothing * sum_logits / total_classes - (1 - smoothing\n                    ) * logits_label\n            else:\n                loss = (lse if not SPLIT else 0.0) - logits_label\n        elif HAS_SMOOTHING:\n            loss = smoothing * ((lse if not SPLIT else 0.0) - sum_logits /\n                total_classes)\n        else:\n            loss = 0.0\n        if not SPLIT:\n            z_loss = lse_square_scale * lse * lse\n            loss += z_loss\n        else:\n            z_loss = 0.0\n    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\n    if not SPLIT:\n        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "72f05a8b-48de-402d-a9fd-f712318b4cba"
  },
  {
    "input": "@triton.jit\ndef _triton_gemm_a16w8_per_channel_kernel(A, B, C, scale_b, bias,\n    zero_points, M, N, K, stride_am, stride_ak, stride_bn, stride_bk,\n    stride_cm, stride_cn, stride_zpk, stride_zpn, stride_scalek,\n    stride_scalen, add_bias: 'tl.constexpr', add_zero_points:\n    'tl.constexpr', BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    BLOCK_K: 'tl.constexpr', GROUP_M: 'tl.constexpr', SPLIT_K: 'tl.constexpr'):\n    pid = tl.program_id(0)\n    pid_z = tl.program_id(1)\n    grid_m = tl.cdiv(M, BLOCK_M)\n    grid_n = tl.cdiv(N, BLOCK_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rbn[:, None] * stride_bn + rk[None, :] * stride_bk)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    if add_zero_points:\n        offs_zero_points = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n        zero_points_ptrs = zero_points + offs_zero_points\n        _ZERO_POINT0 = tl.zeros([1], dtype=zero_points.dtype.element_ty)\n        zero_points_vals = tl.load(zero_points_ptrs, mask=offs_zero_points <\n            N, other=_ZERO_POINT0)\n    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):\n        k_remaining = K - k * (BLOCK_K * SPLIT_K)\n        _A0 = tl.zeros((1, 1), dtype=A.dtype.element_ty)\n        a = tl.load(A, mask=rk[None, :] < k_remaining, other=_A0)\n        _B0 = tl.zeros((1, 1), dtype=B.dtype.element_ty)\n        b = tl.load(B, mask=rk[None, :] < k_remaining, other=_B0)\n        if add_zero_points:\n            b = b - zero_points_vals[:, None]\n        b_fp = b\n        b_fp = tl.trans(b_fp)\n        acc += tl.dot(a, b_fp, out_dtype=tl.float32, allow_tf32=True)\n        A += BLOCK_K * SPLIT_K * stride_ak\n        B += BLOCK_K * SPLIT_K * stride_bk\n    offs_scale = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    scale_ptrs = scale_b + offs_scale\n    _SCALE0 = tl.zeros([1], dtype=scale_b.dtype.element_ty)\n    scales = tl.load(scale_ptrs, mask=offs_scale < N, other=_SCALE0)\n    acc *= scales\n    acc = acc\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if add_bias:\n        offs_bias = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n        bias_ptrs = bias + offs_bias\n        _BIAS0 = tl.zeros([1], dtype=bias.dtype.element_ty)\n        bias_vals = tl.load(bias_ptrs, mask=offs_bias < N, other=_BIAS0)\n        if pid_z == 0:\n            acc += bias_vals[None, :]\n    if SPLIT_K == 1:\n        tl.store(C, acc, mask=mask)\n    else:\n        tl.atomic_add(C, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "61c16c5e-3f50-4d9a-9a88-26c17c352421"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_V(q, v, g, h, o, A, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, scale, T: 'tl.constexpr', K:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BK:\n    'tl.constexpr', BV: 'tl.constexpr'):\n    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    for i_k in range(tl.cdiv(K, BK)):\n        p_q = tl.make_block_ptr(q + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (\n            i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_g = tl.load(p_g, boundary_check=(0, 1))\n        b_qg = b_q * tl.exp(b_g)\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        if i_k >= 0:\n            b_o += tl.dot(b_qg, b_h, allow_tf32=False)\n    p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (i_t *\n        BT, i_v * BV), (BT, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT,\n        0), (BT, BT), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_v, allow_tf32=False)\n    tl.store(p_o, b_o, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a3885e51-0f8b-417c-acc2-947115f79d4b"
  },
  {
    "input": "@triton.jit\ndef selu(input):\n    \"\"\"\n    Applies SELU to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by SELU.\n    \"\"\"\n    scale = 1.0507009873554805\n    alpha = 1.6732632423543772\n    return scale * (tl.maximum(0, input) + tl.minimum(0, alpha * (tl.exp(\n        input) - 1)))\n",
    "category": "Activation Functions",
    "subcategory": "silu",
    "uuid": "c4789302-480c-4d18-9d63-59884da9b8f3"
  },
  {
    "input": "@triton.jit\ndef quantize_row_q8_triton(A, M, K, stride_am, stride_ak, stride_qm,\n    stride_qk, stride_sm, Q, S, MCache: 'tl.constexpr', BLOCK_SIZE_M:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    A_Block_ptr = tl.make_block_ptr(base=A, shape=(M, K), block_shape=(\n        BLOCK_SIZE_M, BLOCK_SIZE_K), offsets=(pid_m * BLOCK_SIZE_M, 0),\n        strides=(stride_am, stride_ak), order=(0, 1))\n    Q_Block_ptr = tl.make_block_ptr(base=Q, shape=(M, K), block_shape=(\n        BLOCK_SIZE_M, BLOCK_SIZE_K), offsets=(pid_m * BLOCK_SIZE_M, 0),\n        strides=(stride_qm, stride_qk), order=(0, 1))\n    S_Block_ptr = tl.make_block_ptr(base=S, shape=(M,), block_shape=(\n        BLOCK_SIZE_M,), offsets=(pid_m * BLOCK_SIZE_M,), strides=(stride_sm\n        ,), order=(0,))\n    a = tl.load(A_Block_ptr)\n    scales = tl.max(tl.abs(a), axis=1) / 127.0\n    doted = a * tl.where(scales > 0, 1 / scales, 0)[:, None]\n    quant = trround(doted)\n    tl.store(Q_Block_ptr, quant)\n    tl.store(S_Block_ptr, scales)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "4226d42d-3880-4c99-b644-990180043dbf"
  },
  {
    "input": "@triton.jit\ndef mask_1d(offs, max):\n    return offs < max\n",
    "category": "Helper Functions",
    "subcategory": "masking",
    "uuid": "f9c8d3af-6f7b-4644-bdfd-cb3ff1bdfc90"
  },
  {
    "input": "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])\n@triton.jit\ndef glu_backward_kernel(output_grad_pointer, input1_pointer, input2_pointer,\n    input1_grad_pointer, input2_grad_pointer, size, param, act_func:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    Calculates the input gradient of the gated linear unit.\n\n    Args:\n        output_grad_pointer: Pointer to the unit's output gradients.\n            The output gradients must be contiguous and contain size elements.\n        input1_pointer: Pointer to the first half of the input that was gated.\n            The first half must be contiguous and contain size elements.\n        input2_pointer: Pointer to the second half of the input that was gated.\n            The second half must be contiguous and contain size elements.\n        input1_grad_pointer: Pointer to a container the first half's gradients are written to.\n            The container must be contiguous and contain size elements.\n        input2_grad_pointer: Pointer to a container the second half's gradients are written to.\n            The container must be contiguous and contain size elements.\n        size: Number of elements in each half of the input.\n        param: Parameter in the case of parameterized activation functions.\n        act_func: Name of activation function to apply.\n            Options are 'sigmoid', 'tanh', 'relu', 'gelu', 'silu',\n            'relu6', 'hardsigmoid', 'hardswish', 'selu', 'mish', and 'leaky_relu'.\n        BLOCK_SIZE: Block size.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    mask = offset < size\n    output_grad = tl.load(output_grad_pointer + offset, mask=mask)\n    input1 = tl.load(input1_pointer + offset, mask=mask)\n    input2 = tl.load(input2_pointer + offset, mask=mask)\n    input1_grad = output_grad * apply_act_func(input2, None, None, None,\n        param, act_func, False)\n    input2_grad = output_grad * input1 * apply_act_func_grad(1, input2,\n        None, None, None, param, act_func, False)\n    tl.store(input1_grad_pointer + offset, input1_grad, mask=mask)\n    tl.store(input2_grad_pointer + offset, input2_grad, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "89edb5b2-4c19-4dc6-9592-b42904b05471"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8), triton.Config({}, num_warps=16), triton.Config({},\n    num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL',\n    'IS_RMS_NORM', 'HAS_BIAS'])\n@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})\n@triton.jit\ndef _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL,\n    DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row,\n    stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps,\n    rows_per_program, IS_RMS_NORM: 'tl.constexpr', BLOCK_N: 'tl.constexpr',\n    HAS_DRESIDUAL: 'tl.constexpr', STORE_DRESIDUAL: 'tl.constexpr',\n    HAS_BIAS: 'tl.constexpr', RECOMPUTE_OUTPUT: 'tl.constexpr'):\n    row_block_id = tl.program_id(0)\n    row_start = row_block_id * rows_per_program\n    cols = tl.arange(0, BLOCK_N)\n    mask = cols < N\n    X += row_start * stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += row_start * stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += row_start * stride_dres_in_row\n    DY += row_start * stride_dy_row\n    DX += row_start * stride_dx_row\n    if RECOMPUTE_OUTPUT:\n        Y += row_start * stride_y_row\n    w = tl.load(W + cols, mask=mask)\n    if RECOMPUTE_OUTPUT and HAS_BIAS:\n        b = tl.load(B + cols, mask=mask, other=0.0)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_BIAS:\n        db = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    row_end = min((row_block_id + 1) * rows_per_program, M)\n    for row in range(row_start, row_end):\n        x = tl.load(X + cols, mask=mask, other=0)\n        dy = tl.load(DY + cols, mask=mask, other=0)\n        if not IS_RMS_NORM:\n            mean = tl.load(Mean + row)\n        rstd = tl.load(Rstd + row)\n        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n        xhat = tl.where(mask, xhat, 0.0)\n        if RECOMPUTE_OUTPUT:\n            y = xhat * w + b if HAS_BIAS else xhat * w\n            tl.store(Y + cols, y, mask=mask)\n        wdy = w * dy\n        dw += dy * xhat\n        if HAS_BIAS:\n            db += dy\n        if not IS_RMS_NORM:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            c2 = tl.sum(wdy, axis=0) / N\n            dx = (wdy - (xhat * c1 + c2)) * rstd\n        else:\n            c1 = tl.sum(xhat * wdy, axis=0) / N\n            dx = (wdy - xhat * c1) * rstd\n        if HAS_DRESIDUAL:\n            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0)\n            dx += dres\n        if STORE_DRESIDUAL:\n            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n        tl.store(DX + cols, dx, mask=mask)\n        X += stride_x_row\n        if HAS_DRESIDUAL:\n            DRESIDUAL += stride_dres_row\n        if STORE_DRESIDUAL:\n            DRESIDUAL_IN += stride_dres_in_row\n        if RECOMPUTE_OUTPUT:\n            Y += stride_y_row\n        DY += stride_dy_row\n        DX += stride_dx_row\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\n    if HAS_BIAS:\n        tl.store(DB + row_block_id * N + cols, db, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "eb2c05dc-803e-4671-9131-01a7be183b78"
  },
  {
    "input": "@triton.jit\ndef rotary_kernel(OUT, X, COS, SIN, CU_SEQLENS, SEQLEN_OFFSETS, seqlen,\n    rotary_dim, seqlen_ro, stride_out_batch, stride_out_seqlen,\n    stride_out_nheads, stride_out_headdim, stride_x_batch, stride_x_seqlen,\n    stride_x_nheads, stride_x_headdim, BLOCK_K: 'tl.constexpr',\n    IS_SEQLEN_OFFSETS_TENSOR: 'tl.constexpr', IS_VARLEN: 'tl.constexpr',\n    INTERLEAVED: 'tl.constexpr', CONJUGATE: 'tl.constexpr', BLOCK_M:\n    'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_batch = tl.program_id(axis=1)\n    pid_head = tl.program_id(axis=2)\n    rotary_dim_half = rotary_dim // 2\n    if not IS_VARLEN:\n        X = X + pid_batch * stride_x_batch + pid_head * stride_x_nheads\n        OUT = OUT + pid_batch * stride_out_batch + pid_head * stride_out_nheads\n    else:\n        start_idx = tl.load(CU_SEQLENS + pid_batch)\n        seqlen = tl.load(CU_SEQLENS + pid_batch + 1) - start_idx\n        X = X + start_idx * stride_x_seqlen + pid_head * stride_x_nheads\n        OUT = (OUT + start_idx * stride_out_seqlen + pid_head *\n            stride_out_nheads)\n    if pid_m * BLOCK_M >= seqlen:\n        return\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    if not IS_SEQLEN_OFFSETS_TENSOR:\n        rm_cs = rm + SEQLEN_OFFSETS\n    else:\n        rm_cs = rm + tl.load(SEQLEN_OFFSETS + pid_batch)\n    rk = tl.arange(0, BLOCK_K)\n    rk_half = tl.arange(0, BLOCK_K // 2)\n    if not INTERLEAVED:\n        X = X + (rm[:, None] * stride_x_seqlen + rk_half[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim + rk_half[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_half[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_half[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half), other=0.0)\n        x1 = tl.load(X + rotary_dim_half * stride_x_headdim, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        o0 = x0 * cos - x1 * sin\n        o1 = x0 * sin + x1 * cos\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk_half[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, o0, mask=(rm[:, None] < seqlen) & (rk_half[None, :] <\n            rotary_dim_half))\n        tl.store(OUT + rotary_dim_half * stride_out_headdim, o1, mask=(rm[:,\n            None] < seqlen) & (rk_half[None, :] < rotary_dim_half))\n    else:\n        rk_swap = rk + (rk + 1) % 2 * 2 - 1\n        rk_repeat = tl.arange(0, BLOCK_K) // 2\n        X0 = X + (rm[:, None] * stride_x_seqlen + rk[None, :] *\n            stride_x_headdim)\n        X1 = X + (rm[:, None] * stride_x_seqlen + rk_swap[None, :] *\n            stride_x_headdim)\n        COS = COS + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])\n        SIN = SIN + (rm_cs[:, None] * rotary_dim + rk_repeat[None, :])\n        cos = tl.load(COS, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=1.0)\n        sin = tl.load(SIN, mask=(rm_cs[:, None] < seqlen_ro) & (rk_repeat[\n            None, :] < rotary_dim_half), other=0.0)\n        x0 = tl.load(X0, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim), other=0.0)\n        x1 = tl.load(X1, mask=(rm[:, None] < seqlen) & (rk_swap[None, :] <\n            rotary_dim), other=0.0)\n        if CONJUGATE:\n            sin = -sin\n        x0_cos = x0 * cos\n        x1_sin = x1 * sin\n        out = tl.where(rk[None, :] % 2 == 0, x0_cos - x1_sin, x0_cos + x1_sin)\n        OUT = OUT + (rm[:, None] * stride_out_seqlen + rk[None, :] *\n            stride_out_headdim)\n        tl.store(OUT, out, mask=(rm[:, None] < seqlen) & (rk[None, :] <\n            rotary_dim))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "1fba8883-c454-4824-91f5-e7fb33dc3dab"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])\n@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'\n    ] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not\n    None})\n@triton.jit\ndef chunk_retention_bwd_kernel_dh(q, do, dh, dh0, dht, scale, H:\n    'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT:\n    'tl.constexpr', STORE_INITIAL_STATE_GRADIENT: 'tl.constexpr',\n    USE_FINAL_STATE_GRADIENT: 'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_i = tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1))\n    for i_t in range(NT - 1, -1, -1):\n        if HEAD_FIRST:\n            p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k *\n                BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        else:\n            p_q = tl.make_block_ptr(q + i_b * T * H * K + i_h * K, (K, T),\n                (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K,\n            V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        d_b = tl.math.exp2(min(BT, T - i_t * BT) * b_b)\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, b_do * d_i[:, None], allow_tf32=False)\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dh0 = tl.make_block_ptr(dh0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh0, b_dh, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "bfaa04f6-e317-4c2c-9afb-b31b0ae8b1f4"
  },
  {
    "input": "@triton.jit\ndef chunk_retention_bwd_kernel_dh(q, do, dh, s_qk_h, s_qk_t, s_qk_d, s_vo_h,\n    s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    o_i = tl.arange(0, BT)\n    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh = d_b * b_dh + tl.dot(b_q, b_do * d_i[:, None], allow_tf32=False)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "1eda0b67-899b-4d27-b940-57efc3afee09"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({},\n    num_warps=2), triton.Config({}, num_warps=4), triton.Config({},\n    num_warps=8)], key=['BV', 'BT'])\n@triton.jit\ndef chunk_gla_bwd_kernel_dA(v, do, dA, scale, T: 'tl.constexpr', H:\n    'tl.constexpr', V: 'tl.constexpr', BT: 'tl.constexpr', BV:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr'):\n    i_t, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    b_dA = tl.zeros([BT, BT], dtype=tl.float32)\n    for i_v in range(tl.cdiv(V, BV)):\n        if HEAD_FIRST:\n            p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (\n                i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v *\n                BV, i_t * BT), (BV, BT), (0, 1))\n        else:\n            p_do = tl.make_block_ptr(do + i_b * T * H * V + i_h * V, (T, V),\n                (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n            p_v = tl.make_block_ptr(v + i_b * T * H * V + i_h * V, (V, T),\n                (1, H * V), (i_v * BV, i_t * BT), (BV, BT), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dA += tl.dot(b_do, b_v)\n    if HEAD_FIRST:\n        p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t *\n            BT, 0), (BT, BT), (1, 0))\n    else:\n        p_dA = tl.make_block_ptr(dA + i_b * T * H * BT + i_h * BT, (T, BT),\n            (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\n    b_dA = tl.where(m_s, b_dA * scale, 0.0)\n    tl.store(p_dA, b_dA, boundary_check=(0, 1))\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "69f062fd-f924-44a5-b6f3-66b9a59b3c3a"
  },
  {
    "input": "@triton.jit\ndef chunk_linear_attn_bwd_kernel_dh(q, do, dh, s_qk_h, s_qk_t, s_qk_d,\n    s_vo_h, s_vo_t, s_vo_d, s_h_h, s_h_t, scale, H: 'tl.constexpr', T:\n    'tl.constexpr', K: 'tl.constexpr', V: 'tl.constexpr', BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', NT: 'tl.constexpr'\n    ):\n    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_t in range(NT - 1, -1, -1):\n        p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (K, T), (s_qk_d, s_qk_t),\n            (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_do = tl.make_block_ptr(do + i_bh * s_vo_h, (T, V), (s_vo_t,\n            s_vo_d), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_dh = tl.make_block_ptr(dh + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_dh, b_dh, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "bbdfda4d-f5ce-4f8d-9466-ad7adb95ac4d"
  },
  {
    "input": "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD, key=['BLOCK_DMODEL',\n    'max_seqlen_q', 'max_seqlen_k'])\n@triton.jit\ndef tuned_bwd_kernel_dq(Q, K, V, B, sm_scale, Out, DO, DQ, DB, L, D,\n    stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n    stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n    stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n    stride_om, stride_ok, stride_dqz, stride_dqh, stride_dqm, stride_dqk,\n    stride_dbz, stride_dbh, stride_dbm, stride_dbn, cu_seqlens_q,\n    cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim,\n    dropout_p, philox_seed, philox_offset_base, BLOCK_M: 'tl.constexpr',\n    BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr', CAUSAL:\n    'tl.constexpr', ENABLE_DROPOUT: 'tl.constexpr', PADDED_HEAD:\n    'tl.constexpr', BIAS_TYPE: 'tl.constexpr'):\n    bare_bwd_kernel_dq(Q, K, V, B, sm_scale, Out, DO, DQ, DB, L, D,\n        stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh,\n        stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn,\n        stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh,\n        stride_om, stride_ok, stride_dqz, stride_dqh, stride_dqm,\n        stride_dqk, stride_dbz, stride_dbh, stride_dbm, stride_dbn,\n        cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k,\n        head_dim, dropout_p, philox_seed, philox_offset_base, BLOCK_M,\n        BLOCK_DMODEL, BLOCK_N, CAUSAL, ENABLE_DROPOUT, PADDED_HEAD=\n        PADDED_HEAD, BIAS_TYPE=BIAS_TYPE)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "2635cd34-3285-490a-bc50-c8f8f03a4a67"
  },
  {
    "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, nheads, dim, dstate,\n    nheads_ngroups_ratio, stride_state_batch, stride_state_head,\n    stride_state_dim, stride_state_dstate, stride_x_batch, stride_x_head,\n    stride_x_dim, stride_dt_batch, stride_dt_head, stride_dt_dim,\n    stride_dt_bias_head, stride_dt_bias_dim, stride_A_head, stride_A_dim,\n    stride_A_dstate, stride_B_batch, stride_B_group, stride_B_dstate,\n    stride_C_batch, stride_C_group, stride_C_dstate, stride_D_head,\n    stride_D_dim, stride_z_batch, stride_z_head, stride_z_dim,\n    stride_out_batch, stride_out_head, stride_out_dim, DT_SOFTPLUS:\n    'tl.constexpr', TIE_HDIM: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr',\n    HAS_DT_BIAS: 'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z:\n    'tl.constexpr', BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    pid_h = tl.program_id(axis=2)\n    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head\n    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head\n    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head\n    if HAS_DT_BIAS:\n        dt_bias_ptr += pid_h * stride_dt_bias_head\n    A_ptr += pid_h * stride_A_head\n    B_ptr += (pid_b * stride_B_batch + pid_h // nheads_ngroups_ratio *\n        stride_B_group)\n    C_ptr += (pid_b * stride_C_batch + pid_h // nheads_ngroups_ratio *\n        stride_C_group)\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head\n    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    if HAS_D:\n        D_ptr += pid_h * stride_D_head\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    if not TIE_HDIM:\n        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n        if DT_SOFTPLUS:\n            dt = tl.where(dt <= 20.0, softplus(dt), dt)\n        A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n            dstate), other=0.0)\n        dA = tl.exp(A * dt[:, None])\n    else:\n        dt = tl.load(dt_ptr)\n        if HAS_DT_BIAS:\n            dt += tl.load(dt_bias_ptr)\n        if DT_SOFTPLUS:\n            dt = tl.where(dt <= 20.0, softplus(dt), dt)\n        A = tl.load(A_ptr)\n        dA = tl.exp(A * dt)\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    if not TIE_HDIM:\n        dB = B[None, :] * dt[:, None]\n    else:\n        dB = B * dt\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n",
    "category": "Linear Operations",
    "subcategory": "linear layers",
    "uuid": "05906634-8b00-4ddd-84c8-ff3f2249aabb"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_retention_bwd_kernel(q, k, v, do, dq, dk, dv,\n    initial_state, s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T,\n    scale, BK: 'tl.constexpr', BV: 'tl.constexpr', DK: 'tl.constexpr', DV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    b_b = 1 - tl.math.pow(2, -5 - i_h * 1.0)\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK)\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV)\n    p_dq = dq + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK)\n    mask_bk = i_k * BK + tl.arange(0, BK) < DK\n    mask_bv = i_v * BV + tl.arange(0, BV) < DV\n    h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        mask_kv = mask_bk[:, None] & mask_bv[None, :]\n        p_init_s = initial_state + i_bh * DK * DV + (i_k * BK + tl.arange(0,\n            BK)[:, None]) * DV + (i_v * BV + tl.arange(0, BV)[None, :])\n        h += tl.load(p_init_s, mask=mask_kv, other=0)\n    for i in range(0, T):\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        h = b_b * h + _k[:, None] * _v[None, :]\n        _d_q = h * _do[None, :]\n        d_q = tl.sum(_d_q, axis=1) * scale\n        tl.store(p_dq, d_q, mask=mask_bk)\n        p_k += DK\n        p_do += DV\n        p_v += DV\n        p_dq += DK\n    tl.debug_barrier()\n    p_q = q + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_k = k + i_bh * s_qk_h + i_k * BK + tl.arange(0, BK) + (T - 1) * DK\n    p_do = do + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_v = v + i_bh * s_vo_h + i_v * BV + tl.arange(0, BV) + (T - 1) * DV\n    p_dk = dk + (i_bh + i_v * B * H) * s_qk_h + i_k * BK + tl.arange(0, BK) + (\n        T - 1) * DK\n    p_dv = dv + (i_bh + i_k * B * H) * s_vo_h + i_v * BV + tl.arange(0, BV) + (\n        T - 1) * DV\n    d_h = tl.zeros([BK, BV], dtype=tl.float32)\n    for _ in range(T):\n        _do = tl.load(p_do, mask=mask_bv, other=0)\n        _q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        _k = tl.load(p_k, mask=mask_bk, other=0)\n        _v = tl.load(p_v, mask=mask_bv, other=0)\n        d_h += _q[:, None] * _do[None, :]\n        d_k = tl.sum(d_h * _v[None, :], axis=1)\n        d_v = tl.sum(d_h * _k[:, None], axis=0)\n        d_h *= b_b\n        tl.store(p_dk, d_k, mask=mask_bk)\n        tl.store(p_dv, d_v, mask=mask_bv)\n        p_do -= DV\n        p_q -= DK\n        p_k -= DK\n        p_v -= DV\n        p_dk -= DK\n        p_dv -= DV\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "b75da8d4-a2be-4fc5-9439-173d95e77db8"
  },
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_s(q, k, s, rk, ck, pk, s_qk_h, s_qk_t, s_qk_d,\n    s_sk_h, s_sk_t, s_sk_m, T, scale, BT: 'tl.constexpr', BK:\n    'tl.constexpr', BM: 'tl.constexpr', DK: 'tl.constexpr', DM:\n    'tl.constexpr', NT: 'tl.constexpr'):\n    i_m, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    n_bh = tl.num_programs(2)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_s = tl.make_block_ptr(s + (i_k * n_bh + i_bh) * s_sk_h, (T, DM), (\n        s_sk_t, s_sk_m), (0, i_m * BM), (BT, BM), (1, 0))\n    p_rk = tl.make_block_ptr(rk + i_bh * s_sk_t * NT, (NT * DM,), (s_sk_m,),\n        (i_m * BM,), (BM,), (0,))\n    p_ck = tl.make_block_ptr(ck + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    p_pk = tl.make_block_ptr(pk + i_bh * s_sk_h, (T, DM), (s_sk_t, s_sk_m),\n        (0, i_m * BM), (BT, BM), (1, 0))\n    o_i = tl.arange(0, BT)\n    m_s = o_i[:, None] >= o_i[None, :]\n    b_hk = tl.zeros([BK, BM], dtype=tl.float32)\n    for _ in range(NT):\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_rk = tl.load(p_rk, boundary_check=(0,))\n        b_ck = tl.load(p_ck, boundary_check=(0, 1))\n        b_pk = tl.load(p_pk, boundary_check=(0, 1))\n        b_inter = tl.dot(b_q, b_hk, allow_tf32=False) * b_rk[None, :]\n        b_intra = tl.dot(tl.where(m_s, tl.dot(b_q, b_k, allow_tf32=False), \n            0), b_ck, allow_tf32=False)\n        b_s = (b_inter + b_intra) * b_pk\n        b_hk = b_hk * b_rk[None, :] + tl.dot(b_k, b_ck, allow_tf32=False)\n        tl.store(p_s, b_s, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_s = tl.advance(p_s, (BT, 0))\n        p_rk = tl.advance(p_rk, (DM,))\n        p_ck = tl.advance(p_ck, (BT, 0))\n        p_pk = tl.advance(p_pk, (BT, 0))\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "6857fb36-2cca-4863-abb0-5f827a27d01b"
  },
  {
    "input": "@triton.jit\ndef _bwd_intra_kernel(Q, K, V, S, DO, DQ, DK, DV, b: 'tl.constexpr', h:\n    'tl.constexpr', n: 'tl.constexpr', d: 'tl.constexpr', e: 'tl.constexpr',\n    BLOCK: 'tl.constexpr', NUM_BLOCK: 'tl.constexpr', CBLOCK:\n    'tl.constexpr', NUM_CBLOCK: 'tl.constexpr'):\n    off_bh = tl.program_id(0)\n    off_block = tl.program_id(1)\n    off_h = off_bh % h\n    qk_offset = off_bh * n * d\n    v_offset = off_bh * n * e\n    o_offset = off_bh * n * e\n    block_offset = off_block * BLOCK + tl.arange(0, BLOCK)\n    Q_trans_block_ptr = Q + qk_offset + block_offset[None, :] * d + tl.arange(\n        0, d)[:, None]\n    K_block_ptr = K + qk_offset + block_offset[:, None] * d + tl.arange(0, d)[\n        None, :]\n    V_trans_block_ptr = V + v_offset + block_offset[None, :] * e + tl.arange(\n        0, e)[:, None]\n    DQ_block_ptr = DQ + qk_offset + block_offset[:, None] * d + tl.arange(0, d\n        )[None, :]\n    DK_trans_block_ptr = DK + qk_offset + block_offset[None, :\n        ] * d + tl.arange(0, d)[:, None]\n    DV_block_ptr = DV + v_offset + block_offset[:, None] * e + tl.arange(0, e)[\n        None, :]\n    DO_block_ptr = DO + o_offset + block_offset[:, None] * e + tl.arange(0, e)[\n        None, :]\n    S_block_ptr = S + off_h\n    s = tl.load(S_block_ptr)\n    array = tl.arange(0, BLOCK)\n    index = array[:, None] - array[None, :]\n    s_index = s * index\n    s_index = tl.where(index >= 0, -s_index, float('-inf'))\n    diag_decay = tl.exp(s_index)\n    diag_decay_trans = tl.trans(diag_decay)\n    k = tl.load(K_block_ptr, mask=block_offset[:, None] < n, other=0.0)\n    v_trans = tl.load(V_trans_block_ptr, mask=block_offset[None, :] < n,\n        other=0.0)\n    do = tl.load(DO_block_ptr, mask=block_offset[:, None] < n, other=0.0)\n    q_trans = tl.load(Q_trans_block_ptr, mask=block_offset[None, :] < n,\n        other=0.0)\n    dqk = tl.dot(do, v_trans) * diag_decay\n    dq_intra = tl.dot(dqk, k)\n    dk_intra_trans = tl.dot(q_trans, dqk)\n    qk_trans = tl.dot(k, q_trans) * diag_decay_trans\n    dv_intra = tl.dot(qk_trans, do)\n    dq = dq_intra\n    dk_trans = dk_intra_trans\n    dv = dv_intra\n    tl.store(DQ_block_ptr, dq, mask=block_offset[:, None] < n)\n    tl.store(DK_trans_block_ptr, dk_trans, mask=block_offset[None, :] < n)\n    tl.store(DV_block_ptr, dv, mask=block_offset[:, None] < n)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "ab3bf55c-beeb-4f52-bef7-aa3285223293"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r,\n    'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in\n    itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)],\n    key=['M'])\n@triton.jit\ndef triton_jagged_sum_kernel_simple_fused_sum_then_buffer(input_ptr_values,\n    input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\n    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\n    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\n    block_start_m = pid_m * BLOCK_SIZE_M\n    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\n    mask_m = offsets_m < M\n    ragged_start, ragged_end = tl.load(input_ptr_offsets + pid_ragged\n        ), tl.load(input_ptr_offsets + (pid_ragged + 1))\n    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n        block_start_ragged = ragged_start + block_pos\n        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n        mask_ragged = offsets_ragged < ragged_end\n        idxs = offsets_ragged[:, None] * M + offsets_m\n        mask = mask_ragged[:, None] & mask_m\n        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n        buffer += tl.sum(input, axis=0)\n    buffer_view = buffer.reshape((BLOCK_SIZE_M,))\n    output_offsets = offsets_m + pid_ragged * M\n    output_mask = output_offsets < M * (pid_ragged + 1)\n    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "d7703b28-6a08-4b26-bd73-fbaee50ff25b"
  },
  {
    "input": "@triton.jit\ndef fused_chunk_retention_fwd_kernel(q, k, v, o, initial_state, final_state,\n    s_qk_h, s_qk_t, s_qk_d, s_vo_h, s_vo_t, s_vo_d, B, H, T, scale, BT:\n    'tl.constexpr', BK: 'tl.constexpr', BV: 'tl.constexpr', DK:\n    'tl.constexpr', DV: 'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr',\n    STORE_FINAL_STATE: 'tl.constexpr', CHECK: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    o_i = tl.arange(0, BT)\n    b_b = tl.math.log2(1 - tl.math.pow(2, -5 - i_h * 1.0))\n    d_b, d_o, d_h = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b\n        ), tl.math.exp2((BT - o_i - 1) * b_b)\n    m_s = o_i[:, None] >= o_i[None, :]\n    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    p_q = tl.make_block_ptr(q + i_bh * s_qk_h, (T, DK), (s_qk_t, s_qk_d), (\n        0, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * s_qk_h, (DK, T), (s_qk_d, s_qk_t), (\n        i_k * BK, 0), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * s_vo_h, (T, DV), (s_vo_t, s_vo_d), (\n        0, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * s_vo_h, (T, DV), (\n        s_vo_t, s_vo_d), (0, i_v * BV), (BT, BV), (1, 0))\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(initial_state + i_bh * DK * DV, (DK, DV), (\n            DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n    for i in range(0, tl.cdiv(T, BT)):\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = b_q * scale\n        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n        b_o = tl.dot(b_s, b_v, allow_tf32=False)\n        if CHECK and i == 0:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        else:\n            b_o += tl.dot(b_q, b_h, allow_tf32=False) * d_o[:, None]\n            b_h = d_b * b_h + tl.dot(b_k, b_v * d_h[:, None], allow_tf32=False)\n        tl.store(p_o, b_o, boundary_check=(0, 1))\n        p_q = tl.advance(p_q, (BT, 0))\n        p_k = tl.advance(p_k, (0, BT))\n        p_v = tl.advance(p_v, (BT, 0))\n        p_o = tl.advance(p_o, (BT, 0))\n    if STORE_FINAL_STATE:\n        p_final = tl.make_block_ptr(final_state + i_bh * DK * DV, (DK, DV),\n            (DV, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_final, b_h, boundary_check=(0, 1))\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "8a3567cc-33f4-4155-a9ac-975c20b03c97"
  },
  {
    "input": "@triton.jit\ndef load_1d(ptr, sz: 'const', n, max, stride=1):\n    \"\"\"Chunk 1d vector (defined by ptr) into 1d grid, where each chunk has size sz. Load the nth chunk. Ie, load [n*sz,...,(n+1)*sz-1].\"\"\"\n    offs = offset_1d(sz, n)\n    mask = mask_1d(offs, max)\n    return tl.load(ptr + offs, mask)\n",
    "category": "Data Movement",
    "subcategory": "loading",
    "uuid": "4ffddfa6-0c99-4416-bbc6-9c27a828408b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config\n    ({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32,\n    'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config\n    ({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': \n    128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.\n    Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': \n    256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': \n    32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.\n    Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1},\n    num_stages=5, num_warps=2)] + get_configs_io_bound(), key=[\n    'CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={\n    'early_config_prune': early_config_prune, 'perf_model':\n    estimate_matmul_time, 'top_k': 10})\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] *\n    args['SPLIT_K']) == 0})\n@triton.jit\ndef kernel_bwd(C, ACT_INPUT, A, B, M, N, K, CACHE_KEY_M, CACHE_KEY_N,\n    CACHE_KEY_K, stride_cm, stride_am, stride_ak, stride_bk, stride_bn,\n    BLOCK_M: 'tl.constexpr', GROUP_M: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', BLOCK_K: 'tl.constexpr', SPLIT_K: 'tl.constexpr',\n    EVEN_K: 'tl.constexpr', ACTIVATION: 'tl.constexpr'):\n    \"\"\"\n    Kernel for computing Out = activation(A x W + C)\n    - Input has shape (M, K)\n    - Weight has shape (K, N)\n    - Output has shape (M, N)\n    - ActInputs (optional) has shape (M, N)\n    'ActInputs' optionally saves the A x W + C intermediate for backward computations\n    This kernel will consolidate over K\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    grid_m = (M + BLOCK_M - 1) // BLOCK_M\n    grid_n = (N + BLOCK_N - 1) // BLOCK_N\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)\n    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)\n    rk = tl.arange(0, BLOCK_K)\n    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\n    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n    for k in range(K, 0, -BLOCK_K):\n        if EVEN_K:\n            a = tl.load(A)\n            b = tl.load(B)\n        else:\n            a = tl.load(A, mask=rk[None, :] < k, other=0.0)\n            b = tl.load(B, mask=rk[:, None] < k, other=0.0)\n        acc += tl.dot(a, b)\n        A += BLOCK_K * stride_ak\n        B += BLOCK_K * stride_bk\n    if ACTIVATION != 'id':\n        act_in_ptrs = ACT_INPUT + ram[:, None] * stride_cm + rbn[None, :]\n        act_input = tl.load(act_in_ptrs)\n    if ACTIVATION == 'gelu':\n        acc *= gelu_grad(act_input)\n    elif ACTIVATION == 'gelu_approx':\n        acc *= gelu_approx_grad(act_input)\n    elif ACTIVATION == 'squared_relu':\n        acc *= squared_relu_grad(act_input)\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    C = C + rm[:, None] * stride_cm + rn[None, :]\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    tl.store(C, acc, mask=mask)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "643f56bb-b134-4ec8-be35-ba16d4248a8c"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK': 1024}, num_stages=1,\n    num_warps=1), triton.Config({'BLOCK': 2048}, num_stages=1, num_warps=8),\n    triton.Config({'BLOCK': 4096}, num_stages=1, num_warps=8), triton.\n    Config({'BLOCK': 8192}, num_stages=1, num_warps=16), triton.Config({\n    'BLOCK': 16384}, num_stages=1, num_warps=16)], key=['N',\n    'CLASS_INDICES', 'log_size_logits', 'BUFFER_DTYPE'])\n@triton.jit\ndef _backward(PROBS, IDX, DPROBS, dprob_stride, DIN, weight, N,\n    WEIGHT_BUFFER, smoothing_factor, log_size_logits, WEIGHTS:\n    'tl.constexpr', CLASS_INDICES: 'tl.constexpr', LABEL_SMOOTHING:\n    'tl.constexpr', IGNORE_INDEX: 'tl.constexpr', BUFFER_DTYPE:\n    'tl.constexpr', BLOCK: 'tl.constexpr'):\n    buffer_dtype = _DTYPE2TRITON[BUFFER_DTYPE.value]\n    row = tl.program_id(0)\n    start_n = tl.program_id(1)\n    cols = tl.arange(0, BLOCK)\n    PROBS = PROBS + row * N\n    probs_start = PROBS + cols + BLOCK * start_n\n    probs = -tl.load(probs_start, mask=cols < N - start_n * BLOCK, other=\n        float('inf'))\n    DIN = DIN + row * N + cols + BLOCK * start_n\n    dout = tl.load(DPROBS + row * dprob_stride)\n    if CLASS_INDICES:\n        idx = tl.load(IDX + row)\n        delta = start_n * BLOCK + cols == idx\n        if IGNORE_INDEX >= 0:\n            use_class = idx == IGNORE_INDEX\n            dout = dout * (1 - use_class)\n        if LABEL_SMOOTHING:\n            if WEIGHTS:\n                weight_ptr = weight + cols + BLOCK * start_n\n                full_weights_val = tl.load(weight_ptr, mask=cols < N - \n                    start_n * BLOCK, other=0.0)\n                weights_val = tl.load(weight + idx)\n                probs = probs / full_weights_val\n            probs = tl.exp(probs)\n            if WEIGHTS:\n                weights_total = tl.load(WEIGHT_BUFFER + row)\n                numerator_contrib = weights_val * (1.0 - smoothing_factor) * (\n                    probs - delta)\n                mean_contrib = (weights_total * probs - full_weights_val\n                    ) * smoothing_factor / N\n            else:\n                numerator_contrib = (1.0 - smoothing_factor) * (probs - delta)\n                mean_contrib = smoothing_factor * probs - smoothing_factor / N\n            din = (numerator_contrib + mean_contrib) * dout\n        else:\n            probs = tl.exp(probs)\n            din = (probs - delta) * dout\n            if WEIGHTS:\n                weight_ptr = weight + idx\n                weights_val = tl.load(weight_ptr)\n                din = weights_val * din\n    else:\n        idx = tl.load(IDX + row * N + cols + BLOCK * start_n, mask=cols < N -\n            start_n * BLOCK, other=0.0)\n        full_weights_val = (1.0 - smoothing_factor\n            ) * idx + smoothing_factor / N\n        weights_total = tl.load(WEIGHT_BUFFER + row)\n        if WEIGHTS:\n            weight_ptr = weight + cols + BLOCK * start_n\n            weights_val = tl.load(weight_ptr, mask=cols < N - start_n *\n                BLOCK, other=0.0)\n            full_weights_val = weights_val * full_weights_val\n        probs = probs / full_weights_val\n        probs = tl.exp(probs)\n        weighted_probs = probs * weights_total\n        weighted_probs_per_class = weighted_probs - full_weights_val\n        din = weighted_probs_per_class * dout\n    tl.store(DIN, din, mask=cols + BLOCK * start_n < N)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "f46b14f9-f0c1-4639-9fde-b3bbcd66d58b"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.\n    Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8),\n    triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32},\n    num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({\n    'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton\n    .Config({'BT': 64}, num_warps=8)], key=['S'])\n@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})\n@triton.jit\ndef chunk_global_cumsum_vector_kernel(s, z, offsets, T: 'tl.constexpr', H:\n    'tl.constexpr', S: 'tl.constexpr', BT: 'tl.constexpr', BS:\n    'tl.constexpr', HEAD_FIRST: 'tl.constexpr', USE_OFFSETS: 'tl.constexpr'):\n    i_s, i_bh = tl.program_id(0), tl.program_id(1)\n    i_b, i_h = i_bh // H, i_bh % H\n    if USE_OFFSETS:\n        start, end = tl.load(offsets + i_b), tl.load(offsets + i_b + 1)\n    else:\n        start, end = i_b * T, i_b * T + T\n    T = end - start\n    o_i = tl.arange(0, BT)\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\n    b_z = tl.zeros([BS], dtype=tl.float32)\n    for i_t in range(tl.cdiv(T, BT)):\n        if HEAD_FIRST:\n            p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + i_bh * T * S, (T, S), (S, 1), (i_t *\n                BT, i_s * BS), (BT, BS), (1, 0))\n        else:\n            p_s = tl.make_block_ptr(s + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n            p_z = tl.make_block_ptr(z + start * H * S + i_h * S, (T, S), (H *\n                S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        b_s = tl.load(p_s, boundary_check=(0, 1))\n        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n        tl.store(p_z, b_c, boundary_check=(0, 1))\n        if i_t >= 0:\n            b_z += tl.sum(b_s, 0)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "0d48035c-c852-4cac-aea0-4dd9d8499ff4"
  },
  {
    "input": "@triton.jit\ndef store_2d(vals, ptr, sz0: 'const', sz1: 'const', n0, n1, max0, max1,\n    stride0=None, stride1=1):\n    \"\"\"Store 2d block into (n0,n1)th chunk of matrix (defined by ptr), where each chunk has size (sz0, sz1)\"\"\"\n    stride0 = stride0 or sz1\n    offs0 = offset_1d(sz0, n0)\n    offs1 = offset_1d(sz1, n1)\n    offs = offset_2d(offs0, offs1, stride0, stride1)\n    mask = mask_2d(offs0, offs1, max0, max1)\n    tl.store(ptr + offs, vals, mask)\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "c909db08-46fe-4e06-b449-09ecc93efcb4"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_compute_ws(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v,\n    desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz,\n    stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk,\n    stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX,\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', HEAD_DIM:\n    'tl.constexpr', STAGE: 'tl.constexpr', ENABLE_TMA: 'tl.constexpr',\n    LOOP_SCHEDULE: 'tl.constexpr'):\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n    qvk_offset = off_z * stride_qz + off_h * stride_qh\n    K_block_ptr = None\n    V_block_ptr = None\n    Q_block_ptr = None\n    O_block_ptr = None\n    if not ENABLE_TMA:\n        Q_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX,\n            HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m *\n            BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n        v_order: 'tl.constexpr' = (0, 1\n            ) if V.dtype.element_ty == tl.float8e5 else (1, 0)\n        V_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX,\n            HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0),\n            block_shape=(BLOCK_N, HEAD_DIM), order=v_order)\n        K_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(\n            HEAD_DIM, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0\n            ), block_shape=(HEAD_DIM, BLOCK_N), order=(0, 1))\n        O_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX,\n            HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m *\n            BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n    qk_scale = sm_scale\n    qk_scale *= 1.44269504\n    with tl.async_task([0]):\n        if ENABLE_TMA:\n            q = tl._experimental_descriptor_load(desc_q, [qvk_offset //\n                stride_qm + start_m * BLOCK_M, 0], [BLOCK_M, HEAD_DIM], Q.\n                dtype.element_ty)\n        else:\n            q = tl.load(Q_block_ptr)\n    if STAGE & 1:\n        acc, l_i, m_i = _attn_fwd_inner_ws(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, desc_k, desc_v, Q, qvk_offset, stride_kn,\n            stride_vn, stride_vk, start_m, qk_scale, BLOCK_M, HEAD_DIM,\n            BLOCK_N, 4 - STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty ==\n            tl.float8e5, ENABLE_TMA, LOOP_SCHEDULE)\n    if STAGE & 2:\n        acc, l_i, m_i = _attn_fwd_inner_ws(acc, l_i, m_i, q, K_block_ptr,\n            V_block_ptr, desc_k, desc_v, Q, qvk_offset, stride_kn,\n            stride_vn, stride_vk, start_m, qk_scale, BLOCK_M, HEAD_DIM,\n            BLOCK_N, 2, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.\n            float8e5, ENABLE_TMA, LOOP_SCHEDULE)\n    with tl.async_task([1, 2]):\n        m_i += tl.math.log2(l_i)\n        acc = acc / l_i[:, None]\n        m_ptrs = M + off_hz * N_CTX + offs_m\n        tl.store(m_ptrs, m_i)\n        if ENABLE_TMA:\n            tl._experimental_descriptor_store(desc_o, acc, [qvk_offset //\n                stride_om + start_m * BLOCK_M, 0])\n        else:\n            tl.store(O_block_ptr, acc)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "ff6c2a96-3b06-42b8-b2fc-65513e69d7c4"
  },
  {
    "input": "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not\n    None})\n@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})\n@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})\n@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.\n    next_power_of_2(args['dstate'])})\n@triton.jit\ndef _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr,\n    A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, dim, dstate,\n    stride_state_batch, stride_state_dim, stride_state_dstate,\n    stride_x_batch, stride_x_dim, stride_dt_batch, stride_dt_dim,\n    stride_dt_bias_dim, stride_A_dim, stride_A_dstate, stride_B_batch,\n    stride_B_dstate, stride_C_batch, stride_C_dstate, stride_D_dim,\n    stride_z_batch, stride_z_dim, stride_out_batch, stride_out_dim,\n    DT_SOFTPLUS: 'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', HAS_DT_BIAS:\n    'tl.constexpr', HAS_D: 'tl.constexpr', HAS_Z: 'tl.constexpr',\n    BLOCK_SIZE_DSTATE: 'tl.constexpr'):\n    pid_m = tl.program_id(axis=0)\n    pid_b = tl.program_id(axis=1)\n    state_ptr += pid_b * stride_state_batch\n    x_ptr += pid_b * stride_x_batch\n    dt_ptr += pid_b * stride_dt_batch\n    B_ptr += pid_b * stride_B_batch\n    C_ptr += pid_b * stride_C_batch\n    if HAS_Z:\n        z_ptr += pid_b * stride_z_batch\n    out_ptr += pid_b * stride_out_batch\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\n    state_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[\n        None, :] * stride_state_dstate)\n    x_ptrs = x_ptr + offs_m * stride_x_dim\n    dt_ptrs = dt_ptr + offs_m * stride_dt_dim\n    if HAS_DT_BIAS:\n        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\n    A_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] *\n        stride_A_dstate)\n    B_ptrs = B_ptr + offs_n * stride_B_dstate\n    C_ptrs = C_ptr + offs_n * stride_C_dstate\n    if HAS_D:\n        D_ptrs = D_ptr + offs_m * stride_D_dim\n    if HAS_Z:\n        z_ptrs = z_ptr + offs_m * stride_z_dim\n    out_ptrs = out_ptr + offs_m * stride_out_dim\n    state = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate), other=0.0)\n    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0)\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0)\n    if DT_SOFTPLUS:\n        dt = tl.log(1.0 + tl.exp(dt))\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] <\n        dstate), other=0.0)\n    dA = tl.exp(A * dt[:, None])\n    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0)\n    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0)\n    if HAS_D:\n        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0)\n    if HAS_Z:\n        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0)\n    dB = B[None, :] * dt[:, None]\n    state = state * dA + dB * x[:, None]\n    tl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None,\n        :] < dstate))\n    out = tl.sum(state * C[None, :], axis=1)\n    if HAS_D:\n        out += x * D\n    if HAS_Z:\n        out *= z * tl.sigmoid(z)\n    tl.store(out_ptrs, out, mask=offs_m < dim)\n",
    "category": "Helper Functions",
    "subcategory": "type conversion",
    "uuid": "b68aa6bd-f1f1-4013-851a-e66abe567d0d"
  },
  {
    "input": "@triton.jit\ndef tl_and_reduce_fn(a, b):\n    return a & b\n",
    "category": "Math Utils",
    "subcategory": "",
    "uuid": "4c624ddd-3492-48db-8e33-e7963d5a8ba8"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N':\n    32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=2), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages\n    =5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32,\n    'BLOCK_SIZE_K': 64}, num_stages=6, num_warps=2), triton.Config({\n    'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128},\n    num_stages=4, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32,\n    'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=5, num_warps=2),\n    triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': \n    128}, num_stages=6, num_warps=2)], key=['M', 'N', 'K', 'PK'])\n@triton.jit\ndef _matmul_partition_k(a_ptr, b_ptr, c_buf_ptr, M, N, K, PK, PK_SIZE,\n    stride_am, stride_ak, stride_bk, stride_bn, stride_cb_m, stride_cb_n,\n    stride_cb_k, BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr',\n    BLOCK_SIZE_K: 'tl.constexpr'):\n    \"\"\"Kernel for computing the matmul C = A x B.\n    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n    \"\"\"\n    pid_m = tl.program_id(axis=0)\n    pid_n = tl.program_id(axis=1)\n    pid_pk = tl.program_id(axis=2)\n    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    offs_k = (pid_pk * PK_SIZE + tl.arange(0, BLOCK_SIZE_K)) % K\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] *\n        stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] *\n        stride_bn)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(PK_SIZE, BLOCK_SIZE_K)):\n        a = tl.load(a_ptrs)\n        b = tl.load(b_ptrs)\n        accumulator += tl.dot(a, b)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\n    acc = accumulator\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    offs_ck = pid_pk\n    c_buf_ptrs = c_buf_ptr + stride_cb_m * offs_cm[:, None, None\n        ] + stride_cb_n * offs_cn[None, :, None] + stride_cb_k * offs_ck[\n        None, None, :]\n    tl.store(c_buf_ptrs, acc[:, :, None])\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "3ac64656-d28d-40fb-9d8a-2e0de9f253a5"
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel32(q, k, v, w, u, o, h0, ht, s_k_h,\n    s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr',\n    K: 'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "fc01a7a9-25f3-43ad-8234-074a8284c86a"
  },
  {
    "input": "@triton.jit\ndef _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m,\n    qk_scale, BLOCK_M: 'tl.constexpr', HEAD_DIM: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr', STAGE: 'tl.constexpr', offs_m: 'tl.constexpr', offs_n:\n    'tl.constexpr', N_CTX: 'tl.constexpr'):\n    if STAGE == 1:\n        lo, hi = 0, start_m * BLOCK_M\n    elif STAGE == 2:\n        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n        lo = tl.multiple_of(lo, BLOCK_M)\n    else:\n        lo, hi = 0, N_CTX\n    K_block_ptr = tl.advance(K_block_ptr, (0, lo))\n    V_block_ptr = tl.advance(V_block_ptr, (lo, 0))\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        k = tl.load(K_block_ptr)\n        qk = tl.dot(q, k)\n        if STAGE == 2:\n            mask = offs_m[:, None] >= start_n + offs_n[None, :]\n            qk = qk * qk_scale + tl.where(mask, 0, -1000000.0)\n            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n            qk -= m_ij[:, None]\n        else:\n            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n            qk = qk * qk_scale - m_ij[:, None]\n        p = tl.math.exp2(qk)\n        l_ij = tl.sum(p, 1)\n        alpha = tl.math.exp2(m_i - m_ij)\n        l_i = l_i * alpha + l_ij\n        acc = acc * alpha[:, None]\n        v = tl.load(V_block_ptr)\n        p = p\n        acc = tl.dot(p, v, acc)\n        m_i = m_ij\n        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\n        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    return acc, l_i, m_i\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "54141078-249c-4ee6-9d70-312cf7f7fb4f"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': b_n,\n    'BLOCK_SIZE_K': b_k}, num_warps=w) for b_n, b_k, w in itertools.product\n    ([(4 ** n) for n in range(7)], [(4 ** n) for n in range(4)], [2, 4, 8])\n    ], key=['N'])\n@triton.jit\ndef triton_sum_kernel_2D_result_dim_1_buffer_then_sum(input_ptr, output_ptr,\n    M: 'tl.constexpr', N: 'tl.constexpr', K: 'tl.constexpr', BLOCK_SIZE_N:\n    'tl.constexpr', BLOCK_SIZE_K: 'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\n    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\n    buffer = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_K), dtype=tl.float32)\n    block_start_k = pid_k * BLOCK_SIZE_K\n    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\n    mask_k = offsets_k < K\n    for block_start_n in range(0, N, BLOCK_SIZE_N):\n        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n        mask_n = offsets_n < N\n        idxs_base = offsets_n[:, None] * K + offsets_k\n        idxs = idxs_base + pid_m * N * K\n        mask = mask_n[:, None] & mask_k\n        input = tl.load(input_ptr + idxs, mask=mask, other=0)\n        buffer += input\n    output = tl.sum(buffer, axis=0)\n    output_offsets = pid_m * K + offsets_k\n    tl.store(output_ptr + output_offsets, output, mask=mask_k)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "0cda86a5-c250-4479-b1f7-c2ede1ba10b2"
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd_fused(Out, A, Weight, Bias, Mean, Rstd, stride, N, eps,\n    BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Out += row * stride\n    A += row * stride\n    mean = 0\n    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0)\n        _mean += a\n    mean = tl.sum(_mean, axis=0) / N\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        a = tl.load(A + cols, mask=cols < N, other=0.0)\n        a = tl.where(cols < N, a - mean, 0.0)\n        _var += a * a\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    tl.store(Mean + row, mean)\n    tl.store(Rstd + row, rstd)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        weight = tl.load(Weight + cols, mask=mask)\n        bias = tl.load(Bias + cols, mask=mask)\n        a = tl.load(A + cols, mask=mask, other=0.0)\n        a_hat = (a - mean) * rstd\n        out = a_hat * weight + bias\n        tl.store(Out + cols, out, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "layer norm",
    "uuid": "b115cdd2-ebef-4233-a8f2-df8bd739353a"
  },
  {
    "input": "@triton.jit\ndef _split_noncontiguous_block(pid_k, pid_n, input, input_slices,\n    input_tiles, grad_output, grad_other, grad_other_tiles, stride_input_m,\n    stride_input_k, stride_grad_output_m, stride_grad_output_n,\n    stride_grad_other_b, stride_grad_other_k, stride_grad_other_n, K, N,\n    next_id, next_next_id, out_dtype: 'tl.constexpr', BLOCK_SIZE:\n    'tl.constexpr', NUM_TILES: 'tl.constexpr', TILE_K: 'tl.constexpr',\n    TILE_N: 'tl.constexpr', TILE_M: 'tl.constexpr', EVEN_K: 'tl.constexpr',\n    EVEN_N: 'tl.constexpr', DETERMINISTIC: 'tl.constexpr'):\n    for _ in range(0, BLOCK_SIZE):\n        if next_id < NUM_TILES and next_id != -1:\n            start_off = tl.load(input_tiles + 5 * next_id + 2)\n            end_off = tl.load(input_tiles + 5 * next_id + 3)\n            length = end_off - start_off\n            if length > 0:\n                type_id = tl.load(input_tiles + 5 * next_id + 1)\n                slice_id = tl.load(input_tiles + 5 * next_id + 0)\n                slice_start = tl.load(input_slices + 5 * slice_id + 2)\n                slice_end = tl.load(input_slices + 5 * slice_id + 3)\n                M = slice_end - slice_start\n                _split_dispatch(pid_k, pid_n, next_id, input + start_off *\n                    stride_input_m, grad_output + start_off *\n                    stride_grad_output_m, grad_other + type_id *\n                    stride_grad_other_b, grad_other_tiles, stride_input_m,\n                    stride_input_k, stride_grad_output_m,\n                    stride_grad_output_n, stride_grad_other_b,\n                    stride_grad_other_k, stride_grad_other_n, K, N, M,\n                    length, out_dtype=out_dtype, BLOCK_LENGTH=TILE_M *\n                    BLOCK_SIZE, TILE_K=TILE_K, TILE_N=TILE_N, TILE_M=TILE_M,\n                    EVEN_K=EVEN_K, EVEN_N=EVEN_N, EVEN_M=False,\n                    DYNAMIC_TILING=True, DETERMINISTIC=DETERMINISTIC)\n            next_id = next_next_id\n            next_next_id += 1\n",
    "category": "Data Movement",
    "subcategory": "broadcasting",
    "uuid": "fd1c5c5e-9429-41fd-9e22-e90fdd2a0baf"
  },
  {
    "input": "@triton.jit\ndef _rms_norm_forward_kernel(Y_ptr, Y_row_stride, X_ptr, X_row_stride,\n    W_ptr, W_row_stride, RSTD_ptr, RSTD_row_stride, n_cols, eps, offset,\n    casting_mode: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    \"\"\"\n    y_i = (x_i / (RMS)) * (offset + wi), RMS = sqrt(sum(x_i^2) / N)\n\n    Reference:\n    1. https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n    2. https://github.com/unslothai/unsloth/blob/fd753fed99ed5f10ef8a9b7139588d9de9ddecfb/unsloth/kernels/rms_layernorm.py#L22\n    3. https://arxiv.org/pdf/1910.07467\n    \"\"\"\n    row_idx = tl.program_id(0)\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    Y_ptr += row_idx * Y_row_stride\n    X_ptr += row_idx * X_row_stride\n    RSTD_ptr += row_idx * RSTD_row_stride\n    X_row = tl.load(X_ptr + col_offsets, mask=mask, other=0)\n    X_row_dtype = X_row.dtype\n    W_row = tl.load(W_ptr + col_offsets, mask=mask, other=0)\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row\n    if casting_mode == _CASTING_MODE_GEMMA:\n        W_row = W_row\n        X_row = X_row\n    if casting_mode == _CASTING_MODE_NONE:\n        eps = eps\n        offset = offset\n    mean_square = tl.sum(X_row * X_row, axis=0) / n_cols\n    rstd = rsqrt(mean_square + eps)\n    tl.store(RSTD_ptr, rstd)\n    X_row = X_row * rstd\n    if casting_mode == _CASTING_MODE_LLAMA:\n        X_row = X_row\n    Y_row = X_row * (offset + W_row)\n    if casting_mode == _CASTING_MODE_GEMMA:\n        Y_row = Y_row\n    tl.store(Y_ptr + col_offsets, Y_row, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "rms norm",
    "uuid": "756c23aa-a2e5-4668-a5b9-af4c5a58823f"
  },
  {
    "input": "@triton.jit\ndef _geglu_tanh_backward_kernel(dc, a, b, stride, n_cols: 'tl.constexpr',\n    BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc += program_id * stride\n    a += program_id * stride\n    b += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b + col_offsets, mask=mask, other=0)\n    sqrt_2_over_pi = 0.7978845608028654\n    a_cubed = a_row * a_row * a_row\n    tanh_arg = sqrt_2_over_pi * (a_row + 0.044715 * a_cubed)\n    tanh_result = tanh(tanh_arg)\n    geglu_a = 0.5 * a_row * (1 + tanh_result)\n    db_row = dc_row * geglu_a\n    term1 = 0.5 * (1 + tanh_result)\n    tanh_sq = tanh_result * tanh_result\n    term2 = 0.5 * a_row * (1 - tanh_sq) * (sqrt_2_over_pi * (1 + 3 * \n        0.044715 * a_row * a_row))\n    da_row = dc_row * b_row * (term1 + term2)\n    tl.store(a + col_offsets, da_row, mask=mask)\n    tl.store(b + col_offsets, db_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "e359efef-cea0-4506-920b-789c3c871d28"
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.\n    Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}),\n    triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])\n@triton.jit\ndef _chunk_scan_bwd_ddAcs_unstable_kernel(dout_ptr, out_ptr, dt_ptr,\n    ddt_ptr, x_ptr, D_ptr, ddA_cumsum_ptr, dD_ptr, chunk_size, hdim, batch,\n    seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head,\n    stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head,\n    stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head,\n    stride_dt_csize, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head,\n    stride_ddt_csize, stride_x_batch, stride_x_seqlen, stride_x_head,\n    stride_x_hdim, stride_D_head, stride_ddA_cs_batch, stride_ddA_cs_chunk,\n    stride_ddA_cs_head, stride_ddA_cs_csize, stride_dD_batch,\n    stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D:\n    'tl.constexpr', D_HAS_HDIM: 'tl.constexpr', SUBTRACT_DDTDT:\n    'tl.constexpr', BLOCK_SIZE_M: 'tl.constexpr', BLOCK_SIZE_N: 'tl.constexpr'\n    ):\n    pid_bc = tl.program_id(axis=1)\n    pid_c = pid_bc // batch\n    pid_b = pid_bc - pid_c * batch\n    pid_h = tl.program_id(axis=2)\n    pid_m = tl.program_id(axis=0)\n    dout_ptr += (pid_b * stride_dout_batch + pid_c * chunk_size *\n        stride_dout_seqlen + pid_h * stride_dout_head)\n    out_ptr += (pid_b * stride_out_batch + pid_c * chunk_size *\n        stride_out_seqlen + pid_h * stride_out_head)\n    dt_ptr += (pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h *\n        stride_dt_head)\n    ddt_ptr += (pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h *\n        stride_ddt_head)\n    ddA_cumsum_ptr += (pid_b * stride_ddA_cs_batch + pid_c *\n        stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head)\n    if HAS_D:\n        x_ptr += (pid_b * stride_x_batch + pid_c * chunk_size *\n            stride_x_seqlen + pid_h * stride_x_head)\n        dD_ptr += (pid_b * stride_dD_batch + pid_c * stride_dD_chunk + \n            pid_h * stride_dD_head + pid_m * stride_dD_csize)\n    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_n = tl.arange(0, BLOCK_SIZE_N)\n    dout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[\n        None, :] * stride_dout_hdim)\n    out_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None,\n        :] * stride_out_hdim)\n    if HAS_D:\n        x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None,\n            :] * stride_x_hdim)\n        if D_HAS_HDIM:\n            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    out = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n        offs_n[None, :] < hdim), other=0.0)\n    if HAS_D:\n        x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (\n            offs_n[None, :] < hdim), other=0.0)\n        if D_HAS_HDIM:\n            dD = tl.sum(dout * x, axis=0)\n            tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n <\n                hdim, other=0.0)\n        else:\n            dD = tl.sum(dout * x)\n            tl.store(dD_ptr, dD)\n            D = tl.load(D_ptr + pid_h * stride_D_head)\n        out -= x * D\n    ddA_cs = tl.sum(dout * out, axis=1)\n    if SUBTRACT_DDTDT:\n        dt = tl.load(dt_ptr + offs_m * stride_dt_csize, mask=offs_m <\n            chunk_size, other=0.0)\n        ddt = tl.load(ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m <\n            chunk_size, other=0.0)\n        ddA_cs -= dt * ddt\n    tl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=\n        offs_m < chunk_size)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "d86643a0-a82b-438d-a2b4-b0ac3991d6ab"
  },
  {
    "input": "@triton.jit\ndef quant_kernel(src_ptr, stride_srcb, stride_srcm, stride_srcn, dst_ptr,\n    stride_dstb, stride_dstm, stride_dstn, output_scale, B, M:\n    'tl.constexpr', N: 'tl.constexpr', np2_M: 'tl.constexpr', np2_N:\n    'tl.constexpr'):\n    \"\"\"\n    quant fp16 tensor to int4\n    \"\"\"\n    batch_id = tl.program_id(axis=0) + tl.program_id(axis=1) * tl.num_programs(\n        axis=0)\n    index_rows = tl.arange(0, np2_M)\n    index_cols = tl.arange(0, np2_N)\n    src_ptrs = src_ptr + batch_id * stride_srcb + index_rows[:, None\n        ] * stride_srcm + index_cols[None, :] * stride_srcn\n    src_mask = (index_rows[:, None] < M) & (index_cols[None, :] < N)\n    src = tl.load(src_ptrs, mask=src_mask, other=0.0)\n    abs_src_val = tl.abs(src)\n    max_src_val = tl.max(abs_src_val)\n    scale = max_src_val / 7.0\n    quant_val = libdevice.llrint(src / scale)\n    quant_val = max(-8, min(quant_val, 7))\n    quant_val = quant_val.reshape(np2_M, np2_N // 2, 2, can_reorder=False)\n    quant_val_even, quant_val_odd = quant_val.split()\n    quant_val_odd = quant_val_odd << 4\n    res = tl.zeros((np2_M, np2_N // 2), dtype=tl.uint8)\n    res = res | quant_val_odd & 240\n    res = res | quant_val_even & 15\n    offs_resm = tl.arange(0, np2_M)\n    offs_resn = tl.arange(0, np2_N // 2)\n    dst_ptrs = dst_ptr + stride_dstb * batch_id + stride_dstm * offs_resm[:,\n        None] + stride_dstn * offs_resn[None, :]\n    res_mask = (offs_resm[:, None] < M) & (offs_resn[None, :] < N // 2)\n    tl.store(dst_ptrs, res, mask=res_mask)\n    tl.store(output_scale + batch_id, scale)\n",
    "category": "Linear Operations",
    "subcategory": "quantization",
    "uuid": "2869a05f-a907-4967-8878-6035b2e1c223"
  },
  {
    "input": "@triton.jit\ndef fwd_inner_chunk(q, k, g, A, s_k_h, s_k_t, s_k_d, scale, B:\n    'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr', K: 'tl.constexpr',\n    BT: 'tl.constexpr', BK: 'tl.constexpr'):\n    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    p_k = tl.make_block_ptr(k + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_g = tl.make_block_ptr(g + i_bh * s_k_h, (T, K), (s_k_t, s_k_d), (i_t *\n        BT, i_k * BK), (BT, BK), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    mask = i_k * BK + tl.arange(0, BK) < K\n    o_i = tl.arange(0, BT)\n    p_q = q + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_gq = g + i_bh * s_k_h + i_k * BK + i_t * BT * K + tl.arange(0, BK)\n    p_A = A + (i_bh + i_k * B * H) * (tl.cdiv(T, BT) * BT * BT\n        ) + i_t * BT * BT + tl.arange(0, BT)\n    for i in range(BT):\n        _q = tl.load(p_q, mask=mask, other=0) * scale\n        gq = tl.load(p_gq, mask=mask, other=0)\n        s = _q[None, :] * b_k * tl.exp(gq[None, :] - b_g)\n        score = tl.sum(s, axis=1)\n        score = tl.where(o_i <= i, score, 0)\n        tl.store(p_A, score)\n        p_q += K\n        p_gq += K\n        p_A += BT\n",
    "category": "Attention Mechanisms",
    "subcategory": "flash attention",
    "uuid": "5c56a733-8590-4433-a2d0-74336bc87fe9"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "57a66f46-575c-4897-9bcf-ca21e1122f77"
  },
  {
    "input": "@triton.autotune(configs=_generate_configs(), key=['N', 'K',\n    'stddev_tile_size_m', 'avg_tile_size_m'], prune_configs_by={\n    'early_config_prune': functools.partial(_early_config_prune, is_weight=\n    False)}, rep=10, use_cuda_graph=True)\n@triton.heuristics({'EVEN_K': lambda args: args['K'] % args['TILE_SIZE_K'] ==\n    0, 'EVEN_N': lambda args: args['N'] % args['TILE_SIZE_N'] == 0,\n    'EQUAL_K': lambda args: args['K'] == args['TILE_SIZE_K']})\n@triton.jit\ndef segment_matmul_kernel(input, input_tiles, other, output, K, N,\n    stride_input_m, stride_input_k, stride_other_b, stride_other_k,\n    stride_other_n, stride_output_m, stride_output_n, stddev_tile_size_m,\n    avg_tile_size_m, out_dtype: 'tl.constexpr', NUM_TILES: 'tl.constexpr',\n    NUM_BLOCKS: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr', TILE_SIZE_M:\n    'tl.constexpr', EVEN_K: 'tl.constexpr', EVEN_N: 'tl.constexpr', EQUAL_K:\n    'tl.constexpr', TILE_SIZE_N: 'tl.constexpr', TILE_SIZE_K: 'tl.constexpr'):\n    TILE_N: 'tl.constexpr' = TILE_SIZE_N\n    TILE_K: 'tl.constexpr' = TILE_SIZE_K\n    TILE_M: 'tl.constexpr' = TILE_SIZE_M\n    GROUP_M: 'tl.constexpr' = 4\n    pid = tl.program_id(axis=0)\n    grid_n = tl.cdiv(N, TILE_N)\n    width = GROUP_M * grid_n\n    group_id = pid // width\n    group_size = min(NUM_BLOCKS - group_id * GROUP_M, GROUP_M)\n    pid_m = group_id * GROUP_M + pid % group_size\n    pid_n = pid % width // group_size\n    next_id = pid_m\n    next_next_id = tl.load(input_tiles + 5 * next_id + 4)\n    if next_next_id == 0:\n        _contiguous_block(input_tiles, next_id, pid_n, input, other, output,\n            K, N, stride_input_m, stride_input_k, stride_other_b,\n            stride_other_k, stride_other_n, stride_output_m,\n            stride_output_n, out_dtype=out_dtype, BLOCK_SIZE=BLOCK_SIZE,\n            TILE_M=TILE_M, TILE_N=TILE_N, TILE_K=TILE_K, EVEN_K=EVEN_K,\n            EVEN_N=EVEN_N, EQUAL_K=EQUAL_K)\n    else:\n        _noncontiguous_block(input_tiles, next_id, next_next_id, pid_n,\n            input, other, output, K, N, stride_input_m, stride_input_k,\n            stride_other_b, stride_other_k, stride_other_n, stride_output_m,\n            stride_output_n, out_dtype=out_dtype, BLOCK_SIZE=BLOCK_SIZE,\n            NUM_TILES=NUM_TILES, TILE_M=TILE_M, TILE_N=TILE_N, TILE_K=\n            TILE_K, EVEN_K=EVEN_K, EVEN_N=EVEN_N)\n",
    "category": "Linear Operations",
    "subcategory": "matrix multiplication",
    "uuid": "2a4927ca-cda6-47d4-b5d9-de57b016c71c"
  },
  {
    "input": "@triton.jit\ndef __triton_round_compute(X, stride_x_n, N, BLOCK_N: 'tl.constexpr'):\n    pid_n = tl.program_id(0)\n    grid_n = tl.num_programs(0)\n    n = tl.arange(0, BLOCK_N) * grid_n + pid_n\n    n_mask = n < N\n    xs = tl.load(X + n * stride_x_n, mask=n_mask)\n    ys = tl.math.round(xs)\n    tl.store(X + n * stride_x_n, ys, mask=n_mask)\n",
    "category": "Math Utils",
    "subcategory": "statistical",
    "uuid": "6be99a9c-963d-41a0-a6b7-4c28823db6bb"
  },
  {
    "input": "@triton.jit\ndef hardswish(input):\n    \"\"\"\n    Applies hard Swish to the input.\n\n    Args:\n        input: Input. The input must be loaded and cannot be a pointer.\n\n    Returns:\n        Input transformed by hard Swish.\n    \"\"\"\n    return input * relu6(input + 3) / 6\n",
    "category": "Activation Functions",
    "subcategory": "relu",
    "uuid": "43d4ae58-d18d-4be1-a8a4-bc6476eed45f"
  },
  {
    "input": "@triton.jit\ndef _bwd_store_dk_dv(dk_ptrs, dv_ptrs, dk, dv, offs_n, offs_d, seqlen_k,\n    headdim, EVEN_M: 'tl.constexpr', EVEN_N: 'tl.constexpr', EVEN_HEADDIM:\n    'tl.constexpr'):\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            tl.store(dv_ptrs, dv)\n            tl.store(dk_ptrs, dk)\n        else:\n            tl.store(dv_ptrs, dv, mask=offs_d[None, :] < headdim)\n            tl.store(dk_ptrs, dk, mask=offs_d[None, :] < headdim)\n    elif EVEN_HEADDIM:\n        tl.store(dv_ptrs, dv, mask=offs_n[:, None] < seqlen_k)\n        tl.store(dk_ptrs, dk, mask=offs_n[:, None] < seqlen_k)\n    else:\n        tl.store(dv_ptrs, dv, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n        tl.store(dk_ptrs, dk, mask=(offs_n[:, None] < seqlen_k) & (offs_d[\n            None, :] < headdim))\n",
    "category": "Data Movement",
    "subcategory": "storing",
    "uuid": "25f26f06-3a16-49df-85ce-e0c6a61c3165"
  },
  {
    "input": "@triton.jit\ndef _single_query_cached_kv_attention_v2(exp_sums, max_logits, out, q,\n    k_cache, v_cache, head_mapping, scale, block_tables, seq_lens,\n    partiton_size, max_num_blocks_per_seq, alibi_slopes, stride_qm,\n    stride_qn, stride_om, stride_on, stride_ok, stride_km, stride_kn,\n    stride_kk, stride_exp_m, stride_exp_n, BLOCK_SIZE: 'tl.constexpr',\n    HEAD_SIZE: 'tl.constexpr'):\n    seq_idx = tl.program_id(axis=1)\n    par_idx = tl.program_id(axis=2)\n    seq_len = tl.load(seq_lens + seq_idx)\n    if par_idx * partiton_size >= seq_len:\n        return\n    num_context_blocks = tl.cdiv(seq_len, BLOCK_SIZE)\n    num_blocks_per_par = partiton_size // BLOCK_SIZE\n    start_block_idx = par_idx * num_blocks_per_par\n    end_block_idx = tl.minimum(start_block_idx + num_blocks_per_par,\n        num_context_blocks)\n    head_idx = tl.program_id(axis=0)\n    kv_head_idx = tl.load(head_mapping + head_idx)\n    if alibi_slopes is None:\n        alibi_slope = 0.0\n    else:\n        alibi_slope = tl.load(alibi_slopes + head_idx)\n    block_offs = tl.arange(0, BLOCK_SIZE)\n    head_size_offs = tl.arange(0, HEAD_SIZE)\n    q = tl.load(q + seq_idx * stride_qm + head_idx * stride_qn + head_size_offs\n        )\n    q = q * scale\n    qkv = tl.zeros([BLOCK_SIZE, HEAD_SIZE], dtype=tl.float32)\n    qk_max = float('-inf')\n    exp_sum = 0.0\n    fp16_0 = tl.zeros([1, 1], dtype=k_cache.dtype.element_ty)\n    base_offs_kv = kv_head_idx * stride_kn + block_offs[:, None\n        ] * stride_kk + head_size_offs[None, :]\n    for block_idx in range(start_block_idx, end_block_idx):\n        physical_block_idx = tl.load(block_tables + seq_idx *\n            max_num_blocks_per_seq + block_idx)\n        mask = (block_offs[:, None] < seq_len - block_idx * BLOCK_SIZE) & (\n            head_size_offs[None, :] < HEAD_SIZE)\n        offs_kv = physical_block_idx * stride_km + base_offs_kv\n        k = tl.load(k_cache + offs_kv, mask=mask, other=fp16_0)\n        v = tl.load(v_cache + offs_kv, mask=mask, other=fp16_0)\n        _qk = tl.sum(q[None, :] * k, axis=1)\n        _qk += alibi_slope * (block_idx * BLOCK_SIZE + block_offs - seq_len + 1\n            )\n        _qk_max = tl.maximum(tl.max(_qk, axis=0), qk_max)\n        qk = tl.where(block_offs[:, None] < seq_len - block_idx *\n            BLOCK_SIZE, _qk[:, None], float('-inf'))\n        _exp_sum = exp_sum * tl.exp(qk_max - _qk_max) + tl.sum(tl.exp(_qk -\n            _qk_max), axis=0)\n        qkv = qkv * (exp_sum * tl.exp(qk_max - _qk_max) / _exp_sum) + tl.exp(\n            qk - _qk_max) / _exp_sum * v\n        qk_max = _qk_max\n        exp_sum = _exp_sum\n    offs_exp = seq_idx * stride_exp_m + head_idx * stride_exp_n + par_idx\n    tl.store(exp_sums + offs_exp, exp_sum)\n    tl.store(max_logits + offs_exp, qk_max)\n    offs_out = (seq_idx * stride_om + head_idx * stride_on + par_idx *\n        stride_ok + head_size_offs)\n    tl.store(out + offs_out, tl.sum(qkv, axis=0))\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "86034176-0475-41c4-be84-e8c5053d32a1"
  },
  {
    "input": "@triton.jit\ndef softmax_kernel_forward(output_ptr, input_ptr, input_row_stride,\n    output_row_stride, n_cols, BLOCK_SIZE: 'tl.constexpr'):\n    row_idx = tl.program_id(0)\n    row_start_ptr = input_ptr + row_idx * input_row_stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    input_ptrs = row_start_ptr + col_offsets\n    mask = col_offsets < n_cols\n    row = tl.load(input_ptrs, mask=mask, other=-float('inf'))\n    causal_mask = col_offsets > row_idx % n_cols\n    row = row + tl.where(causal_mask, -float('inf'), 0.0)\n    row_minus_max = row - tl.max(row, axis=0)\n    numerator = tl.exp(row_minus_max)\n    denominator = tl.sum(numerator, axis=0)\n    softmax_output = numerator / denominator\n    output_row_start_ptr = output_ptr + row_idx * output_row_stride\n    output_ptrs = output_row_start_ptr + col_offsets\n    tl.store(output_ptrs, softmax_output, mask=mask)\n",
    "category": "Activation Functions",
    "subcategory": "softmax",
    "uuid": "c8c10270-fda7-4454-bfc7-fadb158c1f83"
  },
  {
    "input": "@triton.jit\ndef _get_plane_grid_sample_info(gi, ix_in, iy_in, IH, IW, feature_grid_size,\n    C: 'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    offs = gi * 5 + tl.zeros((BLOCK_SIZE,), dtype=tl.int32)\n    BS = tl.load(feature_grid_size + offs + 0)\n    grid_numel = BS * IH * IW * C\n    grid_numel = tl.sum(grid_numel, axis=0) // BLOCK_SIZE\n    ix11 = (ix_in + 1) / 2 * IW - 0.5\n    iy11 = (iy_in + 1) / 2 * IH - 0.5\n    ix = ix11 * (IW > 1)\n    iy = iy11 * (IH > 1)\n    ix0 = _floor(ix)\n    iy0 = _floor(iy)\n    return ix, iy, ix0, iy0, grid_numel\n",
    "category": "Helper Functions",
    "subcategory": "indexing",
    "uuid": "518fce37-f974-40ca-92b6-f2db44f83fb4"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att1(Q, K, sm_scale, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, Att_Out, stride_b_loc_b, stride_b_loc_s, stride_qbs,\n    stride_qh, stride_qd, stride_kbs, stride_kh, stride_kd, att_stride_h,\n    att_stride_bs, kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N:\n    'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    start_n = tl.program_id(2)\n    cur_kv_head = cur_head // kv_group_num\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = max_input_len\n    off_q = cur_batch * stride_qbs + cur_head * stride_qh + offs_d * stride_qd\n    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\n    block_stard_index = start_n * BLOCK_N\n    block_mask = tl.where(block_stard_index < cur_batch_seq_len, 1, 0)\n    for start_mark in range(0, block_mask, 1):\n        q = tl.load(Q + off_q + start_mark)\n        offs_n_new = cur_batch_start_index + offs_n\n        k_loc = tl.load(B_Loc + stride_b_loc_b * cur_batch + stride_b_loc_s *\n            offs_n_new, mask=offs_n_new < cur_batch_end_index, other=0)\n        off_k = k_loc[:, None] * stride_kbs + cur_kv_head * stride_kh + offs_d[\n            None, :] * stride_kd\n        k = tl.load(K + off_k, mask=offs_n_new[:, None] <\n            cur_batch_end_index, other=0.0)\n        att_value = tl.sum(q[None, :] * k, 1)\n        att_value *= sm_scale\n        off_o = cur_head * att_stride_h + (cur_batch_in_all_start_index +\n            offs_n) * att_stride_bs\n        tl.store(Att_Out + off_o, att_value, mask=offs_n_new <\n            cur_batch_end_index)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "a7037f72-868a-46cb-bf65-5847be9bd9ff"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att2_int8v(Prob, V, V_scale, Out, B_Loc, B_Start_Loc,\n    B_Seqlen, max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph,\n    stride_pbs, stride_vbs, stride_vh, stride_vd, stride_vsbs, stride_vsh,\n    stride_vsd, stride_obs, stride_oh, stride_od, BLOCK_DMODEL:\n    'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_head * stride_vh + offs_d[None, :] * stride_vd\n    vs_offs = cur_head * stride_vsh\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        vs_value = tl.load(V_scale + vs_offs + v_loc[:, None] * stride_vsbs,\n            mask=start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value * vs_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "b7d5d7fc-8f9b-4886-858f-47ee2645416c"
  },
  {
    "input": "@triton.jit\ndef _swiglu_backward_kernel(dc_ptr, a_ptr, b_ptr, stride, n_cols:\n    'tl.constexpr', BLOCK_SIZE: 'tl.constexpr'):\n    program_id = tl.program_id(0)\n    dc_ptr += program_id * stride\n    a_ptr += program_id * stride\n    b_ptr += program_id * stride\n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    dc_row = tl.load(dc_ptr + col_offsets, mask=mask, other=0)\n    a_row = tl.load(a_ptr + col_offsets, mask=mask, other=0)\n    b_row = tl.load(b_ptr + col_offsets, mask=mask, other=0)\n    sig_a = tl.sigmoid(a_row)\n    silu_a = a_row * sig_a\n    db_row = dc_row * silu_a\n    da_row = dc_row * (silu_a * (1 - sig_a) + sig_a) * b_row\n    tl.store(a_ptr + col_offsets, da_row, mask=mask)\n    tl.store(b_ptr + col_offsets, db_row, mask=mask)\n",
    "category": "Gradient Operations",
    "subcategory": "backward pass",
    "uuid": "e4e9fed0-f88e-4f5b-9ec4-ba346d27bcfc"
  },
  {
    "input": "@triton.jit\ndef _fwd_kernel_token_att2(Prob, V, Out, B_Loc, B_Start_Loc, B_Seqlen,\n    max_input_len, stride_b_loc_b, stride_b_loc_s, stride_ph, stride_pbs,\n    stride_vbs, stride_vh, stride_vd, stride_obs, stride_oh, stride_od,\n    kv_group_num, BLOCK_DMODEL: 'tl.constexpr', BLOCK_N: 'tl.constexpr'):\n    cur_batch = tl.program_id(0)\n    cur_head = tl.program_id(1)\n    cur_kv_head = cur_head // kv_group_num\n    offs_n = tl.arange(0, BLOCK_N)\n    offs_d = tl.arange(0, BLOCK_DMODEL)\n    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n    cur_batch_start_index = max_input_len - cur_batch_seq_len\n    cur_batch_end_index = cur_batch_seq_len\n    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n    v_loc_off = cur_batch * stride_b_loc_b + (cur_batch_start_index + offs_n\n        ) * stride_b_loc_s\n    p_offs = cur_head * stride_ph + (cur_batch_in_all_start_index + offs_n\n        ) * stride_pbs\n    v_offs = cur_kv_head * stride_vh + offs_d[None, :] * stride_vd\n    acc = tl.zeros([BLOCK_DMODEL], dtype=tl.float32)\n    for start_n in range(0, cur_batch_seq_len, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n        p_value = tl.load(Prob + p_offs + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_loc = tl.load(B_Loc + v_loc_off + start_n * stride_b_loc_s, mask=\n            start_n + offs_n < cur_batch_seq_len, other=0.0)\n        v_value = tl.load(V + v_offs + v_loc[:, None] * stride_vbs, mask=\n            start_n + offs_n[:, None] < cur_batch_seq_len, other=0.0)\n        acc += tl.sum(p_value[:, None] * v_value, 0)\n    acc = acc\n    off_o = cur_batch * stride_obs + cur_head * stride_oh + offs_d * stride_od\n    out_ptrs = Out + off_o\n    tl.store(out_ptrs, acc)\n    return\n",
    "category": "Attention Mechanisms",
    "subcategory": "self attention",
    "uuid": "c4a85dd6-554a-44c4-afca-67c4aaa26bb1"
  }
]
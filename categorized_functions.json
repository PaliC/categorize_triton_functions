[
  {
    "input": "@triton.jit\ndef chunk_abc_fwd_kernel_h(k, v, z, h, h0, ht, s_k_h, s_k_t, s_k_d, s_v_h,\n    s_v_t, s_v_d, s_h_h, s_h_t, s_h_d, T: 'tl.constexpr', K: 'tl.constexpr',\n    V: 'tl.constexpr', BT: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', NT: 'tl.constexpr', NORMK: 'tl.constexpr',\n    USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h, boundary_check=(0, 1))\n    if NORMK:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,), (i_k *\n            BK,), (BK,), (0,))\n    else:\n        p_z0 = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,), (i_v *\n            BV,), (BV,), (0,))\n    b_zp = tl.load(p_z0)\n    for i_t in range(NT):\n        p_k = tl.make_block_ptr(k + i_bh * s_k_h, (K, T), (s_k_d, s_k_t), (\n            i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_v = tl.make_block_ptr(v + i_bh * s_v_h, (T, V), (s_v_t, s_v_d), (\n            i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_h = tl.make_block_ptr(h + i_bh * s_h_h + i_t * K * V, (K, V), (\n            s_h_t, s_h_d), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        if NORMK:\n            p_zc = tl.make_block_ptr(z + i_bh * s_k_h, (T * K,), (s_k_d,),\n                ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[:, None]\n            b_k = tl.exp(b_k - b_zc[:, None])\n        else:\n            p_zc = tl.make_block_ptr(z + i_bh * s_v_h, (T * V,), (s_v_d,),\n                ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n            b_zc = tl.load(p_zc, boundary_check=(0,))\n            b_r, b_zp = tl.exp(b_zp - b_zc), b_zc\n            b_h = b_h * b_r[None, :]\n            b_v = tl.exp(b_v - b_zc[None, :])\n        b_h += tl.dot(b_k, b_v, allow_tf32=False)\n    if STORE_FINAL_STATE:\n        p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k *\n            BK, i_v * BV), (BK, BV), (1, 0))\n        tl.store(p_h, b_h, boundary_check=(0, 1))\n",
    "category": "Attention",
    "subcategory": "Chunked Attention",
    "uuid": 0
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_X': 64}, num_warps=2),\n    triton.Config({'BLOCK_X': 128}, num_warps=2), triton.Config({'BLOCK_X':\n    256}, num_warps=2), triton.Config({'BLOCK_X': 128}, num_warps=4),\n    triton.Config({'BLOCK_X': 256}, num_warps=4)], key=['NUM_COLUMNS'])\n@triton.jit\ndef _padded_copy(a, b, indices, bin_ids, weights, bins, padded_bins,\n    NUM_COLUMNS: 'tl.constexpr', TOP_K: 'tl.constexpr', BLOCK_X:\n    'tl.constexpr', A_TO_B: 'tl.constexpr', SCALE: 'tl.constexpr'):\n    index_a = tl.load(indices + tl.program_id(0))\n    bin_idx = tl.load(bin_ids + tl.program_id(0))\n    offset_in_bin = tl.program_id(0)\n    if bin_idx > 0:\n        offset_in_bin -= tl.load(bins + bin_idx - 1)\n    index_b = offset_in_bin\n    if bin_idx > 0:\n        index_b += tl.load(padded_bins + bin_idx - 1)\n    offset = index_a // TOP_K if A_TO_B else index_a\n    a += tl.multiple_of(offset * NUM_COLUMNS, NUM_COLUMNS)\n    b += tl.multiple_of(index_b * NUM_COLUMNS, NUM_COLUMNS)\n    offsets = tl.max_contiguous(tl.arange(0, BLOCK_X), BLOCK_X)\n    scale = tl.load(weights + index_a) if SCALE else 1\n    iptr = a if A_TO_B else b\n    optr = b if A_TO_B else a\n    for i in range(tl.cdiv(NUM_COLUMNS, BLOCK_X)):\n        mask = offsets < NUM_COLUMNS\n        x = tl.load(iptr + offsets, mask=mask)\n        x = x * scale\n        tl.store(optr + offsets, x, mask=mask)\n        offsets += BLOCK_X\n",
    "category": "Memory Operations",
    "subcategory": "Memory Padding",
    "uuid": 1
  },
  {
    "input": "@triton.jit\ndef _rmsnorm_kernel_fwd(x_ptr, w_ptr, z_ptr, K, eps, BLOCK_SIZE:\n    'tl.constexpr'=8):\n    row_idx = tl.program_id(0)\n    x_row_ptr = x_ptr + row_idx * K\n    w_row_ptr = w_ptr + row_idx * K\n    z_row_ptr = z_ptr + row_idx * K\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for col_index in range(0, K, BLOCK_SIZE):\n        col_offsets = col_index + tl.arange(0, BLOCK_SIZE)\n        x_ptrs = x_row_ptr + col_offsets\n        x = tl.load(x_ptrs, mask=col_offsets < K, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / K\n    rsqrt = 1 / tl.sqrt(var + eps)\n    for col_index in range(0, K, BLOCK_SIZE):\n        col_offsets = col_index + tl.arange(0, BLOCK_SIZE)\n        mask = col_offsets < K\n        x = tl.load(x_row_ptr + col_offsets, mask=mask, other=0.0)\n        w = tl.load(w_ptr + col_offsets, mask=mask)\n        normed = x * rsqrt\n        normed = normed\n        z = normed * w\n        tl.store(z_row_ptr + col_offsets, z, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "RMS Normalization",
    "uuid": 2
  },
  {
    "input": "@triton.jit\ndef fused_recurrent_rwkv6_fwd_kernel32(q, k, v, w, u, o, h0, ht, s_k_h,\n    s_v_h, scale, B: 'tl.constexpr', H: 'tl.constexpr', T: 'tl.constexpr',\n    K: 'tl.constexpr', V: 'tl.constexpr', BK: 'tl.constexpr', BV:\n    'tl.constexpr', USE_INITIAL_STATE: 'tl.constexpr', STORE_FINAL_STATE:\n    'tl.constexpr', REVERSE: 'tl.constexpr'):\n    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h = i_bh % H\n    p_q = q + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_k = k + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_v = v + i_bh * s_v_h + i_v * BV + tl.arange(0, BV) + ((T - 1) * V if\n        REVERSE else 0)\n    p_o = o + (i_bh + i_k * B * H) * s_v_h + i_v * BV + tl.arange(0, BV) + (\n        (T - 1) * V if REVERSE else 0)\n    p_w = w + i_bh * s_k_h + i_k * BK + tl.arange(0, BK) + ((T - 1) * K if\n        REVERSE else 0)\n    p_u = u + i_h * K + tl.arange(0, BK) + i_k * BK\n    mask_bk = i_k * BK + tl.arange(0, BK) < K\n    mask_bv = i_v * BV + tl.arange(0, BV) < V\n    mask_kv = mask_bv[:, None] & mask_bk[None, :]\n    b_h = tl.zeros([BV, BK], dtype=tl.float32)\n    if USE_INITIAL_STATE:\n        p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        b_h += tl.load(p_h0, mask=mask_kv, other=0)\n    b_u = tl.load(p_u, mask=mask_bk, other=0)\n    for _ in range(0, T):\n        b_k = tl.load(p_k, mask=mask_bk, other=0)\n        b_v = tl.load(p_v, mask=mask_bv, other=0)\n        b_q = tl.load(p_q, mask=mask_bk, other=0) * scale\n        b_w = tl.load(p_w, mask=mask_bk, other=0)\n        b_w = tl.exp(b_w)\n        b_kv = b_k[None, :] * b_v[:, None]\n        b_o = (b_h + b_kv * b_u[None, :]) * b_q[None, :]\n        b_o = tl.sum(b_o, axis=1)\n        b_h = b_h * b_w[None, :]\n        b_h += b_kv\n        tl.store(p_o, b_o, mask=mask_bv)\n        p_q += -K if REVERSE else K\n        p_k += -K if REVERSE else K\n        p_o += -V if REVERSE else V\n        p_v += -V if REVERSE else V\n        p_w += -K if REVERSE else K\n    if STORE_FINAL_STATE:\n        p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]\n            ) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n        tl.store(p_ht, b_h, mask=mask_kv)\n",
    "category": "Recurrent Operations",
    "subcategory": "RWKV",
    "uuid": 3
  },
  {
    "input": "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_OUT': 256,\n    'BLOCK_SIZE_HIDDEN': 64}, num_stages=3, num_warps=8), triton.Config({\n    'BLOCK_SIZE_OUT': 256, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 128, 'BLOCK_SIZE_HIDDEN':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 64,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=4, num_warps=4), triton.Config({\n    'BLOCK_SIZE_OUT': 128, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=4,\n    num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 32, 'BLOCK_SIZE_HIDDEN':\n    32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_OUT': 32,\n    'BLOCK_SIZE_HIDDEN': 32}, num_stages=5, num_warps=2), triton.Config({\n    'BLOCK_SIZE_OUT': 64, 'BLOCK_SIZE_HIDDEN': 32}, num_stages=5, num_warps\n    =2)], key=['hidden_size', 'out_size'], restore_value=['weights_ptr'])\n@triton.jit\ndef modifier_kernel(weights_ptr, assumed_wmax_ptr, reduced_assumed_wmax_ptr,\n    upper_end_of_slices_ptr, hidden_size, out_size, num_slices,\n    stride_weights_hidden_size, stride_weights_out_size,\n    stride_assumed_wmax_num_slices, stride_assumed_wmax_out_size,\n    modifier_type: 'tl.constexpr', modifier_weight_res: 'tl.constexpr',\n    modifier_seed, modifier_std: 'tl.constexpr', BLOCK_SIZE_HIDDEN:\n    'tl.constexpr', BLOCK_SIZE_OUT: 'tl.constexpr'):\n    \"\"\"\n    Modifier kernel for the weights.\n    \"\"\"\n    pid = tl.program_id(axis=0)\n    offs_bn = (pid * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)) % out_size\n    offs_assumed_wmax = pid * BLOCK_SIZE_OUT + tl.arange(0, BLOCK_SIZE_OUT)\n    increase_weight_offsets_by = BLOCK_SIZE_HIDDEN * BLOCK_SIZE_OUT\n    weight_random_offsets = tl.arange(0, BLOCK_SIZE_HIDDEN * BLOCK_SIZE_OUT\n        ).reshape((BLOCK_SIZE_HIDDEN, BLOCK_SIZE_OUT), can_reorder=True)\n    ir_range_lower = 0\n    for slice_idx in range(0, num_slices):\n        abs_max_slice_ptrs = (assumed_wmax_ptr + slice_idx *\n            stride_assumed_wmax_num_slices + offs_bn *\n            stride_assumed_wmax_out_size)\n        if modifier_type == 'AddNormal' or (modifier_type == 'Discretize' or\n            modifier_type == 'DiscretizeAddNormal'):\n            assumed_wmax_per_slice = tl.load(reduced_assumed_wmax_ptr +\n                slice_idx)\n        else:\n            assumed_wmax_per_slice = tl.load(abs_max_slice_ptrs, mask=\n                offs_assumed_wmax < out_size, other=float('-inf'))\n            assumed_wmax_per_slice = assumed_wmax_per_slice[None, :]\n        ir_range_upper = tl.load(upper_end_of_slices_ptr + slice_idx)\n        current_lower = ir_range_lower\n        num_k = tl.cdiv(ir_range_upper - ir_range_lower, BLOCK_SIZE_HIDDEN)\n        for k in range(0, num_k):\n            current_upper = min(ir_range_upper, ir_range_lower + (k + 1) *\n                BLOCK_SIZE_HIDDEN, hidden_size)\n            offs_k = current_lower + tl.arange(0, BLOCK_SIZE_HIDDEN)\n            b_ptrs = weights_ptr + (offs_k[:, None] *\n                stride_weights_hidden_size + offs_bn[None, :] *\n                stride_weights_out_size)\n            weight_block = tl.load(b_ptrs, mask=offs_k[:, None] <\n                current_upper, other=0.0)\n            if (modifier_type == 'Discretize' or modifier_type ==\n                'DiscretizeAddNormal') or (modifier_type ==\n                'DiscretizePerChannel' or modifier_type ==\n                'DiscretizeAddNormalPerChannel'):\n                if modifier_weight_res > 0:\n                    n_states = max(modifier_weight_res, 1 / modifier_weight_res\n                        )\n                    res = 2 * assumed_wmax_per_slice / n_states\n                    weight_block = weight_block / res\n                    weight_block = tl.extra.cuda.libdevice.rint(weight_block)\n                    weight_block = weight_block * res\n            if (modifier_type == 'AddNormal' or modifier_type ==\n                'AddNormalPerChannel') or (modifier_type ==\n                'DiscretizeAddNormal' or modifier_type ==\n                'DiscretizeAddNormalPerChannel'):\n                randn_block = tl.randn(modifier_seed + pid,\n                    weight_random_offsets)\n                weight_random_offsets += increase_weight_offsets_by\n                randn_block = (assumed_wmax_per_slice * modifier_std *\n                    randn_block)\n                weight_block += randn_block\n            tl.store(b_ptrs, weight_block, mask=(offs_k[:, None] <\n                current_upper) & (offs_assumed_wmax[None, :] < out_size))\n            current_lower = current_upper\n        ir_range_lower = ir_range_upper\n",
    "category": "Tensor Operations",
    "subcategory": "Weight Modification",
    "uuid": 4
  },
  {
    "input": "@triton.jit\ndef _layer_norm_fwd(X, Y, Mean, Rstd, D, eps, stride_x, stride_y, TRAINING:\n    'tl.constexpr', BLOCK_D: 'tl.constexpr', COMPUTE_MEAN_AND_RSTD:\n    'tl.constexpr'):\n    row = tl.program_id(0)\n    X += row * stride_x\n    Y += row * stride_y\n    cols = tl.arange(0, BLOCK_D)\n    x = tl.load(X + cols, mask=cols < D, other=0.0)\n    if COMPUTE_MEAN_AND_RSTD:\n        mean = tl.sum(x, axis=0) / D\n    else:\n        mean = tl.load(Mean + row)\n    x_mean = tl.where(cols < D, x - mean, 0.0)\n    if COMPUTE_MEAN_AND_RSTD:\n        _var = tl.zeros([BLOCK_D], dtype=tl.float32)\n        _var += x_mean * x_mean\n        var = tl.sum(_var, axis=0) / D\n        rstd = 1 / tl.sqrt(var + eps)\n        if TRAINING:\n            tl.store(Mean + row, mean)\n            tl.store(Rstd + row, rstd)\n    else:\n        rstd = tl.load(Rstd + row)\n    mask = cols < D\n    y = x_mean * rstd\n    tl.store(Y + cols, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "Layer Normalization",
    "uuid": 5
  },
  {
    "input": "@triton.jit\ndef triton_add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE:\n    'tl.constexpr'):\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask = offsets < n_elements\n    x = tl.load(x_ptr + offsets, mask=mask)\n    y = tl.load(y_ptr + offsets, mask=mask)\n    output = x + y\n    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "category": "Tensor Operations",
    "subcategory": "Tensor Addition",
    "uuid": 6
  },
  {
    "input": "@triton.jit\ndef _ragged_hstu_attn_bwd_one_col_block(start_n, seq_len, n_targets, Q, K,\n    V, TS, TW, PW, Bias, DOut, DQ, DK, DV, DBias, DTW, DPW, LOCK, stride_qm,\n    stride_kn, stride_vn, stride_dom, stride_dqm, stride_dkn, stride_dvn,\n    alpha, MAX_SEQ_LEN, num_buckets, max_pos_ind, time_bucket_incr,\n    time_bucket_div, time_delta, MAX_ATTN_LEN: 'tl.constexpr',\n    INVALID_MASK_TYPE: 'tl.constexpr', CAUSAL: 'tl.constexpr', BUCKET_FN:\n    'tl.constexpr', ATTN_BIAS_TYPE: 'tl.constexpr', USE_TIME_BIAS:\n    'tl.constexpr', USE_POS_BIAS: 'tl.constexpr', FUSED_BIAS_BWD:\n    'tl.constexpr', HAS_MAX_POS_IND: 'tl.constexpr', HAS_MULTIPLE_TARGETS:\n    'tl.constexpr', CONTEXTUAL_SEQ_LEN: 'tl.constexpr', ALLOW_TF32:\n    'tl.constexpr', BLOCK_D_Q: 'tl.constexpr', BLOCK_D_V: 'tl.constexpr',\n    BLOCK_M: 'tl.constexpr', BLOCK_N: 'tl.constexpr', UNROLL:\n    'tl.constexpr', ATOMIC_ADD: 'tl.constexpr'):\n    if INVALID_MASK_TYPE == 'lower_triangular':\n        if HAS_MULTIPLE_TARGETS:\n            low = start_n\n            if MAX_ATTN_LEN > 0:\n                high = start_n + MAX_ATTN_LEN + BLOCK_N\n                high = high if high + n_targets < seq_len else seq_len\n            else:\n                high = seq_len\n        else:\n            low = start_n\n            if MAX_ATTN_LEN > 0:\n                high = start_n + MAX_ATTN_LEN + BLOCK_N\n                high = high if high < seq_len else seq_len\n            else:\n                high = seq_len\n        if CONTEXTUAL_SEQ_LEN > 0:\n            contextual_block_end = tl.cdiv(CONTEXTUAL_SEQ_LEN, BLOCK_M\n                ) * BLOCK_M\n            if low < contextual_block_end:\n                low = contextual_block_end\n    elif INVALID_MASK_TYPE == 'upper_triangular':\n        low = 0\n        high = start_n + BLOCK_N\n    offs_m = tl.arange(0, BLOCK_M)\n    offs_qk_d = tl.arange(0, BLOCK_D_Q)\n    offs_v_d = tl.arange(0, BLOCK_D_V)\n    offs_n = start_n + tl.arange(0, BLOCK_N)\n    q_ptrs_trans = Q + (offs_m[None, :] * stride_qm + offs_qk_d[:, None])\n    dq_ptrs_trans = DQ + (offs_m[None, :] * stride_dqm + offs_qk_d[:, None])\n    k_ptrs = K + (offs_n[:, None] * stride_kn + offs_qk_d[None, :])\n    v_ptrs = V + (offs_n[:, None] * stride_vn + offs_v_d[None, :])\n    mask_n = offs_n < seq_len\n    ts_0_ptrs = None\n    ts_1_ptrs = None\n    ts_1 = None\n    off_bias_trans = None\n    bias_ptrs_trans = None\n    dbias_ptrs_trans = None\n    if ATTN_BIAS_TYPE == 'fused' and USE_TIME_BIAS:\n        ts_0_ptrs = TS + offs_m\n        ts_1_ptrs = TS + offs_n\n        if CAUSAL:\n            ts_1 = tl.load(ts_1_ptrs, mask=mask_n)\n        else:\n            ts_1 = tl.load(ts_1_ptrs + 1, mask=mask_n)\n    elif ATTN_BIAS_TYPE == 'separate':\n        off_bias_trans = offs_m[None, :] * seq_len + offs_n[:, None]\n        bias_ptrs_trans = Bias + off_bias_trans\n        dbias_ptrs_trans = DBias + off_bias_trans\n    do_ptrs = DOut + (offs_m[:, None] * stride_dom + offs_v_d[None, :])\n    dv = tl.zeros([BLOCK_N, BLOCK_D_V], dtype=tl.float32)\n    dk = tl.zeros([BLOCK_N, BLOCK_D_Q], dtype=tl.float32)\n    k = tl.load(k_ptrs, mask=mask_n[:, None], other=0.0)\n    v = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0)\n    if HAS_MULTIPLE_TARGETS:\n        if INVALID_MASK_TYPE == 'lower_triangular':\n            pos_offs_n = tl.where(offs_n < seq_len - n_targets, offs_n, \n                seq_len - n_targets)\n        elif INVALID_MASK_TYPE == 'upper_triangular':\n            pos_offs_n = tl.where(offs_n > n_targets - 1, offs_n, n_targets - 1\n                )\n    else:\n        pos_offs_n = offs_n\n    if CONTEXTUAL_SEQ_LEN > 0 and INVALID_MASK_TYPE == 'lower_triangular':\n        for start_m in range(0, CONTEXTUAL_SEQ_LEN, BLOCK_M):\n            start_m = tl.multiple_of(start_m, BLOCK_M)\n            dk, dv = _ragged_hstu_attn_bwd_one_block(start_m=start_m,\n                offs_n=offs_n, offs_m=offs_m, q_ptrs_trans=q_ptrs_trans,\n                dq_ptrs_trans=dq_ptrs_trans, mask_n=mask_n, ts_0_ptrs=\n                ts_0_ptrs, ts_1=ts_1, bias_ptrs_trans=bias_ptrs_trans,\n                dbias_ptrs_trans=dbias_ptrs_trans, do_ptrs=do_ptrs, dk=dk,\n                dv=dv, k=k, v=v, pos_offs_n=pos_offs_n, seq_len=seq_len,\n                n_targets=n_targets, TW=TW, PW=PW, DTW=DTW, DPW=DPW, LOCK=\n                LOCK, stride_qm=stride_qm, stride_dom=stride_dom,\n                stride_dqm=stride_dqm, alpha=alpha, MAX_SEQ_LEN=MAX_SEQ_LEN,\n                num_buckets=num_buckets, max_pos_ind=max_pos_ind,\n                MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n                time_bucket_incr, time_bucket_div=time_bucket_div,\n                time_delta=time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE,\n                CAUSAL=CAUSAL, BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=\n                ATTN_BIAS_TYPE, USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=\n                USE_POS_BIAS, FUSED_BIAS_BWD=FUSED_BIAS_BWD,\n                HAS_MAX_POS_IND=HAS_MAX_POS_IND, HAS_MULTIPLE_TARGETS=\n                HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=CONTEXTUAL_SEQ_LEN,\n                ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N,\n                ATOMIC_ADD=ATOMIC_ADD)\n    for start_m in tl.range(low, high, BLOCK_M, loop_unroll_factor=UNROLL):\n        start_m = tl.multiple_of(start_m, BLOCK_M)\n        dk, dv = _ragged_hstu_attn_bwd_one_block(start_m=start_m, offs_n=\n            offs_n, offs_m=offs_m, q_ptrs_trans=q_ptrs_trans, dq_ptrs_trans\n            =dq_ptrs_trans, mask_n=mask_n, ts_0_ptrs=ts_0_ptrs, ts_1=ts_1,\n            bias_ptrs_trans=bias_ptrs_trans, dbias_ptrs_trans=\n            dbias_ptrs_trans, do_ptrs=do_ptrs, dk=dk, dv=dv, k=k, v=v,\n            pos_offs_n=pos_offs_n, seq_len=seq_len, n_targets=n_targets, TW\n            =TW, PW=PW, DTW=DTW, DPW=DPW, LOCK=LOCK, stride_qm=stride_qm,\n            stride_dom=stride_dom, stride_dqm=stride_dqm, alpha=alpha,\n            MAX_SEQ_LEN=MAX_SEQ_LEN, num_buckets=num_buckets, max_pos_ind=\n            max_pos_ind, MAX_ATTN_LEN=MAX_ATTN_LEN, time_bucket_incr=\n            time_bucket_incr, time_bucket_div=time_bucket_div, time_delta=\n            time_delta, INVALID_MASK_TYPE=INVALID_MASK_TYPE, CAUSAL=CAUSAL,\n            BUCKET_FN=BUCKET_FN, ATTN_BIAS_TYPE=ATTN_BIAS_TYPE,\n            USE_TIME_BIAS=USE_TIME_BIAS, USE_POS_BIAS=USE_POS_BIAS,\n            FUSED_BIAS_BWD=FUSED_BIAS_BWD, HAS_MAX_POS_IND=HAS_MAX_POS_IND,\n            HAS_MULTIPLE_TARGETS=HAS_MULTIPLE_TARGETS, CONTEXTUAL_SEQ_LEN=\n            CONTEXTUAL_SEQ_LEN, ALLOW_TF32=ALLOW_TF32, BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N, ATOMIC_ADD=ATOMIC_ADD)\n    dv_ptrs = DV + (offs_n[:, None] * stride_dvn + offs_v_d[None, :])\n    dk_ptrs = DK + (offs_n[:, None] * stride_dkn + offs_qk_d[None, :])\n    dk = dk * alpha\n    tl.store(dv_ptrs, dv, mask=mask_n[:, None])\n    tl.store(dk_ptrs, dk, mask=mask_n[:, None])\n",
    "category": "Attention",
    "subcategory": "Ragged Attention",
    "uuid": 7
  },
  {
    "input": "@triton.jit\ndef _rms_norm_fwd_fused(X, Y, W, x_stride0, x_stride1, y_stride0, y_stride1,\n    N, eps, BLOCK_SIZE: 'tl.constexpr'):\n    row = tl.program_id(0)\n    Y += row * y_stride0\n    X += row * x_stride0\n    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        x = tl.load(X + cols * x_stride1, mask=cols < N, other=0.0)\n        _var += x * x\n    var = tl.sum(_var, axis=0) / N\n    rstd = 1 / tl.sqrt(var + eps)\n    for off in range(0, N, BLOCK_SIZE):\n        cols = off + tl.arange(0, BLOCK_SIZE)\n        mask = cols < N\n        w = tl.load(W + cols, mask=mask)\n        x = tl.load(X + cols, mask=mask, other=0.0)\n        x_hat = x * rstd\n        y = x_hat * w\n        tl.store(Y + cols * y_stride1, y, mask=mask)\n",
    "category": "Normalization",
    "subcategory": "RMS Normalization",
    "uuid": 8
  },
  {
    "input": "@triton.jit\ndef triton_cross_scan_flex(x: 'tl.tensor', y: 'tl.tensor', x_layout:\n    'tl.constexpr', y_layout: 'tl.constexpr', operation: 'tl.constexpr',\n    onebyone: 'tl.constexpr', scans: 'tl.constexpr', BC: 'tl.constexpr', BH:\n    'tl.constexpr', BW: 'tl.constexpr', DC: 'tl.constexpr', DH:\n    'tl.constexpr', DW: 'tl.constexpr', NH: 'tl.constexpr', NW: 'tl.constexpr'\n    ):\n    i_hw, i_c, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n    i_h, i_w = i_hw // NW, i_hw % NW\n    _mask_h = i_h * BH + tl.arange(0, BH) < DH\n    _mask_w = i_w * BW + tl.arange(0, BW) < DW\n    _mask_hw = _mask_h[:, None] & _mask_w[None, :]\n    _for_C = min(DC - i_c * BC, BC)\n    pos_h = i_h * BH + tl.arange(0, BH)[:, None]\n    pos_w = i_w * BW + tl.arange(0, BW)[None, :]\n    neg_h = DH - i_h * BH - 1 - tl.arange(0, BH)[:, None]\n    neg_w = DW - i_w * BW - 1 - tl.arange(0, BW)[None, :]\n    if scans == 0:\n        HWRoute0 = pos_h * DW + pos_w\n        HWRoute1 = pos_w * DH + pos_h\n        HWRoute2 = neg_h * DW + neg_w\n        HWRoute3 = neg_w * DH + neg_h\n    elif scans == 1:\n        HWRoute0 = pos_h * DW + pos_w\n        HWRoute1 = HWRoute0\n        HWRoute2 = HWRoute0\n        HWRoute3 = HWRoute0\n    elif scans == 2:\n        HWRoute0 = pos_h * DW + pos_w\n        HWRoute1 = HWRoute0\n        HWRoute2 = neg_h * DW + neg_w\n        HWRoute3 = HWRoute2\n    _tmp1 = DC * DH * DW\n    y_ptr_base = y + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if y_layout == 0\n         else i_c * BC)\n    if y_layout == 0:\n        p_y1 = y_ptr_base + HWRoute0\n        p_y2 = y_ptr_base + _tmp1 + HWRoute1\n        p_y3 = y_ptr_base + 2 * _tmp1 + HWRoute2\n        p_y4 = y_ptr_base + 3 * _tmp1 + HWRoute3\n    else:\n        p_y1 = y_ptr_base + HWRoute0 * 4 * DC\n        p_y2 = y_ptr_base + DC + HWRoute1 * 4 * DC\n        p_y3 = y_ptr_base + 2 * DC + HWRoute2 * 4 * DC\n        p_y4 = y_ptr_base + 3 * DC + HWRoute3 * 4 * DC\n    if onebyone == 0:\n        x_ptr_base = x + i_b * _tmp1 + (i_c * BC * DH * DW if x_layout == 0\n             else i_c * BC)\n        if x_layout == 0:\n            p_x = x_ptr_base + HWRoute0\n        else:\n            p_x = x_ptr_base + HWRoute0 * DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _x = tl.load(p_x + _idx_x, mask=_mask_hw)\n                tl.store(p_y1 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, _x, mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, _x, mask=_mask_hw)\n        elif operation == 1:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                _y1 = tl.load(p_y1 + _idx_y, mask=_mask_hw)\n                _y2 = tl.load(p_y2 + _idx_y, mask=_mask_hw)\n                _y3 = tl.load(p_y3 + _idx_y, mask=_mask_hw)\n                _y4 = tl.load(p_y4 + _idx_y, mask=_mask_hw)\n                tl.store(p_x + _idx_x, _y1 + _y2 + _y3 + _y4, mask=_mask_hw)\n    else:\n        x_ptr_base = x + i_b * 4 * _tmp1 + (i_c * BC * DH * DW if x_layout ==\n            0 else i_c * BC)\n        if x_layout == 0:\n            p_x1 = x_ptr_base + HWRoute0\n            p_x2 = p_x1 + _tmp1\n            p_x3 = p_x2 + _tmp1\n            p_x4 = p_x3 + _tmp1\n        else:\n            p_x1 = x_ptr_base + HWRoute0 * 4 * DC\n            p_x2 = p_x1 + DC\n            p_x3 = p_x2 + DC\n            p_x4 = p_x3 + DC\n        if operation == 0:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_y1 + _idx_y, tl.load(p_x1 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y2 + _idx_y, tl.load(p_x2 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y3 + _idx_y, tl.load(p_x3 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n                tl.store(p_y4 + _idx_y, tl.load(p_x4 + _idx_x, mask=\n                    _mask_hw), mask=_mask_hw)\n        else:\n            for idxc in range(_for_C):\n                _idx_x = idxc * DH * DW if x_layout == 0 else idxc\n                _idx_y = idxc * DH * DW if y_layout == 0 else idxc\n                tl.store(p_x1 + _idx_x, tl.load(p_y1 + _idx_y), mask=_mask_hw)\n                tl.store(p_x2 + _idx_x, tl.load(p_y2 + _idx_y), mask=_mask_hw)\n                tl.store(p_x3 + _idx_x, tl.load(p_y3 + _idx_y), mask=_mask_hw)\n                tl.store(p_x4 + _idx_x, tl.load(p_y4 + _idx_y), mask=_mask_hw)\n",
    "category": "Tensor Operations",
    "subcategory": "Matrix Scanning",
    "uuid": 9
  }
]